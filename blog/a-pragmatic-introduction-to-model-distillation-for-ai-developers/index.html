<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:47:08 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">A pragmatic introduction to model distillation for AI developers</title><meta name="description" data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="A pragmatic introduction to model distillation for AI developers" data-next-head=""/><meta property="og:description" data-next-head=""/><meta property="og:url" content="https://labelbox.ghost.io/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/" data-next-head=""/><meta property="og:image" content="https://labelbox.ghost.io/blog/content/images/2024/01/Frame-3593--1-.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="A pragmatic introduction to model distillation for AI developers" data-next-head=""/><meta name="twitter:description" data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.ghost.io/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/" data-next-head=""/><meta property="twitter:image" content="https://labelbox.ghost.io/blog/content/images/2024/01/Frame-3593--1-.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g48[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.eivcj #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.eivcj .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.eivcj .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.eivcj #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.eivcj #image-viewer .close:hover,.eivcj #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.eivcj .modal-content{width:100%;}}/*!sc*/
data-styled.g105[id="ImageModal__ImageModalWrapper-sc-1ey7m7r-0"]{content:"eivcj,"}/*!sc*/
.QsqTL .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.QsqTL .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.QsqTL .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:20px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h2{padding-top:10px;}}/*!sc*/
.QsqTL .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h3{padding-top:10px;}}/*!sc*/
.QsqTL .content h4{font-size:20px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 16px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h4{padding-top:8px;}}/*!sc*/
.QsqTL .content h5{font-size:18px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 14px;}/*!sc*/
.QsqTL .content h6{font-size:16px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 12px;}/*!sc*/
.QsqTL .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.QsqTL .content a:hover{color:#1e40af;}/*!sc*/
.QsqTL .content li{margin-bottom:20px;}/*!sc*/
.QsqTL .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.QsqTL .content ol{list-style:decimal;padding-left:20px;}/*!sc*/
.QsqTL .content .table-container{overflow-x:auto;margin:40px 0;-webkit-overflow-scrolling:touch;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-container{margin:30px -20px;padding:0 20px;}}/*!sc*/
.QsqTL .content table{width:100%;border-collapse:collapse;font-size:16px;background:white;border:1px solid #e5e7eb;border-radius:8px;overflow:hidden;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content table{font-size:14px;}}/*!sc*/
.QsqTL .content .table-container table{margin:0;min-width:600px;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-container table{min-width:700px;border-radius:0;border-left:none;border-right:none;}}/*!sc*/
.QsqTL .content .content table:not(.table-container table){margin:40px 0;min-width:auto;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .content table:not(.table-container table){margin:30px 0;min-width:auto;border-radius:8px;border:1px solid #e5e7eb;}}/*!sc*/
.QsqTL .content thead{background:#fafbfc;border-bottom:1px solid #d1d5db;}/*!sc*/
.QsqTL .content th{padding:16px 20px;text-align:left;font-weight:600;color:#374151;font-size:14px;-webkit-letter-spacing:0.025em;-moz-letter-spacing:0.025em;-ms-letter-spacing:0.025em;letter-spacing:0.025em;border-right:1px solid #f3f4f6;}/*!sc*/
.QsqTL .content th:last-child{border-right:none;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content th{padding:12px 16px;font-size:13px;}}/*!sc*/
.QsqTL .content td{padding:16px 20px;border-bottom:1px solid #f3f4f6;border-right:1px solid #f9fafb;color:#374151;line-height:1.5;}/*!sc*/
.QsqTL .content td:last-child{border-right:none;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content td{padding:12px 16px;}}/*!sc*/
.QsqTL .content tbody tr{-webkit-transition:background-color 0.2s ease;transition:background-color 0.2s ease;}/*!sc*/
.QsqTL .content tbody tr:hover{background-color:#f8fafc;}/*!sc*/
.QsqTL .content tbody tr:last-child td{border-bottom:none;}/*!sc*/
.QsqTL .content .table-wrapper{overflow-x:auto;margin:40px 0;border:1px solid #e5e7eb;border-radius:8px;-webkit-overflow-scrolling:touch;}/*!sc*/
.QsqTL .content .table-wrapper table{margin:0;border:none;border-radius:0;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-wrapper{margin:30px -20px;border-radius:0;border-left:none;border-right:none;}}/*!sc*/
.QsqTL .content code{background:#f1f5f9;padding:2px 6px;border-radius:4px;font-family:'Monaco','Menlo','Ubuntu Mono',monospace;font-size:14px;color:#e11d48;}/*!sc*/
.QsqTL .content pre{background:#1e293b;color:#e2e8f0;padding:20px;border-radius:8px;overflow-x:auto;margin:30px 0;}/*!sc*/
.QsqTL .content pre code{background:transparent;padding:0;color:inherit;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content pre{margin:20px -20px;border-radius:0;padding:16px 20px;}}/*!sc*/
.QsqTL .content blockquote{border-left:4px solid #2563eb;padding:20px 24px;margin:30px 0;background:#f8fafc;border-radius:0 8px 8px 0;font-style:italic;color:#475569;}/*!sc*/
.QsqTL .content blockquote p{margin-bottom:0;}/*!sc*/
.QsqTL .content blockquote p:last-child{margin-bottom:0;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content blockquote{margin:20px 0;padding:16px 20px;}}/*!sc*/
.QsqTL .content hr{border:none;height:1px;background:linear-gradient(to right,transparent,#e5e7eb,transparent);margin:50px 0;}/*!sc*/
.QsqTL .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.QsqTL .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;color:#6b7280;font-style:italic;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.QsqTL .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;border-radius:8px;}/*!sc*/
.QsqTL .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.QsqTL .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100%;height:100%;margin:0 auto;border-radius:8px;}/*!sc*/
.QsqTL .content .kg-bookmark-card{background:white;border-radius:10px;margin-top:60px !important;border:1px solid #e5e7eb;-webkit-transition:border-color 0.3s ease;transition:border-color 0.3s ease;}/*!sc*/
.QsqTL .content .kg-bookmark-card:hover{border-color:#d1d5db;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;color:#262626 !important;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-thumbnail{position:relative;min-width:30%;max-height:100%;overflow:hidden;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-thumbnail img{position:absolute;top:0;left:0;width:100% !important;height:100% !important;-o-object-fit:cover;object-position:left;object-fit:cover;border-radius:0 10px 10px 0;border-left:1px solid #f5f5f5;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;padding:20px;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-title{font-size:1.125rem;line-height:1.3;font-weight:600;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-description{font-size:0.875rem;font-weight:400;line-height:1.4;margin-top:12px;overflow-y:hidden;color:#6b7280;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;font-size:0.9rem;font-weight:400;margin-top:14px;color:#6b7280;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata img{width:22px !important;height:22px !important;margin-right:8px !important;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata .kg-bookmark-author{margin:4px;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata .kg-bookmark-publisher{margin:4px;}/*!sc*/
.QsqTL .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;margin:40px 0;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;margin-bottom:12px;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 6px;border-radius:6px;overflow:hidden;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;object-fit:cover;-webkit-transition:-webkit-transform 0.3s ease;-webkit-transition:transform 0.3s ease;transition:transform 0.3s ease;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row img:hover{-webkit-transform:scale(1.02);-ms-transform:scale(1.02);transform:scale(1.02);}/*!sc*/
data-styled.g112[id="id__PostContentWrapper-sc-1hduup0-0"]{content:"QsqTL,"}/*!sc*/
@media (max-width:767px){.bwsQop.toc-container{display:none;}}/*!sc*/
.bwsQop.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g113[id="id__TocContainer-sc-1hduup0-1"]{content:"bwsQop,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/blog/%5bid%5d-b80b73d0fd88ad55.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="ImageModal__ImageModalWrapper-sc-1ey7m7r-0 eivcj"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All blog posts</a><main class="id__TocContainer-sc-1hduup0-1 bwsQop toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><p class="my-4 text-sm font-medium">Mikiko Bazeley<span class="mx-2">•</span>January 23, 2024</p><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">A pragmatic introduction to model distillation for AI developers</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox.ghost.io/blog/content/images/2024/01/Frame-3593--1-.png"/></div><main class="id__PostContentWrapper-sc-1hduup0-0 QsqTL md:px-24"><div class="content js-toc-content"><p>The landscape of artificial intelligence (AI) and machine learning (ML) is continuously evolving, surfacing innovative techniques that revolutionize how we develop and deploy AI models.&nbsp;</p><p>One such technique gaining significant traction is model distillation. </p><p>Model distillation has been instrumental in driving both open-source innovation of LLMs as well as the adoption of large models (both language and vision) for use cases where task specificity and runtime optimization have been required.&nbsp;</p><p>One of the most well-known examples of model distillation in action is Stanford’s Alpaca, based on Meta’s LLaMa 7B model.&nbsp;</p><p><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html?ref=labelbox.ghost.io">Alpaca</a> was trained in less than 2 months for less than $600 on 52K question and answer pairs generated using OpenAI’s text-davinci-003. At the time of release Alpaca boasted near comparable performance to GPT 3.5.&nbsp;</p><p>Although Alpaca was deprecated, the ability for a small team to create impressive models for a fraction of the cost from large foundation models injected fuel in the ensuing months for developers and teams eager to fine-tune their own powerful, task specific models using open-source models.&nbsp;</p><p>In this blog post our goal is to provide a pragmatic introduction to model distillation.&nbsp;</p><p>Model distillation is a useful technique that we believe will stand the test of time for companies wanting to efficiently and effectively create the best models for their use cases.</p><p>We’ll provide the “why” and introduce the “how” of performing model distillation so that you too can train powerful and efficient models, important components of intelligent applications.&nbsp;</p><p>If you’re an AI developer struggling to adopt Large Language or Large Vision models and trying to understand where model distillation fits in with other techniques like RAG, fine-tuning, and dataset distillation, this is the guide for you.</p><h2 id="introduction">Introduction</h2><h3 id="a-brief-overview">A Brief Overview</h3><p><a href="../../guides/model-distillation/indexc625.html?ref=labelbox.ghost.io" rel="noreferrer"><strong>Model distillation</strong></a>, also known as <a href="../../guides/knowledge-distillation/indexc625.html?ref=labelbox.ghost.io" rel="noreferrer"><strong>knowledge distillation</strong></a>, is a technique that focuses on creating efficient models by transferring knowledge from large, complex models to smaller, deployable ones.&nbsp;</p><p>We can think of the typical process as taking the best of a wide, deep model and transferring it to a narrow, short model.</p><p>This process aims to maintain performance while reducing memory footprint and computation requirements, a form of model compression introduced by Geoffrey Hinton in "<a href="https://arxiv.org/abs/1503.02531?ref=labelbox.ghost.io"><u>Distilling the Knowledge in a Neural Network</u></a>" (2015).&nbsp;</p><h3 id="why-customize-foundation-models">Why Customize Foundation Models</h3><p>Despite the high accuracy of foundational models and ability to perform remarkable feats of <a href="../../guides/zero-shot-learning-few-shot-learning-fine-tuning/indexc625.html?ref=labelbox.ghost.io"><u>zero-shot learning</u></a>, their deployment often faces challenges like increased latency and decreased throughput performance.&nbsp;</p><p>As a result, <a href="../../usecases/large-language-models/indexc625.html?ref=labelbox.ghost.io" rel="noreferrer">large language models (LLMs)</a> with their millions of parameters, are cumbersome and resource-intensive, making them impractical for teams needing near real-time solutions beyond public benchmarks.&nbsp;</p><p>For example, Google gives the estimate that a “<a href="https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html?ref=labelbox.ghost.io"><u>single 175 billion LLM requires 350GB of GPU memory</u></a>”. Many of the most powerful LLMs can be <a href="https://en.wikipedia.org/wiki/Large_language_model?ref=labelbox.ghost.io#:~:text=data.%5B102%5D-,List,-%5Bedit%5D"><u>upwards of 170 billion parameters</u></a> and even LLMs with 10 million parameters will require 20GB of GPU memory (<a href="https://www.anyscale.com/blog/num-every-llm-developer-should-know?ref=labelbox.ghost.io"><u>assuming a roughly 2:1 ratio of GPU memory requirements for serving and the number of parameters</u></a>).&nbsp;This tension necessitates the development of smaller, more specialized models.</p><p>Model distillation emerges as a solution (especially for <a href="../../customers/intuitive-surgical-customer-story/indexc625.html?ref=labelbox.ghost.io">companies that have unique and proprietary data</a>), offering efficient deployment, cost reduction, faster inference, customization, and sustainability.&nbsp;</p><p>By optimizing foundation model sizes, foundation models can become even more widely adopted across a <a href="../../solutions/accelerators/indexc625.html?ref=labelbox.ghost.io">broader, diverse set of use cases</a> in a practical, cost-effective, and environmentally friendly manner.&nbsp;</p><h2 id="understanding-model-distillation">Understanding Model Distillation</h2><h3 id="definition-and-basic-concepts">Definition and Basic Concepts</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2024/01/-hmqy_Wi64wQ0KBF4Y2AIYMmmIU02M30ekFMdnzg0ylQaGb1Sivxg6jvl15wIhMfa1KNONZPUyiD0bblWvfvjE22WW87VryrSnO7ZuZZh3GqYcoMiZ1Pm9HauR_9NIWIMBigEifYyOBFDtcVlgWjHIA.8b7.de" class="kg-image" alt="" loading="lazy" width="1000" height="415" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/-hmqy_Wi64wQ0KBF4Y2AIYMmmIU02M30ekFMdnzg0ylQaGb1Sivxg6jvl15wIhMfa1KNONZPUyiD0bblWvfvjE22WW87VryrSnO7ZuZZh3GqYcoMiZ1Pm9HauR_9NIWIMBigEifYyOBFDtcVlgWjHIA.png 600w, https://labelbox.ghost.io/blog/content/images/2024/01/-hmqy_Wi64wQ0KBF4Y2AIYMmmIU02M30ekFMdnzg0ylQaGb1Sivxg6jvl15wIhMfa1KNONZPUyiD0bblWvfvjE22WW87VryrSnO7ZuZZh3GqYcoMiZ1Pm9HauR_9NIWIMBigEifYyOBFDtcVlgWjHIA.png 1000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Fig 2.1: Knowledge Distillation – Source: </span><a href="https://arxiv.org/abs/2006.05525?ref=labelbox.ghost.io"><u><span class="underline" style="white-space: pre-wrap;">“Knowledge Distillation: A Survey”</span></u></a></figcaption></figure><p>Earlier we described model distillation as a process of transferring knowledge from a large, complex model (<strong>teacher</strong>) to a smaller, more efficient model (<strong>student</strong>).&nbsp;</p><p>This (usually) involves two separate models: the teacher model, large and highly accurate, and the student model, compact and less resource-intensive.&nbsp;</p><h3 id="key-components-of-model-distillation">Key Components of Model Distillation</h3><p>Let’s dig in deeper to the different components involved in model distillation.</p><p>There are a couple key decisions that need to be made, such as:</p><h3 id="picking-teacher-and-student-model-architectures">Picking Teacher and Student Model Architectures</h3><p>Model distillation involves two main elements: the <strong>teacher</strong> model, a large pre-trained model with generally high performance, and the <strong>student</strong> model, a smaller model that learns from the teacher (and that will ultimately be used for downstream applications).&nbsp;</p><p>The student model can vary in structure, from a simplified or quantized version of the teacher model to an entirely different network with an optimized structure.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2024/01/pyPqxhQP8cOgjt65eXXywck3BUTj_23Bf4J3FV0lVQJ-92ZR24A2UO71fpT0MTL05hxGhWpaSfRR2ohqbL_ZIPuKmtjFwSYtCnokGXswvJhafki4JjwEY1TPfRFQ_TJo20fSuyao6VNAUkcqrlvriGY.8b8.de" class="kg-image" alt="" loading="lazy" width="355" height="269"><figcaption><span style="white-space: pre-wrap;">Fig 2.2: Types of Student Model Architectures – Source: </span><a href="https://arxiv.org/abs/2006.05525?ref=labelbox.ghost.io"><u><span class="underline" style="white-space: pre-wrap;">“Knowledge Distillation: A Survey”</span></u></a></figcaption></figure><h3 id="the-distillation-process-explained">The Distillation Process Explained</h3><p>The next decision to be made is the distillation process to use.</p><p>The <strong>distillation process</strong> involves training the smaller neural network (the student) to mimic the behavior of the larger, more complex teacher network by learning from its predictions or internal representations.&nbsp;</p><p>This process is a form of <strong>supervised learning</strong> where the student minimizes the difference between its predictions and those of the teacher model.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2024/01/Fig-2.3_-Knowledge-Distillation-Core-Concepts-v2.png" class="kg-image" alt="" loading="lazy" width="1890" height="1372" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-2.3_-Knowledge-Distillation-Core-Concepts-v2.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-2.3_-Knowledge-Distillation-Core-Concepts-v2.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-2.3_-Knowledge-Distillation-Core-Concepts-v2.png 1600w, https://labelbox.ghost.io/blog/content/images/2024/01/Fig-2.3_-Knowledge-Distillation-Core-Concepts-v2.png 1890w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Fig 2.3: Knowledge Distillation Core Concepts</span></figcaption></figure><h3 id="distillation-knowledge-types">Distillation Knowledge Types</h3><p>What do we mean by <strong>knowledge</strong>?&nbsp;</p><p>The types of knowledge distillation can be categorized into three types: <strong>response-based</strong>, <strong>feature-based</strong>, and <strong>relation-based </strong>distillation.&nbsp;</p><p>Each type focuses on different aspects of knowledge transfer from the teacher to the student model and offers unique advantages and challenges.</p><p>Although the most common and easiest form of knowledge distillation to get started with is <strong>response-based distillation</strong>, it’s helpful to understand the different types of knowledge distillation.&nbsp;</p><p>1. <strong>Response-Based Distillation</strong>: Focuses on the student model mimicking the teacher's predictions. The teacher generates soft labels for each input example, and the student is trained to predict these labels by minimizing the difference in their outputs.&nbsp;</p><ul><li><strong>Pros</strong>: Easy implementation, applicable to various models and datasets.</li><li><strong>Cons</strong>: Only transfers output-related knowledge, doesn’t capture complex internal representations.</li></ul><p>2. <strong>Feature-Based Distillation</strong>: Involves the student model learning the internal features or representations learned by the teacher. The process includes the student minimizing the distance between the features learned by both models.</p><ul><li><strong>Pros</strong>: Helps learn robust representations, applicable across tasks and models.</li><li><strong>Cons</strong>: Computationally expensive, not suitable for tasks where teacher's internal representations aren't transferable.</li></ul><p>3.<strong> Relation-Based Distillation</strong>: This method teaches the student to understand the relationships between inputs and outputs. It involves transferring the underlying relationships between these elements from the teacher to the student model.</p><ul><li><strong>Pros</strong>: Enables learning robust and generalizable input-output relationships.</li><li><strong>Cons</strong>: Computationally intensive, requires greater sophistication and experience for implementation by the ML engineer.</li></ul><h3 id="training-methods">Training Methods</h3><p>The final decision that needs to be made is the training method used to transfer knowledge from the teacher to the student models.&nbsp;</p><p>There are three main training methods in model distillation: <strong>offline</strong>, <strong>online</strong>, and <strong>self-distillation</strong>.&nbsp;</p><p><strong>Offline distillation</strong> involves a pre-trained, frozen teacher model, while <strong>online distillation</strong> trains both teacher and student models simultaneously.&nbsp;</p><p><strong>Self-distillation</strong> uses the same network as both teacher and student, addressing conventional distillation limitations. This method requires the ability to copy the teacher model and be able to update the model, which is not possible with proprietary models or models where the weights and architecture haven’t been published.&nbsp;</p><p>Knowledge of different training methods (offline, online, self-distillation) is essential for AI developers who need to implement and manage the training process in a way that aligns with their project's constraints and goals.&nbsp;</p><p>For instance, an AI developer working in an environment with limited data availability might find self-distillation methods particularly useful.</p><p>Most teams however start with a form of <strong>response-based</strong>, <strong>offline distillation</strong>. Training offline allows the ML Engineer to evaluate model performance and analyze model errors before deploying the student model to production.&nbsp;<br></p><h2 id="benefits-of-model-distillation">Benefits of Model Distillation</h2><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2024/01/Fig-3.0_-Benefits.png" class="kg-image" alt="" loading="lazy" width="2000" height="1109" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-3.0_-Benefits.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-3.0_-Benefits.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-3.0_-Benefits.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/01/Fig-3.0_-Benefits.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Fig 3.1: Benefits of Using Model Distillation</span></figcaption></figure><p>There are many benefits to performing model distillation and using smaller, task specific models rather than a <a href="../../usecases/large-language-models/indexc625.html?ref=labelbox.ghost.io" rel="noreferrer"><u>Large Language Model</u></a> (or Large Vision Model) out of the box.&nbsp;</p><h3 id="efficiency-in-training">Efficiency in Training</h3><p>Model distillation enhances data efficiency, requiring less data for pretraining (and potentially fine-tuning). Model distillation aligns with data-centric AI philosophy to maximize the utility of data.</p><h3 id="efficiency-in-deployment">Efficiency in Deployment</h3><p>Distilled models are smaller and more efficient, ideal for deploying on platforms with <a href="../../research/lightweight-multi-drone-detection-and-3d-localization-via-yolo/indexc625.html?ref=labelbox.ghost.io"><u>limited resources</u></a>. They offer versatility in applications like <a href="../../research/fruit-flower-detection-in-apple-orchards-using-ml/indexc625.html?ref=labelbox.ghost.io"><u>edge computing</u></a> and real-time processing.</p><h3 id="performance-improvements">Performance Improvements</h3><p>Similar to fine-tuning, model distillation can improve accuracy and performance on specific tasks and domains. The resulting smaller models achieve similar performance to larger ones but with quicker response times.</p><h3 id="resource-optimization">Resource Optimization</h3><p>Model distillation leads to cost reduction and sustainability. It reduces computational and storage requirements, making it beneficial for projects with limited budgets and aligning with ethical AI development strategies.</p><h2 id="potential-challenges">Potential Challenges</h2><p>Like many powerful techniques, key considerations must be made in order to effectively and efficiently implement model distillation as part of a team’s AI development process.&nbsp;</p><p>Some key challenges include:</p><ul><li>Selecting appropriate teacher and student models,&nbsp;</li><li>Balancing size, speed, and accuracy of the student model,&nbsp;&nbsp;</li><li>The technical complexity of the distillation process,</li><li>Ensuring models are retrained as necessary to minimize model drift,</li><li>Versioning and curating the data used for student training,</li><li>Ensuring visibility and monitoring of the model performance.</li></ul><p>Some teams in industries like gaming that require real-time learning or edge-constrained devices may find that offline training isn’t sufficient and they need to explore a continuous learning pattern of model development and deployment, such as online distillation.&nbsp;</p><h2 id="considerations-in-using-model-distillation">Considerations In Using Model Distillation</h2><h3 id="difference-between-model-distillation-fine-tuning-and-traditional-model-training">Difference Between Model Distillation, Fine-Tuning, and Traditional Model Training</h3><p>How does model distillation compare to fine-tuning, another popular technique utilized with foundation models? Are they mutually exclusive, complementary, or sequential approaches?</p><p>Model distillation, fine-tuning, and traditional model training (also called pretraining) each have distinct purposes in machine learning.&nbsp;</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.1_-Before-Foundation-Models-1.png" class="kg-image" alt="" loading="lazy" width="1520" height="688" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.1.1_-Before-Foundation-Models-1.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.1.1_-Before-Foundation-Models-1.png 1000w, https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.1_-Before-Foundation-Models-1.png 1520w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Fig 5.1.1: Early Years of Developing Deep Learning Models</span></figcaption></figure><h5 id=""></h5><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.2_-Foundation-Models--1-.png" class="kg-image" alt="" loading="lazy" width="2000" height="728" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.1.2_-Foundation-Models--1-.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.1.2_-Foundation-Models--1-.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-5.1.2_-Foundation-Models--1-.png 1600w, https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.2_-Foundation-Models--1-.png 2362w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Fig 5.1.2: The New Opportunities &amp; Challenges of Foundation Models</span></figcaption></figure><p></p><p><strong>Traditional training</strong> involves learning directly from raw data, often requiring extensive resources, to generate a model from scratch. The drawback of this approach is the significant requirement for supporting Machine Learning Operations infrastructure to bridge the development-production chasm and the cost (both in data acquisition and compute) to approach the same performance of the foundation models available on the market (both proprietary and open-source).&nbsp;</p><p><strong>Fine-tuning</strong> adjusts an existing, pre-trained model using a specific dataset, which can be resource-efficient but relies on human-generated labels. The resulting model is usually a similar size but with improved performance on domains and tasks corresponding to the dataset that was used.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.3_-Improving-Model-Efficiency--1-.png" class="kg-image" alt="" loading="lazy" width="2000" height="593" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.1.3_-Improving-Model-Efficiency--1-.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.1.3_-Improving-Model-Efficiency--1-.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-5.1.3_-Improving-Model-Efficiency--1-.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/01/Fig-5.1.3_-Improving-Model-Efficiency--1-.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Fig 5.1.3: Improving Model Efficiency</span></figcaption></figure><p>Model distillation offers the best of many worlds.&nbsp;</p><p>In comparison to <strong>pre-training</strong> (or <strong>traditional training</strong>), model distillation requires far less data (both raw and labeled). By starting with a powerful foundation model to train a smaller model (student), the developer is guaranteed a starting performance similar to the foundation model on specific tasks and domains but with less computational demand due to the smaller parameter size.&nbsp;</p><p>The <strong>offline</strong>, <strong>response-based</strong> approach for model distillation can be easily performed by a developer using minimal infrastructure and requires very little labeled data to start with, as the labels are provided by the parent mode in the form of a response. This approach is especially beneficial for deploying pipelines requiring the use of one or more foundation models in resource-constrained environments, as it bypasses the need for extensive data or manual labeling.</p><p>With that being said, the use of model distillation and fine-tuning isn’t mutually exclusive and can both be used by the same team and even for the same project.&nbsp;</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.4_-Options-for-Optimizing-Models-1.png" class="kg-image" alt="" loading="lazy" width="2000" height="814" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.1.4_-Options-for-Optimizing-Models-1.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.1.4_-Options-for-Optimizing-Models-1.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-5.1.4_-Options-for-Optimizing-Models-1.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/01/Fig-5.1.4_-Options-for-Optimizing-Models-1.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Fig 5.1.4: Options for Optimizing Models</span></figcaption></figure><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.5_-Leveraging-Model-Distillation---Fine-Tuning-Together-3.png" class="kg-image" alt="" loading="lazy" width="2000" height="732" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.1.5_-Leveraging-Model-Distillation---Fine-Tuning-Together-3.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.1.5_-Leveraging-Model-Distillation---Fine-Tuning-Together-3.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-5.1.5_-Leveraging-Model-Distillation---Fine-Tuning-Together-3.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/01/Fig-5.1.5_-Leveraging-Model-Distillation---Fine-Tuning-Together-3.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Fig 5.1.5: Leveraging Model Distillation &amp; Fine-Tuning Together</span></figcaption></figure><p>You could perform model distillation using a base foundation model to create a smaller, student model from the parent model. Then the student model could be fine-tuned on a new, unique dataset (automatically transformed and ingested as part of a centralized, data catalog).&nbsp;</p><p>In upcoming tutorials, we show how easy it is to use Quantumworks Lab’s <a href="../../product/model/indexc625.html?ref=labelbox.ghost.io"><u>Model</u></a> and <a href="../../product/catalog/indexc625.html?ref=labelbox.ghost.io"><u>Catalog</u></a> to perform model distillation for both <a href="../../usecases/computer-vision/indexc625.html?ref=labelbox.ghost.io" rel="noreferrer"><u>computer vision</u></a> and NLP use cases.</p><h3 id="leveraging-model-distillation-with-rag-prompt-engineering-within-fmops">Leveraging Model Distillation with RAG, Prompt Engineering, within FMOps</h3><p>We’ve established that model distillation and fine-tuning are both used for the purpose of adapting the capabilities of large foundation models to more task specific use cases.&nbsp;</p><p>The ultimate purpose of model distillation is making models more inference-optimized as a form of <strong>model compression</strong> (without significant loss in performance and accuracy within the domain area of interest), whereas the focus of fine-tuning is improving task specific performance (with model size being relatively irrelevant).</p><p>In addition to knowledge distillation, other compression and acceleration methods like <strong>quantization</strong>, <strong>pruning</strong>, and <strong>low-rank factorization</strong> are also employed.</p><p>Model distillation can be used not just with fine-tuning but alongside RAG-based systems and prompt engineering techniques.&nbsp;</p><p><strong>Model distillation</strong>, <strong>fine-tuning</strong>, <strong>RAG</strong> and <strong>prompt engineering</strong> are all considered important tools in the toolkit of <strong>FMOps</strong>, or Foundation Model Operations.&nbsp;</p><h3 id="rag-based-systems">RAG-Based Systems</h3><p><strong>RAG (Retrieval-Augmented Generation)</strong> is a pattern (usually leveraging a <strong>vector database</strong>) that combines a neural retrieval mechanism with a sequence-to-sequence model, allowing it to retrieve relevant documents or data to enhance its responses. This approach enables the model to incorporate a broader range of external information and context, beyond what is contained in its initial training data.&nbsp;</p><p>In RAG-based systems, model distillation helps manage the size and complexity of models, ensuring they remain functional and efficient.&nbsp;</p><p>Similar to fine-tuning, model distillation produces a static model that doesn’t have real-time information on the entire internet. The resulting student model also shouldn’t have access to proprietary data.&nbsp;</p><p>RAG ensures that the right information is fetched and injected into the context of the prompt or as part of the <strong>response</strong>. In essence, RAG is usually pretty quick so the bottleneck in inference speed is usually in the embedding and serving stages (i.e. the student model).&nbsp;</p><h3 id="prompt-engineering">Prompt Engineering</h3><p><strong>Prompt engineering</strong> involves crafting input <strong>prompts</strong> in a way that effectively guides models, especially foundation models based on natural language processing, to produce structured outputs or responses.</p><p>This process is crucial in optimizing the performance of models like GPT-3, as the quality and structure of the input significantly influence the accuracy and relevance of the generated output.</p><p>Prompt engineering should be used in generating the responses used to train the student model from the teacher model and to further structure the inputs and outputs to the student model once it’s deployed in production.</p><h3 id="reinforcement-learning-from-human-feedback-rlhf">Reinforcement Learning From Human Feedback (RLHF)</h3><p><a href="../../guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/indexc625.html?ref=labelbox.ghost.io" rel="noreferrer">Reinforcement Learning from Human Feedback (RLHF)</a> is a machine learning approach where a model is trained to optimize its behavior based on feedback from human interactions, rather than solely relying on predefined reward functions. This method allows for more nuanced and contextually appropriate learning, as it incorporates human judgment and preferences into the training process.</p><p>The feedback from these experts is used to improve either the teacher model by improving the quality of the initial pre-training data, the student model by enhancing and enriching the responses and output dataset used to fine-tune the student model, and by further refining the fine-tuned student model for retraining. </p><p>Midjourney has effectively used RLHF, for example, to continuously collect user feedback through their Discord-based UI. Users can select options to redo runs, create variants, and upscale the images they like the best. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../cdn.document360.io/3040c2b6-fead-4744-a3a9-d56d621c6c7e/Images/Documentation/MJ_UpscaledUI.png" class="kg-image" alt="Image of the Midjourney Discord button interface after upscaling an images" loading="lazy" width="640" height="740"><figcaption><span style="white-space: pre-wrap;">Fig 5.2.3: Iteratively Collecting User Feedback For Retraining Through The UI – Source: "</span><a href="https://docs.midjourney.com/docs/midjourney-discord?ref=labelbox.ghost.io" rel="noreferrer"><u><span class="underline" style="white-space: pre-wrap;">Midjourney Documentation</span></u></a><u><span class="underline" style="white-space: pre-wrap;">"</span></u></figcaption></figure><p></p><h2 id="the-fmops-toolkit">The FMOps Toolkit</h2><p>To tie all the techniques together:&nbsp;</p><ul><li><strong>Model Distillation</strong> – Is the process of creating a smaller, task-specific model from a powerful, larger foundation model, usually by generating responses (aka labels, outputs, or predictions) that are then used for model training or fine-tuning;</li><li><strong>Fine-Tuning</strong> – Is a process for adapting an existing model to a domain specific dataset to improve performance on a specific set of tasks through supervised learning, with the resulting model used in an intelligent application;</li><li><strong>RAG</strong> – Is a pattern for injecting external information into a model, as either part of the context fed to the model through the user prompt or injected dynamically into the response, usually based on some kind of similarity match between the information and the prompt;</li><li><strong>Prompt Engineering – </strong>Is the practice of strategically designing and structuring input prompts to effectively guide an AI model's response, optimizing the model's performance in generating accurate, relevant, and contextually appropriate outputs;</li><li><strong>RLHF</strong> – Is a method of training AI models by using human feedback to reinforce desired behaviors, enabling the model to learn from human preferences and judgments, thereby refining its responses and actions in a more contextually and socially aware manner.</li></ul><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2024/01/Fig-5.2.1_-Foundation-Model-Operations.png" class="kg-image" alt="" loading="lazy" width="2000" height="610" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.2.1_-Foundation-Model-Operations.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.2.1_-Foundation-Model-Operations.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-5.2.1_-Foundation-Model-Operations.png 1600w, https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.2.1_-Foundation-Model-Operations.png 2000w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Fig 5.2.4: Leveraging FMOps To Develop intelligent Applications</span></figcaption></figure><h2 id="practical-applications">Practical Applications</h2><p>The power of model distillation is it’s applicability across a wide range of practical use cases, including:</p><ul><li><strong>Industry-Specific Solutions</strong>: In <a href="../../industries/healthcare-and-life-sciences/indexc625.html?ref=labelbox.ghost.io"><u>healthcare</u></a> for <a href="../../solutions/healthcare-and-life-sciences/medical-imaging/indexc625.html?ref=labelbox.ghost.io"><u>rapid diagnosis</u></a>, and in <a href="../../industries/financial-services-and-insurance/indexc625.html?ref=labelbox.ghost.io"><u>finance</u></a> for real-time fraud detection. Industries as varied as <a href="../../industries/retail-and-ecommerce/indexc625.html?ref=labelbox.ghost.io"><u>retail and ecommerce</u></a>, <a href="../../industries/media-and-internet/indexc625.html?ref=labelbox.ghost.io"><u>media and entertainment</u></a>, and even <a href="../../industries/industrial/indexc625.html?ref=labelbox.ghost.io"><u>industrial</u></a> can benefit from efficient ML workloads.</li><li><strong>Common Use Cases</strong>: Utilized in <a href="../../solutions/computer-vision/indexc625.html?ref=labelbox.ghost.io"><u>computer vision</u></a>, <a href="../../guides/how-to-build-a-content-moderation-model-to-detect-disinformation/indexc625.html?ref=labelbox.ghost.io"><u>natural language processing</u></a>, speech recognition, <a href="../../guides/how-to-build-a-powerful-product-recommendation-system-for-retail/indexc625.html?ref=labelbox.ghost.io"><u>recommendation systems</u></a>, and neural architecture search.</li></ul><p>Reasons why model distillation will become even more important in the future include:</p><ul><li><strong>Compliance and Privacy</strong>: In certain cases, regulations or privacy concerns might restrict the use of cloud-based, large-scale AI models. Distilled models can often be deployed locally, offering a solution that respects privacy and regulatory constraints.</li><li><strong>Energy Efficiency and Sustainability</strong>: The environmental impact of running large-scale AI models is a growing concern. Distilled models require less computational power, which translates to lower energy consumption, aligning with the increasing need for sustainable AI practices.</li><li><strong>Improved Accessibility and User Experience</strong>: For applications like mobile apps or web services, using a distilled model means faster response times and lower bandwidth requirements, leading to a better user experience. This is particularly relevant for generative AI applications that interact directly with end-users, like chatbots or image generators.<br></li></ul><h2 id="getting-started-with-model-distillation-using-labelbox">Getting Started With Model Distillation Using Quantumworks Lab</h2><h3 id="basic-tools-and-platforms">Basic tools and platforms</h3><p>Here’s what you’ll need to get started with model distillation:</p><ul><li><strong>Access to a foundation model: </strong>A relevant foundation model that can be used as the parent model to generate responses from prompts or unstructured data like images or texts. For inspiration, <a href="../../product/model/foundry-models/indexc625.html?ref=labelbox.ghost.io"><u>here are all the models currently available on the Quantumworks Lab platform</u></a>. Models not listed <a href="https://docs.labelbox.com/reference/hugging-face-integration?ref=labelbox.ghost.io"><u>can still be incorporated</u></a> from sites like <a href="../hugging-face-models/indexc625.html?ref=labelbox.ghost.io"><u>HuggingFace</u></a>.&nbsp;</li><li><strong>A training framework</strong>: Libraries like TensorFlow and PyTorch offer functionalities for model distillation, such as training and model pickling.</li><li><strong>A development environment or notebook</strong>: Model distillation can be done locally or in a cloud based IDE or notebook-based environment like Colab or <a href="../../guides/how-to-automatically-ingest-data-from-databricks-into-labelbox/indexc625.html?ref=labelbox.ghost.io"><u>Databricks notebook</u></a> (even an MLOps platform like GCP’s <a href="../../guides/how-to-fine-tune-vertex-ai-models-with-labelbox/indexc625.html?ref=labelbox.ghost.io"><u>Vertex AI</u></a>).&nbsp;</li><li><strong>A platform for automating data and response preparation:</strong> A platform like Quantumworks Lab with capability to automate the data preparation process, generating either labels or predictions. </li><li><strong>A data storage and curation platform:</strong> Once the responses have been generated, a centralized repository for the inputs (or prompts) and the outputs (or responses) will be valuable, especially for future analysis. Quantumworks Lab provides a data curation and storage solution in the form of <a href="../../product/catalog/indexc625.html?ref=labelbox.ghost.io"><u>Catalog</u></a>, which offers rich features for filtering multiple modalities (image, text, video, audio, and geospatial data).&nbsp;</li></ul><h3 id="initial-steps">Initial Steps</h3><ul><li>If you’re just getting started, consider applying model distillation to train a student model in an offline, response-based manner using one of the popular foundation models for either a computer vision or natural language processing use case.</li><li>Otherwise, be sure to decide on the following:<ul><li><strong>Picking the teacher and student models:</strong> What model will be used for the teacher model? What model will be used for the student model (if fine-tuning) or will model training happen from scratch?&nbsp;</li><li><strong>Picking the knowledge type: </strong>What kind of knowledge do you want to capture? Do you want to mimic the responses? The internal features? The relationship between the inputs and outputs?&nbsp;</li><li><strong>Picking the training method</strong>: How will you train the student model? Offline? Continuously or online? Self-train via self-distillation?&nbsp;</li></ul></li></ul><h3 id="next-steps-applying-model-distillation-in-practice">Next Steps: Applying Model Distillation In Practice</h3><p>In the upcoming parts of the series, you’ll get a chance to see how simple and easy it is to get started with model distillation using Quantumworks Lab’s Catalog and our data engine. </p><p>In Part 2 of the series, we’ll demonstrate an end-to-end workflow for computer vision, using model distillation to fine-tune a model with labels created in Quantumworks Lab's data engine using Amazon Rekognition.&nbsp;</p><p>In Part 3 of the series, we’ll demonstrate an end-to-end workflow for NLP, using model distillation to fine-tune a BERT model with labels created in Quantumworks Lab using PaLM2.&nbsp;</p><h1 id="conclusion">Conclusion</h1><p>In this introduction we’ve barely scratched the surface of model distillation. The standard approach to distillation encourages the distilled model to emulate the hidden states of the larger teacher model.</p><p>How can we augment the standard approach with methods that align structured domain knowledge (like we’d see in knowledge graphs, entity-relational graphs, markup-languages, causal models, simulators, process models, etc.). Only time will tell.&nbsp;</p><p>Still, model distillation presents an exciting frontier in the AI world, offering a blend of efficiency and performance.&nbsp;As AI developers, integrating distillation into your workflows can lead to more efficient, cost-effective, and innovative AI solutions.&nbsp;The future of AI is not just in building larger and larger models. The future will be about developing more intelligent applications requiring smaller and smarter task-specific models.</p><p>Model distillation is a key step in that direction.</p><p>We’re happy to help answer any questions. Reach out to us anytime on our&nbsp;<a href="../../sales/indexc625.html?ref=labelbox.ghost.io"><u>contact us</u></a>&nbsp;page.</p></div></main></div></div></div><div class="mt-5 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="my-20 w-full h-[1px] bg-neutral-200"></div><div class="grid grid-cols-12 gap-2"><div class="col-span-12"><h2 class="mb-12 text-center text-3xl md:text-4xl font-medium">Continue reading</h2></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../the-power-of-human-expertise-transforming-audio-and-multimodal-stem-models-with-labelbox-services/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index632b.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Esther Na<span class="mx-2">•</span>March 6, 2025</p></div><a href="../the-power-of-human-expertise-transforming-audio-and-multimodal-stem-models-with-labelbox-services/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">The power of human expertise: Transforming audio and multimodal STEM models with Quantumworks Lab Services</p><p class="text-base max-w-2xl undefined line-clamp-3">In this blog, learn about two AI lab customers who utilized Quantumworks Lab&#x27;s top-tier AI trainers to drive innovation in their audio and multimodal STEM models. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../how-to-generate-industry-specific-data-for-ai-training-with-labelbox/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F02%2FBlog_industry-specific.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F02%2FBlog_industry-specific.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F02%2FBlog_industry-specific.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F02%2FBlog_industry-specific.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F02%2FBlog_industry-specific.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F02%2FBlog_industry-specific.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F02%2FBlog_industry-specific.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F02%2FBlog_industry-specific.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index49ca.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F02%2FBlog_industry-specific.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Quantumworks Lab<span class="mx-2">•</span>February 25, 2025</p></div><a href="../how-to-generate-industry-specific-data-for-ai-training-with-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to generate industry-specific data for AI training with Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">This guide will teach you how to generate domain-specific data with the Quantumworks Lab data factory to train your LLMs and AI models on industry-specific reasoning. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../code-runner-secure-scalable-code-execution-for-model-evaluation-2/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index4144.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Dmytro Apollonin<span class="mx-2">•</span>December 20, 2024</p></div><a href="../code-runner-secure-scalable-code-execution-for-model-evaluation-2/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Code Runner: Secure, scalable code execution for model evaluation</p><p class="text-base max-w-2xl undefined line-clamp-3">Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.</p></a></div></div></div></div></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="Footer__StyledFooter-sc-u68pnv-0 eJChXt"><div class="undefined lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="py-24"><div class=" w-full h-[1px] bg-neutral-200"></div></div><div class="hidden md:block"><img src="../../static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36"/></div><section class="hidden md:grid footer-grid"></section><section class="social-media"></section><div class="text-center "><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">© Quantumworks Lab, Inc <br/>We enable breakthroughs</p><div class="flex flex-row flex-wrap justify-content-center gap-4 mt-4"><a href="https://docs.labelbox.com/page/terms-of-service" class=" " target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Terms of Service</p></a><div class="mx-1 border"></div><a href="https://docs.labelbox.com/page/privacy-notice" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Privacy Notice</p></a><div class="mx-1 border hidden sm:block"></div><a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Copyright Dispute Policy</p></a></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"a-pragmatic-introduction-to-model-distillation-for-ai-developers","id":"65b003b857624b0001cb517a","uuid":"1dee2d77-73fd-4a14-9d58-99d518ca373f","title":"A pragmatic introduction to model distillation for AI developers","html":"\u003cp\u003eThe landscape of artificial intelligence (AI) and machine learning (ML) is continuously evolving, surfacing innovative techniques that revolutionize how we develop and deploy AI models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOne such technique gaining significant traction is model distillation. \u003c/p\u003e\u003cp\u003eModel distillation has been instrumental in driving both open-source innovation of LLMs as well as the adoption of large models (both language and vision) for use cases where task specificity and runtime optimization have been required.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOne of the most well-known examples of model distillation in action is Stanford’s Alpaca, based on Meta’s LLaMa 7B model.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html?ref=labelbox.ghost.io\"\u003eAlpaca\u003c/a\u003e was trained in less than 2 months for less than $600 on 52K question and answer pairs generated using OpenAI’s text-davinci-003. At the time of release Alpaca boasted near comparable performance to GPT 3.5.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAlthough Alpaca was deprecated, the ability for a small team to create impressive models for a fraction of the cost from large foundation models injected fuel in the ensuing months for developers and teams eager to fine-tune their own powerful, task specific models using open-source models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this blog post our goal is to provide a pragmatic introduction to model distillation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel distillation is a useful technique that we believe will stand the test of time for companies wanting to efficiently and effectively create the best models for their use cases.\u003c/p\u003e\u003cp\u003eWe’ll provide the “why” and introduce the “how” of performing model distillation so that you too can train powerful and efficient models, important components of intelligent applications.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you’re an AI developer struggling to adopt Large Language or Large Vision models and trying to understand where model distillation fits in with other techniques like RAG, fine-tuning, and dataset distillation, this is the guide for you.\u003c/p\u003e\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003ch3 id=\"a-brief-overview\"\u003eA Brief Overview\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/model-distillation/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003eModel distillation\u003c/strong\u003e\u003c/a\u003e, also known as \u003ca href=\"https://labelbox.com/guides/knowledge-distillation/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003eknowledge distillation\u003c/strong\u003e\u003c/a\u003e, is a technique that focuses on creating efficient models by transferring knowledge from large, complex models to smaller, deployable ones.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe can think of the typical process as taking the best of a wide, deep model and transferring it to a narrow, short model.\u003c/p\u003e\u003cp\u003eThis process aims to maintain performance while reducing memory footprint and computation requirements, a form of model compression introduced by Geoffrey Hinton in \"\u003ca href=\"https://arxiv.org/abs/1503.02531?ref=labelbox.ghost.io\"\u003e\u003cu\u003eDistilling the Knowledge in a Neural Network\u003c/u\u003e\u003c/a\u003e\" (2015).\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"why-customize-foundation-models\"\u003eWhy Customize Foundation Models\u003c/h3\u003e\u003cp\u003eDespite the high accuracy of foundational models and ability to perform remarkable feats of \u003ca href=\"https://labelbox.com/guides/zero-shot-learning-few-shot-learning-fine-tuning/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ezero-shot learning\u003c/u\u003e\u003c/a\u003e, their deployment often faces challenges like increased latency and decreased throughput performance.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs a result, \u003ca href=\"https://labelbox.com/usecases/large-language-models/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003elarge language models (LLMs)\u003c/a\u003e with their millions of parameters, are cumbersome and resource-intensive, making them impractical for teams needing near real-time solutions beyond public benchmarks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, Google gives the estimate that a “\u003ca href=\"https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html?ref=labelbox.ghost.io\"\u003e\u003cu\u003esingle 175 billion LLM requires 350GB of GPU memory\u003c/u\u003e\u003c/a\u003e”. Many of the most powerful LLMs can be \u003ca href=\"https://en.wikipedia.org/wiki/Large_language_model?ref=labelbox.ghost.io#:~:text=data.%5B102%5D-,List,-%5Bedit%5D\"\u003e\u003cu\u003eupwards of 170 billion parameters\u003c/u\u003e\u003c/a\u003e and even LLMs with 10 million parameters will require 20GB of GPU memory (\u003ca href=\"https://www.anyscale.com/blog/num-every-llm-developer-should-know?ref=labelbox.ghost.io\"\u003e\u003cu\u003eassuming a roughly 2:1 ratio of GPU memory requirements for serving and the number of parameters\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;This tension necessitates the development of smaller, more specialized models.\u003c/p\u003e\u003cp\u003eModel distillation emerges as a solution (especially for \u003ca href=\"https://labelbox.com/customers/intuitive-surgical-customer-story/?ref=labelbox.ghost.io\"\u003ecompanies that have unique and proprietary data\u003c/a\u003e), offering efficient deployment, cost reduction, faster inference, customization, and sustainability.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBy optimizing foundation model sizes, foundation models can become even more widely adopted across a \u003ca href=\"https://labelbox.com/solutions/accelerators/?ref=labelbox.ghost.io\"\u003ebroader, diverse set of use cases\u003c/a\u003e in a practical, cost-effective, and environmentally friendly manner.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"understanding-model-distillation\"\u003eUnderstanding Model Distillation\u003c/h2\u003e\u003ch3 id=\"definition-and-basic-concepts\"\u003eDefinition and Basic Concepts\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/01/-hmqy_Wi64wQ0KBF4Y2AIYMmmIU02M30ekFMdnzg0ylQaGb1Sivxg6jvl15wIhMfa1KNONZPUyiD0bblWvfvjE22WW87VryrSnO7ZuZZh3GqYcoMiZ1Pm9HauR_9NIWIMBigEifYyOBFDtcVlgWjHIA.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"415\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/-hmqy_Wi64wQ0KBF4Y2AIYMmmIU02M30ekFMdnzg0ylQaGb1Sivxg6jvl15wIhMfa1KNONZPUyiD0bblWvfvjE22WW87VryrSnO7ZuZZh3GqYcoMiZ1Pm9HauR_9NIWIMBigEifYyOBFDtcVlgWjHIA.png 600w, https://labelbox.ghost.io/blog/content/images/2024/01/-hmqy_Wi64wQ0KBF4Y2AIYMmmIU02M30ekFMdnzg0ylQaGb1Sivxg6jvl15wIhMfa1KNONZPUyiD0bblWvfvjE22WW87VryrSnO7ZuZZh3GqYcoMiZ1Pm9HauR_9NIWIMBigEifYyOBFDtcVlgWjHIA.png 1000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 2.1: Knowledge Distillation – Source: \u003c/span\u003e\u003ca href=\"https://arxiv.org/abs/2006.05525?ref=labelbox.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003e“Knowledge Distillation: A Survey”\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eEarlier we described model distillation as a process of transferring knowledge from a large, complex model (\u003cstrong\u003eteacher\u003c/strong\u003e) to a smaller, more efficient model (\u003cstrong\u003estudent\u003c/strong\u003e).\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis (usually) involves two separate models: the teacher model, large and highly accurate, and the student model, compact and less resource-intensive.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"key-components-of-model-distillation\"\u003eKey Components of Model Distillation\u003c/h3\u003e\u003cp\u003eLet’s dig in deeper to the different components involved in model distillation.\u003c/p\u003e\u003cp\u003eThere are a couple key decisions that need to be made, such as:\u003c/p\u003e\u003ch3 id=\"picking-teacher-and-student-model-architectures\"\u003ePicking Teacher and Student Model Architectures\u003c/h3\u003e\u003cp\u003eModel distillation involves two main elements: the \u003cstrong\u003eteacher\u003c/strong\u003e model, a large pre-trained model with generally high performance, and the \u003cstrong\u003estudent\u003c/strong\u003e model, a smaller model that learns from the teacher (and that will ultimately be used for downstream applications).\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe student model can vary in structure, from a simplified or quantized version of the teacher model to an entirely different network with an optimized structure.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/01/pyPqxhQP8cOgjt65eXXywck3BUTj_23Bf4J3FV0lVQJ-92ZR24A2UO71fpT0MTL05hxGhWpaSfRR2ohqbL_ZIPuKmtjFwSYtCnokGXswvJhafki4JjwEY1TPfRFQ_TJo20fSuyao6VNAUkcqrlvriGY.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"355\" height=\"269\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 2.2: Types of Student Model Architectures – Source: \u003c/span\u003e\u003ca href=\"https://arxiv.org/abs/2006.05525?ref=labelbox.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003e“Knowledge Distillation: A Survey”\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"the-distillation-process-explained\"\u003eThe Distillation Process Explained\u003c/h3\u003e\u003cp\u003eThe next decision to be made is the distillation process to use.\u003c/p\u003e\u003cp\u003eThe \u003cstrong\u003edistillation process\u003c/strong\u003e involves training the smaller neural network (the student) to mimic the behavior of the larger, more complex teacher network by learning from its predictions or internal representations.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis process is a form of \u003cstrong\u003esupervised learning\u003c/strong\u003e where the student minimizes the difference between its predictions and those of the teacher model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/01/Fig-2.3_-Knowledge-Distillation-Core-Concepts-v2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1890\" height=\"1372\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-2.3_-Knowledge-Distillation-Core-Concepts-v2.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-2.3_-Knowledge-Distillation-Core-Concepts-v2.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-2.3_-Knowledge-Distillation-Core-Concepts-v2.png 1600w, https://labelbox.ghost.io/blog/content/images/2024/01/Fig-2.3_-Knowledge-Distillation-Core-Concepts-v2.png 1890w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 2.3: Knowledge Distillation Core Concepts\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"distillation-knowledge-types\"\u003eDistillation Knowledge Types\u003c/h3\u003e\u003cp\u003eWhat do we mean by \u003cstrong\u003eknowledge\u003c/strong\u003e?\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe types of knowledge distillation can be categorized into three types: \u003cstrong\u003eresponse-based\u003c/strong\u003e, \u003cstrong\u003efeature-based\u003c/strong\u003e, and \u003cstrong\u003erelation-based \u003c/strong\u003edistillation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach type focuses on different aspects of knowledge transfer from the teacher to the student model and offers unique advantages and challenges.\u003c/p\u003e\u003cp\u003eAlthough the most common and easiest form of knowledge distillation to get started with is \u003cstrong\u003eresponse-based distillation\u003c/strong\u003e, it’s helpful to understand the different types of knowledge distillation.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1. \u003cstrong\u003eResponse-Based Distillation\u003c/strong\u003e: Focuses on the student model mimicking the teacher's predictions. The teacher generates soft labels for each input example, and the student is trained to predict these labels by minimizing the difference in their outputs.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePros\u003c/strong\u003e: Easy implementation, applicable to various models and datasets.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCons\u003c/strong\u003e: Only transfers output-related knowledge, doesn’t capture complex internal representations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e2. \u003cstrong\u003eFeature-Based Distillation\u003c/strong\u003e: Involves the student model learning the internal features or representations learned by the teacher. The process includes the student minimizing the distance between the features learned by both models.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePros\u003c/strong\u003e: Helps learn robust representations, applicable across tasks and models.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCons\u003c/strong\u003e: Computationally expensive, not suitable for tasks where teacher's internal representations aren't transferable.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e3.\u003cstrong\u003e Relation-Based Distillation\u003c/strong\u003e: This method teaches the student to understand the relationships between inputs and outputs. It involves transferring the underlying relationships between these elements from the teacher to the student model.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePros\u003c/strong\u003e: Enables learning robust and generalizable input-output relationships.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCons\u003c/strong\u003e: Computationally intensive, requires greater sophistication and experience for implementation by the ML engineer.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"training-methods\"\u003eTraining Methods\u003c/h3\u003e\u003cp\u003eThe final decision that needs to be made is the training method used to transfer knowledge from the teacher to the student models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThere are three main training methods in model distillation: \u003cstrong\u003eoffline\u003c/strong\u003e, \u003cstrong\u003eonline\u003c/strong\u003e, and \u003cstrong\u003eself-distillation\u003c/strong\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOffline distillation\u003c/strong\u003e involves a pre-trained, frozen teacher model, while \u003cstrong\u003eonline distillation\u003c/strong\u003e trains both teacher and student models simultaneously.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSelf-distillation\u003c/strong\u003e uses the same network as both teacher and student, addressing conventional distillation limitations. This method requires the ability to copy the teacher model and be able to update the model, which is not possible with proprietary models or models where the weights and architecture haven’t been published.\u0026nbsp;\u003c/p\u003e\u003cp\u003eKnowledge of different training methods (offline, online, self-distillation) is essential for AI developers who need to implement and manage the training process in a way that aligns with their project's constraints and goals.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor instance, an AI developer working in an environment with limited data availability might find self-distillation methods particularly useful.\u003c/p\u003e\u003cp\u003eMost teams however start with a form of \u003cstrong\u003eresponse-based\u003c/strong\u003e, \u003cstrong\u003eoffline distillation\u003c/strong\u003e. Training offline allows the ML Engineer to evaluate model performance and analyze model errors before deploying the student model to production.\u0026nbsp;\u003cbr\u003e\u003c/p\u003e\u003ch2 id=\"benefits-of-model-distillation\"\u003eBenefits of Model Distillation\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/01/Fig-3.0_-Benefits.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1109\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-3.0_-Benefits.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-3.0_-Benefits.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-3.0_-Benefits.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/01/Fig-3.0_-Benefits.png 2400w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 3.1: Benefits of Using Model Distillation\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThere are many benefits to performing model distillation and using smaller, task specific models rather than a \u003ca href=\"https://labelbox.com/usecases/large-language-models/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eLarge Language Model\u003c/u\u003e\u003c/a\u003e (or Large Vision Model) out of the box.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"efficiency-in-training\"\u003eEfficiency in Training\u003c/h3\u003e\u003cp\u003eModel distillation enhances data efficiency, requiring less data for pretraining (and potentially fine-tuning). Model distillation aligns with data-centric AI philosophy to maximize the utility of data.\u003c/p\u003e\u003ch3 id=\"efficiency-in-deployment\"\u003eEfficiency in Deployment\u003c/h3\u003e\u003cp\u003eDistilled models are smaller and more efficient, ideal for deploying on platforms with \u003ca href=\"https://labelbox.com/research/lightweight-multi-drone-detection-and-3d-localization-via-yolo/?ref=labelbox.ghost.io\"\u003e\u003cu\u003elimited resources\u003c/u\u003e\u003c/a\u003e. They offer versatility in applications like \u003ca href=\"https://labelbox.com/research/fruit-flower-detection-in-apple-orchards-using-ml/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eedge computing\u003c/u\u003e\u003c/a\u003e and real-time processing.\u003c/p\u003e\u003ch3 id=\"performance-improvements\"\u003ePerformance Improvements\u003c/h3\u003e\u003cp\u003eSimilar to fine-tuning, model distillation can improve accuracy and performance on specific tasks and domains. The resulting smaller models achieve similar performance to larger ones but with quicker response times.\u003c/p\u003e\u003ch3 id=\"resource-optimization\"\u003eResource Optimization\u003c/h3\u003e\u003cp\u003eModel distillation leads to cost reduction and sustainability. It reduces computational and storage requirements, making it beneficial for projects with limited budgets and aligning with ethical AI development strategies.\u003c/p\u003e\u003ch2 id=\"potential-challenges\"\u003ePotential Challenges\u003c/h2\u003e\u003cp\u003eLike many powerful techniques, key considerations must be made in order to effectively and efficiently implement model distillation as part of a team’s AI development process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eSome key challenges include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelecting appropriate teacher and student models,\u0026nbsp;\u003c/li\u003e\u003cli\u003eBalancing size, speed, and accuracy of the student model,\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003eThe technical complexity of the distillation process,\u003c/li\u003e\u003cli\u003eEnsuring models are retrained as necessary to minimize model drift,\u003c/li\u003e\u003cli\u003eVersioning and curating the data used for student training,\u003c/li\u003e\u003cli\u003eEnsuring visibility and monitoring of the model performance.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSome teams in industries like gaming that require real-time learning or edge-constrained devices may find that offline training isn’t sufficient and they need to explore a continuous learning pattern of model development and deployment, such as online distillation.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"considerations-in-using-model-distillation\"\u003eConsiderations In Using Model Distillation\u003c/h2\u003e\u003ch3 id=\"difference-between-model-distillation-fine-tuning-and-traditional-model-training\"\u003eDifference Between Model Distillation, Fine-Tuning, and Traditional Model Training\u003c/h3\u003e\u003cp\u003eHow does model distillation compare to fine-tuning, another popular technique utilized with foundation models? Are they mutually exclusive, complementary, or sequential approaches?\u003c/p\u003e\u003cp\u003eModel distillation, fine-tuning, and traditional model training (also called pretraining) each have distinct purposes in machine learning.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.1_-Before-Foundation-Models-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1520\" height=\"688\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.1.1_-Before-Foundation-Models-1.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.1.1_-Before-Foundation-Models-1.png 1000w, https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.1_-Before-Foundation-Models-1.png 1520w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 5.1.1: Early Years of Developing Deep Learning Models\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch5 id=\"\"\u003e\u003c/h5\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.2_-Foundation-Models--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"728\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.1.2_-Foundation-Models--1-.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.1.2_-Foundation-Models--1-.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-5.1.2_-Foundation-Models--1-.png 1600w, https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.2_-Foundation-Models--1-.png 2362w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 5.1.2: The New Opportunities \u0026amp; Challenges of Foundation Models\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTraditional training\u003c/strong\u003e involves learning directly from raw data, often requiring extensive resources, to generate a model from scratch. The drawback of this approach is the significant requirement for supporting Machine Learning Operations infrastructure to bridge the development-production chasm and the cost (both in data acquisition and compute) to approach the same performance of the foundation models available on the market (both proprietary and open-source).\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFine-tuning\u003c/strong\u003e adjusts an existing, pre-trained model using a specific dataset, which can be resource-efficient but relies on human-generated labels. The resulting model is usually a similar size but with improved performance on domains and tasks corresponding to the dataset that was used.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.3_-Improving-Model-Efficiency--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"593\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.1.3_-Improving-Model-Efficiency--1-.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.1.3_-Improving-Model-Efficiency--1-.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-5.1.3_-Improving-Model-Efficiency--1-.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/01/Fig-5.1.3_-Improving-Model-Efficiency--1-.png 2400w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 5.1.3: Improving Model Efficiency\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eModel distillation offers the best of many worlds.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn comparison to \u003cstrong\u003epre-training\u003c/strong\u003e (or \u003cstrong\u003etraditional training\u003c/strong\u003e), model distillation requires far less data (both raw and labeled). By starting with a powerful foundation model to train a smaller model (student), the developer is guaranteed a starting performance similar to the foundation model on specific tasks and domains but with less computational demand due to the smaller parameter size.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe \u003cstrong\u003eoffline\u003c/strong\u003e, \u003cstrong\u003eresponse-based\u003c/strong\u003e approach for model distillation can be easily performed by a developer using minimal infrastructure and requires very little labeled data to start with, as the labels are provided by the parent mode in the form of a response. This approach is especially beneficial for deploying pipelines requiring the use of one or more foundation models in resource-constrained environments, as it bypasses the need for extensive data or manual labeling.\u003c/p\u003e\u003cp\u003eWith that being said, the use of model distillation and fine-tuning isn’t mutually exclusive and can both be used by the same team and even for the same project.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.4_-Options-for-Optimizing-Models-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"814\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.1.4_-Options-for-Optimizing-Models-1.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.1.4_-Options-for-Optimizing-Models-1.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-5.1.4_-Options-for-Optimizing-Models-1.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/01/Fig-5.1.4_-Options-for-Optimizing-Models-1.png 2400w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 5.1.4: Options for Optimizing Models\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.1.5_-Leveraging-Model-Distillation---Fine-Tuning-Together-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"732\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.1.5_-Leveraging-Model-Distillation---Fine-Tuning-Together-3.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.1.5_-Leveraging-Model-Distillation---Fine-Tuning-Together-3.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-5.1.5_-Leveraging-Model-Distillation---Fine-Tuning-Together-3.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/01/Fig-5.1.5_-Leveraging-Model-Distillation---Fine-Tuning-Together-3.png 2400w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 5.1.5: Leveraging Model Distillation \u0026amp; Fine-Tuning Together\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou could perform model distillation using a base foundation model to create a smaller, student model from the parent model. Then the student model could be fine-tuned on a new, unique dataset (automatically transformed and ingested as part of a centralized, data catalog).\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn upcoming tutorials, we show how easy it is to use Quantumworks Lab’s \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e to perform model distillation for both \u003ca href=\"https://labelbox.com/usecases/computer-vision/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003ecomputer vision\u003c/u\u003e\u003c/a\u003e and NLP use cases.\u003c/p\u003e\u003ch3 id=\"leveraging-model-distillation-with-rag-prompt-engineering-within-fmops\"\u003eLeveraging Model Distillation with RAG, Prompt Engineering, within FMOps\u003c/h3\u003e\u003cp\u003eWe’ve established that model distillation and fine-tuning are both used for the purpose of adapting the capabilities of large foundation models to more task specific use cases.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe ultimate purpose of model distillation is making models more inference-optimized as a form of \u003cstrong\u003emodel compression\u003c/strong\u003e (without significant loss in performance and accuracy within the domain area of interest), whereas the focus of fine-tuning is improving task specific performance (with model size being relatively irrelevant).\u003c/p\u003e\u003cp\u003eIn addition to knowledge distillation, other compression and acceleration methods like \u003cstrong\u003equantization\u003c/strong\u003e, \u003cstrong\u003epruning\u003c/strong\u003e, and \u003cstrong\u003elow-rank factorization\u003c/strong\u003e are also employed.\u003c/p\u003e\u003cp\u003eModel distillation can be used not just with fine-tuning but alongside RAG-based systems and prompt engineering techniques.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eModel distillation\u003c/strong\u003e, \u003cstrong\u003efine-tuning\u003c/strong\u003e, \u003cstrong\u003eRAG\u003c/strong\u003e and \u003cstrong\u003eprompt engineering\u003c/strong\u003e are all considered important tools in the toolkit of \u003cstrong\u003eFMOps\u003c/strong\u003e, or Foundation Model Operations.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"rag-based-systems\"\u003eRAG-Based Systems\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eRAG (Retrieval-Augmented Generation)\u003c/strong\u003e is a pattern (usually leveraging a \u003cstrong\u003evector database\u003c/strong\u003e) that combines a neural retrieval mechanism with a sequence-to-sequence model, allowing it to retrieve relevant documents or data to enhance its responses. This approach enables the model to incorporate a broader range of external information and context, beyond what is contained in its initial training data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn RAG-based systems, model distillation helps manage the size and complexity of models, ensuring they remain functional and efficient.\u0026nbsp;\u003c/p\u003e\u003cp\u003eSimilar to fine-tuning, model distillation produces a static model that doesn’t have real-time information on the entire internet. The resulting student model also shouldn’t have access to proprietary data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eRAG ensures that the right information is fetched and injected into the context of the prompt or as part of the \u003cstrong\u003eresponse\u003c/strong\u003e. In essence, RAG is usually pretty quick so the bottleneck in inference speed is usually in the embedding and serving stages (i.e. the student model).\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"prompt-engineering\"\u003ePrompt Engineering\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003ePrompt engineering\u003c/strong\u003e involves crafting input \u003cstrong\u003eprompts\u003c/strong\u003e in a way that effectively guides models, especially foundation models based on natural language processing, to produce structured outputs or responses.\u003c/p\u003e\u003cp\u003eThis process is crucial in optimizing the performance of models like GPT-3, as the quality and structure of the input significantly influence the accuracy and relevance of the generated output.\u003c/p\u003e\u003cp\u003ePrompt engineering should be used in generating the responses used to train the student model from the teacher model and to further structure the inputs and outputs to the student model once it’s deployed in production.\u003c/p\u003e\u003ch3 id=\"reinforcement-learning-from-human-feedback-rlhf\"\u003eReinforcement Learning From Human Feedback (RLHF)\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003eReinforcement Learning from Human Feedback (RLHF)\u003c/a\u003e is a machine learning approach where a model is trained to optimize its behavior based on feedback from human interactions, rather than solely relying on predefined reward functions. This method allows for more nuanced and contextually appropriate learning, as it incorporates human judgment and preferences into the training process.\u003c/p\u003e\u003cp\u003eThe feedback from these experts is used to improve either the teacher model by improving the quality of the initial pre-training data, the student model by enhancing and enriching the responses and output dataset used to fine-tune the student model, and by further refining the fine-tuned student model for retraining. \u003c/p\u003e\u003cp\u003eMidjourney has effectively used RLHF, for example, to continuously collect user feedback through their Discord-based UI. Users can select options to redo runs, create variants, and upscale the images they like the best. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://cdn.document360.io/3040c2b6-fead-4744-a3a9-d56d621c6c7e/Images/Documentation/MJ_UpscaledUI.png\" class=\"kg-image\" alt=\"Image of the Midjourney Discord button interface after upscaling an images\" loading=\"lazy\" width=\"640\" height=\"740\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 5.2.3: Iteratively Collecting User Feedback For Retraining Through The UI – Source: \"\u003c/span\u003e\u003ca href=\"https://docs.midjourney.com/docs/midjourney-discord?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eMidjourney Documentation\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003e\"\u003c/span\u003e\u003c/u\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"the-fmops-toolkit\"\u003eThe FMOps Toolkit\u003c/h2\u003e\u003cp\u003eTo tie all the techniques together:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel Distillation\u003c/strong\u003e – Is the process of creating a smaller, task-specific model from a powerful, larger foundation model, usually by generating responses (aka labels, outputs, or predictions) that are then used for model training or fine-tuning;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-Tuning\u003c/strong\u003e – Is a process for adapting an existing model to a domain specific dataset to improve performance on a specific set of tasks through supervised learning, with the resulting model used in an intelligent application;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRAG\u003c/strong\u003e – Is a pattern for injecting external information into a model, as either part of the context fed to the model through the user prompt or injected dynamically into the response, usually based on some kind of similarity match between the information and the prompt;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrompt Engineering – \u003c/strong\u003eIs the practice of strategically designing and structuring input prompts to effectively guide an AI model's response, optimizing the model's performance in generating accurate, relevant, and contextually appropriate outputs;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRLHF\u003c/strong\u003e – Is a method of training AI models by using human feedback to reinforce desired behaviors, enabling the model to learn from human preferences and judgments, thereby refining its responses and actions in a more contextually and socially aware manner.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.2.1_-Foundation-Model-Operations.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"610\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/01/Fig-5.2.1_-Foundation-Model-Operations.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/01/Fig-5.2.1_-Foundation-Model-Operations.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/01/Fig-5.2.1_-Foundation-Model-Operations.png 1600w, https://labelbox.ghost.io/blog/content/images/2024/01/Fig-5.2.1_-Foundation-Model-Operations.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFig 5.2.4: Leveraging FMOps To Develop intelligent Applications\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"practical-applications\"\u003ePractical Applications\u003c/h2\u003e\u003cp\u003eThe power of model distillation is it’s applicability across a wide range of practical use cases, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIndustry-Specific Solutions\u003c/strong\u003e: In \u003ca href=\"https://labelbox.com/industries/healthcare-and-life-sciences/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehealthcare\u003c/u\u003e\u003c/a\u003e for \u003ca href=\"https://labelbox.com/solutions/healthcare-and-life-sciences/medical-imaging/?ref=labelbox.ghost.io\"\u003e\u003cu\u003erapid diagnosis\u003c/u\u003e\u003c/a\u003e, and in \u003ca href=\"https://labelbox.com/industries/financial-services-and-insurance/?ref=labelbox.ghost.io\"\u003e\u003cu\u003efinance\u003c/u\u003e\u003c/a\u003e for real-time fraud detection. Industries as varied as \u003ca href=\"https://labelbox.com/industries/retail-and-ecommerce/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eretail and ecommerce\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/industries/media-and-internet/?ref=labelbox.ghost.io\"\u003e\u003cu\u003emedia and entertainment\u003c/u\u003e\u003c/a\u003e, and even \u003ca href=\"https://labelbox.com/industries/industrial/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eindustrial\u003c/u\u003e\u003c/a\u003e can benefit from efficient ML workloads.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCommon Use Cases\u003c/strong\u003e: Utilized in \u003ca href=\"https://labelbox.com/solutions/computer-vision/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ecomputer vision\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/guides/how-to-build-a-content-moderation-model-to-detect-disinformation/?ref=labelbox.ghost.io\"\u003e\u003cu\u003enatural language processing\u003c/u\u003e\u003c/a\u003e, speech recognition, \u003ca href=\"https://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail/?ref=labelbox.ghost.io\"\u003e\u003cu\u003erecommendation systems\u003c/u\u003e\u003c/a\u003e, and neural architecture search.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eReasons why model distillation will become even more important in the future include:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCompliance and Privacy\u003c/strong\u003e: In certain cases, regulations or privacy concerns might restrict the use of cloud-based, large-scale AI models. Distilled models can often be deployed locally, offering a solution that respects privacy and regulatory constraints.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnergy Efficiency and Sustainability\u003c/strong\u003e: The environmental impact of running large-scale AI models is a growing concern. Distilled models require less computational power, which translates to lower energy consumption, aligning with the increasing need for sustainable AI practices.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Accessibility and User Experience\u003c/strong\u003e: For applications like mobile apps or web services, using a distilled model means faster response times and lower bandwidth requirements, leading to a better user experience. This is particularly relevant for generative AI applications that interact directly with end-users, like chatbots or image generators.\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"getting-started-with-model-distillation-using-labelbox\"\u003eGetting Started With Model Distillation Using Quantumworks Lab\u003c/h2\u003e\u003ch3 id=\"basic-tools-and-platforms\"\u003eBasic tools and platforms\u003c/h3\u003e\u003cp\u003eHere’s what you’ll need to get started with model distillation:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAccess to a foundation model: \u003c/strong\u003eA relevant foundation model that can be used as the parent model to generate responses from prompts or unstructured data like images or texts. For inspiration, \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehere are all the models currently available on the Quantumworks Lab platform\u003c/u\u003e\u003c/a\u003e. Models not listed \u003ca href=\"https://docs.labelbox.com/reference/hugging-face-integration?ref=labelbox.ghost.io\"\u003e\u003cu\u003ecan still be incorporated\u003c/u\u003e\u003c/a\u003e from sites like \u003ca href=\"https://labelbox.com/blog/hugging-face-models/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eHuggingFace\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eA training framework\u003c/strong\u003e: Libraries like TensorFlow and PyTorch offer functionalities for model distillation, such as training and model pickling.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eA development environment or notebook\u003c/strong\u003e: Model distillation can be done locally or in a cloud based IDE or notebook-based environment like Colab or \u003ca href=\"https://labelbox.com/guides/how-to-automatically-ingest-data-from-databricks-into-labelbox/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eDatabricks notebook\u003c/u\u003e\u003c/a\u003e (even an MLOps platform like GCP’s \u003ca href=\"https://labelbox.com/guides/how-to-fine-tune-vertex-ai-models-with-labelbox/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eVertex AI\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eA platform for automating data and response preparation:\u003c/strong\u003e A platform like Quantumworks Lab with capability to automate the data preparation process, generating either labels or predictions. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eA data storage and curation platform:\u003c/strong\u003e Once the responses have been generated, a centralized repository for the inputs (or prompts) and the outputs (or responses) will be valuable, especially for future analysis. Quantumworks Lab provides a data curation and storage solution in the form of \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e, which offers rich features for filtering multiple modalities (image, text, video, audio, and geospatial data).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"initial-steps\"\u003eInitial Steps\u003c/h3\u003e\u003cul\u003e\u003cli\u003eIf you’re just getting started, consider applying model distillation to train a student model in an offline, response-based manner using one of the popular foundation models for either a computer vision or natural language processing use case.\u003c/li\u003e\u003cli\u003eOtherwise, be sure to decide on the following:\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePicking the teacher and student models:\u003c/strong\u003e What model will be used for the teacher model? What model will be used for the student model (if fine-tuning) or will model training happen from scratch?\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePicking the knowledge type: \u003c/strong\u003eWhat kind of knowledge do you want to capture? Do you want to mimic the responses? The internal features? The relationship between the inputs and outputs?\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePicking the training method\u003c/strong\u003e: How will you train the student model? Offline? Continuously or online? Self-train via self-distillation?\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"next-steps-applying-model-distillation-in-practice\"\u003eNext Steps: Applying Model Distillation In Practice\u003c/h3\u003e\u003cp\u003eIn the upcoming parts of the series, you’ll get a chance to see how simple and easy it is to get started with model distillation using Quantumworks Lab’s Catalog and our data engine. \u003c/p\u003e\u003cp\u003eIn Part 2 of the series, we’ll demonstrate an end-to-end workflow for computer vision, using model distillation to fine-tune a model with labels created in Quantumworks Lab's data engine using Amazon Rekognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn Part 3 of the series, we’ll demonstrate an end-to-end workflow for NLP, using model distillation to fine-tune a BERT model with labels created in Quantumworks Lab using PaLM2.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eIn this introduction we’ve barely scratched the surface of model distillation. The standard approach to distillation encourages the distilled model to emulate the hidden states of the larger teacher model.\u003c/p\u003e\u003cp\u003eHow can we augment the standard approach with methods that align structured domain knowledge (like we’d see in knowledge graphs, entity-relational graphs, markup-languages, causal models, simulators, process models, etc.). Only time will tell.\u0026nbsp;\u003c/p\u003e\u003cp\u003eStill, model distillation presents an exciting frontier in the AI world, offering a blend of efficiency and performance.\u0026nbsp;As AI developers, integrating distillation into your workflows can lead to more efficient, cost-effective, and innovative AI solutions.\u0026nbsp;The future of AI is not just in building larger and larger models. The future will be about developing more intelligent applications requiring smaller and smarter task-specific models.\u003c/p\u003e\u003cp\u003eModel distillation is a key step in that direction.\u003c/p\u003e\u003cp\u003eWe’re happy to help answer any questions. Reach out to us anytime on our\u0026nbsp;\u003ca href=\"https://labelbox.com/sales?ref=labelbox.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;page.\u003c/p\u003e","comment_id":"65b003b857624b0001cb517a","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/01/Frame-3593--1-.png","featured":false,"visibility":"public","created_at":"2024-01-23T10:21:44.000-08:00","updated_at":"2024-12-02T15:49:00.000-08:00","published_at":"2024-01-23T13:19:38.000-08:00","custom_excerpt":"Model distillation is an important technique that can be applied across any domain and use case to develop smaller, task specific models. Learn more about model distillation in this practical guide.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},{"id":"65302fc14e99900001fc0527","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-llms/"},{"id":"65302fd64e99900001fc0529","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-computer-vision/"},{"id":"65303cb64e99900001fc05a5","name":"Labeling automation","slug":"labeling-automation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/labeling-automation/"},{"id":"65b02cf657624b0001cb52a1","name":"Model Distillation","slug":"model-distillation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/model-distillation/"},{"id":"6530313c4e99900001fc0537","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/train-fine-tune-ai/"}],"authors":[{"id":"65aab69e454cd800010e1a79","name":"Mikiko Bazeley","slug":"mikiko","profile_image":"https://labelbox.ghost.io/blog/content/images/2024/01/230829_0520.png","cover_image":null,"bio":null,"website":null,"location":"San Francisco","facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/mikiko/"}],"primary_author":{"id":"65aab69e454cd800010e1a79","name":"Mikiko Bazeley","slug":"mikiko","profile_image":"https://labelbox.ghost.io/blog/content/images/2024/01/230829_0520.png","cover_image":null,"bio":null,"website":null,"location":"San Francisco","facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/mikiko/"},"primary_tag":{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},"url":"https://labelbox.ghost.io/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/","excerpt":"Model distillation is an important technique that can be applied across any domain and use case to develop smaller, task specific models. Learn more about model distillation in this practical guide.","reading_time":15,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},"recommended":[{"id":"67ca1d6cb890cf0001e11bef","uuid":"16aa9964-ac17-468a-bef9-f6dddf71c1a7","title":"The power of human expertise: Transforming audio and multimodal STEM models with Quantumworks Lab Services","slug":"the-power-of-human-expertise-transforming-audio-and-multimodal-stem-models-with-labelbox-services","html":"\u003cp\u003eLeading frontier AI builders leverage domain-and language-specific expertise to differentiate their models across data modalities—including audio, multimodal, image, text, and video—and to train them for more complex tasks. As the capabilities of AI expands, the need for post-training processes like SFT, RLHF, and human evaluation remain strong. These tasks depend on expert human knowledge to guide models toward higher performance.This is where we come in. Quantumworks Lab is the AI data factory that delivers high-quality data across all modalities, including specialized domains like STEM, finance, coding, and law—across a wide range of languages for each.\u0026nbsp; \u003c/p\u003e\u003cp\u003eWith \u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eLabelbox Labeling Services\u003c/u\u003e\u003c/a\u003e, we harness our skilled talent network, \u003ca href=\"https://www.alignerr.com/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eAlignerr\u003c/u\u003e\u003c/a\u003e, to source, vet, and onboard custom teams of human experts who can align models and generate new training data with domain-specific knowledge. Quantumworks Lab can operate and fully-manage a project with Alignerrs and the Quantumworks Lab Platform that generates new training data for our customers, or through \u003ca href=\"https://labelbox.com/services/alignerr-connect/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eAlignerr Connect\u003c/u\u003e\u003c/a\u003e, customers can browse and select top-tier experts to staff their existing projects and utilize their in-house tools and processes. In this blog, we highlight two recent customers who utilized our top-tier human experts to drive innovation in their cutting-edge audio and multimodal STEM models.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"driving-breakthroughs-in-the-ai-audio-landscape\"\u003eDriving breakthroughs in the AI audio landscape\u0026nbsp;\u003c/h2\u003e\u003cp\u003eA growing AI audio startup aimed to enhance its voice, speech, and sound models by training with expert-labeled data. They faced challenges due to the subjectivity of labeling large volumes of complex audio. Quantumworks Lab addressed this through our platform’s custom audio editor and building a team of trainers with expertise in voice acting and speech. Their background enabled them to label nuanced audio segments with greater accuracy than generalists.\u003c/p\u003e\u003cp\u003eOne of the Alignerrs on the project, Jeff K., has a PhD in Theater and Performance Studies. He shared this about his experience on the project:\u003c/p\u003e\u003cp\u003e\u003cem\u003e“Through years of performing and teaching the arts, I've developed a deep understanding of voice dynamics. I have mental checklists for how and where voices change, which makes it natural for me to identify the various emotions in speech and understand their impact on the listener.\" \u003c/em\u003e\u003cbr\u003e\u003cbr\u003eThis specialized level of human expertise was critical in enabling the startup to create high-quality audio datasets and enhance their AI models, driving the adoption of their advanced audio technology. \u003cbr\u003e\u003cbr\u003eWant to explore the full story behind this success? Read more \u003ca href=\"https://labelbox.com/customers/cutting-edge-audio-models-customer-story/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"improving-multimodal-reasoning-capabilities-with-stem-experts\"\u003eImproving multimodal reasoning capabilities with STEM experts\u003c/h2\u003e\u003cp\u003eA leading AI lab aimed to enhance its large language model (LLM) by identifying weaknesses in K-12 STEM education responses, but they needed a diverse team of STEM experts to create new training data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox’s Labeling Services brought together a team of highly skilled experts with PhDs and Masters in STEM, who were tasked with reviewing multimodal prompts and responses spanning various domains, including natural science, physics, earth science, and language comprehension. Each task included reviewing the model’s response to a question that contained metadata such as question format, subject category, and an associated image URL.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe AI trainers then had to rate the answers across a number of different areas, including examining the model’s ability to read text, analyze image, and respond correctly. This collaboration helped the lab pinpoint areas for improvement and boost performance with high-quality, domain-specific STEM data.\u003c/p\u003e\u003cp\u003eIf you are interested in learning more about this work, read more \u003ca href=\"https://labelbox.com/customers/multimodal-STEM-customer-story/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"looking-to-leverage-labelbox%E2%80%99s-expert-ai-trainers\"\u003eLooking to leverage Quantumworks Lab’s expert AI trainers?\u003c/h2\u003e\u003cp\u003eThese two customer stories highlight a few examples of the groundbreaking work Quantumworks Lab is performing with frontier model builders.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you’d like to learn more about the expert teams we offer and are ready to discuss your data needs, \u003ca href=\"https://labelbox.com/sales/?ref=labelbox.ghost.io\"\u003e\u003cu\u003econtact our team\u003c/u\u003e\u003c/a\u003e anytime on how we can help. You can also directly explore profiles of some of our AI trainers \u003ca href=\"https://labelbox.com/services/alignerr-connect/trainers/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"67ca1d6cb890cf0001e11bef","feature_image":"https://labelbox.ghost.io/blog/content/images/2025/03/Blog_CustomerStories-1.png","featured":false,"visibility":"public","created_at":"2025-03-06T14:10:52.000-08:00","updated_at":"2025-03-31T14:44:29.000-07:00","published_at":"2025-03-06T14:19:30.000-08:00","custom_excerpt":"In this blog, learn about two AI lab customers who utilized Quantumworks Lab's top-tier AI trainers to drive innovation in their audio and multimodal STEM models. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"},{"id":"66e9be59d0584c0001886b42","name":"Services","slug":"services","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/services/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"},{"id":"670018ec863cb90001f263e9","name":"Customers","slug":"customers","description":"Quantumworks Lab customer stories","feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/customers/"}],"authors":[{"id":"671a9e7504d48b00016d39a3","name":"Esther Na","slug":"esther","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/esther/"}],"primary_author":{"id":"671a9e7504d48b00016d39a3","name":"Esther Na","slug":"esther","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/esther/"},"primary_tag":{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},"url":"https://labelbox.ghost.io/blog/the-power-of-human-expertise-transforming-audio-and-multimodal-stem-models-with-labelbox-services/","excerpt":"In this blog, learn about two AI lab customers who utilized Quantumworks Lab's top-tier AI trainers to drive innovation in their audio and multimodal STEM models. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Customers transform audio and multimodal AI with Quantumworks Lab Services","meta_description":"In this blog, learn about two AI lab customers who utilized Quantumworks Lab's top-tier AI trainers to drive innovation in their audio and multimodal STEM models. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"67bd005c3d2f94000130cf29","uuid":"3f09970f-5674-4723-9240-ac8ba605d717","title":"How to generate industry-specific data for AI training with Quantumworks Lab","slug":"how-to-generate-industry-specific-data-for-ai-training-with-labelbox","html":"\u003cp\u003eGenerative AI models are becoming increasingly sophisticated thanks to advances in post-training and model alignment tasks. As a result, the demand for models that not only understand language but also grasp the nuances of specific industries is skyrocketing.\u003c/p\u003e\u003cp\u003eWhile general-purpose large language models (LLMs) have made significant strides, they often fall short when faced with tasks requiring deep domain expertise. Many industries require AI systems capable of industry-specific reasoning that can navigate complex, domain-specific scenarios with expert-level understanding.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe journey to creating AI that truly \"gets\" a particular field goes beyond simply feeding it more data. It requires a targeted approach that focuses on enhancing the model's ability to understand and apply the unique language, concepts, and problem-solving frameworks inherent to a particular field. This is where industry-specific reasoning comes in, and it is the key to unlocking the true potential of AI across sectors like finance, law, medicine, and insurance. By building and using higher quality training data, companies are creating a competitive advantage and opening the door to new opportunities.\u003c/p\u003e\u003cp\u003eThis guide will walk you through the process of generating domain-specific data with the Quantumworks Lab data factory to train your LLMs and AI models on industry-specific reasoning. We'll explore real-world examples in finance and law, delve into the intricacies of creating post-training datasets, and provide a step-by-step walkthrough of creating a project in the Quantumworks Lab platform. We will also discuss the importance of selecting the right AI trainers and crafting clear instructions and ontology.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBy the end of this guide, you'll be equipped with the knowledge and tools to embark on your own journey toward creating powerful training data to help you build AI models that are not just intelligent, but industry-smart.\u003c/p\u003e\u003ch2 id=\"examples-of-how-industry-specific-reasoning-is-becoming-a-reality\"\u003eExamples of how industry-specific reasoning is becoming a reality\u003c/h2\u003e\u003cp\u003eThe power of industry-specific reasoning in AI is not merely theoretical; it's being realized today by forward-thinking companies leveraging advanced tools and expert human insights. Before explaining how to use the Quantumworks Lab platform to build a dataset for training models on industry-specific reasons, let's examine two real-world examples where Quantumworks Lab has helped clients in the finance and legal sectors expand the capabilities of foundational models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese examples showcase how targeted data labeling, guided by domain experts, can enhance an LLM's ability to support different industries. And while each example discusses a specific industry’s use case, we’ll later see how the process and usage of Quantumworks Lab can be extended to a much broader range of domains and uses, paving the way for a deeper dive into the practical steps involved in creating your own successful projects.\u003c/p\u003e\u003ch4 id=\"finance-training-models-on-financial-analysis\"\u003e\u003cstrong\u003eFinance: Training models on financial analysis\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eA leading AI lab wanted to improve their LLM’s performance in financial analysis and argumentation. They hoped to train the LLM to provide meaningful insights on any public company when provided a ticker symbol and the latest financial reports of a company. In addition, they wanted the model trained and prepared to answer the most common questions financial analyst might ask.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo reach this level of financial expertise, they needed training on detailed, domain-specific datasets that were accurately labeled by a team of Chartered Financial Analysts (CFAs) and financial experts. The CFAs needed to have advanced industry knowledge and experience reviewing and analyzing financial models and details.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, they faced two major challenges: (1) sourcing financial experts who could accurately evaluate and rank responses through multi-step analyses, and (2) managing a complex process where each piece of data could take an hour or more to properly analyze and fully label.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe company turned to Quantumworks Lab to help source experts with financial expertise. Quantumworks Lab leveraged their Alignerr network of expert human labelers to quickly recruit and onboard a skilled team of financial experts with backgrounds in CFA, MBA, and Masters in Finance. The team built a customized project consisting of a complex ontology, detailed instructions, and numerous attachments per dataset to review and prepare hypothetical questions and scenarios around.\u0026nbsp;\u003c/p\u003e\u003ch4 id=\"legal-automating-case-evaluation-and-data-discovery\"\u003e\u003cstrong\u003eLegal: Automating case evaluation and data discovery\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eA forward-thinking legal tech company wants to revolutionize how plaintiff firms operate. By using AI-driven tools with industry-specific reasoning, they want to significantly enhance their ability to evaluate cases, analyze key facts and claims, and ultimately deliver results for their clients more efficiently and transparently.\u003c/p\u003e\u003cp\u003eTo achieve this, they needed to imbue their foundational AI model with deep legal expertise, particularly in understanding long legal documents, interpreting insurance and medical bills, and assessing case value.\u003c/p\u003e\u003cp\u003eThe company partnered with Quantumworks Lab to create specialized datasets for post-training their model. Quantumworks Lab's network of legal experts reviewed a long list of example prompts that were each associated with many multi-page legal documents. They were tasked with identifying key information based on the prompt, crafting well-reasoned responses supported by evidence found in the documents, and evaluating the models responses for accuracy, safety, and reasoning. They were also used to build a dataset based on understanding insurance and medical billing details and learning how to extra the correct details.\u003c/p\u003e\u003cp\u003eSimilar to the finance example above, this project involved creating a custom ontology and using the flexibility of the Quantumworks Lab text editor to generate responses to sample prompts from industry experts, identify key data in attached documents, and use their own expertise to properly interpret complex documents.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"importance-of-high-quality-training-datasets\"\u003eImportance of high-quality training datasets\u0026nbsp;\u003c/h2\u003e\u003cp\u003eThe above finance and legal examples, while distinct in their domain-specific challenges, underscore a fundamental truth about building industry-specific reasoning into AI: the underlying approach remains consistent across industries, whether it's finance, medicine, insurance, or any other specialized field.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAt its core, the process revolves around creating unique and differentiated datasets that capture the specific nuances, knowledge, and reasoning patterns of the target domain. These datasets serve as the bedrock for training models to perform the desired capabilities, enabling them to go beyond general understanding and develop true expertise.\u0026nbsp;\u003c/p\u003e\u003ch4 id=\"strategies-to-remember-when-planning-your-project\"\u003e\u003cstrong\u003eStrategies to remember when planning your project\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eBefore kicking off a project or starting the process of building a post-training dataset for industry-specific reasoning, organizations should take the time to think through the following steps:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIdentifying key domain-specific competencies required\u003c/li\u003e\u003cli\u003eDefining clear evaluation criteria for expert knowledge\u003c/li\u003e\u003cli\u003eGathering supporting documentation and examples to attach to the project\u003c/li\u003e\u003cli\u003eWriting hypothetical scenarios and questions you want the model to answer\u003c/li\u003e\u003cli\u003eEstablishing metrics for measuring improvement in domain understanding\u003c/li\u003e\u003cli\u003eSetting realistic scope and scale for the training dataset\u003c/li\u003e\u003cli\u003ePreparing a model evaluation process for side-by-side comparisons\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis list may be shortened by some users and for others there may be other key considerations to add; however, we’ve learned that the most successful projects involve advanced planning and often a fair amount of preparation to gather necessary documents, built prompt examples, and think through the key scenarios that require examples and labeled data to sufficiently train the model.\u0026nbsp;\u003c/p\u003e\u003ch4 id=\"selecting-the-right-ai-trainers\"\u003e\u003cstrong\u003eSelecting the right AI trainers\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eThe next big AI breakthroughs will be fueled by unique, high-quality data. The massive quantity of available data from the internet has been used to train all of the most popular AI models—meaning they all have similar core capabilities and areas of weakness.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo properly train these models on new domain-specific knowledge and reasoning it’s imperative that we tap into new information and capture data from domain experts. As a result, the focus must be on properly identifying and recruiting AI trainers with unique skills—like the \u003ca href=\"https://www.alignerr.com/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eAlignerr network\u003c/u\u003e\u003c/a\u003e of highly educated industry experts—to label data, generate new responses, evaluate models, and perform critical model post-training alignment tasks.\u003c/p\u003e\u003cp\u003eBased on our experience from leading the Alignerr network and working with AI labs and model builders to form teams of experts, it’s important to keep the following in mind:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDomain expertise:\u003c/strong\u003e Look for annotators with relevant qualifications, certifications, and experience in the target industry (e.g., CFA, JD, MD).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAnalytical skills:\u003c/strong\u003e Choose individuals who can demonstrate strong analytical and reasoning abilities.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAttention to detail:\u003c/strong\u003e Ensure the annotators are meticulous and capable of identifying subtle nuances in the data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCommunication skills:\u003c/strong\u003e Select annotators who can clearly articulate their reasoning and provide constructive feedback.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrevious success metrics:\u003c/strong\u003e For annotators that have worked on AI training before and used a complete AI platform, then you should be able to review past performance metrics to assist in the evaluations.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAI interviews: \u003c/strong\u003eAbilities can only be determined so much by reviewing resumes and profiles, so using interviews tailored to their specific background provides the most powerful data.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"key-labelbox-features-for-generating-industry-specific-training-data\"\u003eKey Quantumworks Lab features for generating industry-specific training data\u003c/h2\u003e\u003cp\u003eHaving outlined strategic considerations to review before starting a project as well as the keys to selecting the right domain experts, the next crucial step is understanding how to operationalize this process efficiently and effectively. This is where a robust and versatile platform like Quantumworks Lab becomes indispensable.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox Platform is an industry-leading software tool with the flexibility and advanced features needed to translate your vision into a concrete, well-structured labeling project tailored for industry-specific reasoning.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFrom selecting the appropriate editor—often the flexible text editor for tasks involving complex analysis and document reviews—to building a comprehensive ontology with a mix of selection tools and free-form text inputs, Quantumworks Lab can capture the nuances of your target domain. In addition, the built-in quality assurance and project management capabilities ensure that your project stays on track, maintains high accuracy, and ultimately delivers a dataset that enhances your AI model's performance.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLet's briefly explore the key components of the Quantumworks Lab platform that we see used most often in industry-specific reasoning tasks and discuss how they can be leveraged to build the foundation for your industry-specific AI.\u003c/p\u003e\u003ch4 id=\"data-types-editors\"\u003e\u003cstrong\u003eData types \u0026amp; editors\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eLabelbox supports a range of data types, including text, video, image, audio, PDFs, multimodal, and more. For each data type, a customized editor exists to serve as the core interface for labeling that specific data type. While tailored to specific datatypes, the editors are extremely flexible and can be customized to help label and capture the specific data needed for a given industry.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor most industry-specific reasoning projects, we have seen the core \u003ca href=\"https://docs.labelbox.com/docs/text-editor?ref=labelbox.ghost.io\"\u003e\u003cu\u003etext editor\u003c/u\u003e\u003c/a\u003e serve as the key editor. It was the editor used in both the finance and legal examples shared earlier. The editor supports these annotation types: entity, relationships, radio classification, checklist classification, and free-form text classification. The latter is often used to capture detailed information and sample responses from domain experts on a given prompt or to provide detailed information on specific information found or extracted from an attached document.\u0026nbsp;\u003c/p\u003e\u003ch4 id=\"ontology\"\u003e\u003cstrong\u003eOntology\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eThe \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox.ghost.io\"\u003e\u003cu\u003eLabelbox ontology\u003c/u\u003e\u003c/a\u003e defines the structure and categories for labeling data. Ontologies can be reused across different projects and they are required for data labeling, model training, and evaluation. When you are in the editor, the ontology appears in the Tools panel.\u003c/p\u003e\u003cp\u003eOntologies can be customized for any given project and supports classifications, object detection, segmentation, relationships, messaging ranking, prompt rating, step-by-step rating, and more. The available ontology tasks will vary based on the selected editor and data type.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOntologies also allow for hierarchical relationships between classes, allowing you to create complex labeling tasks with detailed information.\u0026nbsp;\u003c/p\u003e\u003ch4 id=\"quality-assurance\"\u003e\u003cstrong\u003eQuality assurance\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eLabelbox offers a suite of quality assurance features to deliver high-quality labeled data, which is particularly crucial when dealing with complex, industry-specific reasoning tasks. Consensus allows you to measure the agreement between multiple labelers on the same data point, providing a statistical measure of confidence in the assigned labels. Benchmarks enable you to incorporate known ground-truth data into your project, allowing you to directly assess labeler accuracy against established standards.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCalibration tools help identify and correct for systematic biases that individual labelers might exhibit, further refining the consistency of your dataset. Moreover, Quantumworks Lab Monitor provides real-time insights into your labeling operations, allowing you to track key metrics, identify trends, and quickly spot any outliers in labeler performance. With Monitor, you can proactively address issues and make adjustments as needed, ensuring that your project stays on course.\u003c/p\u003e\u003ch4 id=\"project-management\"\u003e\u003cstrong\u003eProject management\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eLabelbox provides robust project management tools to manage and track your industry-specific reasoning projects from start to finish. Unlike many labeling services and solutions, the Quantumworks Lab platform offers transparency into project progress, allowing you to monitor the status of individual tasks, track labeler performance, and gain a clear overview of the entire project's health. Real-time communication features enable seamless interaction with your team of expert labelers, facilitating quick clarification of instructions, addressing questions, and providing feedback directly within the platform.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox also allows you to build customized workflows tailored to your specific review and quality control processes, ensuring that each piece of data undergoes the appropriate level of scrutiny before being incorporated into your training set. Furthermore, robust data versioning capabilities provide a comprehensive history of all changes made to your data and ontology, allowing for easy rollback if needed and providing a clear audit trail for maintaining data integrity.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"creating-an-industry-specific-reasoning-project-in-labelbox\"\u003eCreating an industry-specific reasoning project in Quantumworks Lab\u003c/h2\u003e\u003cp\u003eHaving explored the core features of the Quantumworks Lab platform, let’s examine the key steps to creating and executing on a project to generate new data for training your AI models and apps on industry-specific reasoning.\u003c/p\u003e\u003cp\u003eThis section provides a practical roadmap, walking you through each stage of the process, from crafting clear and comprehensive instructions for your expert labelers to setting up your project within the Quantumworks Lab environment, configuring the ideal ontology, managing the labeling operation, and conducting thorough reviews to ensure the highest level of data quality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eUsing these steps as a framework for your project, you can start exploring how the Quantumworks Lab Platform can be used for your unique project. You’ll learn what it takes to build a new training dataset for your specific needs. \u003c/p\u003e\u003ch4 id=\"1-identify-desired-outcomes-and-goals\"\u003e\u003cstrong\u003e1. Identify desired outcomes and goals\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eBefore diving into the mechanics of using Quantumworks Lab, it's crucial to establish a clear understanding of your project's objectives. Training frontier models for industry-specific reasoning requires a well-defined target. What specific problem are you trying to solve? What capabilities do you want your model to possess? Answering these questions will guide your data collection and annotation strategy, ultimately determining the success of your project. For example, are you aiming to build a model that can summarize legal documents, predict financial market trends, or diagnose medical conditions? Each of these scenarios demands a different approach to data and annotation.\u003c/p\u003e\u003cp\u003eThis initial phase focuses on defining the desired outcomes and the key capabilities you want your model to achieve. Let's consider a practical example: building a model to assist legal experts in reviewing case files. The desired outcome might be to reduce the time spent on initial case review by automating the identification of key legal arguments and relevant precedents. This translates into key capabilities like: understanding legal terminology, identifying relationships between different parts of a case file, and summarizing complex legal arguments. We need to capture training data that reflects these capabilities. This might include labeled examples of legal arguments, summaries of past cases, and annotations highlighting the relationships between different legal concepts within a document.\u003c/p\u003e\u003ch4 id=\"2-write-clear-instructions\"\u003e\u003cstrong\u003e2. Write clear instructions\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eThe quality of your training data hinges on the clarity and completeness of the instructions provided to your AI trainers. Ambiguous guidelines lead to inconsistent annotations, which ultimately undermines the performance of your frontier model. This section outlines the key components of effective instruction design, ensuring your team can accurately and efficiently generate the high-quality data you need.\u003c/p\u003e\u003cp\u003eA comprehensive instruction set begins with a clear overview of the task. Explain the project's overall goal and how the annotations contribute to achieving that goal. Frame the task within the context of the larger project, emphasizing the importance of accurate labeling. For our legal case review example, this overview might explain how the labeled data will train a model to assist lawyers in quickly identifying key information within case files, ultimately saving time and resources.\u003c/p\u003e\u003cp\u003eNext, provide a detailed explanation of the ontology and all relevant terminology. Clearly define each label, category, or rating, avoiding jargon or technical terms that the labelers might not understand. Use simple language and provide real-world examples to illustrate each concept. For instance, instead of simply defining \"legal precedent,\" explain it with a concrete example: \"Legal precedent refers to a previous court decision that serves as a guide for similar cases in the future. For example, the 1954 Supreme Court case Brown v. Board of Education established a precedent for desegregating public schools.\" Provide multiple examples for each category to cover a range of scenarios. For financial analysis, this might include defining terms like \"bull market,\" \"bear market,\" and \"market volatility,\" each with illustrative examples from real-world financial news.\u003c/p\u003e\u003cp\u003eAddressing edge cases and exceptions is crucial. Anticipate situations where the correct label might be unclear or ambiguous. Provide specific guidance on how to handle these situations. And it’s important to provide guidance on what a trainer should do if they don’t feel qualified or confident in a given task. The instructions should clearly state: \"If you are unsure about the correct label, or if you encounter a situation not covered in these guidelines, do not guess. Instead, flag the case for review by a subject matter expert.\" This emphasis on accuracy over completeness is essential for maintaining data quality.\u003c/p\u003e\u003cp\u003eFinally, establish a clear process for feedback, questions, and clarification. Provide a designated channel for labelers to ask questions, report issues, or suggest improvements to the guidelines. Regularly review and incorporate feedback to refine the instructions and address any ambiguities that arise during the annotation process. This iterative approach ensures that the labeling process becomes more accurate and efficient over time. A well-defined communication channel also empowers labelers to contribute valuable insights based on their experience with the data.\u003c/p\u003e\u003ch4 id=\"3-create-a-new-project-and-select-the-right-editor\"\u003e\u003cstrong\u003e3. Create a new project and select the right editor\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eYou're ready to create your project within the Quantumworks Lab platform. Begin by logging into your \u003ca href=\"https://app.labelbox.com/home?ref=labelbox.ghost.io\"\u003e\u003cu\u003eLabelbox account\u003c/u\u003e\u003c/a\u003e. Once logged in, click “Create project.”\u003c/p\u003e\u003cp\u003eThe first step is to select the appropriate data modality or task type for your project. This choice is crucial as it determines the tools and interface available to your labelers. For industry-specific reasoning projects involving long-form text analysis, such as reviewing legal documents or analyzing financial reports, the \u003cem\u003eText editor\u003c/em\u003e is often the most suitable option.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe6JVMOwP2YQkAGOA_b1BMZyK0LV3m9Q2h2XMLsFwDVxpjmwIOZS0RUTSfdijAF_LkaewSJmvA_Gh_-30qE8wTAxOawA4Eohobyp2izBk85Tl2f6w4BROD4ls1LVmuYKBy_J3-gQA?key=n3W5WytgaR_uk4G4uepV7qDG\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"379\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eCreate a new project in Quantumworks Lab by selecting the appropriate data modality or task type\u003c/em\u003e\u003c/p\u003e\u003cp\u003eLabelbox also supports other modalities like image, video, audio, and multimodal chat, allowing you to adapt to various data types. For example, if your project involves evaluating the quality of responses from a large language model in a live chat conversation, you may use the multimodal chat editor. If your project involves processing audio recordings of earnings calls, you may select audio. Select the modality that best reflects the nature of your data.\u003c/p\u003e\u003cp\u003eProvide your project with a descriptive and informative \u003cem\u003ename\u003c/em\u003e. This will help you easily identify and manage your project within Labelbox. For our legal case review example, a name like \"Legal Case Review - Contract Analysis\" would be appropriate. For financial analysis, it might be “Financial Report Analysis - Risk Assessment.”\u003c/p\u003e\u003cp\u003eFinally, \u003cem\u003eupload any existing data\u003c/em\u003e that you plan to use in the project. Quantumworks Lab supports various data formats, making it easy to import your data. This might include PDFs, plain text files, CSV files, or even connections to cloud storage. Consider organizing your data into logical batches or datasets before uploading to simplify project management. While uploading, ensure that your data is properly formatted and structured for optimal use within the Quantumworks Lab platform. For large datasets, consider using Quantumworks Lab's data import capabilities to streamline the process.\u003c/p\u003e\u003ch4 id=\"4-customize-your-ontology\"\u003e\u003cstrong\u003e4. Customize your ontology\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eA well-defined ontology is the backbone of your labeling project. It provides the structure and vocabulary for your AI trainers, ensuring consistency and accuracy in their annotations.\u003c/p\u003e\u003cp\u003eNavigate to the \"Ontology\" tab within your newly created project. This is where you'll define the building blocks of your annotation schema.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXf-803iyhcp4sokBV32MavTuHSP0jcDW7l72SZFdzMBoCiUcEnhz-qpo2cIw3qGSdA7Mh4ebaIH1WHQIikgzFJRus8lbpU6tGFtz5VrAdSQhWKz4e8suPlvKSI_cS2Z0qFMerEhig?key=n3W5WytgaR_uk4G4uepV7qDG\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"391\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eExample of an ontology configured to collect both ratings and free responses on a given datarow\u003c/em\u003e\u003c/p\u003e\u003cp\u003eClean, thoughtful ontologies help create high-quality labeled data with minimal errors and inconsistencies. Ontologies are an essential part of the Quantumworks Lab labeling platform. Every time you create a project or a model in Quantumworks Lab, you will need to select an ontology.\u003c/p\u003e\u003cp\u003eAlong with the core objects, classifications, and relationships that you’ll use in the text editor, you can also establish \u003cem\u003ehierarchical relationships\u003c/em\u003e between classes if it makes sense for your domain. This allows you to create a more structured and organized ontology. For instance, \"Breach of Contract\" could be a subclass of a more general class called \"Contractual Issue.\" Hierarchical relationships can help your model learn more general concepts from specific examples. They can also make the labeling process more efficient by allowing labelers to quickly navigate through related concepts. For complex ontologies, consider using Quantumworks Lab's hierarchical labeling features to simplify the annotation process.\u003c/p\u003e\u003cp\u003eFor most of today’s complex tasks for generative AI, consider incorporating \u003cem\u003efree text fields\u003c/em\u003e to capture richer insights from your AI trainers. These fields give the trainers the ability to rewrite prompts or responses, provide detailed feedback, and explain the rationale behind their ratings or labels. This qualitative information can be invaluable for understanding model behavior and improving its performance.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor instance, a free text field could allow a labeler to explain why they chose a particular label, highlight ambiguities in the data, or suggest improvements to the ontology. This feedback loop is crucial for refining your model and ensuring it aligns with your desired outcomes.\u003c/p\u003e\u003ch4 id=\"5-execute-the-project-label-rate-and-align\"\u003e\u003cstrong\u003e5. Execute the project: label, rate and align\u0026nbsp;\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eWith your project configured, data loaded, and a robust ontology in place, you're ready to invite your team to begin executing on the tasks and generating new training data.\u003c/p\u003e\u003cp\u003eFirst, invite your selected AI trainers to the project. Quantumworks Lab makes it easy to add team members and assign them specific roles. Ensure that each annotator has the necessary training and understands the project's goals, the ontology, and the annotation guidelines. Consider providing a brief onboarding session to familiarize annotators with the Quantumworks Lab platform and the specific requirements of your project.\u003c/p\u003e\u003cp\u003eNext, assign data batches to the annotators. Organize your data into manageable batches to streamline the annotation workflow. Quantumworks Lab provides tools for batch management, allowing you to distribute data evenly among your team members. Consider assigning smaller batches initially to allow for early feedback and adjustments to the annotation process. As annotators become more proficient, you can increase the batch size.\u003c/p\u003e\u003cp\u003eMonitor the labeling progress regularly. The \u003ca href=\"https://docs.labelbox.com/docs/monitor?ref=labelbox.ghost.io\"\u003e\u003cu\u003eLabelbox workspace monitor\u003c/u\u003e\u003c/a\u003e provides dashboards and reporting tools that allow you to track the progress of each annotator and identify any potential bottlenecks or issues. Regular monitoring also allows you to provide timely feedback to annotators and address any questions or concerns they may have. This proactive approach helps maintain data quality and ensures that the project stays on track.\u003c/p\u003e\u003cp\u003eUtilize Consensus, Benchmarks, and Calibration to ensure data quality. These features are essential for maintaining consistency and accuracy in your annotations. Consensus involves having multiple annotators label the same data points and then comparing their annotations. Discrepancies can be discussed and resolved, leading to higher quality data.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eBenchmarks\u003c/em\u003e are pre-labeled data points that serve as a gold standard for evaluating annotator performance. Regularly testing annotators on benchmarks can help identify areas where they may need additional training or guidance.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eCalibration\u003c/em\u003e is the process of adjusting the annotation guidelines or training materials based on feedback from annotators and insights gained from consensus and benchmark analysis. This iterative approach ensures that your data quality continuously improves throughout the project lifecycle. By actively managing your team and implementing these quality control measures, you can generate the high-quality training data needed to power your frontier models.\u003c/p\u003e\u003ch4 id=\"6-review-and-perform-qa\"\u003e\u003cstrong\u003e6. Review and perform QA\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eEven with well-defined instructions and diligent annotators, a robust review process is essential for guaranteeing the highest quality training data.\u003c/p\u003e\u003cp\u003eEstablishing a strong review process within Quantumworks Lab allows you to implement multiple layers of quality control. You can configure reviews on a per-project basis, tailoring the process to the specific needs of your task. For example, you might require all annotations to be reviewed by a subject matter expert before being accepted into the training dataset. Alternatively, you could implement a tiered review system, where a subset of annotations are reviewed by a senior annotator, and only those that meet a certain quality threshold are then passed on to a subject matter expert for final review. Quantumworks Lab's review workflows can be customized to fit your specific requirements, allowing you to create a scalable and efficient quality control system. Consider implementing a system where annotations are reviewed by a different annotator than the one who originally labeled the data. This helps to catch potential biases or inconsistencies.\u003c/p\u003e\u003cp\u003eBeyond manual review, Quantumworks Lab offers AutoQA features that can significantly improve data quality. AutoQA leverages machine learning models to automatically identify potential errors or inconsistencies in your annotations. These features can flag annotations that deviate significantly from the consensus, highlight areas where annotators disagree, or identify annotations that are inconsistent with pre-defined rules or constraints. By proactively identifying potential issues, AutoQA allows you to focus your review efforts on the most critical areas, saving time and resources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBy combining manual review with AutoQA, you can create a comprehensive quality assurance system that ensures your training data is accurate, consistent, and reliable. This, in turn, will lead to better performing frontier models capable of industry-specific reasoning.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"disrupting-your-industry-with-advanced-ai-capabilities\"\u003eDisrupting your industry with advanced AI capabilities\u003c/h2\u003e\u003cp\u003eThe pursuit of AI excellence demands a shift from generic to specialized. As we've explored in this guide, building industry-specific reasoning into LLMs and AI models is not just a technical challenge, but a strategic imperative. By leveraging platforms like Quantumworks Lab and embracing a data-centric approach, companies can unlock the true potential of AI and create models that are not just intelligent, but also insightful, reliable, and tailored to the unique demands of their respective industries.\u003c/p\u003e\u003cp\u003eThis guide has provided a roadmap for embarking on this transformative journey. From crafting clear instructions and building robust ontologies to selecting the right AI trainers and leveraging the powerful features of the Quantumworks Lab platform, you now have the foundational knowledge to create your own industry-specific reasoning projects. Remember that the key to success lies in a meticulous, iterative approach, where continuous learning and improvement are paramount.\u003c/p\u003e\u003cp\u003eAs you venture forth, keep in mind that the landscape of AI is constantly evolving. Stay curious, embrace new challenges, and never stop refining your approach. The future of AI is not just about building smarter models, but about building models that truly understand the world in all its specialized complexity. And with Quantumworks Lab as your partner, you're well-equipped to lead the charge toward a future where AI is not just a tool, but a true industry expert.\u003c/p\u003e\u003ch2 id=\"additional-resources\"\u003eAdditional resources:\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eLabelbox documentation\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/customers/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eLabelbox customer stories\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"67bd005c3d2f94000130cf29","feature_image":"https://labelbox.ghost.io/blog/content/images/2025/02/Blog_industry-specific.png","featured":false,"visibility":"public","created_at":"2025-02-24T15:27:24.000-08:00","updated_at":"2025-03-31T14:46:10.000-07:00","published_at":"2025-02-24T16:03:56.000-08:00","custom_excerpt":"This guide will teach you how to generate domain-specific data with the Quantumworks Lab data factory to train your LLMs and AI models on industry-specific reasoning. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"},{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"}],"authors":[{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"}],"primary_author":{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"},"primary_tag":{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},"url":"https://labelbox.ghost.io/blog/how-to-generate-industry-specific-data-for-ai-training-with-labelbox/","excerpt":"This guide will teach you how to generate domain-specific data with the Quantumworks Lab data factory to train your LLMs and AI models on industry-specific reasoning. ","reading_time":17,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Ultimate guide to generating industry-specific AI training data","meta_description":"This guide teaches you how to generate domain-specific data with the Quantumworks Lab data factory to train your AI models on industry-specific knowledge.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6765b8c06f63bf0001f1ca72","uuid":"9f912bc0-54da-4ac6-ab5f-78d8f926463c","title":"Code Runner: Secure, scalable code execution for model evaluation","slug":"code-runner-secure-scalable-code-execution-for-model-evaluation-2","html":"\u003cp\u003eIn the world of large language models (LLMs), evaluating their responses effectively is a fundamental aspect of improving model performance. We’re excited to announce the latest addition to the Quantumworks Lab platform: Code Runner.\u003cstrong\u003e \u003c/strong\u003eThis new capability pushes the boundaries of interactivity by allowing users to execute written code directly within the evaluation workflow.\u003c/p\u003e\u003cp\u003eCode Runner helps eliminate errors, optimizes functionality, and validates outputs, leading to higher-quality datasets. Today, we’ll introduce this new feature and then dive into the technical details of the infrastructure powering this feature, highlighting how it was designed with security, scalability, and\u003cstrong\u003e \u003c/strong\u003erobustness at its core.\u003c/p\u003e\u003ch2 id=\"what-is-code-runner\"\u003e\u003cstrong\u003eWhat is Code Runner?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCode Runner is a new built-in feature of the Quantumworks Lab platform designed to improve the quality of responses and labels generated in any coding-related projects. The new features enables users to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eDirectly execute code found in either model responses or user-written responses \u003c/li\u003e\u003cli\u003eReceive precise outputs including:\u003cul\u003e\u003cli\u003eStandard output (stdout)\u003c/li\u003e\u003cli\u003eStandard error (stderr)\u003c/li\u003e\u003cli\u003eExecution time\u003c/li\u003e\u003cli\u003eWarnings or runtime errors\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy integrating Code Runner into the evaluation pipeline, we aim to simplify the process of verifying the accuracy, efficiency, and functionality of code responses, all without users needing to leave the platform.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeBDN_1bnU_bTrPrWS59SWalVqw22Gxq3AIxNnbsOJmZGPap3weXHYFEgzrlPnEyhVK1GOjzCVClvQycomfMfhQsulqPk4wdQGqniZv8aIaHGP69wzgcFjdDdr5FgooITwNJCsp?key=GRyWmie9kDWaUfN6osDAF8J7\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"389\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOur system automatically detects the language in the text area and suggests the appropriate environment for execution, whether Python or JavaScript (and more to come).\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBut what makes this feature stand out is the sophisticated infrastructure behind it, designed to ensure seamless execution while maintaining strict security and privacy standards.\u003c/p\u003e\u003ch2 id=\"code-runner-infrastructure-a-deep-dive\"\u003e\u003cstrong\u003eCode Runner infrastructure: A deep dive\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAt the heart of Code Runner’s infrastructure lies Google Cloud Run, a fully managed compute platform that runs containerized applications in a secure, scalable manner. Here are the key components and principles driving the system:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Cloud Run for language-specific environments\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eEvery code execution happens in a dedicated Cloud Run instance. Each instance is tailored to a specific programming language environment (e.g., Python, JavaScript, etc.) and is spun up dynamically based on the code type detected in the user response.\u003c/p\u003e\u003cp\u003eThis design includes the following characteristics to ensure security and speed:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIsolation\u003c/strong\u003e: Each execution is fully containerized, completely isolating the runtime environment from others.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTemporary directories\u003c/strong\u003e: Code is executed in a temporary directory within the container, and it is deleted immediately after execution, leaving no trace behind.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLanguage-specific tools\u003c/strong\u003e: Each environment comes preloaded with the necessary packages and libraries to ensure compatibility and speed.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2. Enhanced security with separate GCP projects \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe Cloud Run service is hosted in a separate Google Cloud Platform (GCP) project, distinct from our main infrastructure. This segmentation provides an additional layer of security by isolating code execution from our core services. Even in the unlikely event of a compromise, the blast radius is contained.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Communication via private service connect\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo ensure secure and controlled communication, all interactions between the main evaluation system and the Cloud Run service occur over Private Service Connect, which provides the following advantages: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNo public exposure\u003c/strong\u003e: The Cloud Run endpoint is never exposed to the public internet, reducing the risk of unauthorized access.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOne-way communication\u003c/strong\u003e: The Private Service Connect setup restricts outbound networking from the Cloud Run service, ensuring that executed code cannot make arbitrary network requests. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGranular networking controls\u003c/strong\u003e: The private network allows for precise control over what resources the Cloud Run service can access.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4. Automatic cleanup\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo maintain a lightweight and secure runtime, the system delivers:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEphemeral execution\u003c/strong\u003e: Each execution request is handled in a stateless, temporary environment.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAutomatic deletion\u003c/strong\u003e: Files, logs, and temporary directories are wiped as soon as execution completes, leaving no residual data.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"how-code-runner-works-a-step-by-step-overview\"\u003e\u003cstrong\u003eHow Code Runner works: A step-by-step overview\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow that you have an understanding of the powerful infrastructure underneath Code Runner, here is a summary of how the feature works from start to finish:\u0026nbsp;\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eCode submission\u003c/strong\u003e: A user requests code execution from the evaluation interface.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLanguage detection\u003c/strong\u003e: The system detects the programming language and forwards the request to the corresponding Cloud Run service.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eExecution\u003c/strong\u003e: The Cloud Run instance spins up a container, executes the code in a sandboxed environment, and collects the results.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResult delivery\u003c/strong\u003e: The system returns the output (stdout, stderr, execution time, and any warnings) to the user for analysis.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCleanup\u003c/strong\u003e: The container and all related resources are terminated and deleted.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"advantages-of-labelbox%E2%80%99s-built-in-code-execution\"\u003e\u003cstrong\u003eAdvantages of Quantumworks Lab’s built-in code execution\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCode Runner’s infrastructure was designed specifically to provide the previously discussed benefits and to address several key challenges that other solutions may face:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSecurity\u003c/strong\u003e: By isolating execution environments and ensuring no public exposure, we eliminate a significant attack surface.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability\u003c/strong\u003e: Cloud Run’s serverless nature allows us to scale dynamically with demand, handling thousands of requests efficiently.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eReliability\u003c/strong\u003e: The use of ephemeral containers ensures that each execution starts in a clean slate, avoiding cross-contamination or resource conflicts.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"explore-it-yourself\"\u003e\u003cstrong\u003eExplore it yourself\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWith Code Runner, we’re empowering users to go beyond static evaluations, enabling dynamic, interactive testing that’s as secure as it is scalable. As always, we’re excited to hear your feedback and explore how we can push this feature even further.\u003c/p\u003e\u003cp\u003e If you want to explore Code Runner and other LLM evaluation tools, \u003ca href=\"https://app.labelbox.com/signup?_r=https://landing-page-git-uv-homepage-refresh-dec-24-labelbox.vercel.app/?utm_keyword=Quantumworks Lab\u0026utm_source=house\u0026utm_medium=email\u0026utm_campaign=1224%2520\u0026gclid=CjwKCAiA34S7BhAtEiwACZzv4a9veoKXnMnMvo2rWJvXkH46oHs4Lb5VFQi2ERBN_sQ5kgypV_zfBxoC0yMQAvD_BwE\u0026landingPageAnonymousId=%22e3f2f82f-be24-4045-b2b9-50a49cb801e8%22\u0026referrer_url=https://landing-page-git-uv-homepage-refresh-dec-24-labelbox.vercel.app/\"\u003e\u003cu\u003esign up\u003c/u\u003e\u003c/a\u003e for our platform today.\u0026nbsp;\u003c/p\u003e\u003cp\u003eStay tuned for updates, and happy coding!\u003c/p\u003e","comment_id":"6765b8c06f63bf0001f1ca72","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/12/Labelbox-code-runner--1-.png","featured":false,"visibility":"public","created_at":"2024-12-20T10:34:40.000-08:00","updated_at":"2025-03-12T12:01:43.000-07:00","published_at":"2024-12-20T12:44:45.000-08:00","custom_excerpt":"Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},{"id":"5f9af89dd8118b0039136165","name":"Engineering","slug":"engineering","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/engineering/"},{"id":"65302fc14e99900001fc0527","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-llms/"},{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"},{"id":"65303cb64e99900001fc05a5","name":"Labeling automation","slug":"labeling-automation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/labeling-automation/"},{"id":"6530313c4e99900001fc0537","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/train-fine-tune-ai/"}],"authors":[{"id":"6765bb156f63bf0001f1ca8d","name":"Dmytro Apollonin","slug":"dmytro","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/dmytro/"}],"primary_author":{"id":"6765bb156f63bf0001f1ca8d","name":"Dmytro Apollonin","slug":"dmytro","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/dmytro/"},"primary_tag":{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},"url":"https://labelbox.ghost.io/blog/code-runner-secure-scalable-code-execution-for-model-evaluation-2/","excerpt":"Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}]},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"a-pragmatic-introduction-to-model-distillation-for-ai-developers"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:47:28 GMT -->
</html>