<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/blog/make-videos-queryable-using-foundation-models/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 13:28:34 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Make your videos queryable using foundation models</title><meta name="description" data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Make your videos queryable using foundation models" data-next-head=""/><meta property="og:description" data-next-head=""/><meta property="og:url" content="https://labelbox.ghost.io/blog/make-videos-queryable-using-foundation-models/" data-next-head=""/><meta property="og:image" content="https://labelbox.ghost.io/blog/content/images/2023/03/queryable.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Make your videos queryable using foundation models" data-next-head=""/><meta name="twitter:description" data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.ghost.io/blog/make-videos-queryable-using-foundation-models/" data-next-head=""/><meta property="twitter:image" content="https://labelbox.ghost.io/blog/content/images/2023/03/queryable.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g48[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.eivcj #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.eivcj .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.eivcj .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.eivcj #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.eivcj #image-viewer .close:hover,.eivcj #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.eivcj .modal-content{width:100%;}}/*!sc*/
data-styled.g105[id="ImageModal__ImageModalWrapper-sc-1ey7m7r-0"]{content:"eivcj,"}/*!sc*/
.QsqTL .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.QsqTL .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.QsqTL .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:20px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h2{padding-top:10px;}}/*!sc*/
.QsqTL .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h3{padding-top:10px;}}/*!sc*/
.QsqTL .content h4{font-size:20px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 16px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h4{padding-top:8px;}}/*!sc*/
.QsqTL .content h5{font-size:18px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 14px;}/*!sc*/
.QsqTL .content h6{font-size:16px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 12px;}/*!sc*/
.QsqTL .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.QsqTL .content a:hover{color:#1e40af;}/*!sc*/
.QsqTL .content li{margin-bottom:20px;}/*!sc*/
.QsqTL .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.QsqTL .content ol{list-style:decimal;padding-left:20px;}/*!sc*/
.QsqTL .content .table-container{overflow-x:auto;margin:40px 0;-webkit-overflow-scrolling:touch;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-container{margin:30px -20px;padding:0 20px;}}/*!sc*/
.QsqTL .content table{width:100%;border-collapse:collapse;font-size:16px;background:white;border:1px solid #e5e7eb;border-radius:8px;overflow:hidden;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content table{font-size:14px;}}/*!sc*/
.QsqTL .content .table-container table{margin:0;min-width:600px;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-container table{min-width:700px;border-radius:0;border-left:none;border-right:none;}}/*!sc*/
.QsqTL .content .content table:not(.table-container table){margin:40px 0;min-width:auto;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .content table:not(.table-container table){margin:30px 0;min-width:auto;border-radius:8px;border:1px solid #e5e7eb;}}/*!sc*/
.QsqTL .content thead{background:#fafbfc;border-bottom:1px solid #d1d5db;}/*!sc*/
.QsqTL .content th{padding:16px 20px;text-align:left;font-weight:600;color:#374151;font-size:14px;-webkit-letter-spacing:0.025em;-moz-letter-spacing:0.025em;-ms-letter-spacing:0.025em;letter-spacing:0.025em;border-right:1px solid #f3f4f6;}/*!sc*/
.QsqTL .content th:last-child{border-right:none;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content th{padding:12px 16px;font-size:13px;}}/*!sc*/
.QsqTL .content td{padding:16px 20px;border-bottom:1px solid #f3f4f6;border-right:1px solid #f9fafb;color:#374151;line-height:1.5;}/*!sc*/
.QsqTL .content td:last-child{border-right:none;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content td{padding:12px 16px;}}/*!sc*/
.QsqTL .content tbody tr{-webkit-transition:background-color 0.2s ease;transition:background-color 0.2s ease;}/*!sc*/
.QsqTL .content tbody tr:hover{background-color:#f8fafc;}/*!sc*/
.QsqTL .content tbody tr:last-child td{border-bottom:none;}/*!sc*/
.QsqTL .content .table-wrapper{overflow-x:auto;margin:40px 0;border:1px solid #e5e7eb;border-radius:8px;-webkit-overflow-scrolling:touch;}/*!sc*/
.QsqTL .content .table-wrapper table{margin:0;border:none;border-radius:0;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-wrapper{margin:30px -20px;border-radius:0;border-left:none;border-right:none;}}/*!sc*/
.QsqTL .content code{background:#f1f5f9;padding:2px 6px;border-radius:4px;font-family:'Monaco','Menlo','Ubuntu Mono',monospace;font-size:14px;color:#e11d48;}/*!sc*/
.QsqTL .content pre{background:#1e293b;color:#e2e8f0;padding:20px;border-radius:8px;overflow-x:auto;margin:30px 0;}/*!sc*/
.QsqTL .content pre code{background:transparent;padding:0;color:inherit;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content pre{margin:20px -20px;border-radius:0;padding:16px 20px;}}/*!sc*/
.QsqTL .content blockquote{border-left:4px solid #2563eb;padding:20px 24px;margin:30px 0;background:#f8fafc;border-radius:0 8px 8px 0;font-style:italic;color:#475569;}/*!sc*/
.QsqTL .content blockquote p{margin-bottom:0;}/*!sc*/
.QsqTL .content blockquote p:last-child{margin-bottom:0;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content blockquote{margin:20px 0;padding:16px 20px;}}/*!sc*/
.QsqTL .content hr{border:none;height:1px;background:linear-gradient(to right,transparent,#e5e7eb,transparent);margin:50px 0;}/*!sc*/
.QsqTL .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.QsqTL .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;color:#6b7280;font-style:italic;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.QsqTL .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;border-radius:8px;}/*!sc*/
.QsqTL .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.QsqTL .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100%;height:100%;margin:0 auto;border-radius:8px;}/*!sc*/
.QsqTL .content .kg-bookmark-card{background:white;border-radius:10px;margin-top:60px !important;border:1px solid #e5e7eb;-webkit-transition:border-color 0.3s ease;transition:border-color 0.3s ease;}/*!sc*/
.QsqTL .content .kg-bookmark-card:hover{border-color:#d1d5db;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;color:#262626 !important;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-thumbnail{position:relative;min-width:30%;max-height:100%;overflow:hidden;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-thumbnail img{position:absolute;top:0;left:0;width:100% !important;height:100% !important;-o-object-fit:cover;object-position:left;object-fit:cover;border-radius:0 10px 10px 0;border-left:1px solid #f5f5f5;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;padding:20px;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-title{font-size:1.125rem;line-height:1.3;font-weight:600;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-description{font-size:0.875rem;font-weight:400;line-height:1.4;margin-top:12px;overflow-y:hidden;color:#6b7280;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;font-size:0.9rem;font-weight:400;margin-top:14px;color:#6b7280;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata img{width:22px !important;height:22px !important;margin-right:8px !important;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata .kg-bookmark-author{margin:4px;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata .kg-bookmark-publisher{margin:4px;}/*!sc*/
.QsqTL .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;margin:40px 0;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;margin-bottom:12px;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 6px;border-radius:6px;overflow:hidden;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;object-fit:cover;-webkit-transition:-webkit-transform 0.3s ease;-webkit-transition:transform 0.3s ease;transition:transform 0.3s ease;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row img:hover{-webkit-transform:scale(1.02);-ms-transform:scale(1.02);transform:scale(1.02);}/*!sc*/
data-styled.g112[id="id__PostContentWrapper-sc-1hduup0-0"]{content:"QsqTL,"}/*!sc*/
@media (max-width:767px){.bwsQop.toc-container{display:none;}}/*!sc*/
.bwsQop.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g113[id="id__TocContainer-sc-1hduup0-1"]{content:"bwsQop,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/blog/%5bid%5d-b80b73d0fd88ad55.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="ImageModal__ImageModalWrapper-sc-1ey7m7r-0 eivcj"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All blog posts</a><main class="id__TocContainer-sc-1hduup0-1 bwsQop toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><p class="my-4 text-sm font-medium">Manu Sharma<span class="mx-2">•</span>March 21, 2023</p><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">Make your videos queryable using foundation models</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox.ghost.io/blog/content/images/2023/03/queryable.png"/></div><main class="id__PostContentWrapper-sc-1hduup0-0 QsqTL md:px-24"><div class="content js-toc-content"><p>In this tutorial, I'll demonstrate how easy it is to enrich all of your video content with foundation models to perform tasks such as:</p><ul><li>Searching a wide range video content for specific types of videos</li><li>Understand the nature of video content within your data lake</li><li>Explore videos that contain specific keywords, or talk about a topic, or appear similar to some other videos</li><li>Select data that will improve your ML models, because it is rare or shown as an  edge-case scenario</li><li>Apply zero/few shot learning and weak supervision to label your data</li></ul><p>For this post, I utilized the most recent foundation models provided by OpenAI, Meta and Hugging Face to analyze and enrich a series of videos. <a href="../../product/catalog/indexc625.html?ref=labelbox.ghost.io">Quantumworks Lab Catalog</a> is used as the data platform to store all the metadata, search across metadata and embedding space, as well as perform zero shot classification.</p><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe src="https://fast.wistia.net/embed/iframe/l2e64x3dac" title="cooking_query_optimized Video" allow="autoplay; fullscreen" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" msallowfullscreen="" width="960" height="540"></iframe>
<script src="../../../fast.wistia.net/assets/external/E-v1.js" async=""></script><figcaption><p><span style="white-space: pre-wrap;">Easily search videos by combining foundation models with Quantumworks Lab Catalog</span></p></figcaption></figure><p>Videos are complex data structures that consist of a sequence of frames that are composed of many pixels. This complexity makes it challenging to develop efficient search algorithms that can quickly and accurately locate specific content within a video.</p><p>Additionally, due to variability in content, videos can vary significantly in terms of their content, making it challenging to develop search algorithms that work across different types of videos. For example, searching for a specific object or event within a sports highlight video may require a different approach than searching for the same object or event within a security camera footage. For object or scene detection, one obvious approach is to chunk the video into smaller pieces, downsample the video, generate and index inferences at frame level. However, various use cases just require global video level understanding (e.g. Instagram or Youtube shorts).</p><p>Let’s get started and take a look at how it works on a few real-world examples. For starters, I'll walk through how you can find cooking videos quickly by searching for text related to <code>cooking</code> or <code>recipe</code> in the transcript summary and combining this with foundation models. </p><h3 id="1-prepare-and-understand-data">1) Prepare and understand data</h3><p>I found the <em>QUERYD</em> dataset that contains a curated list of youtube videos. To understand what kind of videos they were, I downloaded the these videos and imported them into Quantumworks Lab Catalog.</p><figure class="kg-card kg-image-card"><img src="../../../labelbox.ghost.io/blog/content/images/2023/03/image-20230319-205645.png" class="kg-image" alt="" loading="lazy" width="2000" height="1111" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2023/03/image-20230319-205645.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2023/03/image-20230319-205645.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2023/03/image-20230319-205645.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2023/03/image-20230319-205645.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>A few initial observations from my dataset:</p><ul><li>Most videos are less than 900 seconds in duration.</li><li>The video content has a high degree of variation from podcasts, sports to cooking. Although these are common topics, widely available over internet.</li><li>Most videos have both audio and voice.</li></ul><h3 id="2-select-ai-models-to-generate-insights">2) Select AI models to generate insights</h3><p>Based on the preliminary understanding of the data, I chose the following foundation models to generate metadata in order to make the videos queryable.</p><ul><li><strong>OpenAI Whisper for video transcription</strong></li></ul><p>Whisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data collected from the web. It enables transcription in multiple languages, as well as translation from those languages into English</p><ul><li><strong>OpenAI GPT 3.5 for summarizing transcription</strong></li></ul><p>OpenAI's GPT-3.5 model is a language model that can perform a wide range of natural language processing tasks including summarization, language generation, translation, question-answering, text completion and more.</p><ul><li><a href="https://openai.com/blog/new-and-improved-embedding-model?ref=labelbox.ghost.io"><strong>OpenAI Generation 2 embeddings</strong></a><strong> for similarity search</strong></li></ul><p>Released in December 2022, the latest generation of embedding model by OpenAI, <code>text-embedding-ada-002</code>, replaces five separate models for text search, text similarity, and code search, and outperforms the previous most capable model, Davinci, at most tasks, while being priced 99.8% lower.</p><ul><li><strong>TimeSformer by Facebook for zero shot video classification</strong></li></ul><p>TimeSformer is a video processing model developed by Facebook AI Research (FAIR) and Meta (formerly Facebook Reality Labs) that can analyze and understand video data. TimeSformer model pre-trained on <a href="https://www.deepmind.com/open-source/kinetics?ref=labelbox.ghost.io"><u>Kinetics-400</u></a>. It was introduced in the paper <a href="https://arxiv.org/abs/2102.05095?ref=labelbox.ghost.io"><u>TimeSformer: Is Space-Time Attention All You Need for Video Understanding?</u></a> by Tong et al. and first released in <a href="https://github.com/facebookresearch/TimeSformer?ref=labelbox.ghost.io"><u>this repository</u></a>.</p><ul><li><a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2?ref=labelbox.ghost.io"><strong>Sentence-transformers/All-mpnet-base-v2</strong></a><strong> for text similarity search</strong></li></ul><p>This is a <a href="https://www.sbert.net/?ref=labelbox.ghost.io"><u>sentence-transformers</u></a> model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.</p><h3 id="3-generate-metadata-and-embeddings">3) Generate metadata and embeddings</h3><p>Below are the python functions to call OpenAI APIs to transcribe audio, summarize text and generate embeddings.</p><pre><code class="language-python">@retry(wait=wait_random(min=1, max=5), stop=stop_after_attempt(3))
def ask_gpt(prompt):
    try:
        response = openai.Completion.create(
          model="text-davinci-003",
          prompt="Summarize this in a short paragraph:\n {}".format(prompt),
          temperature=0.7,
          max_tokens=64,
          top_p=1.0,
          frequency_penalty=0.0,
          presence_penalty=0.0
        )
        transcript_summary = response.choices[0].text
        return transcript_summary
    except Exception as e:
        print(e)
        raise(e)

@retry(wait=wait_random(min=1, max=5), stop=stop_after_attempt(5))
def ask_whisper(mp3_file):
    try:
        audio_file = open(mp3_file, "rb")
        transcript = openai.Audio.transcribe("whisper-1", audio_file)
        transcript_text = transcript.text
        return transcript_text
    except Exception as e:
        print(e)
        raise(e)

@retry(wait=wait_random(min=1, max=5), stop=stop_after_attempt(5))
def embed_text(text):
    try:
        e = openai.Embedding.create(
            model = "text-embedding-ada-002",
            input = text
        )
        embeddings = []
        for item in e.data:
            embeddings.append(item.embedding)
        return embeddings
    except Exception as e:
        print(e)
        raise(e)</code></pre><p>Here’s a code snippet to generate inferences using Timesformer model on a local machine with a GPU using HuggingFace.</p><pre><code class="language-python">def predict_video_action(video_file):
    try:
        container = av.open(video_file, "rb")
        indices = sample_frame_indices(clip_len=60, frame_sample_rate=1, seg_len=container.streams.video[0].frames)
        video = read_video_pyav(container, indices)
        inputs = video_processor(list(video), return_tensors="pt").to("cuda:0")

        with torch.no_grad():
            outputs = video_model(**inputs)
            logits = outputs.logits

        predicted_class_idx = logits.argmax(-1).item()
        video_classification = video_model.config.id2label[predicted_class_idx]
    except Exception as e:
        print(e)
        
device = "cuda:0" if torch.cuda.is_available() else "cpu"
video_processor = AutoImageProcessor.from_pretrained("facebook/timesformer-base-finetuned-k400")
video_model = TimesformerForVideoClassification.from_pretrained("facebook/timesformer-base-finetuned-k400")
video_model = video_model.to(device)</code></pre><p>Now we process all of the videos and call the functions above to generate metadata.</p><pre><code class="language-python">for datarow in dataset:
    try:
        ### Transcribe the audio
        transcript_text = ask_whisper(mp3_file)

        ### Ask GPT to summarize the transcription
        transcript_summary = ask_gpt(transcript_text)

        ### Generate embedding on raw transcription
        embedding = embed_text(transcript_text)

        ### Timesformer video action recognition
        video_action = predict_video_action(video_file)
        
        data_rows.append({
            "row_data": video_url,
            "global_key": global_key,
            "metadata_fields": [
                {"name": transcript_schema.name, "value": transcript_text},
                {"name": transcript_summary_schema.name, "value": transcript_summary},
                {"name": video_classification_schema.name, "value": video_action}
                ],
            "attachments": [{
              "type": "TEXT",
              "value": transcript_text
              }]
        })
        
        text_embeddings.append({"id": global_key, "vector": embedding})
    
    except Exception as e:
        print(e)
        return None
        </code></pre><p>Now we create a new dataset and import metadata generated above.</p><pre><code class="language-python">dataset = client.create_dataset(name = "QUERYD dataset")
task = dataset.create_data_rows(data_rows)
task.wait_till_done()
print(task.errors)</code></pre><p>Now we import <a href="https://github.com/Quantumworks Lab/advlib?ref=labelbox.ghost.io">embeddings</a> using the code snippet below. (P.S. Python SDK support will be released soon)</p><pre><code class="language-python">openai_embedding_ndjson = list()
for i in tqdm(text_embeddings):
  global_key = i["id"]
  vector = i["vector"]
  datarow_id = client.get_data_row_ids_for_global_keys(global_key)["results"][0]
  openai_embedding_ndjson.append({"id": datarow_id, "vector": vector})

with open('openai_embedding.ndjson', 'w') as f:
    ndjson.dump(openai_embedding_ndjson, f)
    
!advtool embeddings import cea4f70f-1d4a-4315-a4f7-0f1d4a131563 openai_embedding.ndjson
</code></pre><h3 id="4-explore-your-results">4) Explore your results</h3><p>By leveraging foundation models, I am been able to take an initial video dataset and find a lot of potential interesting use cases, with a few examples included below for inspiration.</p><p><strong>Finding cooking videos using metadata and embeddings</strong></p><p>I can discover cooking videos from this dataset quickly by searching for <code>cooking</code> or <code>recipe</code> in the transcript summary. I found better results than just searching in the raw transcript text. The reason for this is because the raw transcript is summarized using GPT 3.5 model which is then subsequently used in its summary.</p><figure class="kg-card kg-embed-card"><iframe src="https://fast.wistia.net/embed/iframe/l2e64x3dac" title="cooking_query_optimized Video" allow="autoplay; fullscreen" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" msallowfullscreen="" width="960" height="540"></iframe>
<script src="../../../fast.wistia.net/assets/external/E-v1.js" async=""></script></figure><p><strong>Fine tuning search query to generate better results</strong></p><figure class="kg-card kg-embed-card"><iframe src="https://fast.wistia.net/embed/iframe/0llbj2c970" title="progressive_optimized Video" allow="autoplay; fullscreen" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" msallowfullscreen="" width="960" height="540"></iframe>
<script src="../../../fast.wistia.net/assets/external/E-v1.js" async=""></script></figure><p><strong>Finding videos about people associated with playing, games sports</strong></p><figure class="kg-card kg-embed-card"><iframe src="https://fast.wistia.net/embed/iframe/otlv9pzec4" title="kids_playing_optimized Video" allow="autoplay; fullscreen" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" msallowfullscreen="" width="960" height="540"></iframe>
<script src="../../../fast.wistia.net/assets/external/E-v1.js" async=""></script></figure><p><strong>Let’s try similarity search by embedding Timesformer outputs with all-mpnet-base-v2 transformer model. </strong></p><p>Next, I'll demonstrate searching for text metadata generated by the Timesformer model.</p><p>Text search has one major limitation. The user has to imagine things to look for in words. It is surprisingly hard to come up with a comprehensive list of words that may describe a thing. Semantic search powered by embeddings provide an elegant solution. Here’s a demonstration of just this. Use text search to find positive examples and then use similarity search to find more stuff like the anchors.</p><figure class="kg-card kg-embed-card"><iframe src="https://fast.wistia.net/embed/iframe/ce27aic70f" title="timesformer_optimized Video" allow="autoplay; fullscreen" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" msallowfullscreen="" width="960" height="540"></iframe>
<script src="../../../fast.wistia.net/assets/external/E-v1.js" async=""></script></figure><p><br><strong>Zero shot classification in Catalog</strong></p><p>Zero-shot classification is a type of machine learning approach that allows a model to recognize and classify objects or concepts that it has never seen before. Quantumworks Lab provides an intuitive and effective way to perform zero shot classification with an integrated human in the loop workflow to ensure high data quality.</p><p>In the video below, I'll demonstrate classifying the results of a complex query (metadata + cosine distance based similarity) manually. This can be automated programmatically using slices.</p><figure class="kg-card kg-embed-card"><iframe src="https://fast.wistia.net/embed/iframe/2fpj2nvjva" title="cooking_classification_optimized Video" allow="autoplay; fullscreen" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" msallowfullscreen="" width="960" height="540"></iframe>
<script src="../../../fast.wistia.net/assets/external/E-v1.js" async=""></script></figure><h3 id="try-it-out">Try it out </h3><p>I hope this walkthrough was informative and inspired a few potential video use cases. By utilizing these techniques, you'll be able to accelerate downstream workflows such as classifying high-volumes of videos for further labeling, or enriching your videos with additional metadata to derive faster insights. As a next step, you can check out this dataset and give it a try for yourself in <a href="../../datasets/queryd-a-video-dataset-with-textual-and-audio-narrations/indexc625.html?ref=labelbox.ghost.io">Quantumworks Lab Catalog</a>.</p></div></main></div></div></div><div class="mt-5 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="my-20 w-full h-[1px] bg-neutral-200"></div><div class="grid grid-cols-12 gap-2"><div class="col-span-12"><h2 class="mb-12 text-center text-3xl md:text-4xl font-medium">Continue reading</h2></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../bringing-ai-to-the-browser-sam2-for-interactive-image-segmentation/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index447c.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Stanislav Issayenko<span class="mx-2">•</span>December 19, 2024</p></div><a href="../bringing-ai-to-the-browser-sam2-for-interactive-image-segmentation/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Bringing AI to the browser: SAM2 for interactive image segmentation</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage the power of the SAM2 in a web browser, enabling interactive image segmentation without the need for powerful servers or specialized hardware.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../enhanced-labelbox-video-editors-adds-deeply-nested-classifications/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fhuman-in-the-loop-6.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fhuman-in-the-loop-6.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fhuman-in-the-loop-6.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fhuman-in-the-loop-6.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fhuman-in-the-loop-6.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fhuman-in-the-loop-6.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fhuman-in-the-loop-6.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fhuman-in-the-loop-6.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/indexca1d.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fhuman-in-the-loop-6.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Quantumworks Lab<span class="mx-2">•</span>November 14, 2024</p></div><a href="../enhanced-labelbox-video-editors-adds-deeply-nested-classifications/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Enhanced video editor adds  deeply nested classifications to improve visibility and efficiency</p><p class="text-base max-w-2xl undefined line-clamp-3">Streamline video data labeling with new features for deeply nested classifications and overall improved video editor functionality.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../bring-your-own-models-to-labelbox-with-new-custom-model-integration/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fllm-fine-tuning-1.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fllm-fine-tuning-1.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fllm-fine-tuning-1.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fllm-fine-tuning-1.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fllm-fine-tuning-1.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fllm-fine-tuning-1.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fllm-fine-tuning-1.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fllm-fine-tuning-1.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/indexa6a1.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F11%2Fllm-fine-tuning-1.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Quantumworks Lab<span class="mx-2">•</span>October 8, 2024</p></div><a href="../bring-your-own-models-to-labelbox-with-new-custom-model-integration/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Bring your own models to Quantumworks Lab with new custom model integration</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn about our latest product enhancement that lets you easily integrate custom models and candidates into Quantumworks Lab with just a few clicks.</p></a></div></div></div></div></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"make-videos-queryable-using-foundation-models","id":"6419179a59bbe9003d69c1cb","uuid":"d5e08c0d-199e-445e-a0d7-391214a4bc56","title":"Make your videos queryable using foundation models","html":"\u003cp\u003eIn this tutorial, I'll demonstrate how easy it is to enrich all of your video content with foundation models to perform tasks such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSearching a wide range video content for specific types of videos\u003c/li\u003e\u003cli\u003eUnderstand the nature of video content within your data lake\u003c/li\u003e\u003cli\u003eExplore videos that contain specific keywords, or talk about a topic, or appear similar to some other videos\u003c/li\u003e\u003cli\u003eSelect data that will improve your ML models, because it is rare or shown as an  edge-case scenario\u003c/li\u003e\u003cli\u003eApply zero/few shot learning and weak supervision to label your data\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor this post, I utilized the most recent foundation models provided by OpenAI, Meta and Hugging Face to analyze and enrich a series of videos. \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox.ghost.io\"\u003eLabelbox Catalog\u003c/a\u003e is used as the data platform to store all the metadata, search across metadata and embedding space, as well as perform zero shot classification.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/l2e64x3dac\" title=\"cooking_query_optimized Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEasily search videos by combining foundation models with Quantumworks Lab Catalog\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eVideos are complex data structures that consist of a sequence of frames that are composed of many pixels. This complexity makes it challenging to develop efficient search algorithms that can quickly and accurately locate specific content within a video.\u003c/p\u003e\u003cp\u003eAdditionally, due to variability in content, videos can vary significantly in terms of their content, making it challenging to develop search algorithms that work across different types of videos. For example, searching for a specific object or event within a sports highlight video may require a different approach than searching for the same object or event within a security camera footage. For object or scene detection, one obvious approach is to chunk the video into smaller pieces, downsample the video, generate and index inferences at frame level. However, various use cases just require global video level understanding (e.g. Instagram or Youtube shorts).\u003c/p\u003e\u003cp\u003eLet’s get started and take a look at how it works on a few real-world examples. For starters, I'll walk through how you can find cooking videos quickly by searching for text related to \u003ccode\u003ecooking\u003c/code\u003e or \u003ccode\u003erecipe\u003c/code\u003e in the transcript summary and combining this with foundation models. \u003c/p\u003e\u003ch3 id=\"1-prepare-and-understand-data\"\u003e1) Prepare and understand data\u003c/h3\u003e\u003cp\u003eI found the \u003cem\u003eQUERYD\u003c/em\u003e dataset that contains a curated list of youtube videos. To understand what kind of videos they were, I downloaded the these videos and imported them into Quantumworks Lab Catalog.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/03/image-20230319-205645.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1111\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2023/03/image-20230319-205645.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2023/03/image-20230319-205645.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2023/03/image-20230319-205645.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2023/03/image-20230319-205645.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eA few initial observations from my dataset:\u003c/p\u003e\u003cul\u003e\u003cli\u003eMost videos are less than 900 seconds in duration.\u003c/li\u003e\u003cli\u003eThe video content has a high degree of variation from podcasts, sports to cooking. Although these are common topics, widely available over internet.\u003c/li\u003e\u003cli\u003eMost videos have both audio and voice.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"2-select-ai-models-to-generate-insights\"\u003e2) Select AI models to generate insights\u003c/h3\u003e\u003cp\u003eBased on the preliminary understanding of the data, I chose the following foundation models to generate metadata in order to make the videos queryable.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI Whisper for video transcription\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data collected from the web. It enables transcription in multiple languages, as well as translation from those languages into English\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI GPT 3.5 for summarizing transcription\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOpenAI's GPT-3.5 model is a language model that can perform a wide range of natural language processing tasks including summarization, language generation, translation, question-answering, text completion and more.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://openai.com/blog/new-and-improved-embedding-model?ref=labelbox.ghost.io\"\u003e\u003cstrong\u003eOpenAI Generation 2 embeddings\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e for similarity search\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eReleased in December 2022, the latest generation of embedding model by OpenAI, \u003ccode\u003etext-embedding-ada-002\u003c/code\u003e, replaces five separate models for text search, text similarity, and code search, and outperforms the previous most capable model, Davinci, at most tasks, while being priced 99.8% lower.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTimeSformer by Facebook for zero shot video classification\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTimeSformer is a video processing model developed by Facebook AI Research (FAIR) and Meta (formerly Facebook Reality Labs) that can analyze and understand video data. TimeSformer model pre-trained on \u003ca href=\"https://www.deepmind.com/open-source/kinetics?ref=labelbox.ghost.io\"\u003e\u003cu\u003eKinetics-400\u003c/u\u003e\u003c/a\u003e. It was introduced in the paper \u003ca href=\"https://arxiv.org/abs/2102.05095?ref=labelbox.ghost.io\"\u003e\u003cu\u003eTimeSformer: Is Space-Time Attention All You Need for Video Understanding?\u003c/u\u003e\u003c/a\u003e by Tong et al. and first released in \u003ca href=\"https://github.com/facebookresearch/TimeSformer?ref=labelbox.ghost.io\"\u003e\u003cu\u003ethis repository\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://huggingface.co/sentence-transformers/all-mpnet-base-v2?ref=labelbox.ghost.io\"\u003e\u003cstrong\u003eSentence-transformers/All-mpnet-base-v2\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e for text similarity search\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis is a \u003ca href=\"https://www.sbert.net/?ref=labelbox.ghost.io\"\u003e\u003cu\u003esentence-transformers\u003c/u\u003e\u003c/a\u003e model: It maps sentences \u0026amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\u003c/p\u003e\u003ch3 id=\"3-generate-metadata-and-embeddings\"\u003e3) Generate metadata and embeddings\u003c/h3\u003e\u003cp\u003eBelow are the python functions to call OpenAI APIs to transcribe audio, summarize text and generate embeddings.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e@retry(wait=wait_random(min=1, max=5), stop=stop_after_attempt(3))\ndef ask_gpt(prompt):\n    try:\n        response = openai.Completion.create(\n          model=\"text-davinci-003\",\n          prompt=\"Summarize this in a short paragraph:\\n {}\".format(prompt),\n          temperature=0.7,\n          max_tokens=64,\n          top_p=1.0,\n          frequency_penalty=0.0,\n          presence_penalty=0.0\n        )\n        transcript_summary = response.choices[0].text\n        return transcript_summary\n    except Exception as e:\n        print(e)\n        raise(e)\n\n@retry(wait=wait_random(min=1, max=5), stop=stop_after_attempt(5))\ndef ask_whisper(mp3_file):\n    try:\n        audio_file = open(mp3_file, \"rb\")\n        transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n        transcript_text = transcript.text\n        return transcript_text\n    except Exception as e:\n        print(e)\n        raise(e)\n\n@retry(wait=wait_random(min=1, max=5), stop=stop_after_attempt(5))\ndef embed_text(text):\n    try:\n        e = openai.Embedding.create(\n            model = \"text-embedding-ada-002\",\n            input = text\n        )\n        embeddings = []\n        for item in e.data:\n            embeddings.append(item.embedding)\n        return embeddings\n    except Exception as e:\n        print(e)\n        raise(e)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eHere’s a code snippet to generate inferences using Timesformer model on a local machine with a GPU using HuggingFace.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef predict_video_action(video_file):\n    try:\n        container = av.open(video_file, \"rb\")\n        indices = sample_frame_indices(clip_len=60, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n        video = read_video_pyav(container, indices)\n        inputs = video_processor(list(video), return_tensors=\"pt\").to(\"cuda:0\")\n\n        with torch.no_grad():\n            outputs = video_model(**inputs)\n            logits = outputs.logits\n\n        predicted_class_idx = logits.argmax(-1).item()\n        video_classification = video_model.config.id2label[predicted_class_idx]\n    except Exception as e:\n        print(e)\n        \ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nvideo_processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\nvideo_model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\nvideo_model = video_model.to(device)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow we process all of the videos and call the functions above to generate metadata.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efor datarow in dataset:\n    try:\n        ### Transcribe the audio\n        transcript_text = ask_whisper(mp3_file)\n\n        ### Ask GPT to summarize the transcription\n        transcript_summary = ask_gpt(transcript_text)\n\n        ### Generate embedding on raw transcription\n        embedding = embed_text(transcript_text)\n\n        ### Timesformer video action recognition\n        video_action = predict_video_action(video_file)\n        \n        data_rows.append({\n            \"row_data\": video_url,\n            \"global_key\": global_key,\n            \"metadata_fields\": [\n                {\"name\": transcript_schema.name, \"value\": transcript_text},\n                {\"name\": transcript_summary_schema.name, \"value\": transcript_summary},\n                {\"name\": video_classification_schema.name, \"value\": video_action}\n                ],\n            \"attachments\": [{\n              \"type\": \"TEXT\",\n              \"value\": transcript_text\n              }]\n        })\n        \n        text_embeddings.append({\"id\": global_key, \"vector\": embedding})\n    \n    except Exception as e:\n        print(e)\n        return None\n        \u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow we create a new dataset and import metadata generated above.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003edataset = client.create_dataset(name = \"QUERYD dataset\")\ntask = dataset.create_data_rows(data_rows)\ntask.wait_till_done()\nprint(task.errors)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow we import \u003ca href=\"https://github.com/Quantumworks Lab/advlib?ref=labelbox.ghost.io\"\u003eembeddings\u003c/a\u003e using the code snippet below. (P.S. Python SDK support will be released soon)\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eopenai_embedding_ndjson = list()\nfor i in tqdm(text_embeddings):\n  global_key = i[\"id\"]\n  vector = i[\"vector\"]\n  datarow_id = client.get_data_row_ids_for_global_keys(global_key)[\"results\"][0]\n  openai_embedding_ndjson.append({\"id\": datarow_id, \"vector\": vector})\n\nwith open('openai_embedding.ndjson', 'w') as f:\n    ndjson.dump(openai_embedding_ndjson, f)\n    \n!advtool embeddings import cea4f70f-1d4a-4315-a4f7-0f1d4a131563 openai_embedding.ndjson\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"4-explore-your-results\"\u003e4) Explore your results\u003c/h3\u003e\u003cp\u003eBy leveraging foundation models, I am been able to take an initial video dataset and find a lot of potential interesting use cases, with a few examples included below for inspiration.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFinding cooking videos using metadata and embeddings\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eI can discover cooking videos from this dataset quickly by searching for \u003ccode\u003ecooking\u003c/code\u003e or \u003ccode\u003erecipe\u003c/code\u003e in the transcript summary. I found better results than just searching in the raw transcript text. The reason for this is because the raw transcript is summarized using GPT 3.5 model which is then subsequently used in its summary.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/l2e64x3dac\" title=\"cooking_query_optimized Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFine tuning search query to generate better results\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/0llbj2c970\" title=\"progressive_optimized Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFinding videos about people associated with playing, games sports\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/otlv9pzec4\" title=\"kids_playing_optimized Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eLet’s try similarity search by embedding Timesformer outputs with all-mpnet-base-v2 transformer model. \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eNext, I'll demonstrate searching for text metadata generated by the Timesformer model.\u003c/p\u003e\u003cp\u003eText search has one major limitation. The user has to imagine things to look for in words. It is surprisingly hard to come up with a comprehensive list of words that may describe a thing. Semantic search powered by embeddings provide an elegant solution. Here’s a demonstration of just this. Use text search to find positive examples and then use similarity search to find more stuff like the anchors.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ce27aic70f\" title=\"timesformer_optimized Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003e\u003cstrong\u003eZero shot classification in Catalog\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eZero-shot classification is a type of machine learning approach that allows a model to recognize and classify objects or concepts that it has never seen before. Quantumworks Lab provides an intuitive and effective way to perform zero shot classification with an integrated human in the loop workflow to ensure high data quality.\u003c/p\u003e\u003cp\u003eIn the video below, I'll demonstrate classifying the results of a complex query (metadata + cosine distance based similarity) manually. This can be automated programmatically using slices.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/2fpj2nvjva\" title=\"cooking_classification_optimized Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"try-it-out\"\u003eTry it out \u003c/h3\u003e\u003cp\u003eI hope this walkthrough was informative and inspired a few potential video use cases. By utilizing these techniques, you'll be able to accelerate downstream workflows such as classifying high-volumes of videos for further labeling, or enriching your videos with additional metadata to derive faster insights. As a next step, you can check out this dataset and give it a try for yourself in \u003ca href=\"https://labelbox.com/datasets/queryd-a-video-dataset-with-textual-and-audio-narrations/?ref=labelbox.ghost.io\"\u003eLabelbox Catalog\u003c/a\u003e.\u003c/p\u003e","comment_id":"6419179a59bbe9003d69c1cb","feature_image":"https://labelbox.ghost.io/blog/content/images/2023/03/queryable.png","featured":false,"visibility":"public","created_at":"2023-03-20T19:34:02.000-07:00","updated_at":"2024-10-04T09:46:17.000-07:00","published_at":"2023-03-20T21:00:11.000-07:00","custom_excerpt":"Learn how to quickly make your videos queryable by using foundation models (such as OpenAI, Meta and Hugging Face) alongside Quantumworks Lab Catalog. Explore all your videos that contain specific keywords or audio and derive faster business insights.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"653030aa4e99900001fc052d","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/use-ai/"},{"id":"65302ef44e99900001fc0519","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox.ghost.io/blog/tag/industry-any/"},{"id":"653030c34e99900001fc0531","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/using-computer-vision/"},{"id":"653030324e99900001fc052b","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox.ghost.io/blog/tag/explore-manage-data/"},{"id":"6700193d863cb90001f263ed","name":"Computer vision","slug":"computer-vision","description":"CV related features, best practices, and information","feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/computer-vision/"}],"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":"https://labelbox.ghost.io/blog/content/images/2019/07/ManuSharma-20180730-square.jpg","cover_image":null,"bio":"Founder \u0026 CEO","website":null,"location":"San Francisco","facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/manu/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":"https://labelbox.ghost.io/blog/content/images/2019/07/ManuSharma-20180730-square.jpg","cover_image":null,"bio":"Founder \u0026 CEO","website":null,"location":"San Francisco","facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/manu/"},"primary_tag":{"id":"653030aa4e99900001fc052d","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/use-ai/"},"url":"https://labelbox.ghost.io/blog/make-videos-queryable-using-foundation-models/","excerpt":"Learn how to quickly make your videos queryable by using foundation models (such as OpenAI, Meta and Hugging Face) alongside Quantumworks Lab Catalog. Explore all your videos that contain specific keywords or audio and derive faster business insights.","reading_time":6,"access":true,"comments":false,"og_image":"https://labelbox.ghost.io/blog/content/images/2023/03/queryable-2.png","og_title":null,"og_description":null,"twitter_image":"https://labelbox.ghost.io/blog/content/images/2023/03/queryable-1.png","twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},"recommended":[{"id":"6759fc88a9b5bd0001989d30","uuid":"58632854-1031-4ac5-8fe3-87324337cbe7","title":"Bringing AI to the browser: SAM2 for interactive image segmentation","slug":"bringing-ai-to-the-browser-sam2-for-interactive-image-segmentation","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eIn computer vision, image segmentation is crucial for object recognition, image editing, autonomous driving, and other common applications. The Segment Anything Model 2 (SAM2) pushes the boundaries of interactive image segmentation by allowing users to segment objects in images with minimal input.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTraditionally, running such models required powerful servers or specialized hardware. However, with advancements in web technologies and machine learning libraries, it's now possible to run complex models like SAM2 directly in a browser.\u003c/p\u003e\u003cp\u003eIn this article, we explore how to implement the SAM2 model in a web browser using ONNX Runtime Web (ort). We delve into the architecture of SAM2, how to load and run the model in a browser, and how to create an interactive user interface for real-time image segmentation.\u003c/p\u003e\u003ch2 id=\"understanding-the-sam2-architecture\"\u003eUnderstanding the SAM2 architecture\u003c/h2\u003e\u003ch3 id=\"encoder-decoder-framework\"\u003eEncoder-decoder framework\u003c/h3\u003e\u003cp\u003eSAM2 uses an encoder-decoder architecture:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEncoder:\u003c/strong\u003e Processes the input image to generate a high-dimensional embedding that captures essential features.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDecoder:\u003c/strong\u003e Takes the embedding and user-provided points (positive and negative) to generate segmentation masks.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis architecture allows for interactive and highly accurate segmentation, while also allowing users to\u0026nbsp; iteratively refine the segmentation by adding more points.\u003c/p\u003e\u003ch3 id=\"why-run-sam2-in-the-browser\"\u003eWhy Run SAM2 in the Browser?\u003c/h3\u003e\u003cp\u003eRunning SAM2 in the browser offers several key benefits:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrivacy:\u003c/strong\u003e Images are processed locally, ensuring user data isn't sent to external servers.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAccessibility:\u003c/strong\u003e Users can access the segmentation tool without installing specialized software.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInteractivity:\u003c/strong\u003e Real-time feedback enhances the user experience, allowing for quick iterations.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"implementing-the-encoder-encoderjs\"\u003eImplementing the encoder (encoder.js)\u003c/h2\u003e\u003ch3 id=\"initialization\"\u003eInitialization\u003c/h3\u003e\u003cp\u003eThe encoder loads the ONNX model and prepares it for inference:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003econst ENCODER_MODEL_URL = 'https://storage.googleapis.com/lb-artifacts-testing-public/sam2/sam2_hiera_tiny.encoder.ort';\n\nclass SAM2Encoder {\n  constructor() {\n    this.session = null;\n  }\n\n  async initialize() {\n    this.session = await ort.InferenceSession.create(ENCODER_MODEL_URL);\n    console.log('Encoder model loaded successfully');\n  }\n}\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"image-preprocessing\"\u003eImage preprocessing\u003c/h3\u003e\u003cp\u003eBefore passing the image to the encoder, it must be resized and normalized:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003e Resize:\u003c/strong\u003e Adjust the image to 1024x1024 pixels.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNormalize:\u003c/strong\u003e Scale pixel values to the [-1, 1] range.\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003eimageDataToTensor(image) {\n  const canvas = document.createElement('canvas');\n  const ctx = canvas.getContext('2d');\n  canvas.width = canvas.height = 1024;\n\n  ctx.drawImage(image, 0, 0, 1024, 1024);\n  const imageData = ctx.getImageData(0, 0, 1024, 1024).data;\n  const inputArray = new Float32Array(3 * 1024 * 1024);\n\n  for (let i = 0; i \u0026lt; 1024 * 1024; i++) {\n    inputArray[i] = (imageData[i * 4] / 255.0) * 2 - 1; // R channel\n    inputArray[i + 1024 * 1024] = (imageData[i * 4 + 1] / 255.0) * 2 - 1; // G channel\n    inputArray[i + 2 * 1024 * 1024] = (imageData[i * 4 + 2] / 255.0) * 2 - 1; // B channel\n  }\n\n  return new ort.Tensor('float32', inputArray, [1, 3, 1024, 1024]);\n}\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"encoding-process\"\u003eEncoding process\u003c/h3\u003e\u003cp\u003eThe encode method runs the model and generates the image embedding:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003easync encode(image) {\n  const tensor = this.imageDataToTensor(image);\n  const feeds = { image: tensor };\n  const results = await this.session.run(feeds);\n  this.lastEmbeddings = results.image_embed;\n  return this.lastEmbeddings;\n}\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"implementing-the-decoder-decoderjs\"\u003eImplementing the decoder (decoder.js)\u003c/h2\u003e\u003ch3 id=\"initialization-1\"\u003eInitialization\u003c/h3\u003e\u003cp\u003eSimilar to the encoder, the decoder loads its ONNX model:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003econst DECODER_MODEL_URL = 'https://storage.googleapis.com/lb-artifacts-testing-public/sam2/sam2_hiera_tiny.decoder.onnx';\n\nclass SAM2Predictor {\n  constructor() {\n    this.session = null;\n  }\n\n  async initialize() {\n    this.session = await ort.InferenceSession.create(DECODER_MODEL_URL);\n    console.log('Decoder model loaded successfully');\n  }\n}\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"preparing-inputs\"\u003ePreparing inputs\u003c/h3\u003e\u003cp\u003eThe decoder requires several inputs, including the image embedding and user interaction points:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePoint coordinates (point_coords):\u003c/strong\u003e The (x, y) positions of user clicks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePoint labels (point_labels):\u003c/strong\u003e Indicates positive (foreground) or negative (background) points.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMask input (mask_input):\u003c/strong\u003e An initial mask, set to zeros if not using a previous mask.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOther tensors:\u003c/strong\u003e Additional required inputs like has_mask_input, high_res_feats_0, and high_res_feats_1.\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003eprepareInputs(embedding, points) {\n  const numLabels = 1;\n  const numPoints = points.length;\n  const pointCoordsData = [];\n  const pointLabelsData = [];\n\n  for (let point of points) {\n    pointCoordsData.push([point.x, point.y]);\n    pointLabelsData.push(point.type);\n  }\n\n  return {\n    image_embed: embedding,\n    point_coords: new ort.Tensor('float32', Float32Array.from(pointCoordsData.flat()), [numLabels, numPoints, 2]),\n    point_labels: new ort.Tensor('float32', Float32Array.from(pointLabelsData), [numLabels, numPoints]),\n    mask_input: new ort.Tensor('float32', new Float32Array(numLabels * 1 * 256 * 256), [numLabels, 1, 256, 256]),\n    has_mask_input: new ort.Tensor('float32', new Float32Array([0.0]), [numLabels]),\n    high_res_feats_0: new ort.Tensor('float32', new Float32Array(1 * 32 * 256 * 256), [1, 32, 256, 256]),\n    high_res_feats_1: new ort.Tensor('float32', new Float32Array(1 * 64 * 128 * 128), [1, 64, 128, 128]),\n  };\n}\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"prediction-process\"\u003ePrediction Process\u003c/h3\u003e\u003cp\u003eThe predict method runs the decoder model to generate the segmentation mask:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003easync predict(embedding, inputPoints) {\n  const inputs = this.prepareInputs(embedding, inputPoints);\n  const results = await this.session.run(inputs);\n  return results;\n}\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"building-the-user-interface-appjs\"\u003eBuilding the user interface (app.js)\u003c/h2\u003e\u003ch3 id=\"setting-up-the-canvas\"\u003eSetting up the canvas\u003c/h3\u003e\u003cp\u003eWe use two HTML canvas elements:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSource canvas (sourceCanvas):\u003c/strong\u003e Displays the uploaded image and interaction points.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMask canvas (maskCanvas):\u003c/strong\u003e Overlays the segmentation mask.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"handling-user-interactions\"\u003eHandling user interactions\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eImage upload\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhen a user uploads an image two things happen:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe image is drawn on the sourceCanvas.\u003c/li\u003e\u003cli\u003eThe encoder generates embeddings from the image.\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003eimageInput.addEventListener('change', async (e) =\u0026gt; {\n  // Load image and draw on canvas\n  // Encode the image\n  embedding = await encoder.encode(img);\n});\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eAdding interaction points\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eUsers can click on the image to add positive or negative points:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePositive points:\u003c/strong\u003e Indicate areas to include in the segmentation.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNegative points:\u003c/strong\u003e Indicate areas to exclude.\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003esourceCanvas.addEventListener('click', async (e) =\u0026gt; {\n  const x = e.clientX - rect.left;\n  const y = e.clientY - rect.top;\n  const point = { x: x, y: y, type: isNegative ? 0 : 1 };\n  points.push(point);\n\n  // Draw point on canvas\n  drawPoint(sourceCtx, point);\n\n  // Run prediction\n  const results = await predictor.predict(embedding, points);\n\n  // Draw mask\n  drawMaskOnCanvas(maskCanvas, results['masks'], imageWidth, imageHeight);\n});\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eToggling point types\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eA button allows users to switch between positive and negative points:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003enegativeBtn.addEventListener('click', () =\u0026gt; {\n  isNegative = !isNegative;\n  negativeBtn.textContent = `Negative Points: ${isNegative ? 'ON' : 'OFF'}`;\n});\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"drawing-points-and-masks\"\u003eDrawing points and masks\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eDrawing points\u003c/strong\u003e\u003c/p\u003e\u003cp\u003ePoints are drawn on the sourceCanvas to provide visual feedback:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003efunction drawPoint(ctx, point) {\n  ctx.fillStyle = point.type === 1 ? 'green' : 'red';\n  ctx.beginPath();\n  ctx.arc(point.x, point.y, 5, 0, 2 * Math.PI);\n  ctx.fill();\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eDrawing masks\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eMasks are drawn on the maskCanvas to display the segmentation result:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003efunction drawMaskOnCanvas(maskCanvas, maskData, imageWidth, imageHeight) {\n  // Process the mask tensor and draw it over the image\n}\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"running-the-application\"\u003eRunning the application\u003c/h2\u003e\u003ch3 id=\"step-by-step-guide\"\u003eStep-by-step guide\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eSet up the server:\u003c/strong\u003e Start your local server to host the model files.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInclude scripts:\u003c/strong\u003e In your HTML file, include ort and your JavaScript modules.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOpen the application:\u003c/strong\u003e Access the HTML file through your browser.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eUpload an image:\u003c/strong\u003e Use the file input to select an image.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInteract with the image:\u003c/strong\u003e Click on the image to add points and see the segmentation mask update in real-time.\u003c/li\u003e\u003c/ol\u003e\u003ch3 id=\"demonstration\"\u003eDemonstration\u003c/h3\u003e\u003cp\u003eCheck out and play around with an interactive demo \u003ca href=\"https://storage.googleapis.com/lb-artifacts-testing-public/sam2/index.html?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eIn this article, we've demonstrated how to run the Segment Anything Model 2 (SAM2) directly in the web browser. By leveraging ONNX Runtime Web and thoughtful implementation of the encoder and decoder, we've created an interactive image segmentation tool that runs entirely on the user-side.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis approach opens doors to privacy-preserving applications and makes advanced machine learning models more accessible. As web technologies continue to evolve, we can expect even more sophisticated models to run efficiently in the browser.\u003c/p\u003e\u003cp\u003eAt Quantumworks Lab, we’ve adopted a hybrid strategy, running SAM’s encoder on the server while executing its decoder right in the browser. This approach ensures real-time image segmentation, preserves user privacy, and expands access to cutting-edge machine learning. As web technologies advance, we look forward to delivering even more powerful models efficiently and securely in the browser.\u003c/p\u003e\u003ch2 id=\"check-out-these-additional-resources\"\u003eCheck out these additional resources:\u0026nbsp;\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eONNX Runtime Web Documentation:\u003c/strong\u003e \u003ca href=\"https://onnxruntime.ai/docs/api/javascript/index.html?ref=labelbox.ghost.io\"\u003e\u003cu\u003eONNX Runtime Web\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSAM2 Model Details:\u003c/strong\u003e \u003ca href=\"https://segment-anything.com/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eSegment Anything Model (SAM)\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/Quantumworks Lab/sam2-web?ref=labelbox.ghost.io\"\u003e\u003cu\u003elink\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDemo:\u003c/strong\u003e \u003ca href=\"https://storage.googleapis.com/lb-artifacts-testing-public/sam2/index.html?ref=labelbox.ghost.io\"\u003e\u003cu\u003elink\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"6759fc88a9b5bd0001989d30","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/12/Interactive-Image-Segmentation-in-the-Browser-with-SAM2.png","featured":false,"visibility":"public","created_at":"2024-12-11T12:56:40.000-08:00","updated_at":"2024-12-20T08:52:19.000-08:00","published_at":"2024-12-19T12:18:40.000-08:00","custom_excerpt":"Learn how to leverage the power of the SAM2 in a web browser, enabling interactive image segmentation without the need for powerful servers or specialized hardware.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"5f9af89dd8118b0039136165","name":"Engineering","slug":"engineering","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/engineering/"},{"id":"653030c34e99900001fc0531","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/using-computer-vision/"},{"id":"6700193d863cb90001f263ed","name":"Computer vision","slug":"computer-vision","description":"CV related features, best practices, and information","feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/computer-vision/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"},{"id":"653030aa4e99900001fc052d","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/use-ai/"}],"authors":[{"id":"6764713c6f63bf0001f1ca64","name":"Stanislav Issayenko","slug":"stanislav","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/stanislav/"}],"primary_author":{"id":"6764713c6f63bf0001f1ca64","name":"Stanislav Issayenko","slug":"stanislav","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/stanislav/"},"primary_tag":{"id":"5f9af89dd8118b0039136165","name":"Engineering","slug":"engineering","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/engineering/"},"url":"https://labelbox.ghost.io/blog/bringing-ai-to-the-browser-sam2-for-interactive-image-segmentation/","excerpt":"Learn how to leverage the power of the SAM2 in a web browser, enabling interactive image segmentation without the need for powerful servers or specialized hardware.","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6732aad9d02a4800018d3449","uuid":"472cfb24-5495-47f5-8d72-91aa098617b9","title":"Enhanced video editor adds  deeply nested classifications to improve visibility and efficiency","slug":"enhanced-labelbox-video-editors-adds-deeply-nested-classifications","html":"\u003cp\u003eIn response to customer feedback, we’ve added exciting new features to our video editor to improve data labeling visibility and efficiency. Key updates allow you to  create deeply nested classifications, to visualize classifications directly on the editor timeline, and to easily skip to specific frames. In addition, we've introduced a handful of essential fixes and enhancements to existing functionality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eRead on for details of how these updates are helping AI model builders accelerate and improve generative AI video tasks like text-to-video, video captioning, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCreate and visualize deeply nested classifications\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eClassifications have been a core feature in the Quantumworks Lab video editor and ontology tools for years; however, they could not be arbitrarily nested. The lack of nested classifications made it difficult for you to create classification hierarchies that best reflected the real-world issues you want to model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, imagine you are tasked with analyzing a large set of medical videos to find a variety of potential issues. You need to categorize the videos based on different medical conditions and then have sub-classifications under each to provide a more precise and granular analysis of the videos. Without this hierarchical formation, you would be limited to a flat structure, making it difficult to organize and search for specific segments within videos.\u003c/p\u003e\u003cp\u003eWith this update, you can now create complex classification hierarchies for any ontology!\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd_tI6qiXSky21dgMoZ4t9YuNnaim83j30bbC-Ga6WPrFALErmHByJwld_D32_RWtS_o_nwskImazNMOCXiCVOfq2wxO0xNnw6doFSD2ShBebAE_SuoWZE7jGos-vl8VREh0Odv?key=suV4c3bwYAUNwogpcRnWYF0d\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"589\" height=\"350\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eCreate as many sub-classifications as you need within a hierarchical format to label easier and more precisely.\u0026nbsp;\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn addition to the nested classifications, updates to the video editor make it easy to delete, edit, rearrange, and combine sub-classifications by visualizing per-frame classifications on the video editor’s timeline. You can view classes at a simple glance, helping streamline the annotation and review process. This is especially helpful for GenAI use cases like text-to-video and video-to-text.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdJAU5P6v6iM8xUVMl80v8IG8IDWK_2pWMwF5KdOlJIzKzuCchKREaht72GVIhdKoa9i7bWCGq1Yp2PJjZWA_nxohYTwU4MxbjwWxroHeCElJ6uj8-_JPoar0YZTySXAaoUmPWFHQ?key=suV4c3bwYAUNwogpcRnWYF0d\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"681\" height=\"73\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eVisualize classifications at a glance directly on the video editor timeline.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eAdditional updates enhance the video labeling experience\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOther significant features to enhance your video labeling experiences include:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSkip N frames:\u003c/strong\u003e For longer videos with specific labeling needs (ex: labeling every 5th frame), users can select how many frames they want to skip.\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eBug around toggle on/off feature addressed: \u003c/strong\u003eUsers can now accurately toggle on or off specific classifications in a given video.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDesigned to precisely annotate keyframes, these features provide the utmost flexibility, accuracy, and efficiency when labeling complex video data.\u003cem\u003e\u0026nbsp;\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eGet started today with video labeling\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLabelbox is dedicated to continuous improvement based on customer feedback to make our tools the best it can be for your team. Ready to test out how to label your video data with ease? Simply \u003ca href=\"https://app.labelbox.com/home?_r=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026gclid=Cj0KCQiA57G5BhDUARIsACgCYnwYIWcntlazP2RZRqnKXOghCTuLTxFKOFiM7p_W8PWzao9Z6ACXQOkaAhPtEALw_wcB\u0026landingPageAnonymousId=%22ce322bab-56ea-43b8-a113-000851a3ebaf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003elogin to Quantumworks Lab\u003c/u\u003e\u003c/a\u003e, navigate to the Annotate tab, and select video as the data modality in your Quantumworks Lab platform homepage to get started. \u003c/p\u003e\u003cp\u003eIf you only have a few minutes, learn more about the power of deeply nested classifications in our video editor with this \u003ca href=\"https://app.arcade.software/flows/Du1WfyRQ8tllcLE8UBQK/view?ref=labelbox.ghost.io\"\u003e\u003cu\u003equick, click-through demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eContact our team anytime with questions or if you are ready to discuss your AI video labeling needs and discover how Quantumworks Lab might be able to help.\u003c/p\u003e","comment_id":"6732aad9d02a4800018d3449","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/11/human-in-the-loop-6.png","featured":false,"visibility":"public","created_at":"2024-11-11T17:09:45.000-08:00","updated_at":"2024-11-19T10:57:16.000-08:00","published_at":"2024-11-14T08:42:11.000-08:00","custom_excerpt":"Streamline video data labeling with new features for deeply nested classifications and overall improved video editor functionality.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"5e1517c870aad900380bc812","name":"Release Notes","slug":"release-notes","description":"IMPORTANT: Don't change the name of this Tag, it is referenced inside the theme's code in a way that filters out all of the posts that contain it.\nIf you want to change this name, you should also change it in the loop_index.hbs file in the theme's repository.","feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/release-notes/"},{"id":"6700193d863cb90001f263ed","name":"Computer vision","slug":"computer-vision","description":"CV related features, best practices, and information","feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/computer-vision/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"},{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"}],"authors":[{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"}],"primary_author":{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"},"primary_tag":{"id":"5e1517c870aad900380bc812","name":"Release Notes","slug":"release-notes","description":"IMPORTANT: Don't change the name of this Tag, it is referenced inside the theme's code in a way that filters out all of the posts that contain it.\nIf you want to change this name, you should also change it in the loop_index.hbs file in the theme's repository.","feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/release-notes/"},"url":"https://labelbox.ghost.io/blog/enhanced-labelbox-video-editors-adds-deeply-nested-classifications/","excerpt":"Streamline video data labeling with new features for deeply nested classifications and overall improved video editor functionality.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"67056486eeb2b1000180d421","uuid":"4a823dc0-8f30-4e9d-92b1-dce96b0c3937","title":"Bring your own models to Quantumworks Lab with new custom model integration","slug":"bring-your-own-models-to-labelbox-with-new-custom-model-integration","html":"\u003cp\u003eIn the fast-paced world of AI development, integrating your own models should be quick and easy. That's why Quantumworks Lab just made a major update that will change the way you work with custom models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’re thrilled to announce that with just a few clicks, you can now seamlessly integrate your custom models into the Quantumworks Lab platform to enhance prediction, accelerate model evaluation, and improve data enrichment. The built-in user interface puts the control directly in your hands, eliminating the need for manual gonboarding.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"new-custom-model-integration\"\u003eNew custom model integration\u003c/h2\u003e\u003cp\u003ePreviously, incorporating your own models into Quantumworks Lab required reaching out to our customer solutions team for manual setup. This process could add additional time to your project and limit your flexibility in experimenting with different models.\u003c/p\u003e\u003cp\u003eNow, you can effortlessly add custom models to Quantumworks Lab through a self-serve, product-integrated user interface. Whether you're working with LLMs, classification models, text analysis, or NER, you can seamlessly connect your models for prediction tasks, enhancing your workflow efficiency and security.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/10/Screenshot-2024-10-08-at-2.19.59-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1088\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/10/Screenshot-2024-10-08-at-2.19.59-PM.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/10/Screenshot-2024-10-08-at-2.19.59-PM.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/10/Screenshot-2024-10-08-at-2.19.59-PM.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/10/Screenshot-2024-10-08-at-2.19.59-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eNew option to create a Custom Model from the Model homepage\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"benefits-of-rapid-model-integration\"\u003eBenefits of rapid model integration\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAccelerate model development\u003c/strong\u003e: Quickly integrate and test your custom models for various use cases, including model-assisted labeling and data enrichment.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhance evaluation capabilities\u003c/strong\u003e: Perform comprehensive human evaluations on your custom models to assess performance, safety, alignment, and more.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSeamless integration with Foundry\u003c/strong\u003e: Leverage your custom models within Foundry to predict labels, enrich data, and generate responses, all within your existing Quantumworks Lab workflows.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eExpand model choices for chat arena evaluations\u003c/strong\u003e: Utilize custom models in the GenAI multimodal chat editor to compare up to 10 models together simultaneously with live, multi-turn conversations.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"getting-started-with-custom-models\"\u003eGetting started with custom models\u0026nbsp;\u003c/h2\u003e\u003cp\u003eAdding a custom model is simple. Follow these quick steps:\u003c/p\u003e\u003col\u003e\u003cli\u003eDeploy your model on an accessible HTTP endpoint.\u003c/li\u003e\u003cli\u003eNavigate to the Quantumworks Lab Models page.\u003c/li\u003e\u003cli\u003eClick \"Create\" and select the \"Custom Model\" option.\u003c/li\u003e\u003cli\u003eChoose the data modality or task type for your model.\u003c/li\u003e\u003cli\u003eEnter your model's information, including name, endpoint, and optional description.\u003c/li\u003e\u003cli\u003eClick \"Create model\" and start leveraging its power within Labelbox.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfTWB-iSODl0qxtRbzleOtAmz3H83Y-Ol7zKz9HWgB20X8mMTN71WtHTmCOtmIDzimX9tm5hkLPS1ts2CX3rtWANSWUZfRvCc58kCQ2oP3GkO5-vDrhIozcwxTMaFjuKRaXTG0pbV_Io2pTSPrnWqTpztk0?key=PJyR73HPDyymyP5W3PSixg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"385\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSetup screen for custom model integration to enter in name and HTTP endpoint.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRead our \u003ca href=\"https://docs.labelbox.com/docs/custom-model-integration?ref=labelbox.ghost.io\"\u003e\u003cu\u003eCustom model integration documentation\u003c/u\u003e\u003c/a\u003e for more information.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"learn-more\"\u003eLearn more\u003c/h2\u003e\u003cp\u003eThis new feature empowers you to take full advantage of Quantumworks Lab's capabilities with your own models, driving faster innovation and more effective AI development. We're excited to see the groundbreaking applications you create!\u003c/p\u003e\u003cp\u003eReady to experiment with custom models in Quantumworks Lab? Simply \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003elogin to Quantumworks Lab\u003c/u\u003e\u003c/a\u003e and navigate to the Models page from the Quantumworks Lab homepage to get started.\u003c/p\u003e\u003cp\u003eOnly have a few minutes, then click through our \u003ca href=\"https://app.arcade.software/flows/L2902Vp01JstzZviNV8U/view?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003einteractive demo\u003c/a\u003e to see how custom models can be added with just a few clicks. \u003c/p\u003e\u003cp\u003eAnd we’re happy to help answer any questions. Reach out to us anytime on our \u003ca href=\"https://labelbox.com/sales?ref=labelbox.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e page.\u0026nbsp;\u003c/p\u003e","comment_id":"67056486eeb2b1000180d421","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/11/llm-fine-tuning-1.png","featured":false,"visibility":"public","created_at":"2024-10-08T09:57:42.000-07:00","updated_at":"2024-11-18T16:57:48.000-08:00","published_at":"2024-10-08T14:22:42.000-07:00","custom_excerpt":"Learn about our latest product enhancement that lets you easily integrate custom models and candidates into Quantumworks Lab with just a few clicks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"},{"id":"65302ef44e99900001fc0519","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox.ghost.io/blog/tag/industry-any/"}],"authors":[{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"}],"primary_author":{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"},"primary_tag":{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},"url":"https://labelbox.ghost.io/blog/bring-your-own-models-to-labelbox-with-new-custom-model-integration/","excerpt":"Learn about our latest product enhancement that lets you easily integrate custom models and candidates into Quantumworks Lab with just a few clicks.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":"Integrate custom models into Quantumworks Lab ","twitter_description":"Our latest feature lets you integrate custom models and candidates into Quantumworks Lab with just a few clicks. Use them for model-assisted labeling or model evaluations. ","meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}]},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"make-videos-queryable-using-foundation-models"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script>
    

    

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>

                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="/static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer>
</body>
<!-- Mirrored from labelbox.com/blog/make-videos-queryable-using-foundation-models/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 13:28:34 GMT -->
</html>