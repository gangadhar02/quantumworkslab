<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/blog/6-key-llms-to-power-your-text-based-ai-applications/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:15:08 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">6 key LLMs to power your text-based AI applications</title><meta name="description" content="Large language models (LLMs) have revolutionized the field of AI and natural language processing (NLP). Dive into six popular LLMs and explore their intended use cases, limitations, and possible real-world use cases." data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="6 key LLMs to power your text-based AI applications" data-next-head=""/><meta property="og:description" content="Large language models (LLMs) have revolutionized the field of AI and natural language processing (NLP). Dive into six popular LLMs and explore their intended use cases, limitations, and possible real-world use cases." data-next-head=""/><meta property="og:url" content="https://labelbox.ghost.io/blog/6-key-llms-to-power-your-text-based-ai-applications/" data-next-head=""/><meta property="og:image" content="https://labelbox.ghost.io/blog/content/images/2023/09/Group-2629.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="6 key LLMs to power your text-based AI applications" data-next-head=""/><meta name="twitter:description" content="Large language models (LLMs) have revolutionized the field of AI and natural language processing (NLP). Dive into six popular LLMs and explore their intended use cases, limitations, and possible real-world use cases." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.ghost.io/blog/6-key-llms-to-power-your-text-based-ai-applications/" data-next-head=""/><meta property="twitter:image" content="https://labelbox.ghost.io/blog/content/images/2023/09/Group-2629.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g48[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.eivcj #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.eivcj .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.eivcj .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.eivcj #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.eivcj #image-viewer .close:hover,.eivcj #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.eivcj .modal-content{width:100%;}}/*!sc*/
data-styled.g105[id="ImageModal__ImageModalWrapper-sc-1ey7m7r-0"]{content:"eivcj,"}/*!sc*/
.QsqTL .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.QsqTL .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.QsqTL .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:20px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h2{padding-top:10px;}}/*!sc*/
.QsqTL .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h3{padding-top:10px;}}/*!sc*/
.QsqTL .content h4{font-size:20px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 16px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h4{padding-top:8px;}}/*!sc*/
.QsqTL .content h5{font-size:18px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 14px;}/*!sc*/
.QsqTL .content h6{font-size:16px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 12px;}/*!sc*/
.QsqTL .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.QsqTL .content a:hover{color:#1e40af;}/*!sc*/
.QsqTL .content li{margin-bottom:20px;}/*!sc*/
.QsqTL .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.QsqTL .content ol{list-style:decimal;padding-left:20px;}/*!sc*/
.QsqTL .content .table-container{overflow-x:auto;margin:40px 0;-webkit-overflow-scrolling:touch;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-container{margin:30px -20px;padding:0 20px;}}/*!sc*/
.QsqTL .content table{width:100%;border-collapse:collapse;font-size:16px;background:white;border:1px solid #e5e7eb;border-radius:8px;overflow:hidden;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content table{font-size:14px;}}/*!sc*/
.QsqTL .content .table-container table{margin:0;min-width:600px;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-container table{min-width:700px;border-radius:0;border-left:none;border-right:none;}}/*!sc*/
.QsqTL .content .content table:not(.table-container table){margin:40px 0;min-width:auto;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .content table:not(.table-container table){margin:30px 0;min-width:auto;border-radius:8px;border:1px solid #e5e7eb;}}/*!sc*/
.QsqTL .content thead{background:#fafbfc;border-bottom:1px solid #d1d5db;}/*!sc*/
.QsqTL .content th{padding:16px 20px;text-align:left;font-weight:600;color:#374151;font-size:14px;-webkit-letter-spacing:0.025em;-moz-letter-spacing:0.025em;-ms-letter-spacing:0.025em;letter-spacing:0.025em;border-right:1px solid #f3f4f6;}/*!sc*/
.QsqTL .content th:last-child{border-right:none;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content th{padding:12px 16px;font-size:13px;}}/*!sc*/
.QsqTL .content td{padding:16px 20px;border-bottom:1px solid #f3f4f6;border-right:1px solid #f9fafb;color:#374151;line-height:1.5;}/*!sc*/
.QsqTL .content td:last-child{border-right:none;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content td{padding:12px 16px;}}/*!sc*/
.QsqTL .content tbody tr{-webkit-transition:background-color 0.2s ease;transition:background-color 0.2s ease;}/*!sc*/
.QsqTL .content tbody tr:hover{background-color:#f8fafc;}/*!sc*/
.QsqTL .content tbody tr:last-child td{border-bottom:none;}/*!sc*/
.QsqTL .content .table-wrapper{overflow-x:auto;margin:40px 0;border:1px solid #e5e7eb;border-radius:8px;-webkit-overflow-scrolling:touch;}/*!sc*/
.QsqTL .content .table-wrapper table{margin:0;border:none;border-radius:0;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-wrapper{margin:30px -20px;border-radius:0;border-left:none;border-right:none;}}/*!sc*/
.QsqTL .content code{background:#f1f5f9;padding:2px 6px;border-radius:4px;font-family:'Monaco','Menlo','Ubuntu Mono',monospace;font-size:14px;color:#e11d48;}/*!sc*/
.QsqTL .content pre{background:#1e293b;color:#e2e8f0;padding:20px;border-radius:8px;overflow-x:auto;margin:30px 0;}/*!sc*/
.QsqTL .content pre code{background:transparent;padding:0;color:inherit;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content pre{margin:20px -20px;border-radius:0;padding:16px 20px;}}/*!sc*/
.QsqTL .content blockquote{border-left:4px solid #2563eb;padding:20px 24px;margin:30px 0;background:#f8fafc;border-radius:0 8px 8px 0;font-style:italic;color:#475569;}/*!sc*/
.QsqTL .content blockquote p{margin-bottom:0;}/*!sc*/
.QsqTL .content blockquote p:last-child{margin-bottom:0;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content blockquote{margin:20px 0;padding:16px 20px;}}/*!sc*/
.QsqTL .content hr{border:none;height:1px;background:linear-gradient(to right,transparent,#e5e7eb,transparent);margin:50px 0;}/*!sc*/
.QsqTL .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.QsqTL .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;color:#6b7280;font-style:italic;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.QsqTL .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;border-radius:8px;}/*!sc*/
.QsqTL .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.QsqTL .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100%;height:100%;margin:0 auto;border-radius:8px;}/*!sc*/
.QsqTL .content .kg-bookmark-card{background:white;border-radius:10px;margin-top:60px !important;border:1px solid #e5e7eb;-webkit-transition:border-color 0.3s ease;transition:border-color 0.3s ease;}/*!sc*/
.QsqTL .content .kg-bookmark-card:hover{border-color:#d1d5db;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;color:#262626 !important;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-thumbnail{position:relative;min-width:30%;max-height:100%;overflow:hidden;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-thumbnail img{position:absolute;top:0;left:0;width:100% !important;height:100% !important;-o-object-fit:cover;object-position:left;object-fit:cover;border-radius:0 10px 10px 0;border-left:1px solid #f5f5f5;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;padding:20px;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-title{font-size:1.125rem;line-height:1.3;font-weight:600;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-description{font-size:0.875rem;font-weight:400;line-height:1.4;margin-top:12px;overflow-y:hidden;color:#6b7280;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;font-size:0.9rem;font-weight:400;margin-top:14px;color:#6b7280;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata img{width:22px !important;height:22px !important;margin-right:8px !important;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata .kg-bookmark-author{margin:4px;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata .kg-bookmark-publisher{margin:4px;}/*!sc*/
.QsqTL .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;margin:40px 0;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;margin-bottom:12px;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 6px;border-radius:6px;overflow:hidden;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;object-fit:cover;-webkit-transition:-webkit-transform 0.3s ease;-webkit-transition:transform 0.3s ease;transition:transform 0.3s ease;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row img:hover{-webkit-transform:scale(1.02);-ms-transform:scale(1.02);transform:scale(1.02);}/*!sc*/
data-styled.g112[id="id__PostContentWrapper-sc-1hduup0-0"]{content:"QsqTL,"}/*!sc*/
@media (max-width:767px){.bwsQop.toc-container{display:none;}}/*!sc*/
.bwsQop.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g113[id="id__TocContainer-sc-1hduup0-1"]{content:"bwsQop,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/blog/%5bid%5d-b80b73d0fd88ad55.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="ImageModal__ImageModalWrapper-sc-1ey7m7r-0 eivcj"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All blog posts</a><main class="id__TocContainer-sc-1hduup0-1 bwsQop toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><p class="my-4 text-sm font-medium">Quantumworks Lab<span class="mx-2">•</span>September 6, 2023</p><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">6 key LLMs to power your text-based AI applications</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox.ghost.io/blog/content/images/2023/09/Group-2629.png"/></div><main class="id__PostContentWrapper-sc-1hduup0-0 QsqTL md:px-24"><div class="content js-toc-content"><p>Large language models (LLMs) have revolutionized the field of AI and natural language processing (NLP). LLMs are trained on massive datasets of text, containing millions or even billions of data points. These models have shattered barriers of what was once thought possible in natural language understanding and generation.&nbsp;</p><p>To effectively assess the performance and reliability of powerful LLMs, it's crucial to have standardized methods for measuring a model's capabilities across complex tasks like code generation and reasoning.&nbsp; This requires benchmarks, but traditional methods have faced criticism for issues like data contamination, inability to provide objective evaluations, scalability challenges, and other drawbacks.</p><p>Quantumworks Lab overcomes the limitations of traditional benchmarking with our human-centric evaluation approach through <a href="../../leaderboards/indexc625.html?ref=labelbox.ghost.io#viewall"><u>Quantumworks Lab Leaderboards</u></a>. By leveraging our modern AI <a href="../../why-labelbox/indexc625.html?ref=labelbox.ghost.io"><u>data factory</u></a>—consisting of a robust platform, scientific processes, and human experts from our Alignerr network of domain and language specialists—we offer accurate assessments for nuanced tasks such as factual accuracy, contextual understanding, and advanced reasoning.</p><p>As a follow-up to <a href="../6-cutting-edge-foundation-models-for-computer-vision-and-how-to-use-them/indexc625.html?ref=labelbox.ghost.io">6 cutting edge foundation models for computer vision</a>, this blog post dives into six popular LLMs and explores their intended use cases, limitations, and possible real-world use cases. With Quantumworks Lab’s data factory, you can explore, fine-tune, evaluate, compare, and leverage the LLMs listed below to accelerate model development.&nbsp;</p><h2 id="openai-chatgpt"><strong>OpenAI ChatGPT</strong> </h2><figure class="kg-card kg-image-card"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_HcMc6fsGevyok1iZJULSn1Fm2STRdT3pHmiHVNUHiffZ5NDZXFVfuochnl3FlYRiScvjqFiRjAyR0_7JhOLvJloVCasHCTgSzuGuMgiaDYzdFj7UuuU8vU0INePRkD8KZqI7Z-rBnBdDbuHmEOZINPfpN9wlS5vWUVDDQhZ3tx1s6XbYUWq9ZKWmUzLcBTF9efXfPb0Lr5grbBX8lm5-ekB-jeChaOplKB8ymwulYnjHyk-RnxgHAry8HbGKjuqn1usDftVEiRqBksf0k7mOQng4rTU5zBZewLM_UIneEwu_JqL_wD86SFwTs1naDY0XAIEFqqSZVVBqTsZLBalotvd7gDIr_KBycmNljUVgLtl7VCbgNJFjq4CTSoHrvGoMMF1GbvslmHUxOt-meP8ZtUHTh57OdWnrJinVqmV3oMQVrk0BKflTjYak-DEg5v-IaLOtUIS6LkqQujo4dJN8N9Y3Fsh2IYkDpcxPPk4w3r-KSiR0YOZHzQbVqwjGbtDYHxkXYz=s1600?key=wXuGg0YnVa8ZarPr3u5LhJXy" class="kg-image" alt="" loading="lazy" width="624" height="351"></figure><p><a href="https://openai.com/research/gpt-4?ref=labelbox.ghost.io" rel="noreferrer">Chat</a><a href="https://openai.com/research/gpt-4?ref=labelbox.ghost.io">GPT</a> is a large multi-modal model that accepts both image and text inputs and provides text outputs. OpenAI has released a couple different model variations, with the most prominent being GPT-3, GPT- 4,&nbsp; GPT-4o mini, and GPT-4o. Each generation of GPT models has built upon the successes of its predecessors, pushing the boundaries of AI and natural language processing. With improved accuracy, multimodal capabilities, and advanced reasoning skills, GPT has become increasingly human-like and versatile to broader applications.&nbsp;&nbsp;</p><p>The latest version of GPT is a highly advanced model with capabilities in in-depth reasoning, multilingualism, and handling complex tasks like creative writing, literary analysis, coding, and structured scientific explanations. OpenAI also claims that their model generates text twice as fast and is 50% more cost-effective, all while enhancing its ability to perform more human-like behaviors.&nbsp;</p><p>OpenAI has described their mission as striving to “ensure that artificial general intelligence—AI systems that are generally smarter than humans—benefits all of humanity.” Their goal is closely tied to developing models that prioritize human alignment and steerability, suggesting that safety is an integral part of their advancement process.&nbsp;&nbsp;</p><p>While ChatGPT offers a wide range of capabilities and valuable applications, it still has some limitations. Since the model is trained to respond in the most likely way based on a prompt, it doesn't actually "know" anything and can sometimes generate inaccurate or hallucinated responses. For this reason, it's important to be selective about when to use the model for critical tasks. Additionally, ChatGPT can still produce non-original text or harmful content, which may inadvertently reinforce biases and contribute to other negative outcomes.</p><p><strong>Sample use case: &nbsp;</strong></p><p>In customer support, GPT can power intelligent chatbots that provide 24/7 assistance, handling inquiries, troubleshooting common issues, and offering personalized responses based on customer data. Additionally, it can help automate the creation of knowledge base articles and FAQs, ensuring consistency and accessibility for both customers and support agents.</p><h2 id="google-gemini"><strong>Google Gemini</strong></h2><figure class="kg-card kg-image-card"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_GrUxjlR3o5qRF4Kj3-a8XaXem9HtCFPbUgB6Im88VNvXJRz9GZOQWjn36COJmFRUJHCbEUldRfjJRoTDjIVIlcts9SQR6X7ShJaW_yR_NXS__V8Jlv9BbX54W7XJw8Va4Ldv9QoeP6kcFtlEqF8trxOcC0mHpViniTghtW3yEn1UrY3_27v49xcxR-SMYYHmBPEx6-WWcuR-S5uOxTOfqQTSz8cyIT0UtVYJMXh8GLe7DrEVONCczdIz5pEnYjl9Hl3DVF_K7YfBd-ProxkOYd7uyGohrAzkV3yVlTNcIFlywR5g4ch9W29FeYRDUm0guJEZUL1nxqPlkZPbNovSvgEw3oz9dRctpfWlVIJu4agrG4l_59wJHrF0KSjyj9aQeMMa-_oYO-fGv5bAdq2aBjrFzTUO_1p5eVjZSqX_him7gOMsqYGSFgw1F8sfwbOlKZRlVuwi5PfQq36TOguCGRe1wWU5-iHwZM63vh5WWC6qEwKrQ8C8HlnqZaw2UPJXCuoP89=s1600?key=wXuGg0YnVa8ZarPr3u5LhJXy" class="kg-image" alt="" loading="lazy" width="624" height="351"></figure><p><a href="https://deepmind.google/technologies/gemini/?ref=labelbox.ghost.io" rel="noreferrer">Gemini</a> is Google DeepMind's cutting-edge multimodal language model, excelling in text, image, audio, and video understanding. It pushes the boundaries of reasoning, multilingualism, function calling, and long-context performance. Google has released a line of versatile Gemini models from 1.0 Nano, 1.0 Ultra, 1.5 Pro, 1.5 Pro with Deep Research, and 2.0 Flash Experimental in the last couple of years.&nbsp;</p><p>Gemini excels in tasks like code generation, delivering factually accurate responses, and solving complex math equations, making it ideal for a range of advanced technical use cases. With benchmark scores of 80% or higher in these areas, Gemini paves the way&nbsp; for the development of AI agents capable of memorizing, reasoning, and planning to complete tasks on your behalf.</p><p>Although very powerful, the limitations for Gemini include struggles with creativity and reasoning with challenging datasets. Its capabilities around audio also could be improved with a lower benchmark score of<a href="https://deepmind.google/technologies/gemini/?ref=labelbox.ghost.io"><u> 40.1% for automatic speech translation.&nbsp;</u></a></p><p>Google seems focused on enhancing&nbsp; Gemini’s&nbsp; performance, reducing latency, and expanding the model’s native capabilities.&nbsp;</p><p><strong>Sample use case:&nbsp;</strong></p><p>In the finance industry, Gemini can extract key financial data, such as revenues, expenses, and budget figures, helping to identify inconsistencies or outliers. It can also streamline financial processes by categorizing transactions, automating routine tasks, and generating reports like expense summaries or P&amp;L statements, saving time and improving efficiency.</p><h2 id="meta-llama"><strong>Meta Llama </strong></h2><figure class="kg-card kg-image-card"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_GJJRDK4oLg-LwYxffEZDynVCqQW523-uFkY3gJ23Q_cANePjWtvhtP8SkHOLbgTaC33mPviZQgkQz-lIasCf6zNsm4kkH4Ge_zvplzOPLRC-JcLLjPrrkB_xI3HUXAKOgfZLWGBvH7rk14Ldy2H1WyAFcE67EwrzUQ8YME7Ms8t3m5wqrpCGNtl8Nw27MBbrVRPjYaFkt0rOJLsz87WiMzTr5tMHbg3lui7ZB5gzzk8CfGU-PzjTaDCytNLZ4bzzbJkycVnUkqi3GnLdxhI25WZuEyLWMH5VgIq5Ib_9ThNyMgU09O5wrfN9UQIukGM5uQvY0YBFNGueHcGU8VSzjoKJ5K5XImEKqaPIqSema1q-MnUC6LmeOuvHQ4I6kq3nQrbOu-5JXlPdk-LfyYDcq0BzMCqTuq78dcanTrw-zrn3pShhm_D4rRvZw6bbsDR-6cyIbtGkAZJA0XUeWRkxO0ylwTzhlweuUL-UEOGf--HqEhxrBEH_E4U1ZVD8xmFcvDN-2m=s1600?key=wXuGg0YnVa8ZarPr3u5LhJXy" class="kg-image" alt="" loading="lazy" width="624" height="351"></figure><p>Meta, a leader in the open-source space, offers a range of<a href="https://www.llama.com/?ref=labelbox.ghost.io" rel="noreferrer"> Llama models</a>—3.1, 3.2, and 3.3—each with distinct capabilities: multilingual support in 3.1 and 3.3, and multimodal features in 3.2. With the public release of these models, Meta aims to drive growth, foster exploration, and compete with leading closed-source LLMs in the space.&nbsp;</p><p>Llama excels at instruction-following, long-context processing, coding, and multilingual translation tasks, with benchmark scores for these tasks ranging from the<a href="https://www.llama.com/?ref=labelbox.ghost.io"><u> high 80s to the high 90s</u></a>. During Llama’s research and iterative post-training process, Meta noted they prioritized human evaluation to have more accurate comparisons in real-world scenarios.&nbsp;</p><p>Designed to offer developers a more versatile model option, Llama is the most popular open-source AI model. It was built to empower users to create custom solutions that align with their unique goals and ideas. It integrates seamlessly into a larger ecosystem, managing various components, including external tool integrations. This makes Llama an ideal choice for developers who need to fully customize an LLM for their specific needs, train it on new datasets, and perform additional fine-tuning.</p><p>Like other large language models, Llama's knowledge and capabilities are limited to what has been explicitly included in its training. As a result, it requires continuous fine-tuning with high-quality data; without this, biases and inaccuracies can emerge, particularly in niche areas where training data is scarce.</p><p><strong>Sample use case:&nbsp;</strong></p><p>With Llama’s impressive multilingual capabilities, it can be used across a variety of real-world use cases such as language translation, question answering, and content creation. For example, Llama can be used to build a chatbot to handle customer inquiries across different languages. This can be particularly useful for retailers that have global customers or businesses that operate in multiple countries and need to communicate with people across different languages.</p><h2 id="anthropic-claude"><strong>Anthropic Claude </strong></h2><figure class="kg-card kg-image-card"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_HesXDKIZR8GmMvogih-Zyfhwjo7_ROOd5mXmTBZjzAPsUtCNePoZhQKFWY_g9QpOEgRwiHE5cRRAtVntO6h8e4d8ezF9Q7TnHQBSel7e37wEEmZGatwnT78IFFiKes5nd3Dz5l7aTy6iEk1YbsOOS4QcfgKQBkrQEs_4qC708OPefKlXrQp5pjhfwmcVbnUrT2YOqIxvzcrnGOsotfIxg7qpBdxb6xwz75QzCjeDgtQGx2MoS1ulXaHZU5mHneTaB_5kcl5jQMWpvRh8sKkZbkFz0YTTQYjjy4x23aqiAJiVo_-_sHaU9bnX9T7vfhhGlmcisA2lKbvGDBuc0MMqfScetHZqiUna_Xlt0GhuRgmyUlu59ltSJ5EgIfRNMF-mBmA-NTMHbQpT0i_domjGT2dMw133YUeBklvxyrOe-fjeVA8mNoTzfkqDohb-YiGI8A_Pntk7svvuhT2HJcgy6POg4_0APP15kmFbE0u7GwkUy4mXm7MF1vQ7duY2pJVhw4wC1i=s1600?key=wXuGg0YnVa8ZarPr3u5LhJXy" class="kg-image" alt="" loading="lazy" width="624" height="351"></figure><p>Anthropic’s <a href="https://www.anthropic.com/claude?ref=labelbox.ghost.io" rel="noreferrer">Claude</a> is an LLM with exceptional vision capabilities and excels in advanced reasoning, language nuances, humor, and complex instructions. The latest version, Claude 3.5, is particularly strong at generating content with a natural, engaging tone, and, according to Anthropic, operates twice as fast as Claude 3 Opus, outperforming it by 26% on internal agentic coding evaluation tasks.</p><p>Claude 3.5 demonstrates strong performance across a variety of tasks, with <a href="https://www.anthropic.com/claude/sonnet?ref=labelbox.ghost.io"><u>benchmark scores</u></a> of 93.7% in coding, 70.4% in vision question answering, and 88.3% in reasoning over text. These capabilities have made this model a popular choice for businesses with critical use-cases.&nbsp;</p><p>Unlike some other AI models, Claude 3.5 cannot directly access the internet for real-time information, meaning its capabilities are limited to the knowledge built into its training. Additionally, users have reported occasional messaging limitations that can vary based on demand, potentially impacting accessibility.</p><p><strong>Sample use case:&nbsp;</strong></p><p>For coding use cases, Claude 3.5 has the capabilities to support your entire software development lifecycle, from initial design and coding to bug fixes, maintenance, and optimization. It also excels at automating repetitive tasks like code refactoring and testing, improving productivity and consistency. This can be useful in significantly accelerating your development process and reducing errors.</p><h2 id="xai-grok"><strong>xAI Grok</strong></h2><figure class="kg-card kg-image-card"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_GOkLSJ2HX4NbuIcOvlmrYxAxWbfFZi_6Xt9bq5VOm17tZ6tcPbODHyupKNSwYi4TGCmTPM_24qN0rVCRSk0dYzc0RMHi1M8oImdvWh7VxicoVj0XVXnSHQcuEGq7Q5AkXLcUC41lOAQrdSOqTMFUoF4LEZOSRU8Z4uObEqWsprPxc_PnqXIXCzFteekfwdvr-qxHAMICrHsNYgRAU_eeBtE85QAFnVwA2dL5NFE8VtF59TfQ0qGDYrCOQRWMVmqhaBnJkhlQL59SCqnLeV6w-X9yg9b-J7XZcPv2_E1VYnAgnuEmmwNGoAbSxcvBNnYaU9aeNvJbZCxWUCduB1O940cR7K7A4wLinG2qlMIWMjIQ6l9wTu76ZjI9Ev0CKVmVGnDjs2WsjH7MNuEV0ybLVeZe0x_kPjzF4r8nZP7hlrHDqo1vjdpOfe4XocMW32cwaSCVqDWCS6HyrLSle1C5BrS1zC4moZdDsY7pJaO3pFzClSltu04_q1hhCXx41kVbGz0IAM=s1600?key=wXuGg0YnVa8ZarPr3u5LhJXy" class="kg-image" alt="" loading="lazy" width="624" height="351"></figure><p>Built by xAI, <a href="https://x.ai/grok?ref=labelbox.ghost.io" rel="noreferrer">Grok </a>is a newer player in the LLM space, but it has already made significant strides in reasoning, coding, chat, and vision capabilities. What sets Grok apart is its focus on delivering responses with humor and wit—an emphasis not commonly found in other LLMs.</p><p>Available on X and inspired by characters like the Hitchhiker's Guide to the Galaxy and JARVIS from Iron Man, Grok is advertised as an “<a href="https://help.x.com/en/using-x/about-grok?ref=labelbox.ghost.io"><u>AI assistant with a twist of humor and a dash of rebellion”</u></a> and is useful for completing tasks around answering questions, problem solving, and brainstorming while keeping you entertained.&nbsp;</p><p>Through human evaluators called “AI tutors,” responses generated by Grok are evaluated in two main areas: instruction following and factuality. The model is also evaluated against various academic <a href="https://x.ai/blog/grok-2?ref=labelbox.ghost.io"><u>benchmarks</u></a> and has achieved performance levels on par with other leading LLMs, particularly excelling in areas like science, general knowledge, and math, while outperforming competitors in vision-based tasks.</p><p>Since Grok is still in the early stages of development, it may occasionally provide factually incorrect information, misinterpret details, or miss important context. It also lacks the multimodal and voice capabilities that many of its competitors currently offer.</p><p><strong>Sample use case:&nbsp;</strong></p><p>For e-commerce use cases, Grok can power recommendation engines by analyzing customer preferences and browsing behaviors to suggest products, services, or content. Whether integrated into e-commerce websites, streaming platforms, or learning management systems, it can offer users tailored suggestions that enhance engagement and drive sales.</p><h2 id="mistral-pixtral-large"><strong>&nbsp;Mistral Pixtral Large</strong></h2><figure class="kg-card kg-image-card"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_G5fpmwiVGzXEuVuchyQv-JdXKlIQAhzcfZBpH9Re54aFZ3ysQG2E1sXs_2wdtLQFfnuEMJ2MIow3BTeBwHeLJ3UpFXTBqvHv8NJJcpbR8jcsZm-HaoDTRMijWANeg-qUBTKPwVa4rs1i3GnCtgM5rcnFz36xymksyOo-8TYn5tRHcpqAnFrSeh_Xt8jZ38FA7v8A6JkaMuuIgSL06KlZahuTeg4d-bKcD6aRzbMxnxwWqGvmdnW4jV5hCbMdWe8RYQEUoJm-ZY4n6hUno48DJjilfMkPcRDJv_FvWAiXHucmbhV1tfniFoU_QD9qu2Qb9S-ip4BbzZAJvKLCgUIQRiLaWL169hQp86FB1l-O2cs92CFz0UHB1bc3POLtnUX9Jm6wAbQujdrpocksDXIStqIjWHl5GdqtaV4ZWJJPM2rD_XhelNetHblYh1NaDs9QlrdoPng9b_HXlkkDbc1IxUKeRF2B9fb_cMbPmPmd5vmqzK4z-P2CrNpvE8c57zCGTdQwCg=s1600?key=wXuGg0YnVa8ZarPr3u5LhJXy" class="kg-image" alt="" loading="lazy" width="624" height="351"></figure><p><a href="https://mistral.ai/news/pixtral-large/?ref=labelbox.ghost.io" rel="noreferrer">Pixtral Large</a> is an open-source, multimodal model built on Mistral’s Large 2, combining its robust text capabilities with new features like multilingual OCR and the ability to understand images, including charts.&nbsp;</p><p>When evaluated against key benchmarks, Pixtral outperforms in complex mathematical reasoning with visual data, along with improvements in long-context understanding and more accurate function calling, making it particularly well-suited for agentic workflows. This makes it a favorable choice for enterprise use cases such as knowledge exploration and sharing, semantic understanding of documents, task automation, and improved customer experiences.&nbsp;</p><p>Designed to prioritize co-designing models and product interfaces, Pixtral was trained with high-impact front-end applications in mind. For example, Pixtral Large powers Mistral’s AI tool, Le Chat, which integrates text, vision, and interactive functionalities into a unified platform, making it ideal for diverse use cases like research, ideation, and automation.</p><p>However, as a relatively new player in the space, Pixtral Large lacks the established infrastructure and years of fine-tuning that other LLMs benefit from. This means it may require time to build user trust and could still experience occasional inaccuracies, hallucinations, and glitches.</p><p><strong>Sample use case:&nbsp;</strong></p><p>In the legal industry, Pixel Large can streamline automation by analyzing large volumes of legal documents, contracts, and case files to extract key information, summarize content, and identify relevant precedents. This can significantly reduce manual review time, improve accuracy in legal research, and help law firms manage complex cases more efficiently.</p><p></p><hr><h2 id="explore-llm-development-with-labelbox"><strong>Explore LLM Development with Quantumworks Lab</strong></h2><p>Large language models have changed the way AI builders and business users build more powerful models. You can now leverage cutting edge LLMs to accelerate the development of common enterprise AI applications at scale across a variety of industries.</p><p>&nbsp;Looking to unlock the full potential of LLMs?&nbsp;</p><ul><li>Leverage Quantumworks Lab’s <a href="../../product/platform/indexc625.html?ref=labelbox.ghost.io"><u>end-to-end platform</u></a> and <a href="../../services/labeling/indexc625.html?ref=labelbox.ghost.io"><u>network of expert human evaluators</u></a> across diverse domains and languages to run a fully functional data factory for advanced LLM training.</li><li>Explore our <a href="../../leaderboards/indexc625.html?ref=labelbox.ghost.io"><u>leaderboards</u></a> and view the latest evaluations across AI modalities and applications.</li></ul><p>If you’d like to evaluate your LLM as part of the next leaderboard update or have a question, feel free to contact us <a href="../../sales/indexc625.html?ref=labelbox.ghost.io"><u>here</u></a>.</p><h2 id="sources">Sources</h2><p>Anthropic. (n.d.). Claude Sonnet. Anthropic.<a href="https://www.anthropic.com/news/claude-3-5-sonnet?ref=labelbox.ghost.io"> https://www.anthropic.com/news/claude-3-5-sonnet</a></p><p>DeepMind. (n.d.). Gemini. DeepMind.<a href="https://deepmind.google/technologies/gemini/?ref=labelbox.ghost.io"> https://deepmind.google/technologies/gemini/</a></p><p>DeepMind. (n.d.). Gemini Pro. DeepMind.<a href="https://deepmind.google/technologies/gemini/?ref=labelbox.ghost.io"> https://deepmind.google/technologies/gemini/</a></p><p>DeepMind<strong>.</strong> (2023). Gemini v1.5 report. DeepMind.<a href="https://arxiv.org/abs/2403.05530?ref=labelbox.ghost.io"> https://arxiv.org/abs/2403.05530</a></p><p>Llama. (n.d.). Llama models. Llama.<a href="https://www.llama.com/?ref=labelbox.ghost.io"> https://www.llama.com/</a></p><p>Mistral AI. (2023, December 4). Pixtral Large: The next step in generative AI. Mistral AI.<a href="https://www.infoq.com/news/2024/12/pixtral-large-mistral-ai/?ref=labelbox.ghost.io"> https://www.infoq.com/news/2024/12/pixtral-large-mistral-ai/</a></p><p>OpenAI<strong>.</strong> (n.d.). GPT-4: Model overview. OpenAI.<a href="https://platform.openai.com/docs/models/gp?ref=labelbox.ghost.io"> https://platform.openai.com/docs/models/gp</a></p><p>OpenAI<strong>.</strong> (2023, January 30). Planning for AGI and beyond. OpenAI.<a href="https://openai.com/index/planning-for-agi-and-beyond/?ref=labelbox.ghost.io"> https://openai.com/index/planning-for-agi-and-beyond/</a></p><p>VentureBeat. (2023, December 4). Mistral unleashes Pixtral Large and upgrades Le Chat into full-on ChatGPT competitor. VentureBeat.<a href="https://www.perplexity.ai/page/mistral-releases-pixtral-large-TnUT77WxSueYxDHNAteYGw?ref=labelbox.ghost.io"> https://www.perplexity.ai/page/mistral-releases-pixtral-large-TnUT77WxSueYxDHNAteYGw</a></p><p>xAI. (n.d.). About Grok. xAI.<a href="https://x.ai/?ref=labelbox.ghost.io"> https://x.ai/</a></p><p>xAI. (n.d.). About xAI. xAI.<a href="https://x.ai/about?ref=labelbox.ghost.io"> https://x.ai/about</a></p><p>xAI<strong>.</strong> (2024, March 18). Grok 1.5v release. xAI.<a href="https://x.ai/?ref=labelbox.ghost.io"> https://x.ai/</a></p></div></main></div></div></div><div class="mt-5 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="my-20 w-full h-[1px] bg-neutral-200"></div><div class="grid grid-cols-12 gap-2"><div class="col-span-12"><h2 class="mb-12 text-center text-3xl md:text-4xl font-medium">Continue reading</h2></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../code-runner-secure-scalable-code-execution-for-model-evaluation-2/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index4144.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Dmytro Apollonin<span class="mx-2">•</span>December 20, 2024</p></div><a href="../code-runner-secure-scalable-code-execution-for-model-evaluation-2/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Code Runner: Secure, scalable code execution for model evaluation</p><p class="text-base max-w-2xl undefined line-clamp-3">Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../advance-llm-reasoning-with-advanced-fact-checking-and-prompt-rating-tools/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2Fllm-reasoning--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2Fllm-reasoning--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2Fllm-reasoning--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2Fllm-reasoning--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2Fllm-reasoning--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2Fllm-reasoning--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2Fllm-reasoning--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2Fllm-reasoning--2-.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index1614.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2Fllm-reasoning--2-.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Quantumworks Lab<span class="mx-2">•</span>December 17, 2024</p></div><a href="../advance-llm-reasoning-with-advanced-fact-checking-and-prompt-rating-tools/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Advance LLM reasoning with advanced fact-checking and prompt rating tools</p><p class="text-base max-w-2xl undefined line-clamp-3">Quantumworks Lab&#x27;s new fact-checking and prompt-rating tools improve LLM accuracy and reasoning capabilities by allowing users to evaluate responses, correct errors, and flag bad prompts.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../inside-the-matrix-a-look-into-the-math-behind-ai/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FMatrices.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FMatrices.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FMatrices.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FMatrices.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FMatrices.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FMatrices.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FMatrices.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FMatrices.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index45ff.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FMatrices.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Michał Jóźwiak<span class="mx-2">•</span>December 11, 2024</p></div><a href="../inside-the-matrix-a-look-into-the-math-behind-ai/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Inside the matrix: A look into the math behind AI</p><p class="text-base max-w-2xl undefined line-clamp-3">Matrices are crucial in AI for processing multi-dimensional data in areas like machine learning and computer vision. They represent linear maps and transform input into output, making them central to many AI methods.</p></a></div></div></div></div></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"6-key-llms-to-power-your-text-based-ai-applications","id":"64f0a5d11058bd00013d54d2","uuid":"da7fd016-9835-44ee-abdf-4f8d6ce6bb25","title":"6 key LLMs to power your text-based AI applications","html":"\u003cp\u003eLarge language models (LLMs) have revolutionized the field of AI and natural language processing (NLP). LLMs are trained on massive datasets of text, containing millions or even billions of data points. These models have shattered barriers of what was once thought possible in natural language understanding and generation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo effectively assess the performance and reliability of powerful LLMs, it's crucial to have standardized methods for measuring a model's capabilities across complex tasks like code generation and reasoning.\u0026nbsp; This requires benchmarks, but traditional methods have faced criticism for issues like data contamination, inability to provide objective evaluations, scalability challenges, and other drawbacks.\u003c/p\u003e\u003cp\u003eLabelbox overcomes the limitations of traditional benchmarking with our human-centric evaluation approach through \u003ca href=\"https://labelbox.com/leaderboards/?ref=labelbox.ghost.io#viewall\"\u003e\u003cu\u003eLabelbox Leaderboards\u003c/u\u003e\u003c/a\u003e. By leveraging our modern AI \u003ca href=\"https://labelbox.com/why-labelbox/?ref=labelbox.ghost.io\"\u003e\u003cu\u003edata factory\u003c/u\u003e\u003c/a\u003e—consisting of a robust platform, scientific processes, and human experts from our Alignerr network of domain and language specialists—we offer accurate assessments for nuanced tasks such as factual accuracy, contextual understanding, and advanced reasoning.\u003c/p\u003e\u003cp\u003eAs a follow-up to \u003ca href=\"https://labelbox.com/blog/6-cutting-edge-foundation-models-for-computer-vision-and-how-to-use-them/?ref=labelbox.ghost.io\"\u003e6 cutting edge foundation models for computer vision\u003c/a\u003e, this blog post dives into six popular LLMs and explores their intended use cases, limitations, and possible real-world use cases. With Quantumworks Lab’s data factory, you can explore, fine-tune, evaluate, compare, and leverage the LLMs listed below to accelerate model development.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"openai-chatgpt\"\u003e\u003cstrong\u003eOpenAI ChatGPT\u003c/strong\u003e \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdlaObVViqtxJuwxjfFNrFt6QLuvt4NZbejbHndga_bYgWduYwSOpTIiyT4YNFiIgojkBhJEhvCqTljpnwOnMrzkfrfho0oZ2c6-bLas-u7tSH3XEfs00fEGSUYEXjm6Vw0iXFmDQ?key=wXuGg0YnVa8ZarPr3u5LhJXy\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"\u003e\u003c/figure\u003e\u003cp\u003e\u003ca href=\"https://openai.com/research/gpt-4?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003eChat\u003c/a\u003e\u003ca href=\"https://openai.com/research/gpt-4?ref=labelbox.ghost.io\"\u003eGPT\u003c/a\u003e is a large multi-modal model that accepts both image and text inputs and provides text outputs. OpenAI has released a couple different model variations, with the most prominent being GPT-3, GPT- 4,\u0026nbsp; GPT-4o mini, and GPT-4o. Each generation of GPT models has built upon the successes of its predecessors, pushing the boundaries of AI and natural language processing. With improved accuracy, multimodal capabilities, and advanced reasoning skills, GPT has become increasingly human-like and versatile to broader applications.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe latest version of GPT is a highly advanced model with capabilities in in-depth reasoning, multilingualism, and handling complex tasks like creative writing, literary analysis, coding, and structured scientific explanations. OpenAI also claims that their model generates text twice as fast and is 50% more cost-effective, all while enhancing its ability to perform more human-like behaviors.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOpenAI has described their mission as striving to “ensure that artificial general intelligence—AI systems that are generally smarter than humans—benefits all of humanity.” Their goal is closely tied to developing models that prioritize human alignment and steerability, suggesting that safety is an integral part of their advancement process.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile ChatGPT offers a wide range of capabilities and valuable applications, it still has some limitations. Since the model is trained to respond in the most likely way based on a prompt, it doesn't actually \"know\" anything and can sometimes generate inaccurate or hallucinated responses. For this reason, it's important to be selective about when to use the model for critical tasks. Additionally, ChatGPT can still produce non-original text or harmful content, which may inadvertently reinforce biases and contribute to other negative outcomes.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSample use case: \u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn customer support, GPT can power intelligent chatbots that provide 24/7 assistance, handling inquiries, troubleshooting common issues, and offering personalized responses based on customer data. Additionally, it can help automate the creation of knowledge base articles and FAQs, ensuring consistency and accessibility for both customers and support agents.\u003c/p\u003e\u003ch2 id=\"google-gemini\"\u003e\u003cstrong\u003eGoogle Gemini\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfTHHMLsfAETQqanlYAaSLH-q-UxJljq8UfRjVjsUq7fx6FbW4NhJijp8cgLk2xW9At_y7ZdH-HkWwa-WVwO4wXm-UM39SIhwojunYfinVX-bu4gWm9CzCFNQ_SBpKtUX7HDOH0Lg?key=wXuGg0YnVa8ZarPr3u5LhJXy\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"\u003e\u003c/figure\u003e\u003cp\u003e\u003ca href=\"https://deepmind.google/technologies/gemini/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003eGemini\u003c/a\u003e is Google DeepMind's cutting-edge multimodal language model, excelling in text, image, audio, and video understanding. It pushes the boundaries of reasoning, multilingualism, function calling, and long-context performance. Google has released a line of versatile Gemini models from 1.0 Nano, 1.0 Ultra, 1.5 Pro, 1.5 Pro with Deep Research, and 2.0 Flash Experimental in the last couple of years.\u0026nbsp;\u003c/p\u003e\u003cp\u003eGemini excels in tasks like code generation, delivering factually accurate responses, and solving complex math equations, making it ideal for a range of advanced technical use cases. With benchmark scores of 80% or higher in these areas, Gemini paves the way\u0026nbsp; for the development of AI agents capable of memorizing, reasoning, and planning to complete tasks on your behalf.\u003c/p\u003e\u003cp\u003eAlthough very powerful, the limitations for Gemini include struggles with creativity and reasoning with challenging datasets. Its capabilities around audio also could be improved with a lower benchmark score of\u003ca href=\"https://deepmind.google/technologies/gemini/?ref=labelbox.ghost.io\"\u003e\u003cu\u003e 40.1% for automatic speech translation.\u0026nbsp;\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003eGoogle seems focused on enhancing\u0026nbsp; Gemini’s\u0026nbsp; performance, reducing latency, and expanding the model’s native capabilities.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSample use case:\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the finance industry, Gemini can extract key financial data, such as revenues, expenses, and budget figures, helping to identify inconsistencies or outliers. It can also streamline financial processes by categorizing transactions, automating routine tasks, and generating reports like expense summaries or P\u0026amp;L statements, saving time and improving efficiency.\u003c/p\u003e\u003ch2 id=\"meta-llama\"\u003e\u003cstrong\u003eMeta Llama \u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeuAysF6oOmgp4GsltRbIHyuIo2tOWzDTJIfnxHxHw6GMjpOJednSlFmI32oVama7pbRulYPXA84XlJ6FHdZ7jpHOphIlNcTMQpDfKHl5FaQRm4RjlyBJUwmTWDHOL92o29QTCFGA?key=wXuGg0YnVa8ZarPr3u5LhJXy\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"\u003e\u003c/figure\u003e\u003cp\u003eMeta, a leader in the open-source space, offers a range of\u003ca href=\"https://www.llama.com/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003e Llama models\u003c/a\u003e—3.1, 3.2, and 3.3—each with distinct capabilities: multilingual support in 3.1 and 3.3, and multimodal features in 3.2. With the public release of these models, Meta aims to drive growth, foster exploration, and compete with leading closed-source LLMs in the space.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLlama excels at instruction-following, long-context processing, coding, and multilingual translation tasks, with benchmark scores for these tasks ranging from the\u003ca href=\"https://www.llama.com/?ref=labelbox.ghost.io\"\u003e\u003cu\u003e high 80s to the high 90s\u003c/u\u003e\u003c/a\u003e. During Llama’s research and iterative post-training process, Meta noted they prioritized human evaluation to have more accurate comparisons in real-world scenarios.\u0026nbsp;\u003c/p\u003e\u003cp\u003eDesigned to offer developers a more versatile model option, Llama is the most popular open-source AI model. It was built to empower users to create custom solutions that align with their unique goals and ideas. It integrates seamlessly into a larger ecosystem, managing various components, including external tool integrations. This makes Llama an ideal choice for developers who need to fully customize an LLM for their specific needs, train it on new datasets, and perform additional fine-tuning.\u003c/p\u003e\u003cp\u003eLike other large language models, Llama's knowledge and capabilities are limited to what has been explicitly included in its training. As a result, it requires continuous fine-tuning with high-quality data; without this, biases and inaccuracies can emerge, particularly in niche areas where training data is scarce.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSample use case:\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith Llama’s impressive multilingual capabilities, it can be used across a variety of real-world use cases such as language translation, question answering, and content creation. For example, Llama can be used to build a chatbot to handle customer inquiries across different languages. This can be particularly useful for retailers that have global customers or businesses that operate in multiple countries and need to communicate with people across different languages.\u003c/p\u003e\u003ch2 id=\"anthropic-claude\"\u003e\u003cstrong\u003eAnthropic Claude \u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcHdgisLe4Zp-9NwVS4c_6uubu9PZ61-N58YyScXINz9E0Du5mWAV1p67G1bQOImGmbaT3sRFaXqzPiX3CLJ7YOqtPxDmK244xxF1h99g9ZA4U_FiLEYFwOALSvNsmezeCPjt85?key=wXuGg0YnVa8ZarPr3u5LhJXy\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"\u003e\u003c/figure\u003e\u003cp\u003eAnthropic’s \u003ca href=\"https://www.anthropic.com/claude?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003eClaude\u003c/a\u003e is an LLM with exceptional vision capabilities and excels in advanced reasoning, language nuances, humor, and complex instructions. The latest version, Claude 3.5, is particularly strong at generating content with a natural, engaging tone, and, according to Anthropic, operates twice as fast as Claude 3 Opus, outperforming it by 26% on internal agentic coding evaluation tasks.\u003c/p\u003e\u003cp\u003eClaude 3.5 demonstrates strong performance across a variety of tasks, with \u003ca href=\"https://www.anthropic.com/claude/sonnet?ref=labelbox.ghost.io\"\u003e\u003cu\u003ebenchmark scores\u003c/u\u003e\u003c/a\u003e of 93.7% in coding, 70.4% in vision question answering, and 88.3% in reasoning over text. These capabilities have made this model a popular choice for businesses with critical use-cases.\u0026nbsp;\u003c/p\u003e\u003cp\u003eUnlike some other AI models, Claude 3.5 cannot directly access the internet for real-time information, meaning its capabilities are limited to the knowledge built into its training. Additionally, users have reported occasional messaging limitations that can vary based on demand, potentially impacting accessibility.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSample use case:\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor coding use cases, Claude 3.5 has the capabilities to support your entire software development lifecycle, from initial design and coding to bug fixes, maintenance, and optimization. It also excels at automating repetitive tasks like code refactoring and testing, improving productivity and consistency. This can be useful in significantly accelerating your development process and reducing errors.\u003c/p\u003e\u003ch2 id=\"xai-grok\"\u003e\u003cstrong\u003exAI Grok\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXc-yEGMgHKenbrUOmtgEcgHIggNJNOgJwXfFsRUTX1SngO6FopDn1Uuv73rKatAgvXuP11M1847wkY9Ajg4CUziO7YmXNK3P0h1T-x1jF6JZpXAehHoe-LmkLUtd_ngFK9s2X3ZIQ?key=wXuGg0YnVa8ZarPr3u5LhJXy\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"\u003e\u003c/figure\u003e\u003cp\u003eBuilt by xAI, \u003ca href=\"https://x.ai/grok?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003eGrok \u003c/a\u003eis a newer player in the LLM space, but it has already made significant strides in reasoning, coding, chat, and vision capabilities. What sets Grok apart is its focus on delivering responses with humor and wit—an emphasis not commonly found in other LLMs.\u003c/p\u003e\u003cp\u003eAvailable on X and inspired by characters like the Hitchhiker's Guide to the Galaxy and JARVIS from Iron Man, Grok is advertised as an “\u003ca href=\"https://help.x.com/en/using-x/about-grok?ref=labelbox.ghost.io\"\u003e\u003cu\u003eAI assistant with a twist of humor and a dash of rebellion”\u003c/u\u003e\u003c/a\u003e and is useful for completing tasks around answering questions, problem solving, and brainstorming while keeping you entertained.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThrough human evaluators called “AI tutors,” responses generated by Grok are evaluated in two main areas: instruction following and factuality. The model is also evaluated against various academic \u003ca href=\"https://x.ai/blog/grok-2?ref=labelbox.ghost.io\"\u003e\u003cu\u003ebenchmarks\u003c/u\u003e\u003c/a\u003e and has achieved performance levels on par with other leading LLMs, particularly excelling in areas like science, general knowledge, and math, while outperforming competitors in vision-based tasks.\u003c/p\u003e\u003cp\u003eSince Grok is still in the early stages of development, it may occasionally provide factually incorrect information, misinterpret details, or miss important context. It also lacks the multimodal and voice capabilities that many of its competitors currently offer.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSample use case:\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor e-commerce use cases, Grok can power recommendation engines by analyzing customer preferences and browsing behaviors to suggest products, services, or content. Whether integrated into e-commerce websites, streaming platforms, or learning management systems, it can offer users tailored suggestions that enhance engagement and drive sales.\u003c/p\u003e\u003ch2 id=\"mistral-pixtral-large\"\u003e\u003cstrong\u003e\u0026nbsp;Mistral Pixtral Large\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcWaDbGOT_JsKZrhyp4ZfaNVaPBZSbRnfutvJHlx4SuPGgwrHrF6W2oIGCpGIudeHoOUQd_p-5X5vyfEp8wvradyBmMpaHsTuxk2OE54RUuTMqvl5uw4Rj7HBX_jY2LnIlkphVV?key=wXuGg0YnVa8ZarPr3u5LhJXy\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"\u003e\u003c/figure\u003e\u003cp\u003e\u003ca href=\"https://mistral.ai/news/pixtral-large/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003ePixtral Large\u003c/a\u003e is an open-source, multimodal model built on Mistral’s Large 2, combining its robust text capabilities with new features like multilingual OCR and the ability to understand images, including charts.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen evaluated against key benchmarks, Pixtral outperforms in complex mathematical reasoning with visual data, along with improvements in long-context understanding and more accurate function calling, making it particularly well-suited for agentic workflows. This makes it a favorable choice for enterprise use cases such as knowledge exploration and sharing, semantic understanding of documents, task automation, and improved customer experiences.\u0026nbsp;\u003c/p\u003e\u003cp\u003eDesigned to prioritize co-designing models and product interfaces, Pixtral was trained with high-impact front-end applications in mind. For example, Pixtral Large powers Mistral’s AI tool, Le Chat, which integrates text, vision, and interactive functionalities into a unified platform, making it ideal for diverse use cases like research, ideation, and automation.\u003c/p\u003e\u003cp\u003eHowever, as a relatively new player in the space, Pixtral Large lacks the established infrastructure and years of fine-tuning that other LLMs benefit from. This means it may require time to build user trust and could still experience occasional inaccuracies, hallucinations, and glitches.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSample use case:\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the legal industry, Pixel Large can streamline automation by analyzing large volumes of legal documents, contracts, and case files to extract key information, summarize content, and identify relevant precedents. This can significantly reduce manual review time, improve accuracy in legal research, and help law firms manage complex cases more efficiently.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"explore-llm-development-with-labelbox\"\u003e\u003cstrong\u003eExplore LLM Development with Quantumworks Lab\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eLarge language models have changed the way AI builders and business users build more powerful models. You can now leverage cutting edge LLMs to accelerate the development of common enterprise AI applications at scale across a variety of industries.\u003c/p\u003e\u003cp\u003e\u0026nbsp;Looking to unlock the full potential of LLMs?\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eLeverage Quantumworks Lab’s \u003ca href=\"https://labelbox.com/product/platform/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eend-to-end platform\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox.ghost.io\"\u003e\u003cu\u003enetwork of expert human evaluators\u003c/u\u003e\u003c/a\u003e across diverse domains and languages to run a fully functional data factory for advanced LLM training.\u003c/li\u003e\u003cli\u003eExplore our \u003ca href=\"https://labelbox.com/leaderboards/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eleaderboards\u003c/u\u003e\u003c/a\u003e and view the latest evaluations across AI modalities and applications.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you’d like to evaluate your LLM as part of the next leaderboard update or have a question, feel free to contact us \u003ca href=\"https://labelbox.com/sales/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"sources\"\u003eSources\u003c/h2\u003e\u003cp\u003eAnthropic. (n.d.). Claude Sonnet. Anthropic.\u003ca href=\"https://www.anthropic.com/news/claude-3-5-sonnet?ref=labelbox.ghost.io\"\u003e https://www.anthropic.com/news/claude-3-5-sonnet\u003c/a\u003e\u003c/p\u003e\u003cp\u003eDeepMind. (n.d.). Gemini. DeepMind.\u003ca href=\"https://deepmind.google/technologies/gemini/?ref=labelbox.ghost.io\"\u003e https://deepmind.google/technologies/gemini/\u003c/a\u003e\u003c/p\u003e\u003cp\u003eDeepMind. (n.d.). Gemini Pro. DeepMind.\u003ca href=\"https://deepmind.google/technologies/gemini/?ref=labelbox.ghost.io\"\u003e https://deepmind.google/technologies/gemini/\u003c/a\u003e\u003c/p\u003e\u003cp\u003eDeepMind\u003cstrong\u003e.\u003c/strong\u003e (2023). Gemini v1.5 report. DeepMind.\u003ca href=\"https://arxiv.org/abs/2403.05530?ref=labelbox.ghost.io\"\u003e https://arxiv.org/abs/2403.05530\u003c/a\u003e\u003c/p\u003e\u003cp\u003eLlama. (n.d.). Llama models. Llama.\u003ca href=\"https://www.llama.com/?ref=labelbox.ghost.io\"\u003e https://www.llama.com/\u003c/a\u003e\u003c/p\u003e\u003cp\u003eMistral AI. (2023, December 4). Pixtral Large: The next step in generative AI. Mistral AI.\u003ca href=\"https://www.infoq.com/news/2024/12/pixtral-large-mistral-ai/?ref=labelbox.ghost.io\"\u003e https://www.infoq.com/news/2024/12/pixtral-large-mistral-ai/\u003c/a\u003e\u003c/p\u003e\u003cp\u003eOpenAI\u003cstrong\u003e.\u003c/strong\u003e (n.d.). GPT-4: Model overview. OpenAI.\u003ca href=\"https://platform.openai.com/docs/models/gp?ref=labelbox.ghost.io\"\u003e https://platform.openai.com/docs/models/gp\u003c/a\u003e\u003c/p\u003e\u003cp\u003eOpenAI\u003cstrong\u003e.\u003c/strong\u003e (2023, January 30). Planning for AGI and beyond. OpenAI.\u003ca href=\"https://openai.com/index/planning-for-agi-and-beyond/?ref=labelbox.ghost.io\"\u003e https://openai.com/index/planning-for-agi-and-beyond/\u003c/a\u003e\u003c/p\u003e\u003cp\u003eVentureBeat. (2023, December 4). Mistral unleashes Pixtral Large and upgrades Le Chat into full-on ChatGPT competitor. VentureBeat.\u003ca href=\"https://www.perplexity.ai/page/mistral-releases-pixtral-large-TnUT77WxSueYxDHNAteYGw?ref=labelbox.ghost.io\"\u003e https://www.perplexity.ai/page/mistral-releases-pixtral-large-TnUT77WxSueYxDHNAteYGw\u003c/a\u003e\u003c/p\u003e\u003cp\u003exAI. (n.d.). About Grok. xAI.\u003ca href=\"https://x.ai/?ref=labelbox.ghost.io\"\u003e https://x.ai/\u003c/a\u003e\u003c/p\u003e\u003cp\u003exAI. (n.d.). About xAI. xAI.\u003ca href=\"https://x.ai/about?ref=labelbox.ghost.io\"\u003e https://x.ai/about\u003c/a\u003e\u003c/p\u003e\u003cp\u003exAI\u003cstrong\u003e.\u003c/strong\u003e (2024, March 18). Grok 1.5v release. xAI.\u003ca href=\"https://x.ai/?ref=labelbox.ghost.io\"\u003e https://x.ai/\u003c/a\u003e\u003c/p\u003e","comment_id":"64f0a5d11058bd00013d54d2","feature_image":"https://labelbox.ghost.io/blog/content/images/2023/09/Group-2629.png","featured":false,"visibility":"public","created_at":"2023-08-31T07:38:09.000-07:00","updated_at":"2024-12-18T11:20:26.000-08:00","published_at":"2023-09-06T10:15:50.000-07:00","custom_excerpt":"Large language models (LLMs) have revolutionized the field of AI and natural language processing (NLP). Dive into six popular LLMs and explore their intended use cases, limitations, and possible real-world use cases.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/blog/6-key-llms-to-power-your-text-based-ai-applications","tags":[{"id":"65302fc14e99900001fc0527","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-llms/"},{"id":"653030b74e99900001fc052f","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/using-llms/"},{"id":"6530313c4e99900001fc0537","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/train-fine-tune-ai/"},{"id":"65302ef44e99900001fc0519","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox.ghost.io/blog/tag/industry-any/"},{"id":"67001958863cb90001f263ef","name":"NLP","slug":"nlp","description":"Blogs on natural language processing (NLP)","feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/nlp/"}],"authors":[{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"}],"primary_author":{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"},"primary_tag":{"id":"65302fc14e99900001fc0527","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-llms/"},"url":"https://labelbox.ghost.io/blog/6-key-llms-to-power-your-text-based-ai-applications/","excerpt":"Large language models (LLMs) have revolutionized the field of AI and natural language processing (NLP). Dive into six popular LLMs and explore their intended use cases, limitations, and possible real-world use cases.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":"6 key LLMs to power your text-based AI applications","og_description":"Large language models (LLMs) have revolutionized the field of AI and natural language processing (NLP). Dive into six popular LLMs and explore their intended use cases, limitations, and possible real-world use cases.","twitter_image":null,"twitter_title":"6 key LLMs to power your text-based AI applications","twitter_description":"Large language models (LLMs) have revolutionized the field of AI and natural language processing (NLP). Dive into six popular LLMs and explore their intended use cases, limitations, and possible real-world use cases.","meta_title":"6 key LLMs to power your text-based AI applications","meta_description":"Large language models (LLMs) have revolutionized the field of AI and natural language processing (NLP). Dive into six popular LLMs and explore their intended use cases, limitations, and possible real-world use cases.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},"recommended":[{"id":"6765b8c06f63bf0001f1ca72","uuid":"9f912bc0-54da-4ac6-ab5f-78d8f926463c","title":"Code Runner: Secure, scalable code execution for model evaluation","slug":"code-runner-secure-scalable-code-execution-for-model-evaluation-2","html":"\u003cp\u003eIn the world of large language models (LLMs), evaluating their responses effectively is a fundamental aspect of improving model performance. We’re excited to announce the latest addition to the Quantumworks Lab platform: Code Runner.\u003cstrong\u003e \u003c/strong\u003eThis new capability pushes the boundaries of interactivity by allowing users to execute written code directly within the evaluation workflow.\u003c/p\u003e\u003cp\u003eCode Runner helps eliminate errors, optimizes functionality, and validates outputs, leading to higher-quality datasets. Today, we’ll introduce this new feature and then dive into the technical details of the infrastructure powering this feature, highlighting how it was designed with security, scalability, and\u003cstrong\u003e \u003c/strong\u003erobustness at its core.\u003c/p\u003e\u003ch2 id=\"what-is-code-runner\"\u003e\u003cstrong\u003eWhat is Code Runner?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCode Runner is a new built-in feature of the Quantumworks Lab platform designed to improve the quality of responses and labels generated in any coding-related projects. The new features enables users to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eDirectly execute code found in either model responses or user-written responses \u003c/li\u003e\u003cli\u003eReceive precise outputs including:\u003cul\u003e\u003cli\u003eStandard output (stdout)\u003c/li\u003e\u003cli\u003eStandard error (stderr)\u003c/li\u003e\u003cli\u003eExecution time\u003c/li\u003e\u003cli\u003eWarnings or runtime errors\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy integrating Code Runner into the evaluation pipeline, we aim to simplify the process of verifying the accuracy, efficiency, and functionality of code responses, all without users needing to leave the platform.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeBDN_1bnU_bTrPrWS59SWalVqw22Gxq3AIxNnbsOJmZGPap3weXHYFEgzrlPnEyhVK1GOjzCVClvQycomfMfhQsulqPk4wdQGqniZv8aIaHGP69wzgcFjdDdr5FgooITwNJCsp?key=GRyWmie9kDWaUfN6osDAF8J7\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"389\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOur system automatically detects the language in the text area and suggests the appropriate environment for execution, whether Python or JavaScript (and more to come).\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBut what makes this feature stand out is the sophisticated infrastructure behind it, designed to ensure seamless execution while maintaining strict security and privacy standards.\u003c/p\u003e\u003ch2 id=\"code-runner-infrastructure-a-deep-dive\"\u003e\u003cstrong\u003eCode Runner infrastructure: A deep dive\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAt the heart of Code Runner’s infrastructure lies Google Cloud Run, a fully managed compute platform that runs containerized applications in a secure, scalable manner. Here are the key components and principles driving the system:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Cloud Run for language-specific environments\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eEvery code execution happens in a dedicated Cloud Run instance. Each instance is tailored to a specific programming language environment (e.g., Python, JavaScript, etc.) and is spun up dynamically based on the code type detected in the user response.\u003c/p\u003e\u003cp\u003eThis design includes the following characteristics to ensure security and speed:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIsolation\u003c/strong\u003e: Each execution is fully containerized, completely isolating the runtime environment from others.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTemporary directories\u003c/strong\u003e: Code is executed in a temporary directory within the container, and it is deleted immediately after execution, leaving no trace behind.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLanguage-specific tools\u003c/strong\u003e: Each environment comes preloaded with the necessary packages and libraries to ensure compatibility and speed.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2. Enhanced security with separate GCP projects \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe Cloud Run service is hosted in a separate Google Cloud Platform (GCP) project, distinct from our main infrastructure. This segmentation provides an additional layer of security by isolating code execution from our core services. Even in the unlikely event of a compromise, the blast radius is contained.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Communication via private service connect\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo ensure secure and controlled communication, all interactions between the main evaluation system and the Cloud Run service occur over Private Service Connect, which provides the following advantages: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNo public exposure\u003c/strong\u003e: The Cloud Run endpoint is never exposed to the public internet, reducing the risk of unauthorized access.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOne-way communication\u003c/strong\u003e: The Private Service Connect setup restricts outbound networking from the Cloud Run service, ensuring that executed code cannot make arbitrary network requests. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGranular networking controls\u003c/strong\u003e: The private network allows for precise control over what resources the Cloud Run service can access.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4. Automatic cleanup\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo maintain a lightweight and secure runtime, the system delivers:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEphemeral execution\u003c/strong\u003e: Each execution request is handled in a stateless, temporary environment.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAutomatic deletion\u003c/strong\u003e: Files, logs, and temporary directories are wiped as soon as execution completes, leaving no residual data.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"how-code-runner-works-a-step-by-step-overview\"\u003e\u003cstrong\u003eHow Code Runner works: A step-by-step overview\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow that you have an understanding of the powerful infrastructure underneath Code Runner, here is a summary of how the feature works from start to finish:\u0026nbsp;\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eCode submission\u003c/strong\u003e: A user requests code execution from the evaluation interface.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLanguage detection\u003c/strong\u003e: The system detects the programming language and forwards the request to the corresponding Cloud Run service.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eExecution\u003c/strong\u003e: The Cloud Run instance spins up a container, executes the code in a sandboxed environment, and collects the results.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResult delivery\u003c/strong\u003e: The system returns the output (stdout, stderr, execution time, and any warnings) to the user for analysis.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCleanup\u003c/strong\u003e: The container and all related resources are terminated and deleted.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"advantages-of-labelbox%E2%80%99s-built-in-code-execution\"\u003e\u003cstrong\u003eAdvantages of Quantumworks Lab’s built-in code execution\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCode Runner’s infrastructure was designed specifically to provide the previously discussed benefits and to address several key challenges that other solutions may face:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSecurity\u003c/strong\u003e: By isolating execution environments and ensuring no public exposure, we eliminate a significant attack surface.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability\u003c/strong\u003e: Cloud Run’s serverless nature allows us to scale dynamically with demand, handling thousands of requests efficiently.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eReliability\u003c/strong\u003e: The use of ephemeral containers ensures that each execution starts in a clean slate, avoiding cross-contamination or resource conflicts.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"explore-it-yourself\"\u003e\u003cstrong\u003eExplore it yourself\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWith Code Runner, we’re empowering users to go beyond static evaluations, enabling dynamic, interactive testing that’s as secure as it is scalable. As always, we’re excited to hear your feedback and explore how we can push this feature even further.\u003c/p\u003e\u003cp\u003e If you want to explore Code Runner and other LLM evaluation tools, \u003ca href=\"https://app.labelbox.com/signup?_r=https://landing-page-git-uv-homepage-refresh-dec-24-labelbox.vercel.app/?utm_keyword=Quantumworks Lab\u0026utm_source=house\u0026utm_medium=email\u0026utm_campaign=1224%2520\u0026gclid=CjwKCAiA34S7BhAtEiwACZzv4a9veoKXnMnMvo2rWJvXkH46oHs4Lb5VFQi2ERBN_sQ5kgypV_zfBxoC0yMQAvD_BwE\u0026landingPageAnonymousId=%22e3f2f82f-be24-4045-b2b9-50a49cb801e8%22\u0026referrer_url=https://landing-page-git-uv-homepage-refresh-dec-24-labelbox.vercel.app/\"\u003e\u003cu\u003esign up\u003c/u\u003e\u003c/a\u003e for our platform today.\u0026nbsp;\u003c/p\u003e\u003cp\u003eStay tuned for updates, and happy coding!\u003c/p\u003e","comment_id":"6765b8c06f63bf0001f1ca72","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/12/Labelbox-code-runner--1-.png","featured":false,"visibility":"public","created_at":"2024-12-20T10:34:40.000-08:00","updated_at":"2025-03-12T12:01:43.000-07:00","published_at":"2024-12-20T12:44:45.000-08:00","custom_excerpt":"Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6765bb156f63bf0001f1ca8d","name":"Dmytro Apollonin","slug":"dmytro","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/dmytro/"}],"tags":[{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},{"id":"5f9af89dd8118b0039136165","name":"Engineering","slug":"engineering","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/engineering/"},{"id":"65302fc14e99900001fc0527","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-llms/"},{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"},{"id":"65303cb64e99900001fc05a5","name":"Labeling automation","slug":"labeling-automation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/labeling-automation/"},{"id":"6530313c4e99900001fc0537","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6765bb156f63bf0001f1ca8d","name":"Dmytro Apollonin","slug":"dmytro","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/dmytro/"},"primary_tag":{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},"url":"https://labelbox.ghost.io/blog/code-runner-secure-scalable-code-execution-for-model-evaluation-2/","excerpt":"Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6760b00aa9b5bd0001989da5","uuid":"d216f74b-532e-4556-8777-7630b460b9dd","title":"Advance LLM reasoning with advanced fact-checking and prompt rating tools","slug":"advance-llm-reasoning-with-advanced-fact-checking-and-prompt-rating-tools","html":"\u003cp\u003eLarge language models (LLMs) have made significant strides in recent years, but significant opportunities still exist to improve their reasoning and accuracy. Frontier models are expected to think critically, explain their logic, and produce reliable and accurate results.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo address these challenges, we are thrilled to announce two new features to assist AI teams in the advancement of frontier and task-specific models. We have expanded our multi-step reasoning tool to make it easy for raters to review the accuracy of each part of a complex response. In addition, a new prompt rating feature allows you to analyze prompts for compliance with specific guidelines to ensure raters spend time on valid responses and report poor prompts.\u0026nbsp;\u003c/p\u003e\u003cp\u003eRead on to learn more about how these features can help improve your model’s critical thinking and generate more accurate responses. You can see them in action as well through the interactive demos below.\u003c/p\u003e\u003ch2 id=\"simplify-the-evaluation-of-complex-prompts-and-responses\"\u003eSimplify the evaluation of complex prompts and responses\u003c/h2\u003e\u003cp\u003eLast month, we announced the release of a powerful new annotation type in our \u003ca href=\"https://labelbox.com/blog/new-multimodal-chat-evaluations-experience/?ref=labelbox.ghost.io\"\u003e\u003cu\u003emultimodal chat solution\u003c/u\u003e\u003c/a\u003e (MMC), \u003ca href=\"https://labelbox.com/blog/multi-step-reasoning-teach-llms-to-think-critically/?ref=labelbox.ghost.io\"\u003e\u003cu\u003emulti-step reasoning\u003c/u\u003e\u003c/a\u003e. Multi-step reasoning improves LLM training by automating the breakdown of complex responses into smaller, manageable steps. Individual evaluators can then score and, when necessary, rewrite a specific step, leading to improved model understanding and more accurate outputs.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOur comprehensive Quantumworks Lab platform now includes these two key features:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eFact-checking tasks:\u003c/strong\u003e Labelers can assess the accuracy of complex reasoning responses by guiding the Quantumworks Lab platform to automatically split complex responses into smaller, manageable pieces of information. Each piece of information can be individually rated—with options to include justifications and corrections for disputed claims.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrompt rating tasks\u003c/strong\u003e: Issues with the prompt itself can now be instantly flagged for not meeting pre-defined criteria, such as being unratable, false, offensive, controversial, or self-contained. Quantumworks Lab's customizable ontology also allows for additional criteria to be added. When a prompt is flagged, any required tasks associated with it becomes optional, giving labelers the ability to skip bad prompts and focus on high-value entries.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox's new features are the latest example of how the team is committed to generating the highest-quality data and model evaluations in the industry. By adding another powerful feature in our set of quality control tools, we can help you achieve greater precision in your data and develop more accurate AI models.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"how-does-fact-checking-and-prompt-rating-work-in-labelbox\"\u003eHow does fact checking and prompt rating work in Quantumworks Lab?\u003c/h2\u003e\u003cp\u003eWith the addition of these new features in Quantumworks Lab’s multimodal chat editor, you can now easily determine the veracity of model responses as well as identify and flag any issues with a given prompt.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHere’s how to use the fact-checking feature in Quantumworks Lab’s platform:\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003eCreate a new project using the Multimodal chat task type and click to edit or create the ontology.\u0026nbsp;\u003c/li\u003e\u003cli\u003eGo to “Message step tasks” and select the radio button next to “Factual.” Give the task a name and review the options. Click save when you are done to complete the ontology configuration.\u0026nbsp;\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd7V-dojiecR1_Sjz87op6kX81-Hwp89Qwe5-rcFwxFp1rB-_JHzm5rLOdgFSGLpsedZGT-2mdZWmqDxrPDdmGlvWYfhtvqiT_AK7_9HyeazchClJ-SUnwpvhtLLQrCA8joWKC4kA?key=1KGbUP_5-jRgULByV3dYLmKI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"216\" height=\"236\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSelect the fact check statements task in the ontology setup to automatically classify your model’s response accuracy.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003col start=\"3\"\u003e\u003cli\u003eAfter choosing a model(s) to evaluate and clicking Start labeling, enter a prompt to generate a model response (or multiple responses if evaluating more than one model).\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce the response is generated, click on “Fact check statements” on the left-hand side of the screen if it is not already selected. The multimodal chat editor will automatically split responses into individual steps and allow you to classify them as “ Accurate”, “Inaccurate”, “Disputed”, “Unsupported”, “Can’t confidently assess”, or “No factual information”.\u0026nbsp;\u003c/li\u003e\u003cli\u003eEvaluate and rate each step individually. If you select either “Accurate”, “Inaccurate” or “Disputed”, you will be asked to input additional justification.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdGirCisJZBpbIN0FoZuPVoU8VYld52OBfGbIZHBOtCYI1GTMIPE1NTWldR_kS_7kFr5VE6UuBlhOum1HmX0vHR6B9FaOMU7G0NJDw4RMK5VkA4m0i85ohvHc-6n5Vi0bgmjufe?key=1KGbUP_5-jRgULByV3dYLmKI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"271\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\u0026nbsp;If the step has been marked as either “Accurate”, “Inaccurate” or “Disputed”, the user is prompted to add a justification to the rating. For all other classifications, you will not be asked to add additional information.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003col start=\"6\"\u003e\u003cli\u003eIterate through this process until all steps have been fact checked.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThe new fact-checking feature provides a straightforward and effective process to generate high-quality and accurate responses.\u0026nbsp;\u003c/p\u003e\u003cp\u003eSee Quantumworks Lab’s new fact-checking feature in action \u003ca href=\"https://app.arcade.software/flows/tpITHjGYUAPC7Anoo4oO/view?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHere’s how to use the prompt rating feature in Quantumworks Lab’s platform:\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003eCreate a new project using the Multimodal chat task type, and click to create or edit the ontology.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWithin the ontology configuration screen, add a “Prompt rating task” to the project. Enter a name for the task and then review and edit the options. Options can be configured using checklists, radio buttons, or free text fields.\u0026nbsp;If any of these pre-defined criteria are selected during labeling, then the entire conversation will be marked unratable and can be skipped.\u0026nbsp;\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXf_0QBvmaPhj0MWMiX-16Woaafcy8FU386HrHN91dwc7YDaEg8UPYMz3YwBOSkwgSeCDcsgZuj5Am1zPhCEkcSVQIQyQjflhUY38kQ8hZ0Fjh8-vs-5n0eVDZeM_ZzTu7P-zm4ngQ?key=1KGbUP_5-jRgULByV3dYLmKI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"180\" height=\"260\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eConfigure a prompt rating task in the ontology setup to easily rate the quality of your prompt.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003col start=\"3\"\u003e\u003cli\u003eAfter choosing a model(s) to evaluate and clicking Start labeling, enter a prompt to generate a model response(s).\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce the response is generated, you can flag any issues with the prompt. If any of the pre-defined options for the prompt issue are selected, the red asterisk will be removed from the response task and the labeler will have the option to skip labeling for that response.\u0026nbsp; \u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdZHRLrS0iyKx-1kSgMX6LGNF5G2Q_JcsR_CLOmPItHp9C8j1leHSAayHU4MOrIjYHkmPJr3duqTMCi9nfMNl5MGiGVPZ4IqRe1GFVG8KeurxIOc9q5AOdho8s4RxsaP01InAcw?key=1KGbUP_5-jRgULByV3dYLmKI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"275\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe red asterisk next to tasks under \"Response tasks\" indicates that labeling is still required, as a predefined option for \"Issues with the prompt?\" has not been selected. If an issue was flagged for the prompt, the asterisk would disappear, making labeling for that task optional.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBy carefully crafting and evaluating prompts, we can significantly improve the overall quality and relevance of LLM outputs. In addition we can help improve the efficiency and utility of the time spent rating responses.\u003c/p\u003e\u003cp\u003eSee Quantumworks Lab's new prompt rating feature in action \u003ca href=\"https://app.arcade.software/flows/CDRZAk0Xwaj2l6p91bEq/view?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehere.\u003c/u\u003e\u003c/a\u003e\u0026nbsp; \u003c/p\u003e\u003ch2 id=\"achieve-further-advanced-reasoning-with-fact-checking-and-prompt-rating\"\u003eAchieve further advanced reasoning with fact-checking and prompt rating\u003c/h2\u003e\u003cp\u003eBy ensuring data quality and accuracy with our new quality control mechanisms, Quantumworks Lab can generate key datasets to train LLMs on complex reasoning and decision-making. Critical steps towards agentic reasoning that are supported by Quantumworks Lab’s fact-checking and prompt rating features include:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDirectly improve accuracy: \u003c/strong\u003eFact-checking and prompt rating enhance LLM data quality by identifying and correcting inaccuracies and ensuring clear prompts.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProvide valuable human feedback: \u003c/strong\u003eBoth features help bridge the gap between human and machine intelligence by serving as human-in-the-loop processes that provide expert guidance to the model's learning workflows.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRefine reasoning: \u003c/strong\u003eBy providing tools for justifications and corrections, labelers enable the model to learn from its mistakes, resulting in more accurate and reliable responses.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"the-future-of-ai-starts-with-labelbox\"\u003eThe future of AI starts with Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThe addition of fact-checking and prompt rating tools marks a major advancement in training LLMs for complex and agentic reasoning tasks. These quality control features enable granular rating and classification of both prompts and model responses, ensuring the generation of high-quality, accurate training data.\u003c/p\u003e\u003cp\u003eWant to learn more? \u003c/p\u003e\u003cul\u003e\u003cli\u003eTry a \u003ca href=\"https://labelbox.com/product-demos/?ref=labelbox.ghost.io\"\u003e\u003cu\u003equick, interactive tour\u003c/u\u003e\u003c/a\u003e\u0026nbsp; into the demos for our fact checking and prompt rating features\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/multi-step-reasoning-teach-llms-to-think-critically/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eLearn more\u003c/u\u003e\u003c/a\u003e about our multi-step reasoning feature and how it helps train LLMs to think more critically.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eContact our team\u003c/u\u003e\u003c/a\u003e anytime with questions or if you are ready to discuss your LLM training needs and how Quantumworks Lab might be able to help.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"\"\u003e\u003c/h1\u003e","comment_id":"6760b00aa9b5bd0001989da5","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/12/llm-reasoning--2-.png","featured":false,"visibility":"public","created_at":"2024-12-16T14:56:10.000-08:00","updated_at":"2025-06-16T10:09:59.000-07:00","published_at":"2024-12-17T12:31:57.000-08:00","custom_excerpt":"Quantumworks Lab's new fact-checking and prompt-rating tools improve LLM accuracy and reasoning capabilities by allowing users to evaluate responses, correct errors, and flag bad prompts.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"}],"tags":[{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},{"id":"65302fc14e99900001fc0527","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-llms/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"},{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"}],"primary_author":{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"},"primary_tag":{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},"url":"https://labelbox.ghost.io/blog/advance-llm-reasoning-with-advanced-fact-checking-and-prompt-rating-tools/","excerpt":"Quantumworks Lab's new fact-checking and prompt-rating tools improve LLM accuracy and reasoning capabilities by allowing users to evaluate responses, correct errors, and flag bad prompts.","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6757cf3acb24830001b1d4b1","uuid":"3ff80ee2-cd3b-4596-9a78-88c340d62032","title":"Inside the matrix: A look into the math behind AI","slug":"inside-the-matrix-a-look-into-the-math-behind-ai","html":"\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eMatrices are omnipresent in math and computer science, both theoretical and applied. They are often used as data structures, such as in graph theory. They are a computational workhorse in many AI fields, such as deep learning, computer vision and natural language processing. Why is that? Why would a rectangular array of numbers, with famously unintuitive multiplication rules, be so prevalent in AI?\u003c/p\u003e\u003cp\u003eAI methods (with emphasis on machine learning) are all about processing multi-dimensional data. A lot of that processing is done in a linear way - input data points are multiplied by scalars and added together to create output data. While that sounds limiting, a lot can be achieved with just that, for example:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLinear layers in neural networks (excluding possibly non-linear activations)\u003c/li\u003e\u003cli\u003ePrincipal component analysis\u003c/li\u003e\u003cli\u003eWord embeddings\u003c/li\u003e\u003cli\u003eImage processing\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis particular kind of processing data has a name in mathematics - linear map, which is a concept from linear algebra. I will formally define it later in this blog. For now it suffices to say that you can represent any linear map as a matrix, and matrix operations are intimately connected to operations on linear maps. This is the main reason why matrices are so ubiquitous in AI.\u003c/p\u003e\u003cp\u003eWhile you don't necessarily need to know linear algebra to do machine learning, it is very helpful to have a good intuition for the concepts. This blog is an attempt to demystify matrices and linear algebra surrounding them in a way that strikes a balance between mathematical rigor and intuitive understanding. Contrary to most introductory material on the subject, we won't restrict ourselves to the usual Rn spaces, but I'll still give examples in R2 and R3 for clarity. If you don't know what R\u003csup\u003e2\u003c/sup\u003e, R\u003csup\u003e3\u003c/sup\u003e and R\u003csup\u003en\u003c/sup\u003e are, don't worry - we'll get to that.\u003c/p\u003e\u003cp\u003eI will start with formally defining what a matrix is. We will gradually build up to understanding matrix multiplication, and by the end of the article we will have covered all the necessary concepts.\u003c/p\u003e\u003ch2 id=\"definition-of-a-matrix\"\u003e\u003cstrong\u003eDefinition of a matrix\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eA matrix is a rectangular array of numbers. The numbers are called elements of the matrix. The horizontal lines of elements are called rows, the vertical lines are called columns.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeE-ZtcxxSEfqHBbKD496nYV8uU8Gh85QW_nlkNA9LBe7PJhOm-N4b6LLx7oDfUFppbj0Mza3dQKv5GezLOb3OL6DyY0Uot0pxn_NU8xBOhUkRPx9AyUPX1eqzltwPcs519ZmAphg?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"306\" height=\"215\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e\u0026nbsp;A real-valued matrix with 2 rows and 3 columns.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThere are many operations we can perform on matrices, but for the purpose of this article we only need to know three:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAddition:\u003c/strong\u003e Element-wise addition, only matrices of the same dimensions can be added\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalar multiplication:\u003c/strong\u003e Multiplying each element by a scalar\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMatrix multiplication:\u003c/strong\u003e Multiplying two matrices, resulting in a matrix with dimensions equal to the number of rows in the second matrix and number of columns in the first matrix\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeHPKl6kqvGJrURhsqbjIz4gZ4lEl_ScId7w3a07BTMrr9O1wn30BKljl5ovYAENHIJbqGxa_tnuZk6Qud6OURSD2PnfD-t8azHRkm90mILIkvwE8m3tnw0kSBA9cuHuihQvSms?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"257\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eMatrix addition and scalar multiplication.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe first two operations are pretty straightforward, but matrix multiplication is a bit more complex. There are several algorithms to do it, all resulting in the same end product. \u003c/p\u003e\u003cp\u003eOne of the most common algorithms is this: For element in j-th row and k-th column of the resulting matrix, take the element-wise product of j-th row of the first matrix and k-th column of the second matrix, and sum all the elements of the resulting vector.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeR0jhCg2ufM0vZv_Yyp_6PklvhKVGJ_Zft_vzWMWywsN_GYnzEoeOH4McE3p1D4HLojhYrTzysbjYfAgQ2DxqoSwX0Iaxo4XeWN4tHvSBaNLBJgPreIUuPveGDeQcd_zoA9aLF?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"155\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eMatrix multiplication. To compute the element in the first row and first column of the resulting matrix (10), take the first row of the first matrix (2, 3, 4) and the first column of the second matrix (2, 2, 0), multiply them element-wise and sum the result (2 * 2 + 3 * 2 + 4 * 0 = 10). Note that the resulting matrix has dimensions equal to the number of rows in the first matrix and number of columns in the second matrix.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWhy is matrix multiplication not simply element-wise (which actually exists and is called Hadamard product)? What is the intuition behind it? To answer that, we need to understand several concepts from linear algebra.\u003c/p\u003e\u003ch2 id=\"what-is-a-vector\"\u003e\u003cstrong\u003eWhat is a vector?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eA vector has many definitions. Perhaps the most well known is the one from physics: an arrow in space with a direction and a magnitude, usually represented as coordinates in a space.\u003c/p\u003e\u003cp\u003eIf you are a programmer, you might know it as an array-like data structure, that is an ordered collection of elements. Both of these definitions are most likely known to you if you are working with AI systems, in which vectors often play a crucial role, e.g. as features in a dataset or as low-dimensional representations of high-dimensional data (embeddings).\u003c/p\u003e\u003cp\u003eWhat interests us here is the mathematical definition: A vector is an element of a vector space. What is a vector space then?\u003c/p\u003e\u003cp\u003eTo avoid spelling the whole mathematical definition here, it suffices to say that a vector space is a set of elements with following properties:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIt is possible to add any two elements and the result is also in the set.\u003c/li\u003e\u003cli\u003eIt is possible to multiply any element by a scalar (a real number) and the result is also in the set.\u003c/li\u003e\u003cli\u003eThere exists an element called zero vector, which is such that adding it to any element does not change the latter.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese two operations are connected by a distributive property, which states that scalar multiplication of a sum is the sum of scalar multiplications: a * (u + v) = a * u + a * v.\u003c/p\u003e\u003cp\u003eAs you have probably noticed, this is analogous to the operations of addition and multiplication on real numbers. The important thing to note is that the vector space definition does not require the vector by vector multiplication to be defined. It is defined for some vector spaces, but we won't concern ourselves with it in this blog.\u003c/p\u003e\u003cp\u003eAll of that is pretty abstract, so let's consider some examples. Possibly the most well known vector spaces are R\u003csup\u003e2\u003c/sup\u003e and R\u003csup\u003e3\u003c/sup\u003e, which are spaces of vectors with 2 and 3 coordinates respectively. The generalization of those is R\u003csup\u003en\u003c/sup\u003e, which is a space of vectors with n coordinates. The elements (vectors) of those spaces have a geometric interpretation as arrows or points. Notice how this aligns with the interpretation of vectors as known from physics.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXciYwCq1HZTGnHkYJWxNJex6c52A-UpTxDRkM1DbAOj2n-Vx9r5PPFqosNeQ6MjMzmjExEuWL6aDpUHJUKVF70rDWeMtbhdOFoJPjAR66btoxgXO5fCsyRl9vbAsJZiWvsmm568gw?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"544\" height=\"238\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eImages of R\u003c/em\u003e\u003c/i\u003e\u003csup style=\"white-space: pre-wrap;\"\u003e\u003cspan\u003e2\u003c/span\u003e\u003c/sup\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e and R\u003c/em\u003e\u003c/i\u003e\u003csup style=\"white-space: pre-wrap;\"\u003e\u003cspan\u003e3\u003c/span\u003e\u003c/sup\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e, with some vectors marked. It is common to draw vectors as arrows starting from the origin, but the vector is not defined by its anchor point. Graphically, all vectors with the same length and direction are the same vector.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"linear-combination-of-vectors-span-and-linear-independence\"\u003e\u003cstrong\u003eLinear combination of vectors, span and linear independence\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe know that vectors can be added and multiplied by scalars. A linear combination of vectors is a sum of scalar multiples of those vectors. Consider these examples in R\u003csup\u003e2\u003c/sup\u003e and R\u003csup\u003e3\u003c/sup\u003e:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXetIiIpcEGjnii7eRWzOwH3onH7IgJEGl8tkJ42x1IkjQI41rhP_n4hWW5EJLfCzV8e-oLFcfEVvAklY5tcdUDBjTGekkfJsxFDgVhm5VGXrfLRdm5lfQSi0pm90gEGugHW3XQ9Kg?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"505\" height=\"339\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eGeometric interpretation of linear combination of two vectors in R\u003c/em\u003e\u003c/i\u003e\u003csup style=\"white-space: pre-wrap;\"\u003e\u003cspan\u003e2\u003c/span\u003e\u003c/sup\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eA span of a finite set of vectors is a set of all possible linear combinations of those vectors. Geometrically, a span of one vector in R\u003csup\u003e2\u003c/sup\u003e and R\u003csup\u003e3\u003c/sup\u003e is a line through the origin (all scalar multiples of the vector), while a span of two vectors \u003cstrong\u003emight be\u003c/strong\u003e a plane through the origin (which is the whole space in case of R\u003csup\u003e2\u003c/sup\u003e).\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeS90Xw5U7e8SCrauz6PfboZLkMATSOTsQ8p-ugGkVlC4tiuXekAuTzHV6FYhXZHPHQOSXuiQ_poclCAKdRYm2bXKK_IQdRlI4Po-HHG1pllr_EmFwSaQLXIjQaw86d92NtsnMIdQ?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"419\" height=\"369\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eGeometric interpretation of the span of two vectors in R\u003c/em\u003e\u003c/i\u003e\u003csup style=\"white-space: pre-wrap;\"\u003e\u003cspan\u003e3\u003c/span\u003e\u003c/sup\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e. Their span is a plane through the origin (which is admittedly hard to draw).\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWhy might? Consider two vectors lying on the same line. No matter what scalar coefficients we take for the linear combination, the result will lie on the same line. Their individual spans are identical to the span of the set of those two vectors. We say that those two vectors are \u003cstrong\u003elinearly dependent\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eLet's consider a set of three vectors in R\u003csup\u003e3\u003c/sup\u003e. They can either all lie on the same line, all lie on the same plane, or they can be such that their span is the whole space. In the first two cases, the vectors are linearly dependent, in the last case they are \u003cstrong\u003elinearly independent\u003c/strong\u003e. Two vectors in R\u003csup\u003e2\u003c/sup\u003e spanning the whole space are also linearly independent.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXf8_KYv6dtal1uf07fhGwyO7Dq3aBbTydEuW84z2V0WOO-uFZInEdskzS6ztAWm5pvUevcUSSC-0PxpiH5F0X-4iNc4D98z157Vb7nfHcsb-3fE6VtEEu0AQZTN5I37wYxixRmuOQ?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"256\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eGeometric interpretation of spans of linearly dependent vectors in R\u003c/em\u003e\u003c/i\u003e\u003csup style=\"white-space: pre-wrap;\"\u003e\u003cspan\u003e3\u003c/span\u003e\u003c/sup\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e. All three vectors on the second image lie on the same plane (which contains the origin).\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRigorously speaking, a set of vectors in vector space is linearly independent if no vector in the set can be written as a linear combination of the other vectors in the set (in other words, it does not lie in the span of the rest of the vectors). Intuitively, vectors are linearly independent if each one of them contributes a unique \"direction\" to the span.\u003c/p\u003e\u003cp\u003eLinearly independent vectors have a very important property: each vector in their span can be \u003cstrong\u003euniquely\u003c/strong\u003e represented as a linear combination of those vectors.\u003c/p\u003e\u003ch2 id=\"basis\"\u003e\u003cstrong\u003eBasis\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNot all vector spaces can be spanned by a finite set of vectors. For example, the space of all polynomials (which with proper definition of addition and scalar multiplication is valid vector space) is infinite-dimensional, because no matter how many polynomials we have, we can always find a polynomial that has a higher degree than all of them.\u003c/p\u003e\u003cp\u003eContrarily, if we can find a finite set of vectors that spans the whole space, we say that the vector space is finite-dimensional. Only these spaces are relevant for the purposes of this blog.\u003c/p\u003e\u003cp\u003eEvery finite-dimensional vector space has a basis. A basis is a set of linearly independent vectors that span the whole space. In R\u003csup\u003e2\u003c/sup\u003e, a basis can consist of two non-collinear vectors. In R\u003csup\u003e3\u003c/sup\u003e, a basis can consist of three vectors that do not lie on the same plane. Note that a basis is not necessarily unique - e.g. in R\u003csup\u003en\u003c/sup\u003e, we can choose different vectors that will still span the same space and be linearly independent.\u003c/p\u003e\u003cp\u003eNotice how I stated that a basis in R\u003csup\u003e2\u003c/sup\u003e consists of two vectors, while in R\u003csup\u003e3\u003c/sup\u003e it consists of three vectors. A set of three vectors in R\u003csup\u003e2\u003c/sup\u003e is guaranteed to be linearly dependent, so it cannot be a basis. On the other hand, one vector cannot span R\u003csup\u003e2\u003c/sup\u003e. Likewise, we cannot find four linearly independent vectors in R\u003csup\u003e3\u003c/sup\u003e, but less than three vectors cannot span R\u003csup\u003e3\u003c/sup\u003e.\u003c/p\u003e\u003cp\u003eSkipping the proof, it is a true statement that every basis of a finite-dimensional vector space has the same number of vectors. This number is called the dimension of the vector space. By this, R\u003csup\u003en\u003c/sup\u003e has a dimension of n, which plays nicely with our geometric interpretation of R\u003csup\u003en\u003c/sup\u003e as n-dimensional space.\u003c/p\u003e\u003cp\u003eFor R\u003csup\u003en\u003c/sup\u003e, we define a standard basis as a set of n vectors, where each vector has exactly one non-zero coordinate, which is 1. For example, in R\u003csup\u003e3\u003c/sup\u003e, the standard basis is {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.\u003c/p\u003e\u003cp\u003eWhy do we care about a basis? Since a basis is a set of linearly independent vectors that span the whole space, any vector in that space can be \u003cstrong\u003euniquely\u003c/strong\u003e represented as a linear combination of vectors from the basis. That means that for every finite-dimensional vector space, no matter how abstract or exotic, we can always represent any of its vectors as a list of numbers (coefficients of linear combination of chosen basis).\u003c/p\u003e\u003ch2 id=\"linear-maps\"\u003e\u003cstrong\u003eLinear maps\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow we are ready to define a linear map. A linear map (sometimes also called linear transformation), defined for vector spaces V and W, is a function that takes a vector from V as an input and returns a vector from W. It must satisfy two properties:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAdditivity:\u003c/strong\u003e f(u + v) = f(u) + f(v)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHomogeneity:\u003c/strong\u003e f(a * u) = a * f(u)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese properties ensure that linear maps preserve the linear structure of vector spaces - parallel vectors remain parallel, and origin remains at the origin.\u003c/p\u003e\u003cp\u003eWhile it may seem that linear maps are a very restricted class of functions, they can be used to represent a wide variety of transformations. For example, a linear map in R\u003csup\u003e2\u003c/sup\u003e can be used to represent:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRotation\u003c/li\u003e\u003cli\u003eScaling\u003c/li\u003e\u003cli\u003eReflection\u003c/li\u003e\u003cli\u003eShearing\u003c/li\u003e\u003cli\u003eAny combination of the above\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOne operation you cannot represent with a linear map is translation (moving every point by the same vector), since the origin must remain at the origin.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfcM7o_gKJk2c7hx0wjIEoZmu2OHveEQvO065rsVLYH5Ca5-v_vyWj9OGREyFacWuopomsh0gm9WLDkvOewl5S8JPJmeq1TRQwQrRab4klX2oJT-Tg0fqJ6rkBA3B6zps5tNFLIbg?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"366\" height=\"318\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eShear operation in R\u003c/em\u003e\u003c/i\u003e\u003csup style=\"white-space: pre-wrap;\"\u003e\u003cspan\u003e2\u003c/span\u003e\u003c/sup\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAs mentioned in the introduction, they are also extremely prevalent in machine learning, e.g. in neural networks. While I am not going to prove this, the connections (weights) between linear layers are equivalent to linear maps from R\u003csup\u003en\u003c/sup\u003e to R\u003csup\u003em\u003c/sup\u003e, where n is the dimension of the input and m is the dimension of the output. Since you can represent these connections as a matrix, it's a hint that \u003cstrong\u003ematrices are intimately connected to linear maps\u003c/strong\u003e. We'll come back to this later.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeCp-3F2w7zsnKxYYjLu9Kp02nGlruiHhAbHii9zsGLP8xUvIQjpe9LIjxCrrV8nW5kaLXPYl7iwYc8hXUtfjOBnTS8sDwn-nWbgXz_DsQIN7UjOHG4rlRcGLoD5AOy5DVCrQD9uA?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"178\" height=\"324\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eTwo fully connected layers in a neural network without activation functions or biases. They are equivalent to a linear map from R\u003c/em\u003e\u003c/i\u003e\u003csup style=\"white-space: pre-wrap;\"\u003e\u003cspan\u003e5\u003c/span\u003e\u003c/sup\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e to R\u003c/em\u003e\u003c/i\u003e\u003csup style=\"white-space: pre-wrap;\"\u003e\u003cspan\u003e7\u003c/span\u003e\u003c/sup\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e - each input is a vector in R\u003c/em\u003e\u003c/i\u003e\u003csup style=\"white-space: pre-wrap;\"\u003e\u003cspan\u003e5\u003c/span\u003e\u003c/sup\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e, and each output is a vector in R\u003c/em\u003e\u003c/i\u003e\u003csup style=\"white-space: pre-wrap;\"\u003e\u003cspan\u003e7\u003c/span\u003e\u003c/sup\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe can define a set of operations on linear maps (let T and S be linear maps, and c an arbitrary scalar):\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAddition: \u003c/strong\u003e(T + S)(v) = T(v) + S(v)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalar multiplication:\u003c/strong\u003e (c * T)(v) = c * T(v)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComposition: \u003c/strong\u003e(T * S)(v) = T(S(v))\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eA very important property of linear maps is that \u003cstrong\u003ea linear map is fully defined by its values on basis vectors\u003c/strong\u003e, for a chosen basis of input space. Let's say we have a linear map T and a basis B = {b\u003csub\u003e1\u003c/sub\u003e, b\u003csub\u003e2\u003c/sub\u003e, ..., b\u003csub\u003en\u003c/sub\u003e} of input space V. For any v in V, we can write v as a linear combination of basis vectors: v = c\u003csub\u003e1\u003c/sub\u003e * b\u003csub\u003e1\u003c/sub\u003e + c\u003csub\u003e2\u003c/sub\u003e * b\u003csub\u003e2\u003c/sub\u003e + ... + c\u003csub\u003en\u003c/sub\u003e * b\u003csub\u003en\u003c/sub\u003e. Then T(v) = c\u003csub\u003e1\u003c/sub\u003e * T(b\u003csub\u003e1\u003c/sub\u003e) + c\u003csub\u003e2\u003c/sub\u003e * T(b\u003csub\u003e2\u003c/sub\u003e) + ... + c\u003csub\u003en\u003c/sub\u003e * T(b\u003csub\u003en\u003c/sub\u003e). This means that if we know what T does to each vector in the basis, we know what T does to any vector in the space.\u003c/p\u003e\u003ch2 id=\"matrix-as-representation-of-linear-map\"\u003e\u003cstrong\u003eMatrix as representation of linear map\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe know that every linear map T: V -\u0026gt; W (where V has dimension n and W has dimension m) and for a chosen basis Bv = {v\u003csub\u003e1\u003c/sub\u003e, v\u003csub\u003e2\u003c/sub\u003e, ..., v\u003csub\u003en\u003c/sub\u003e} of V, T is fully defined by values of T(v\u003csub\u003e1\u003c/sub\u003e), T(v\u003csub\u003e2\u003c/sub\u003e), ..., T(v\u003csub\u003en\u003c/sub\u003e).\u003c/p\u003e\u003cp\u003eLet's take a closer look at one of those values, say T(v\u003csub\u003e1\u003c/sub\u003e). We know that T(v\u003csub\u003e1\u003c/sub\u003e) is a vector in W, so we can write it as a linear combination of a chosen basis of W: T(v\u003csub\u003e1\u003c/sub\u003e) = d\u003csub\u003e1\u003c/sub\u003e * w\u003csub\u003e1\u003c/sub\u003e + d\u003csub\u003e2\u003c/sub\u003e * w\u003csub\u003e2\u003c/sub\u003e + ... + d\u003csub\u003em\u003c/sub\u003e * w\u003csub\u003em\u003c/sub\u003e. We can represent the coefficients of this linear combination as a list of numbers: [d\u003csub\u003e1\u003c/sub\u003e, d\u003csub\u003e2\u003c/sub\u003e, ..., d\u003csub\u003em\u003c/sub\u003e]. This list is uniquely determined by the linear map T and the chosen basis of V, and it is called the column vector representation of T(v\u003csub\u003e1\u003c/sub\u003e).\u003c/p\u003e\u003cp\u003eWe can do the same for T(v\u003csub\u003e2\u003c/sub\u003e), T(v\u003csub\u003e3\u003c/sub\u003e), and so on, up to T(v\u003csub\u003en\u003c/sub\u003e). In this way, we can associate with our linear map T a matrix A, where the j-th column is the column vector representation of T(v\u003csub\u003ej\u003c/sub\u003e). This matrix A is called the matrix representation of the linear map T. That is, \u003cstrong\u003ea matrix with m rows and n columns can be interpreted as a representation of a linear map from linear space of dimension n to linear space of dimension m\u003c/strong\u003e. When representing a linear map with a matrix, choice of bases is important - different bases will yield different matrices. That is why when not clear from the context, we must specify the bases for both the input and output spaces.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe36MNl-rRXhG4KwL8YAyQToQLcJq13rDEanIpgYDVG9ge7XCFf-IEVdHSvJdKPzMLQV_H4xzGzERyFyurj_kIzXuZqY0RQpAt9wri3SCB2oYnRHMcnTlJnFqULPTq_4gjZMakp_g?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"318\" height=\"242\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eA representation of a particular linear map T: V -\u0026gt; W as a matrix. V has dimension 3, while W has dimension 2. Thus, the matrix has 2 rows and 3 columns.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIt is important to note that such a matrix can be created for any linear map between finite-dimensional vector spaces - it is not limited to R\u003csup\u003en\u003c/sup\u003e. The common misconception is that the columns or rows of such a matrix are vectors, but in general they are not - they are coefficients of linear combinations. It is a coincidence that in the case of T: R\u003csup\u003en\u003c/sup\u003e -\u0026gt; R\u003csup\u003em\u003c/sup\u003e and if we choose the standard basis for R\u003csup\u003em\u003c/sup\u003e, the columns of the matrix representation are vectors in R\u003csup\u003en\u003c/sup\u003e.\u003c/p\u003e\u003ch2 id=\"matrix-operations-as-operations-on-linear-maps\"\u003e\u003cstrong\u003eMatrix operations as operations on linear maps\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf matrices represent linear maps, do the matrix operations represent operations on linear maps? It turns out that they do.\u003c/p\u003e\u003cp\u003eIt is easy (albeit a bit tedious) to prove, that if A and B are matrices representing linear maps T and S, then:\u003c/p\u003e\u003cul\u003e\u003cli\u003eA + B represents the linear map T + S\u003c/li\u003e\u003cli\u003ec * A represents the linear map c * T\u003c/li\u003e\u003cli\u003eA * B represents the linear map T * S\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eKeep in mind that matrix multiplication is not commutative, i.e. A * B is not necessarily the same as B * A (even if both make sense dimension-wise).\u003c/p\u003e\u003cul\u003e\u003cli\u003eMatrix addition is only defined for matrices of the same dimensions, which is consistent with the fact that addition of linear maps is only defined for maps between the same spaces.\u003c/li\u003e\u003cli\u003eMatrix multiplication by scalar is defined for any matrix, which is consistent with the fact that scalar multiplication of linear maps is defined for any linear map.\u003c/li\u003e\u003cli\u003eMatrix multiplication is defined for any two matrices, provided that the number of columns in the first matrix is the same as the number of rows in the second matrix. This is consistent with the fact that composition of linear maps is only defined for maps where the output space of the first map is the same as the input space of the second map.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile adding and multiplying by scalar are pretty intuitive operations, matrix multiplication is not that obvious. \u003cstrong\u003eIt has actually been defined so that composition of linear maps is represented by multiplication of their matrices\u003c/strong\u003e, which you are encouraged to verify.\u003c/p\u003e\u003cp\u003eAside from linear map composition, matrix multiplication can be used to apply a linear map to a vector. Say we have a linear map T: V -\u0026gt; W represented by matrix A, and a vector v in V. First we need to represent v as a nx1 column matrix. We'll call it Mat(v). Then we can compute A * Mat(v), which will be a vector in W.\u003c/p\u003e\u003cp\u003eThis type of matrix multiplication can be computed as follows, where a\u003csub\u003ej\u003c/sub\u003e is the j-th column of A, while m\u003csub\u003e1j\u003c/sub\u003e is the j-th element of Mat(v): \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/12/Screenshot-2024-12-13-at-11.22.37-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"631\" height=\"199\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/12/Screenshot-2024-12-13-at-11.22.37-AM.png 600w, https://labelbox.ghost.io/blog/content/images/2024/12/Screenshot-2024-12-13-at-11.22.37-AM.png 631w\"\u003e\u003c/figure\u003e\u003cp\u003eApplying T to v can be written as T(v) = c\u003csub\u003e1\u003c/sub\u003e * T(v\u003csub\u003e1\u003c/sub\u003e) + c\u003csub\u003e2\u003c/sub\u003e * T(v\u003csub\u003e2\u003c/sub\u003e) + ... + c\u003csub\u003en\u003c/sub\u003e * T(v\u003csub\u003en\u003c/sub\u003e).\u003c/p\u003e\u003cp\u003eRemembering that a column j of matrix A represents T(v\u003csub\u003ej\u003c/sub\u003e), we can see that multiplying A by Mat(v) is indeed equivalent to applying T to v.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXevqSOQivzWeKcTeOPhUmNT4z0psVVGyrYtls1oLPQGJQnCcVW8QlGLQuM2-iS5fNegBS3CtgeAQ9PKokTe3FmxOHAnoui6DdLwTr7eUN0NPFP9qafWCA5-a1Qo18nur2ApU4g_?key=ODPo2fm0CLrLSvUFi5qEnIvd\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"248\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eA particular way to perform matrix multiplication when the second operand is a column matrix.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe have now covered all the necessary concepts from linear algebra to understand matrix multiplication. We have seen that matrices can be interpreted as representations of linear maps, and that matrix operations can be interpreted as operations on linear maps. We have also seen that matrix multiplication is not just an arbitrary operation, but rather a composition of linear maps. While this blog barely scratched the surface of math involved in creating AI systems, hopefully it gave you a good intuition on the subject of linear maps and matrices.\u003c/p\u003e\u003cp\u003eIn conclusion, matrices are a fundamental tool in the AI toolkit, enabling efficient data manipulation and transformation. Whether you're building a simple linear regression model or a complex deep learning architecture, a solid grasp of matrix operations will empower you to create more effective and efficient AI solutions.\u003c/p\u003e","comment_id":"6757cf3acb24830001b1d4b1","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/12/Matrices.png","featured":false,"visibility":"public","created_at":"2024-12-09T21:18:50.000-08:00","updated_at":"2025-03-12T11:59:32.000-07:00","published_at":"2024-12-11T10:38:57.000-08:00","custom_excerpt":"Matrices are crucial in AI for processing multi-dimensional data in areas like machine learning and computer vision. They represent linear maps and transform input into output, making them central to many AI methods.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6706c454eeb2b1000180d550","name":"Michał Jóźwiak","slug":"michal","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/michal/"}],"tags":[{"id":"5f9af89dd8118b0039136165","name":"Engineering","slug":"engineering","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/engineering/"},{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},{"id":"65302fc14e99900001fc0527","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-llms/"}],"primary_author":{"id":"6706c454eeb2b1000180d550","name":"Michał Jóźwiak","slug":"michal","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/michal/"},"primary_tag":{"id":"5f9af89dd8118b0039136165","name":"Engineering","slug":"engineering","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/engineering/"},"url":"https://labelbox.ghost.io/blog/inside-the-matrix-a-look-into-the-math-behind-ai/","excerpt":"Matrices are crucial in AI for processing multi-dimensional data in areas like machine learning and computer vision. They represent linear maps and transform input into output, making them central to many AI methods.","reading_time":14,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}]},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"6-key-llms-to-power-your-text-based-ai-applications"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/blog/6-key-llms-to-power-your-text-based-ai-applications/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:15:31 GMT -->
</html>