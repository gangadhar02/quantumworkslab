<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/blog/use-automation-to-reduce-your-labeling-time-and-spend/?ref=labelbox.ghost.io by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:53:46 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Use automation to reduce your labeling time and spend</title><meta name="description" content="Quantumworks Lab is excited to introduce our image auto-segmentation tool to help teams quickly generate and edit quality segmentation masks in seconds. " data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Use automation to reduce your labeling time and spend" data-next-head=""/><meta property="og:description" content="Quantumworks Lab is excited to introduce our image auto-segmentation tool to help teams quickly generate and edit quality segmentation masks in seconds. " data-next-head=""/><meta property="og:url" content="https://labelbox.ghost.io/blog/use-automation-to-reduce-your-labeling-time-and-spend/" data-next-head=""/><meta property="og:image" content="https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--65-.gif" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Use automation to reduce your labeling time and spend" data-next-head=""/><meta name="twitter:description" content="Quantumworks Lab is excited to introduce our image auto-segmentation tool to help teams quickly generate and edit quality segmentation masks in seconds. " data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.ghost.io/blog/use-automation-to-reduce-your-labeling-time-and-spend/" data-next-head=""/><meta property="twitter:image" content="https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--65-.gif" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g48[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.eivcj #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.eivcj .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.eivcj .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.eivcj #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.eivcj #image-viewer .close:hover,.eivcj #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.eivcj .modal-content{width:100%;}}/*!sc*/
data-styled.g105[id="ImageModal__ImageModalWrapper-sc-1ey7m7r-0"]{content:"eivcj,"}/*!sc*/
.QsqTL .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.QsqTL .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.QsqTL .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:20px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h2{padding-top:10px;}}/*!sc*/
.QsqTL .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h3{padding-top:10px;}}/*!sc*/
.QsqTL .content h4{font-size:20px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 16px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h4{padding-top:8px;}}/*!sc*/
.QsqTL .content h5{font-size:18px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 14px;}/*!sc*/
.QsqTL .content h6{font-size:16px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 12px;}/*!sc*/
.QsqTL .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.QsqTL .content a:hover{color:#1e40af;}/*!sc*/
.QsqTL .content li{margin-bottom:20px;}/*!sc*/
.QsqTL .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.QsqTL .content ol{list-style:decimal;padding-left:20px;}/*!sc*/
.QsqTL .content .table-container{overflow-x:auto;margin:40px 0;-webkit-overflow-scrolling:touch;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-container{margin:30px -20px;padding:0 20px;}}/*!sc*/
.QsqTL .content table{width:100%;border-collapse:collapse;font-size:16px;background:white;border:1px solid #e5e7eb;border-radius:8px;overflow:hidden;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content table{font-size:14px;}}/*!sc*/
.QsqTL .content .table-container table{margin:0;min-width:600px;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-container table{min-width:700px;border-radius:0;border-left:none;border-right:none;}}/*!sc*/
.QsqTL .content .content table:not(.table-container table){margin:40px 0;min-width:auto;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .content table:not(.table-container table){margin:30px 0;min-width:auto;border-radius:8px;border:1px solid #e5e7eb;}}/*!sc*/
.QsqTL .content thead{background:#fafbfc;border-bottom:1px solid #d1d5db;}/*!sc*/
.QsqTL .content th{padding:16px 20px;text-align:left;font-weight:600;color:#374151;font-size:14px;-webkit-letter-spacing:0.025em;-moz-letter-spacing:0.025em;-ms-letter-spacing:0.025em;letter-spacing:0.025em;border-right:1px solid #f3f4f6;}/*!sc*/
.QsqTL .content th:last-child{border-right:none;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content th{padding:12px 16px;font-size:13px;}}/*!sc*/
.QsqTL .content td{padding:16px 20px;border-bottom:1px solid #f3f4f6;border-right:1px solid #f9fafb;color:#374151;line-height:1.5;}/*!sc*/
.QsqTL .content td:last-child{border-right:none;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content td{padding:12px 16px;}}/*!sc*/
.QsqTL .content tbody tr{-webkit-transition:background-color 0.2s ease;transition:background-color 0.2s ease;}/*!sc*/
.QsqTL .content tbody tr:hover{background-color:#f8fafc;}/*!sc*/
.QsqTL .content tbody tr:last-child td{border-bottom:none;}/*!sc*/
.QsqTL .content .table-wrapper{overflow-x:auto;margin:40px 0;border:1px solid #e5e7eb;border-radius:8px;-webkit-overflow-scrolling:touch;}/*!sc*/
.QsqTL .content .table-wrapper table{margin:0;border:none;border-radius:0;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-wrapper{margin:30px -20px;border-radius:0;border-left:none;border-right:none;}}/*!sc*/
.QsqTL .content code{background:#f1f5f9;padding:2px 6px;border-radius:4px;font-family:'Monaco','Menlo','Ubuntu Mono',monospace;font-size:14px;color:#e11d48;}/*!sc*/
.QsqTL .content pre{background:#1e293b;color:#e2e8f0;padding:20px;border-radius:8px;overflow-x:auto;margin:30px 0;}/*!sc*/
.QsqTL .content pre code{background:transparent;padding:0;color:inherit;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content pre{margin:20px -20px;border-radius:0;padding:16px 20px;}}/*!sc*/
.QsqTL .content blockquote{border-left:4px solid #2563eb;padding:20px 24px;margin:30px 0;background:#f8fafc;border-radius:0 8px 8px 0;font-style:italic;color:#475569;}/*!sc*/
.QsqTL .content blockquote p{margin-bottom:0;}/*!sc*/
.QsqTL .content blockquote p:last-child{margin-bottom:0;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content blockquote{margin:20px 0;padding:16px 20px;}}/*!sc*/
.QsqTL .content hr{border:none;height:1px;background:linear-gradient(to right,transparent,#e5e7eb,transparent);margin:50px 0;}/*!sc*/
.QsqTL .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.QsqTL .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;color:#6b7280;font-style:italic;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.QsqTL .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;border-radius:8px;}/*!sc*/
.QsqTL .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.QsqTL .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100%;height:100%;margin:0 auto;border-radius:8px;}/*!sc*/
.QsqTL .content .kg-bookmark-card{background:white;border-radius:10px;margin-top:60px !important;border:1px solid #e5e7eb;-webkit-transition:border-color 0.3s ease;transition:border-color 0.3s ease;}/*!sc*/
.QsqTL .content .kg-bookmark-card:hover{border-color:#d1d5db;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;color:#262626 !important;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-thumbnail{position:relative;min-width:30%;max-height:100%;overflow:hidden;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-thumbnail img{position:absolute;top:0;left:0;width:100% !important;height:100% !important;-o-object-fit:cover;object-position:left;object-fit:cover;border-radius:0 10px 10px 0;border-left:1px solid #f5f5f5;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;padding:20px;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-title{font-size:1.125rem;line-height:1.3;font-weight:600;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-description{font-size:0.875rem;font-weight:400;line-height:1.4;margin-top:12px;overflow-y:hidden;color:#6b7280;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;font-size:0.9rem;font-weight:400;margin-top:14px;color:#6b7280;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata img{width:22px !important;height:22px !important;margin-right:8px !important;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata .kg-bookmark-author{margin:4px;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata .kg-bookmark-publisher{margin:4px;}/*!sc*/
.QsqTL .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;margin:40px 0;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;margin-bottom:12px;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 6px;border-radius:6px;overflow:hidden;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;object-fit:cover;-webkit-transition:-webkit-transform 0.3s ease;-webkit-transition:transform 0.3s ease;transition:transform 0.3s ease;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row img:hover{-webkit-transform:scale(1.02);-ms-transform:scale(1.02);transform:scale(1.02);}/*!sc*/
data-styled.g112[id="id__PostContentWrapper-sc-1hduup0-0"]{content:"QsqTL,"}/*!sc*/
@media (max-width:767px){.bwsQop.toc-container{display:none;}}/*!sc*/
.bwsQop.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g113[id="id__TocContainer-sc-1hduup0-1"]{content:"bwsQop,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/blog/%5bid%5d-b80b73d0fd88ad55.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="ImageModal__ImageModalWrapper-sc-1ey7m7r-0 eivcj"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All blog posts</a><main class="id__TocContainer-sc-1hduup0-1 bwsQop toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><p class="my-4 text-sm font-medium">Quantumworks Lab<span class="mx-2">•</span>July 6, 2022</p><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">Use automation to reduce your labeling time and spend</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--65-.gif"/></div><main class="id__PostContentWrapper-sc-1hduup0-0 QsqTL md:px-24"><div class="content js-toc-content"><p>It’s no secret that developing and maintaining a performant machine learning (ML) model requires a huge amount of data. However, labeling training data from scratch can be time intensive, require expert labeling and review teams, and can quickly become expensive, especially for teams still developing best practices. Efficiently speeding up the data labeling process can be a challenge. This is where automation comes in. Incorporating automation into your workflow is one of the most effective ways to produce high-quality data, fast.</p><p>While creating any object from scratch can be time intensive, segmentation masks are often the most time consuming annotation type. On average, segmentation masks take around 3x longer to create than any other object type. Automation, such as ML editor-assisted tools, can help reduce the time spent manually labeling data. It lets humans focus on QA and review processes, rather than spending time creating labels from scratch. </p><p>With this in mind, Quantumworks Lab is excited to introduce our image auto-segmentation tool to help teams quickly generate and edit quality segmentation masks in just seconds. </p><h2 id="create-quality-masks-in-a-fraction-of-the-time-with-our-auto-labeling-tool">Create quality masks in a fraction of the time with our auto-labeling tool</h2><p>Regardless of whether you’re labeling data for your first model or you already have many models in production, you can use our new image auto-segmentation tool to save time and jump-start your labeling. Here’s how to best use our tool to get the most out of your automated labeling workflow. </p><ol><li>After setting up your image segmentation project and creating your ontology, select our image auto-segmentation tool from the toolbar. You can easily adjust the contrast and brightness of the image to make the area of interest more visible.</li></ol><figure class="kg-card kg-image-card"><img src="../../../labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--63-.gif" class="kg-image" alt="" loading="lazy" width="1851" height="1080" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2022/07/ezgif.com-gif-maker--63-.gif 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2022/07/ezgif.com-gif-maker--63-.gif 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2022/07/ezgif.com-gif-maker--63-.gif 1600w, https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--63-.gif 1851w" sizes="(min-width: 720px) 720px"></figure><p>2. Draw a bounding box over the area you wish to segment. For a faster workflow, you can draw multiple bounding boxes at once and our auto-segmentation tool will queue them for labeling.</p><figure class="kg-card kg-image-card"><img src="../../../labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--52-.gif" class="kg-image" alt="" loading="lazy" width="1745" height="1080" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2022/07/ezgif.com-gif-maker--52-.gif 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2022/07/ezgif.com-gif-maker--52-.gif 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2022/07/ezgif.com-gif-maker--52-.gif 1600w, https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--52-.gif 1745w" sizes="(min-width: 720px) 720px"></figure><p>3. In less than a second, our tool will return a quality segmentation mask. You can use our pen tool to add to or subtract from the mask prediction. If you accidentally create a mask over an area that you don’t want, you can use our auto-erase tool by drawing a bounding box over the area to delete the mask.</p><figure class="kg-card kg-image-card"><img src="../../../labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--61--1.gif" class="kg-image" alt="" loading="lazy" width="1860" height="1080" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2022/07/ezgif.com-gif-maker--61--1.gif 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2022/07/ezgif.com-gif-maker--61--1.gif 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2022/07/ezgif.com-gif-maker--61--1.gif 1600w, https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--61--1.gif 1860w" sizes="(min-width: 720px) 720px"></figure><p><br>For a more step-by-step tutorial, you can watch the demo below or read more in our <a href="https://docs.labelbox.com/docs/image-annotations?ref=labelbox.ghost.io#segmentation-mask">documentation</a>.</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/L5SCqnre-d4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" title="Image auto-segmentation tool - Demo Walkthrough"></iframe></figure><h2 id="integrate-a-model-to-make-labeling-faster-and-more-accurate-as-you-iterate">Integrate a model to make labeling faster and more accurate as you iterate</h2><p>While ML-assisted tools can be great for teams who are just getting started on labeling their data, teams who adopt more advanced techniques like pre-labeling can take automation to a whole new level.</p><p>By using your model or a model of your choice, you can dramatically accelerate your labeling efficiency with pre-labels. Pre-labeling decreases labeling costs as the model gets smarter with every iteration, leaving teams more time to focus manual labeling on edge cases or areas where the model might not be performing as well. It’s not only faster and less expensive, but delivers better model performance. </p><p><a href="https://labelbox.ghost.io/blog/imagebiopsy-lab-uses-mal-to-increase-efficiency-by-160/">Read this case study</a> to learn more about our pre-labeling workflow powered by model-assisted labeling (MAL) and how one of our customers used MAL to increase their labeling efficiency by 160%.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_Fjp-x-4Gz1FUwHUm8WrWCpmTlVQIRZ7ICTcKhRgYdjzDkHD-bOkTPd3S1hUH4OJePgHF5I4V02SvsJOYUgmOUXAnA3pCzAUyR2sIzDt2a645qw2aVlHMZl223BIM-BOvXlaEgxQD2NYCRHt_n51_E9KsqsuGAA2uXrU8Y6zPpRCYOF2dADpBT4UU6UuRyrf7gCbT4NzTnWEHTHzMiNtcaFRLdWXvkXS7lbZ8kahUXkW6TIQLrXtBwAbKFWBMZGdBL69egxsOXksTLknRTLq77uQ-66Jc6KbufTz2A0gA86w_NqpgvyVMQC1_zA-AeaplL7J4EkBRyNCTWfIx8Ek4HwqExeShY3sP2ZeJVArcb6XWQlsHLfZJorJuRA2K6gwlUP7V-HXf8MhekV2ZMSrpjGKGJ6cRkNhm4XmLYrtbcIWfAgpC_T_442k7rK293AM045GQwLpyZzZ_3YqrpCHIRTb2eR66bY1L_eNz_Dl_J6vSq2EoBtVFZxXKl4kyUDYyyts7sQ=s1600" class="kg-image" alt="" loading="lazy"><figcaption><span style="white-space: pre-wrap;">Model-assisted labeling can help lower labeling costs and improve after every iteration.</span></figcaption></figure><p>Our image auto-segmentation tool and model-assisted labeling workflow is currently available to all customers. We’re excited for our users to experience the ways in which automation can dramatically cut costs and increase labeling efficiency. </p><p>Try our auto-segmentation tool on an <a href="https://app.labelbox.com/projects?ref=labelbox.ghost.io">image labeling project</a> or get started with <a href="https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox.ghost.io">model-assisted labeling</a> today. </p></div></main></div></div></div><div class="mt-5 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="my-20 w-full h-[1px] bg-neutral-200"></div><div class="grid grid-cols-12 gap-2"><div class="col-span-12"><h2 class="mb-12 text-center text-3xl md:text-4xl font-medium">Continue reading</h2></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../code-runner-secure-scalable-code-execution-for-model-evaluation-2/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index4144.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Dmytro Apollonin<span class="mx-2">•</span>December 20, 2024</p></div><a href="../code-runner-secure-scalable-code-execution-for-model-evaluation-2/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Code Runner: Secure, scalable code execution for model evaluation</p><p class="text-base max-w-2xl undefined line-clamp-3">Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../inside-the-data-factory-how-labelbox-produces-the-highest-quality-data-at-scale/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F09%2FBlog.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F09%2FBlog.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F09%2FBlog.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F09%2FBlog.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F09%2FBlog.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F09%2FBlog.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F09%2FBlog.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F09%2FBlog.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index3a98.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F09%2FBlog.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Quantumworks Lab<span class="mx-2">•</span>September 12, 2024</p></div><a href="../inside-the-data-factory-how-labelbox-produces-the-highest-quality-data-at-scale/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Inside the data factory: How Quantumworks Lab produces the highest quality data at scale</p><p class="text-base max-w-2xl undefined line-clamp-3">Inside the Quantumworks Lab AI data factory: take a look at the key tools, techniques, and processes that ensure top-quality data production, highlighting best practices for measuring and managing data quality.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../what-the-focus-on-multimodal-generative-and-3d-ai-mean-for-the-future-at-cvpr-2024/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F07%2FYT-Thumbnails--31-.jpg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F07%2FYT-Thumbnails--31-.jpg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F07%2FYT-Thumbnails--31-.jpg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F07%2FYT-Thumbnails--31-.jpg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F07%2FYT-Thumbnails--31-.jpg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F07%2FYT-Thumbnails--31-.jpg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F07%2FYT-Thumbnails--31-.jpg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F07%2FYT-Thumbnails--31-.jpg&amp;w=3840&amp;q=70 3840w" src="../../_next/image/indexb5d1.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F07%2FYT-Thumbnails--31-.jpg&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Mikiko Bazeley<span class="mx-2">•</span>July 2, 2024</p></div><a href="../what-the-focus-on-multimodal-generative-and-3d-ai-mean-for-the-future-at-cvpr-2024/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">What the focus on multimodal, generative, and 3D AI means for the future at CVPR 2024</p><p class="text-base max-w-2xl undefined line-clamp-3">CVPR 2024 recap: Exploring breakthroughs in multimodal, generative, and 3D AI, and their transformative implications for the future of artificial intelligence.</p></a></div></div></div></div></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="Footer__StyledFooter-sc-u68pnv-0 eJChXt"><div class="undefined lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="py-24"><div class=" w-full h-[1px] bg-neutral-200"></div></div><div class="hidden md:block"><img src="../../static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36"/></div><section class="hidden md:grid footer-grid"></section><section class="social-media"></section><div class="text-center "><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">© Quantumworks Lab, Inc <br/>We enable breakthroughs</p><div class="flex flex-row flex-wrap justify-content-center gap-4 mt-4"><a href="https://docs.labelbox.com/page/terms-of-service" class=" " target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Terms of Service</p></a><div class="mx-1 border"></div><a href="https://docs.labelbox.com/page/privacy-notice" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Privacy Notice</p></a><div class="mx-1 border hidden sm:block"></div><a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Copyright Dispute Policy</p></a></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"use-automation-to-reduce-your-labeling-time-and-spend","id":"62bf4d25b57632003d6f234a","uuid":"8d923b93-2809-4277-9196-391389546bd5","title":"Use automation to reduce your labeling time and spend","html":"\u003cp\u003eIt’s no secret that developing and maintaining a performant machine learning (ML) model requires a huge amount of data. However, labeling training data from scratch can be time intensive, require expert labeling and review teams, and can quickly become expensive, especially for teams still developing best practices. Efficiently speeding up the data labeling process can be a challenge. This is where automation comes in. Incorporating automation into your workflow is one of the most effective ways to produce high-quality data, fast.\u003c/p\u003e\u003cp\u003eWhile creating any object from scratch can be time intensive, segmentation masks are often the most time consuming annotation type. On average, segmentation masks take around 3x longer to create than any other object type. Automation, such as ML editor-assisted tools, can help reduce the time spent manually labeling data. It lets humans focus on QA and review processes, rather than spending time creating labels from scratch. \u003c/p\u003e\u003cp\u003eWith this in mind, Quantumworks Lab is excited to introduce our image auto-segmentation tool to help teams quickly generate and edit quality segmentation masks in just seconds. \u003c/p\u003e\u003ch2 id=\"create-quality-masks-in-a-fraction-of-the-time-with-our-auto-labeling-tool\"\u003eCreate quality masks in a fraction of the time with our auto-labeling tool\u003c/h2\u003e\u003cp\u003eRegardless of whether you’re labeling data for your first model or you already have many models in production, you can use our new image auto-segmentation tool to save time and jump-start your labeling. Here’s how to best use our tool to get the most out of your automated labeling workflow. \u003c/p\u003e\u003col\u003e\u003cli\u003eAfter setting up your image segmentation project and creating your ontology, select our image auto-segmentation tool from the toolbar. You can easily adjust the contrast and brightness of the image to make the area of interest more visible.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--63-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1851\" height=\"1080\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2022/07/ezgif.com-gif-maker--63-.gif 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2022/07/ezgif.com-gif-maker--63-.gif 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2022/07/ezgif.com-gif-maker--63-.gif 1600w, https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--63-.gif 1851w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e2. Draw a bounding box over the area you wish to segment. For a faster workflow, you can draw multiple bounding boxes at once and our auto-segmentation tool will queue them for labeling.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--52-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1745\" height=\"1080\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2022/07/ezgif.com-gif-maker--52-.gif 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2022/07/ezgif.com-gif-maker--52-.gif 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2022/07/ezgif.com-gif-maker--52-.gif 1600w, https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--52-.gif 1745w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e3. In less than a second, our tool will return a quality segmentation mask. You can use our pen tool to add to or subtract from the mask prediction. If you accidentally create a mask over an area that you don’t want, you can use our auto-erase tool by drawing a bounding box over the area to delete the mask.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--61--1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1860\" height=\"1080\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2022/07/ezgif.com-gif-maker--61--1.gif 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2022/07/ezgif.com-gif-maker--61--1.gif 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2022/07/ezgif.com-gif-maker--61--1.gif 1600w, https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--61--1.gif 1860w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003eFor a more step-by-step tutorial, you can watch the demo below or read more in our \u003ca href=\"https://docs.labelbox.com/docs/image-annotations?ref=labelbox.ghost.io#segmentation-mask\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/L5SCqnre-d4?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" title=\"Image auto-segmentation tool - Demo Walkthrough\"\u003e\u003c/iframe\u003e\u003c/figure\u003e\u003ch2 id=\"integrate-a-model-to-make-labeling-faster-and-more-accurate-as-you-iterate\"\u003eIntegrate a model to make labeling faster and more accurate as you iterate\u003c/h2\u003e\u003cp\u003eWhile ML-assisted tools can be great for teams who are just getting started on labeling their data, teams who adopt more advanced techniques like pre-labeling can take automation to a whole new level.\u003c/p\u003e\u003cp\u003eBy using your model or a model of your choice, you can dramatically accelerate your labeling efficiency with pre-labels. Pre-labeling decreases labeling costs as the model gets smarter with every iteration, leaving teams more time to focus manual labeling on edge cases or areas where the model might not be performing as well. It’s not only faster and less expensive, but delivers better model performance. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.ghost.io/blog/imagebiopsy-lab-uses-mal-to-increase-efficiency-by-160/\"\u003eRead this case study\u003c/a\u003e to learn more about our pre-labeling workflow powered by model-assisted labeling (MAL) and how one of our customers used MAL to increase their labeling efficiency by 160%.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/IUTVBkC6Eza-J4S5nKg_vrzRt9HR2t_CKq39bEqHC_6jCIxXvaV2YGGEYgfe0cg5QPbUysYuq8CFLAuUXCpNtp5GHgVRJhn4gbZFBcPy6hvIl_pL51Ucij5jP6ws4Svq47TcSv75qHpfuikas0Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eModel-assisted labeling can help lower labeling costs and improve after every iteration.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eOur image auto-segmentation tool and model-assisted labeling workflow is currently available to all customers. We’re excited for our users to experience the ways in which automation can dramatically cut costs and increase labeling efficiency. \u003c/p\u003e\u003cp\u003eTry our auto-segmentation tool on an \u003ca href=\"https://app.labelbox.com/projects?ref=labelbox.ghost.io\"\u003eimage labeling project\u003c/a\u003e or get started with \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox.ghost.io\"\u003emodel-assisted labeling\u003c/a\u003e today. \u003c/p\u003e","comment_id":"62bf4d25b57632003d6f234a","feature_image":"https://labelbox.ghost.io/blog/content/images/2022/07/ezgif.com-gif-maker--65-.gif","featured":false,"visibility":"public","created_at":"2022-07-01T12:38:13.000-07:00","updated_at":"2023-10-26T14:12:05.000-07:00","published_at":"2022-07-06T00:00:00.000-07:00","custom_excerpt":"Quantumworks Lab is excited to introduce our image auto-segmentation tool to help teams quickly generate and edit quality segmentation masks in seconds. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/blog/use-automation-to-reduce-your-labeling-time-and-spend/","authors":[{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"}],"tags":[{"id":"65303cb64e99900001fc05a5","name":"Labeling automation","slug":"labeling-automation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/labeling-automation/"},{"id":"65302fd64e99900001fc0529","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-computer-vision/"}],"primary_author":{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"},"primary_tag":{"id":"65303cb64e99900001fc05a5","name":"Labeling automation","slug":"labeling-automation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/labeling-automation/"},"url":"https://labelbox.ghost.io/blog/use-automation-to-reduce-your-labeling-time-and-spend/","excerpt":"Quantumworks Lab is excited to introduce our image auto-segmentation tool to help teams quickly generate and edit quality segmentation masks in seconds. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":"Use automation to reduce your labeling time and spend","og_description":"Quantumworks Lab is excited to introduce our image auto-segmentation tool to help teams quickly generate and edit quality segmentation masks in seconds. ","twitter_image":null,"twitter_title":"Use automation to reduce your labeling time and spend","twitter_description":"Quantumworks Lab is excited to introduce our image auto-segmentation tool to help teams quickly generate and edit quality segmentation masks in seconds. ","meta_title":"Use automation to reduce your labeling time and spend","meta_description":"Quantumworks Lab is excited to introduce our image auto-segmentation tool to help teams quickly generate and edit quality segmentation masks in seconds. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},"recommended":[{"id":"6765b8c06f63bf0001f1ca72","uuid":"9f912bc0-54da-4ac6-ab5f-78d8f926463c","title":"Code Runner: Secure, scalable code execution for model evaluation","slug":"code-runner-secure-scalable-code-execution-for-model-evaluation-2","html":"\u003cp\u003eIn the world of large language models (LLMs), evaluating their responses effectively is a fundamental aspect of improving model performance. We’re excited to announce the latest addition to the Quantumworks Lab platform: Code Runner.\u003cstrong\u003e \u003c/strong\u003eThis new capability pushes the boundaries of interactivity by allowing users to execute written code directly within the evaluation workflow.\u003c/p\u003e\u003cp\u003eCode Runner helps eliminate errors, optimizes functionality, and validates outputs, leading to higher-quality datasets. Today, we’ll introduce this new feature and then dive into the technical details of the infrastructure powering this feature, highlighting how it was designed with security, scalability, and\u003cstrong\u003e \u003c/strong\u003erobustness at its core.\u003c/p\u003e\u003ch2 id=\"what-is-code-runner\"\u003e\u003cstrong\u003eWhat is Code Runner?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCode Runner is a new built-in feature of the Quantumworks Lab platform designed to improve the quality of responses and labels generated in any coding-related projects. The new features enables users to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eDirectly execute code found in either model responses or user-written responses \u003c/li\u003e\u003cli\u003eReceive precise outputs including:\u003cul\u003e\u003cli\u003eStandard output (stdout)\u003c/li\u003e\u003cli\u003eStandard error (stderr)\u003c/li\u003e\u003cli\u003eExecution time\u003c/li\u003e\u003cli\u003eWarnings or runtime errors\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy integrating Code Runner into the evaluation pipeline, we aim to simplify the process of verifying the accuracy, efficiency, and functionality of code responses, all without users needing to leave the platform.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeBDN_1bnU_bTrPrWS59SWalVqw22Gxq3AIxNnbsOJmZGPap3weXHYFEgzrlPnEyhVK1GOjzCVClvQycomfMfhQsulqPk4wdQGqniZv8aIaHGP69wzgcFjdDdr5FgooITwNJCsp?key=GRyWmie9kDWaUfN6osDAF8J7\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"389\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOur system automatically detects the language in the text area and suggests the appropriate environment for execution, whether Python or JavaScript (and more to come).\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBut what makes this feature stand out is the sophisticated infrastructure behind it, designed to ensure seamless execution while maintaining strict security and privacy standards.\u003c/p\u003e\u003ch2 id=\"code-runner-infrastructure-a-deep-dive\"\u003e\u003cstrong\u003eCode Runner infrastructure: A deep dive\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAt the heart of Code Runner’s infrastructure lies Google Cloud Run, a fully managed compute platform that runs containerized applications in a secure, scalable manner. Here are the key components and principles driving the system:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Cloud Run for language-specific environments\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eEvery code execution happens in a dedicated Cloud Run instance. Each instance is tailored to a specific programming language environment (e.g., Python, JavaScript, etc.) and is spun up dynamically based on the code type detected in the user response.\u003c/p\u003e\u003cp\u003eThis design includes the following characteristics to ensure security and speed:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIsolation\u003c/strong\u003e: Each execution is fully containerized, completely isolating the runtime environment from others.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTemporary directories\u003c/strong\u003e: Code is executed in a temporary directory within the container, and it is deleted immediately after execution, leaving no trace behind.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLanguage-specific tools\u003c/strong\u003e: Each environment comes preloaded with the necessary packages and libraries to ensure compatibility and speed.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2. Enhanced security with separate GCP projects \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe Cloud Run service is hosted in a separate Google Cloud Platform (GCP) project, distinct from our main infrastructure. This segmentation provides an additional layer of security by isolating code execution from our core services. Even in the unlikely event of a compromise, the blast radius is contained.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Communication via private service connect\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo ensure secure and controlled communication, all interactions between the main evaluation system and the Cloud Run service occur over Private Service Connect, which provides the following advantages: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNo public exposure\u003c/strong\u003e: The Cloud Run endpoint is never exposed to the public internet, reducing the risk of unauthorized access.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOne-way communication\u003c/strong\u003e: The Private Service Connect setup restricts outbound networking from the Cloud Run service, ensuring that executed code cannot make arbitrary network requests. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGranular networking controls\u003c/strong\u003e: The private network allows for precise control over what resources the Cloud Run service can access.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4. Automatic cleanup\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo maintain a lightweight and secure runtime, the system delivers:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEphemeral execution\u003c/strong\u003e: Each execution request is handled in a stateless, temporary environment.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAutomatic deletion\u003c/strong\u003e: Files, logs, and temporary directories are wiped as soon as execution completes, leaving no residual data.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"how-code-runner-works-a-step-by-step-overview\"\u003e\u003cstrong\u003eHow Code Runner works: A step-by-step overview\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow that you have an understanding of the powerful infrastructure underneath Code Runner, here is a summary of how the feature works from start to finish:\u0026nbsp;\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eCode submission\u003c/strong\u003e: A user requests code execution from the evaluation interface.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLanguage detection\u003c/strong\u003e: The system detects the programming language and forwards the request to the corresponding Cloud Run service.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eExecution\u003c/strong\u003e: The Cloud Run instance spins up a container, executes the code in a sandboxed environment, and collects the results.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResult delivery\u003c/strong\u003e: The system returns the output (stdout, stderr, execution time, and any warnings) to the user for analysis.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCleanup\u003c/strong\u003e: The container and all related resources are terminated and deleted.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"advantages-of-labelbox%E2%80%99s-built-in-code-execution\"\u003e\u003cstrong\u003eAdvantages of Quantumworks Lab’s built-in code execution\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCode Runner’s infrastructure was designed specifically to provide the previously discussed benefits and to address several key challenges that other solutions may face:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSecurity\u003c/strong\u003e: By isolating execution environments and ensuring no public exposure, we eliminate a significant attack surface.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability\u003c/strong\u003e: Cloud Run’s serverless nature allows us to scale dynamically with demand, handling thousands of requests efficiently.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eReliability\u003c/strong\u003e: The use of ephemeral containers ensures that each execution starts in a clean slate, avoiding cross-contamination or resource conflicts.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"explore-it-yourself\"\u003e\u003cstrong\u003eExplore it yourself\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWith Code Runner, we’re empowering users to go beyond static evaluations, enabling dynamic, interactive testing that’s as secure as it is scalable. As always, we’re excited to hear your feedback and explore how we can push this feature even further.\u003c/p\u003e\u003cp\u003e If you want to explore Code Runner and other LLM evaluation tools, \u003ca href=\"https://app.labelbox.com/signup?_r=https://landing-page-git-uv-homepage-refresh-dec-24-labelbox.vercel.app/?utm_keyword=Quantumworks Lab\u0026utm_source=house\u0026utm_medium=email\u0026utm_campaign=1224%2520\u0026gclid=CjwKCAiA34S7BhAtEiwACZzv4a9veoKXnMnMvo2rWJvXkH46oHs4Lb5VFQi2ERBN_sQ5kgypV_zfBxoC0yMQAvD_BwE\u0026landingPageAnonymousId=%22e3f2f82f-be24-4045-b2b9-50a49cb801e8%22\u0026referrer_url=https://landing-page-git-uv-homepage-refresh-dec-24-labelbox.vercel.app/\"\u003e\u003cu\u003esign up\u003c/u\u003e\u003c/a\u003e for our platform today.\u0026nbsp;\u003c/p\u003e\u003cp\u003eStay tuned for updates, and happy coding!\u003c/p\u003e","comment_id":"6765b8c06f63bf0001f1ca72","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/12/Labelbox-code-runner--1-.png","featured":false,"visibility":"public","created_at":"2024-12-20T10:34:40.000-08:00","updated_at":"2025-03-12T12:01:43.000-07:00","published_at":"2024-12-20T12:44:45.000-08:00","custom_excerpt":"Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},{"id":"5f9af89dd8118b0039136165","name":"Engineering","slug":"engineering","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/engineering/"},{"id":"65302fc14e99900001fc0527","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-llms/"},{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"},{"id":"65303cb64e99900001fc05a5","name":"Labeling automation","slug":"labeling-automation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/labeling-automation/"},{"id":"6530313c4e99900001fc0537","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/train-fine-tune-ai/"}],"authors":[{"id":"6765bb156f63bf0001f1ca8d","name":"Dmytro Apollonin","slug":"dmytro","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/dmytro/"}],"primary_author":{"id":"6765bb156f63bf0001f1ca8d","name":"Dmytro Apollonin","slug":"dmytro","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/dmytro/"},"primary_tag":{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},"url":"https://labelbox.ghost.io/blog/code-runner-secure-scalable-code-execution-for-model-evaluation-2/","excerpt":"Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66e258daab75a10001bf83b1","uuid":"151a5a79-4c0f-47a1-90e5-a574a525bea4","title":"Inside the data factory: How Quantumworks Lab produces the highest quality data at scale","slug":"inside-the-data-factory-how-labelbox-produces-the-highest-quality-data-at-scale","html":"\u003cp\u003eIn pursuit of AGI and beyond, data quality is not just a checkbox for frontier AI labs—it's the cornerstone of innovation and a critical competitive advantage. The quality of training data determines the success or failure of these cutting-edge models.\u003c/p\u003e\u003cp\u003eWe passionately believe that during this decade, data quality will be the most important factor in advancing model capabilities. Very little is publicly discussed when it comes to measuring data quality and producing high-quality human data efficiently, so we wanted to shed light on our approach and share insights into the strategies and practices we use to achieve these standards.\u003c/p\u003e\u003cp\u003eIn this post, we look deep inside the Quantumworks Lab AI data factory, revealing important tools, techniques and processes that are the bedrock for producing the highest-grade data at scale. We cover just some of the best practices we follow for measuring and managing data quality. We hope this post sparks some valuable insights, while keeping in mind that we utilize many more advanced strategies to operate the entirety of our modern AI data factory.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-full\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/09/Frame-3794.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"750\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/09/Frame-3794.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/09/Frame-3794.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/09/Frame-3794.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/09/Frame-3794.png 2400w\"\u003e\u003c/figure\u003e\u003ch2 id=\"measuring-quality\"\u003eMeasuring quality\u003c/h2\u003e\u003cp\u003ePrecision and accuracy are two foundational pillars for measuring data quality. Think of these as the dynamic duo for data quality measurement. Let's break them down in a way that's easy to understand:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePrecision: Hitting a target consistently\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eImagine you're playing darts. Precision is like throwing multiple darts and having them all cluster tightly together on the board. It doesn't matter if they're in the bullseye or not – what matters is that they're close to each other. In the world of AI data, precision means getting consistent results when collecting human opinions or preferences. It's about reliability and repeatability.\u003c/p\u003e\u003cp\u003eFor example, if multiple people rate the same AI-generated text, high precision would mean their ratings are very similar to each other. This consistency is crucial because it shows that your data collection process is reliable and produces strong, clear signals.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAccuracy: Hitting the right target\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eNow, let's go back to our dart game. Accuracy is about hitting the bullseye – or whatever your specific target is. In AI data quality, accuracy means how close your collected data is to the \"truth\" or the desired outcome.\u003c/p\u003e\u003cp\u003eHowever, here's where it gets tricky with generative AI: often, there isn't a single clear-cut \"right answer.\" In such cases, the model creator is the ultimate judge in deciding the right answer aligned to their view of the world.\u0026nbsp; That's why we focus on how quickly we can adjust and improve our accuracy over time. It's like learning to aim better with each round of darts you play.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy Both Matter\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the world of AI data quality, we need both precision and accuracy.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHigh precision ensures that our data is consistent and reliable. When generating human data for subjective tasks (human preferences, evals, RLHF), the human data factory must produce data with high precision. Every RLHF or evaluation task starts with instructions. Labeling instructions captures a clear point of view from the model creator’s perspective. Instructions also imply that there is a rating criteria and therefore generated data should indicate high precision. Low precision is most likely caused by poor instructions including low coverage of edgecase examples, or poor training, onboarding and execution of labeling projects in production.\u003c/p\u003e\u003cp\u003eGiven this context, precision demonstrates a data factory's capability to produce data that has strong signal and consistency.\u003c/p\u003e\u003cp\u003eGood accuracy (or the ability to improve accuracy quickly) ensures that we're collecting our preferred data to train and evaluate our AI models in the way that we believe is most effective and correct. It is also essential to measure the rate of change of accuracy after a few rounds of calibration (feedback).\u0026nbsp;\u003c/p\u003e\u003cp\u003eBy measuring and improving both precision and accuracy, we can create a solid foundation for high-quality data – the fuel that powers better, more reliable AI systems. An ideal data factory is highly precise and can quickly calibrate to any desired accuracy level.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn the next two sections, we look at specific ways to measure the precision and accuracy of your data.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"precision-metrics\"\u003ePrecision metrics\u003c/h2\u003e\u003cp\u003ePrecision metrics focus on consistency and agreement among labelers, which are primarily derived from Quantumworks Lab's built-in consensus capability.Labelbox uses and tests the effectiveness of over 15 similar metrics for a wide range of supported annotations. Here we share some of our preferred metrics currently.\u003c/p\u003e\u003ch3 id=\"inter-rater-agreement-ira\"\u003e\u003cstrong\u003eInter-rater agreement (IRA)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eInter-rater agreement measures how much consensus there is between different raters (labelers) who are assessing the same data. While there are several methods to calculate IRA, such as Cohen's Kappa and Fleiss' Kappa, we'll focus on Krippendorff's Alpha due to its versatility and robustness in AI data labeling contexts.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"krippendorf%E2%80%99s-alpha\"\u003e\u003cstrong\u003eKrippendorf’s alpha\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eKrippendorff's Alpha is a popular metric used to assess the agreement among raters because it works well for two or more raters, can handle missing data, and supports nominal, ordinal, and ranking data types. Its values range from -1 to 1, with the following interpretations.\u003c/p\u003e\u003cp\u003eInterpretation\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:39.25pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eKrippendorff's Alpha\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eLevel of Agreement\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eInterpretation\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:39.25pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAlpha = 1\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003ePerfect agreement\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAll raters provided the exact same ratings for each item evaluated.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:39.25pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAlpha ≥ 0.80\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eSatisfactory agreement\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eReliable ratings, acceptable for drawing conclusions based on the rated data.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:53.5pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAlpha between 0.67 and 0.79\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eModerate agreement\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eOutcomes should be interpreted with caution, as the agreement is not strong enough for definitive conclusions.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:67pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAlpha \u0026lt; 0.67\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003ePoor agreement\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eData are often deemed unreliable for drawing conclusions, suggesting raters are not applying the coding scheme consistently or the scheme itself may be flawed or data is skewed.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:39.25pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAlpha = 0\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eNo agreement\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eNo agreement among raters beyond what would be expected by chance, similar to a random rating pattern.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:39.25pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAlpha \u0026lt; 0\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eSystematic disagreement\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eRaters are systematically inclined in opposite directions.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"standard-deviation-of-ratings\"\u003e\u003cstrong\u003eStandard deviation of ratings\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eStandard deviation measures the dispersion of a set of ratings from their mean (average) value. In the context of AI data quality, it quantifies how much variation or spread exists in the ratings given by different AI trainers for the same item or task.\u003c/p\u003e\u003cp\u003eInterpretation\u003c/p\u003e\u003cul\u003e\u003cli\u003eLower values indicate higher precision (more clustered ratings).\u003c/li\u003e\u003cli\u003eHigher values suggest more disagreement or variability among raters.\u003c/li\u003e\u003cli\u003eThe scale of interpretation depends on the rating scale used.\u003c/li\u003e\u003cli\u003eIt's sensitive to outliers, which might skew the interpretation in small sample sizes.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"percent-agreement\"\u003e\u003cstrong\u003ePercent agreement\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003ePercent agreement is a straightforward measure of inter-rater reliability that calculates the proportion of times different raters agree in their judgments. This is particularly useful in classification tasks (enums).\u003c/p\u003e\u003cp\u003eInterpretation\u003c/p\u003e\u003cul\u003e\u003cli\u003eRanges from 0% to 100%, with higher percentages indicating better agreement.\u003c/li\u003e\u003cli\u003eGenerally, values above 75-80% are considered good, but this threshold can vary based on task complexity.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"accuracy-metrics\"\u003eAccuracy metrics\u003c/h2\u003e\u003cp\u003eAccuracy metrics assess how close the labelers' responses are to the ground truth. These are primarily derived from Quantumworks Lab's benchmark feature.\u003c/p\u003e\u003cp\u003eFor preference ranking, selection or side by side evaluation tasks, you often do not have an initial ground truth available. It must be created. One of the best ways to create it is using consensus to pick a winner and then verify with highly trusted humans. Again having high precision is paramount in creating ground truth and ultimately measuring accuracy.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"accuracy-score\"\u003e\u003cstrong\u003eAccuracy score\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWhat it is\u003c/strong\u003e: The proportion of correct responses compared to the ground truth.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHow it's calculated\u003c/strong\u003e: (Number of correct responses / Total number of responses) * 100\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e: Ranges from 0% to 100%, with higher percentages indicating better accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eExample\u003c/strong\u003e: If a labeler correctly classifies 90 out of 100 benchmark tasks, their accuracy score would be 90%.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"mean-absolute-error-mae\"\u003e\u003cstrong\u003eMean absolute error (MAE)\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWhat it is\u003c/strong\u003e: The average absolute difference between predicted values and actual values.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHow it's calculated\u003c/strong\u003e: Sum of absolute differences between predictions and actual values, divided by the number of predictions.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e: Lower values indicate better accuracy. The scale depends on the range of the values being predicted.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eExample\u003c/strong\u003e: If labelers are rating the quality of AI-generated text on a scale of 1-10, MAE would show how far off, on average, their ratings are from the ground truth.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"f1-score\"\u003e\u003cstrong\u003eF1 score\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWhat it is\u003c/strong\u003e: A balanced measure of precision and recall, useful for classification tasks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHow it's calculated\u003c/strong\u003e: 2 * ((Precision * Recall) / (Precision + Recall))\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e: Ranges from 0 to 1, with 1 being the best possible score.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExample\u003c/strong\u003e: Useful for tasks like sentiment analysis, where both correctly identifying positive sentiments (precision) and not missing any positive sentiments (recall) are important.\u003c/p\u003e\u003ch2 id=\"metrics-for-various-annotations\"\u003eMetrics for various annotations\u003c/h2\u003e\u003cp\u003eThe choice of metric for evaluating data quality depends heavily on the type of annotation task. Different annotation types require different evaluation approaches to accurately assess precision and accuracy. Here are some of the most common annotation types and their corresponding metrics:\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:25.75pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eTask\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eMetrics\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eWhy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eExample\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:53.5pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eLikert scale responses\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eKrippendorff's Alpha\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eWell-suited for ordinal data, robust to missing data\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eMeasuring the consistency of ratings on a 1-5 scale\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:53.5pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#444746;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eRankings\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eKendall's Tau, ELO, Spearman's rank correlation\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAssess agreement between rankings and handle relative comparisons effectively\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eEvaluating consistency in human preferences for AI-generated content or comparing model performance across different tasks\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:81.25pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eCategorical classifications\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eCohen's Kappa (two raters), Fleiss' Kappa (multiple raters)\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eDesigned for nominal data, account for chance agreement\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAssessing inter-rater reliability in categorizing AI-generated text into genres\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:67pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eFree text responses\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003ea) LLM as a judge, b) BLEU score (translation tasks), c) ROUGE score (summarization tasks)\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eRequire sophisticated methods that understand semantic meaning and context\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eEvaluating the quality, relevance, or correctness of free-text responses to prompts\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:67pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eBinary classifications\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAccuracy, F1 Score, Matthews Correlation Coefficient\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eWell-suited for tasks with two possible outcomes\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eEvaluating the performance of AI-generated images against a benchmark\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:67pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eMulti-label classifications\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eHamming Loss, Jaccard Index\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eCan handle cases where multiple labels can be assigned to a single instance\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAssessing the accuracy and overlap of assigned topic tags to an AI-generated article\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:67pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eNumerical predictions\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eMean Absolute Error (MAE), Root Mean Square Error (RMSE)\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eQuantify the difference between predicted and actual numerical values\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eMeasuring the accuracy of age estimates for people in AI-generated images\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"managing-quality\"\u003eManaging quality\u003c/h2\u003e\u003cp\u003eThe adage \"what gets measured gets managed\" is particularly relevant in AI data quality. With real-time quality measurements at our disposal, the next challenge becomes how to effectively improve and manage quality. Below are some common scenarios in production that our teams have to intervene and correct.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"scenarios-and-strategies\"\u003e\u003cstrong\u003eScenarios and strategies\u003c/strong\u003e\u003c/h3\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:25.75pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eScenario\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eStrategy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:53.5pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eLow precision\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eImplement more rigorous annotator training and clearer guidelines. Conduct calibration sessions to align annotators' understanding. Use concrete examples to illustrate edge cases and potential areas of confusion.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:53.5pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eHigh precision, low accuracy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eRevisit and refine the gold standard or ground truth. Employ expert review to identify systematic biases. Adjust training materials to address any misalignments between annotator output and desired outcomes.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:67pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eLow precision, high accuracy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eFoster more collaboration and knowledge sharing among annotators. Implement a peer review system where annotators can learn from each other's approaches. Encourage discussions about challenging cases to build a collective understanding.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:67pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eLow accuracy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eConduct a comprehensive review of training materials and annotation guidelines. Consider retraining the entire team with a focus on accuracy. Implement a more rigorous quality control process, potentially involving multiple layers of review.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-full\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/09/Frame-3797.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"750\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/09/Frame-3797.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/09/Frame-3797.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/09/Frame-3797.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/09/Frame-3797.png 2400w\"\u003e\u003c/figure\u003e\u003ch2 id=\"beyond-precision-and-accuracy-operational-efficiency-and-trust\"\u003eBeyond precision and accuracy: Operational efficiency and trust\u003c/h2\u003e\u003cp\u003eWhile precision and accuracy are crucial for data quality, it's essential to consider other factors that influence the overall effectiveness and efficiency of AI data labeling processes. These additional indicators provide valuable insights into resource allocation, workflow optimization, and labeler reliability.\u003c/p\u003e\u003ch3 id=\"operational-efficiency-indicators\"\u003e\u003cstrong\u003eOperational efficiency indicators\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eAchieving high precision and accuracy at any cost is often undesirable, as AI teams typically operate within specific data budgets to achieve expected value in terms of new model capability. The following metrics help balance quality with efficiency:\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:25.75pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eMetric\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eDescription\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:25.75pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eLabeling time\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eMeasures the time taken by individual AI trainers to complete tasks.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:25.75pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eReview time\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eTracks the duration of the review process for labeled data.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:25.75pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eRework rate\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eThe percentage of tasks that require revision after initial labeling or review.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:25.75pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eThroughput\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eThe number of tasks completed per unit of time (e.g., per hour or per day).\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNotes\u003c/strong\u003e: Outliers on either side of the mean for these metrics often reveal important insights. For example, consistently fast labelers with high accuracy might be candidates for more complex tasks, while those with long labeling times might need additional training or support.\u003c/p\u003e\u003ch3 id=\"alignerr-trust-score\"\u003e\u003cstrong\u003eAlignerr trust score\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe Alignerr trust score is a sophisticated metric designed to evaluate and quantify the reliability of individual expert AI trainers (also known as an \u003ca href=\"https://www.alignerr.com/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003eAlignerr\u003c/a\u003e). This multidimensional score incorporates various factors such as historical accuracy, consistency, task completion rate, and the ability to handle complex assignments.\u003c/p\u003e\u003cp\u003eIn practice, the Alignerr trust score plays a crucial role in optimizing workflow and maintaining high data quality standards. High-trust AI trainers may be prioritized for more critical or complex tasks, while those with lower scores might receive additional training opportunities or be assigned to tasks with higher levels of oversight. This selective task distribution helps to improve overall data quality without necessarily increasing review overhead. Moreover, the trust score serves as a valuable feedback mechanism, providing AI trainers with insights into their performance while encouraging continuous improvement.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-full\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/09/Frame-3798.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"750\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/09/Frame-3798.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/09/Frame-3798.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/09/Frame-3798.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/09/Frame-3798.png 2400w\"\u003e\u003c/figure\u003e\u003ch2 id=\"operational-aspects-of-ai-data-quality-management\"\u003e\u003cstrong\u003eOperational aspects of AI data quality management\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eEnsuring high-quality data for AI training and evaluation goes beyond metrics and measurements. It requires robust operational processes, innovative technologies, and a skilled workforce. This next section explores key operational aspects that contribute to maintaining and improving data quality.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/09/Frame-3799.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"750\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/09/Frame-3799.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/09/Frame-3799.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/09/Frame-3799.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/09/Frame-3799.png 2400w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"multi-step-review-and-rework\"\u003e\u003cstrong\u003eMulti-step review and rework\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eTo enhance data quality, we employ a multi-step review and rework process that draws inspiration from proven scientific methods. One such approach is the double-entry method, commonly used in data entry to reduce errors:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Initial labeling\u003c/strong\u003e: Two independent labelers perform the same task without knowledge of each other's work.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Comparison\u003c/strong\u003e: The results are automatically compared to identify discrepancies.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Expert review\u003c/strong\u003e: Where discrepancies exist, an expert reviewer examines both entries and makes a final determination.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4) Rework\u003c/strong\u003e: If necessary, the task is sent back for rework with specific feedback.\u003c/p\u003e\u003cp\u003eThis process significantly reduces the likelihood of errors and biases, as it requires multiple independent verifications before data is accepted. Additionally, we implement other quality control measures such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRandom spot checks by senior AI trainers\u003c/li\u003e\u003cli\u003ePeriodic recalibration sessions to ensure consistency across the team\u003c/li\u003e\u003cli\u003eAutomated checks for logical inconsistencies or outliers\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy incorporating these scientific approaches into our workflow, we can consistently produce high-quality data that meets the rigorous standards required for AI training and evaluation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/09/Frame-3796.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"750\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/09/Frame-3796.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/09/Frame-3796.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/09/Frame-3796.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/09/Frame-3796.png 2400w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"llm-as-a-judge\"\u003e\u003cstrong\u003eLLM as a judge\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eLeveraging the power of Large Language Models (LLMs) can greatly enhance our quality control processes, particularly for text-based tasks. We use fine-tuned LLMs to assess the similarity between annotator-provided explanations and ground truth responses. This approach offers several advantages:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Scalability\u003c/strong\u003e: LLMs can process large volumes of text quickly, allowing for comprehensive quality checks.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Consistency\u003c/strong\u003e: Unlike human reviewers, LLMs apply the same criteria consistently across all evaluations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Semantic understanding\u003c/strong\u003e: Fine-tuned LLMs can capture nuanced similarities in meaning, even when the exact wording differs.\u003c/p\u003e\u003cp\u003eOur process for using LLMs as judges involves:\u003c/p\u003e\u003cp\u003e1) Fine-tuning an LLM on a dataset of high-quality, expert-verified responses for specific task types.\u003c/p\u003e\u003cp\u003e2) Using the fine-tuned model to generate similarity scores between annotator responses and ground truth.\u003c/p\u003e\u003cp\u003e3) Flagging responses that fall below a certain similarity threshold for human review.\u003c/p\u003e\u003cp\u003eThis LLM-assisted approach allows us to efficiently identify potential quality issues while reducing the workload on human reviewers, who can focus their attention on the most challenging cases.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/09/Frame-3800.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"750\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/09/Frame-3800.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/09/Frame-3800.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/09/Frame-3800.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/09/Frame-3800.png 2400w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"setting-the-standard-for-expert-ai-trainers\"\u003eSetting the standard for expert AI trainers\u003c/h2\u003e\u003cp\u003eWhile software and AI technologies are critical for data quality management, the most significant quality gains often come from highly skilled AI trainers (aka human raters). These experts bring nuanced understanding and critical thinking skills that are essential for handling complex generative AI data.\u003c/p\u003e\u003cp\u003eThe impact of expert AI trainers is particularly evident in areas requiring deep domain expertise, such as STEM fields, advanced coding, and teaching AI systems complex skills like planning and reasoning. When training AI models to perform complex mathematical proofs, optimize code, or develop advanced problem-solving strategies, human expertise often surpasses current AI capabilities.\u003c/p\u003e\u003cp\u003eWith the Alignerr network, Quantumworks Lab maintains exceptionally high standards in our recruitment process, with an acceptance rate of just 3%. Our rigorous selection process includes:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Initial screening\u003c/strong\u003e: Looking for advanced degrees in STEM fields, extensive coding experience, or backgrounds in cognitive science and AI development.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Skills assessment\u003c/strong\u003e: Evaluating critical thinking, pattern recognition, and problem-solving abilities crucial for high-quality AI data annotation in complex domains.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Expertise tests\u003c/strong\u003e: Simulating real-world scenarios in STEM problem-solving, code optimization, or designing complex reasoning tasks for AI.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4) Technical interviews\u003c/strong\u003e: Assessing depth of knowledge in specialty areas and understanding of AI and machine learning concepts.\u003c/p\u003e\u003cp\u003eBy investing in top-tier human expertise across crucial domains, we ensure our data quality exceeds what can be achieved through software and AI alone, providing our clients with a competitive edge in developing next-generation AI capabilities.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/09/Frame-3801.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"750\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2024/09/Frame-3801.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2024/09/Frame-3801.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2024/09/Frame-3801.png 1600w, https://labelbox.ghost.io/blog/content/images/size/w2400/2024/09/Frame-3801.png 2400w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"curating-mission-specific-expert-ai-trainer-teams\"\u003eCurating mission-specific expert AI trainer teams\u0026nbsp;\u003c/h2\u003e\u003cp\u003eAt Quantumworks Lab, we take a unique approach to team formation for each customer project. Rather than assigning available annotators ad hoc, we create and curate dedicated teams specifically tailored to each mission. This approach ensures deep familiarity with the project and fosters a sense of shared purpose among team members.\u003c/p\u003e\u003cp\u003eKey aspects of our team curation process include:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Dedicated team formation\u003c/strong\u003e: We assemble a team of experts whose skills and experience align closely with the project's specific requirements.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Minimum hour commitment\u003c/strong\u003e: Team members are required to dedicate a minimum number of hours to the project. This ensures they gain the necessary context and develop proficiency in the specific task domain.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Context building\u003c/strong\u003e: Through intensive onboarding and ongoing training, we help the team build a comprehensive understanding of the customer's goals, challenges, and quality expectations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4) Mission-specific motivation\u003c/strong\u003e: We cultivate a shared sense of purpose within the team, aligning their efforts with the project's broader objectives and potential impact.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5) Continuous improvement\u003c/strong\u003e: Regular feedback sessions and performance reviews help the team refine their approach and continuously enhance their skills.\u003c/p\u003e\u003cp\u003eThis dedicated team model not only leads to higher quality outputs but also results in increased efficiency over time as the team develops deep expertise in the customer's specific domain. By creating a focused, motivated team with a strong grasp of the project's context, we ensure that each customer receives the highest level of service and the best possible results for their AI initiatives.\u003c/p\u003e\u003ch2 id=\"bringing-it-all-together\"\u003eBringing it all together\u003c/h2\u003e\u003cp\u003eIn this post, we explored some of the key ways that Quantumworks Lab helps customers capitalize on an AI data factory and our approach to delivering high-quality data at scale. This strategy includes:\u003c/p\u003e\u003cp\u003e1) Precision and accuracy metrics tailored to various annotation types\u003c/p\u003e\u003cp\u003e2) Adaptive quality management strategies for different scenarios\u003c/p\u003e\u003cp\u003e3) Operational efficiency indicators to balance quality with cost-effectiveness\u003c/p\u003e\u003cp\u003e4) The Alignerr trust score for optimizing workflow and maintaining high standards\u003c/p\u003e\u003cp\u003e5) Multi-step review processes and LLM-assisted quality control\u003c/p\u003e\u003cp\u003e6) A rigorous selection process for expert AI trainers\u003c/p\u003e\u003cp\u003e7) Curated, mission-specific teams dedicated to each customer's unique needs\u003c/p\u003e\u003cp\u003eWhat sets Quantumworks Lab apart is the scientific approach to data quality and operating an AI data factory at scale. Just last month, over 50 million annotations were created with over 200,000 human hours.\u0026nbsp;By continuously monitoring and analyzing data quality as it's produced, we enable immediate interventions and adjustments. This real-time approach allows AI teams to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eQuickly identify and address quality issues before they compound\u003c/li\u003e\u003cli\u003eProvide instant feedback to AI trainers, fostering rapid improvement\u003c/li\u003e\u003cli\u003eAdapt to changing project requirements on the fly\u003c/li\u003e\u003cli\u003eEnsure consistent, high-quality outputs throughout the entire data production process\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOur confidence in this system is so strong that we offer something truly unique in the industry: a data quality guarantee. Customers only pay for data that meets the agreed-upon Service Level Agreement (SLA) for quality, throughput, and efficiency. This guarantee underscores our commitment to delivering not just data, but value and results for our customers.\u003c/p\u003e\u003ch2 id=\"data-as-the-bedrock-for-agi-and-beyond\"\u003eData as the bedrock for AGI and beyond\u003c/h2\u003e\u003cp\u003eAs the AI landscape evolves and frontier labs continue their rapid pace of innovation to drive us closer towards AGI, Quantumworks Lab remains committed to providing the highest quality data and most efficient services, ensuring that data quality never becomes the limiting factor in the pursuit of transformative AI technologies. We’re continuing to invest heavily in cutting-edge data science and alignment techniques, pushing the boundaries of what's possible in data quality and service performance.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe hope you found this post helpful for gaining a deeper understanding of how a data factory helps ensure data quality and accelerate AI development process. If you're interested in learning more, feel free to \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try out the platform, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox.ghost.io\"\u003e\u003cu\u003econtact our team\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"references\"\u003eReferences\u003c/h2\u003e\u003cp\u003eWe’ve compiled a list of articles and research papers that have influenced our approach.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cu\u003e1) \u003c/u\u003e\u003c/em\u003e\u003ca href=\"https://arxiv.org/abs/1801.02546?ref=labelbox.ghost.io\"\u003e\u003cem\u003e\u003cu\u003e[1801.02546] Quality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques and Assurance Actions\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e\u0026nbsp;\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cu\u003e2) \u003c/u\u003e\u003c/em\u003e\u003ca href=\"https://arxiv.org/abs/2311.04345?ref=labelbox.ghost.io\"\u003e\u003cem\u003e\u003cu\u003e[2311.04345] A Taxonomy of Rater Disagreements: Surveying Challenges \u0026amp; Opportunities from the Perspective of Annotating Online Toxicity\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cu\u003e3) \u003c/u\u003e\u003c/em\u003e\u003ca href=\"https://archivepp.com/storage/models/article/Ax8JTUZ2hr0L7lYSZrZnCfnjHXEPcXw4I4idWZIgrqjsVNTOhaW0dRQ6x4T6/survey-of-agreement-between-raters-for-nominal-data-using-krippendorffs-alpha.pdf?ref=labelbox.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eSurvey of agreement between raters for nominal data using krippendorff's Alpha\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cu\u003e4) \u003c/u\u003e\u003c/em\u003e\u003ca href=\"https://arxiv.org/abs/1912.10107?ref=labelbox.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAssessing Data Quality of Annotations with Krippendorff Alpha For Applications in Computer Vision\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cu\u003e5) \u003c/u\u003e\u003c/em\u003e\u003ca href=\"https://aclanthology.org/E14-1058/?ref=labelbox.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eWeighted Krippendorff’s alpha is a more reliable metrics for multi-coders ordinal annotations: experimental studies on emotion, opinion and coreference annotation - ACL Anthology\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cu\u003e6)\u003c/u\u003e\u003c/em\u003e\u003ca href=\"https://arxiv.org/abs/2212.09503?ref=labelbox.ghost.io\"\u003e\u003cem\u003e\u003cu\u003e[2212.09503] Measuring Annotator Agreement Generally across Complex Structured, Multi-object, and Free-text Annotation Tasks\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cu\u003e7) \u003c/u\u003e\u003c/em\u003e\u003ca href=\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/?ref=labelbox.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eThinking about High-Quality Human Data | Lil'Log\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e","comment_id":"66e258daab75a10001bf83b1","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/09/Blog.png","featured":false,"visibility":"public","created_at":"2024-09-11T19:58:34.000-07:00","updated_at":"2025-06-15T09:12:25.000-07:00","published_at":"2024-09-11T20:23:00.000-07:00","custom_excerpt":"Inside the Quantumworks Lab AI data factory: take a look at the key tools, techniques, and processes that ensure top-quality data production, highlighting best practices for measuring and managing data quality.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"65302ef44e99900001fc0519","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox.ghost.io/blog/tag/industry-any/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"},{"id":"65303cb64e99900001fc05a5","name":"Labeling automation","slug":"labeling-automation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/labeling-automation/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"}],"authors":[{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"}],"primary_author":{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"},"primary_tag":{"id":"65302ef44e99900001fc0519","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox.ghost.io/blog/tag/industry-any/"},"url":"https://labelbox.ghost.io/blog/inside-the-data-factory-how-labelbox-produces-the-highest-quality-data-at-scale/","excerpt":"Inside the Quantumworks Lab AI data factory: take a look at the key tools, techniques, and processes that ensure top-quality data production, highlighting best practices for measuring and managing data quality.","reading_time":14,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6683870bb813720001afba9e","uuid":"36e607bc-9e2b-45d7-afd2-86524ef04531","title":"What the focus on multimodal, generative, and 3D AI means for the future at CVPR 2024","slug":"what-the-focus-on-multimodal-generative-and-3d-ai-mean-for-the-future-at-cvpr-2024","html":"\u003ch2 id=\"if-cvpr-is-the-pulse-of-sota-then-innovation-is-stronger-than-ever\"\u003eIf CVPR is the pulse of SOTA, then innovation is stronger than ever\u003c/h2\u003e\u003cp\u003eReflecting on the \u003ca href=\"https://labelbox.com/blog/recapping-cvpr-2023-key-takeaways-from-labelbox/?ref=labelbox.ghost.io\"\u003ekey takeaways from CVPR 2023\u003c/a\u003e and observing the advancements at CVPR 2024, it's evident that innovation in computer vision and AI is surging forward at an unprecedented pace. This year’s CVPR has been groundbreaking, breaking previous records with over 10,000 attendees and more than 2,500 research papers presented. This scale underscores the growing interest and investment in computer vision and pattern recognition research.\u003c/p\u003e\u003cp\u003eAnd if the \u003ca href=\"https://cvpr.thecvf.com/Conferences/2024/News/Wrap_Release?ref=labelbox.ghost.io\"\u003esheer number of papers and attendees\u003c/a\u003e isn’t convincing enough, maybe all the tweets showing packed rooms, attendees zooming in from hallways because of capacity challenges, and the packs of attendees wearing orange and green CVPR badges does.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXe7DJ6N01RF6xpvQOEBpvvYzCZbRyXQQGfWrXSFLoMzMP2YBBKv7ml4mwqFBlDZyFaKwOYkvygCOmXn19s1AwuMol1rwPgOiiOKQRAhO-5yJ6WOCBL5xemrQPaLrKLZkuFHmX5sVqEH_5Eh1Y2kPoin6W_A?key=zIKMTHkzae1s7CVvY4HBYQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1200\"\u003e\u003c/figure\u003e\u003ch1 id=\"the-most-popular-workshops-according-to-attendees\"\u003eThe most popular workshops according to attendees*\u003c/h1\u003e\u003cp\u003eEach day we would take an informal survey of the hundreds of attendees that visited the Quantumworks Lab booth, asking the following question: \u003c/p\u003e\u003cblockquote\u003e\u003cstrong\u003e“Which workshop was your favorite or stood out the most?”\u0026nbsp;\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThe clear winners were the following, all of which have recordings available (most likely because participants who weren’t lucky enough to make it in were encouraged to watch virtually).\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/8k7vdqgrye\" title=\"Clip 0: LBX Booth Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe top workshops (in no particular order) were:\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"ai-for-3d-generation\"\u003e\u003cstrong\u003eAI for 3D Generation\u003c/strong\u003e  \u003c/h2\u003e\u003cp\u003eThe \u003ca href=\"https://cvpr.thecvf.com/virtual/2024/workshop/23633?ref=labelbox.ghost.io\"\u003eworkshop on generative models for 3D data\u003c/a\u003e focused on the development of algorithms capable of creating realistic, high-quality 3D content at scale. It aimed to address the challenges in generating and controlling complex 3D environments, including human interactions and scene dynamics. The discussions covered the latest advancements in 3D object synthesis, intuitive control mechanisms, realistic human actions, and the ethical implications of artificial 3D content generation. This session featured speakers from top institutions and companies, including the University of Oxford, Stanford University, Carnegie Mellon University, Meta AI, Stability AI, Snap Research, NVIDIA Research, Adobe Research, Columbia University, etc.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/07/Screenshot-2024-07-02-at-9.40.46-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"588\" height=\"585\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://x.com/SergeyTulyakov/status/1802721597461262374?ref=labelbox.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOriginal tweet\u003c/span\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"towards-3d-foundation-models-progress-and-prospects\"\u003e\u003cstrong\u003eTowards 3D Foundation Models: Progress and Prospects\u003c/strong\u003e \u003c/h2\u003e\u003cp\u003eThe \u003ca href=\"https://cvpr.thecvf.com/virtual/2024/workshop/23670?ref=labelbox.ghost.io\"\u003eworkshop on 3D foundation models\u003c/a\u003e focused on developing scalable, generalizable, and adaptable 3D AI frameworks. It aimed to address the challenges of creating foundational 3D models for applications like 3D content creation, AR/VR, robotics, and autonomous driving. Discussions included necessary datasets, training tasks, architecture consensus, and potential applications. The session featured speakers from leading institutions and companies, including the University of Oxford, Stanford University, Columbia University, Technical University of Munich, UC San Diego, Adobe Research, KAUST, Allen Institute for AI, University of Toronto, CUHK Shenzhen, and Naver Labs.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/07/Screenshot-2024-07-02-at-9.41.37-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"599\" height=\"503\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://x.com/haosu_twitr/status/1803146639126810971?ref=labelbox.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOriginal tweet\u003c/span\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"the-future-of-generative-visual-art\"\u003e\u003cstrong\u003eThe Future of Generative Visual Art \u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe \u003ca href=\"https://cvpr.thecvf.com/virtual/2024/workshop/23670?ref=labelbox.ghost.io\"\u003eworkshop on generative visual art\u003c/a\u003e, part of the 4th AI for Creative Video Editing and Understanding (CVEU) series, focused on advancing machine learning technologies for creative visual content creation and understanding. It aimed to explore intuitive video editing tools, human-AI collaboration, reducing production costs, and addressing AI biases. The session featured speakers from top institutions and companies, including OpenAI, Carnegie Mellon University, Adobe Research, RunwayML, DeepMind, ByteDance, Google, Meta, Pika, and Eyeline Studios. \u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2024/07/Screenshot-2024-07-02-at-9.42.57-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"591\" height=\"901\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://x.com/jon_barron/status/1801282433243062420?ref=labelbox.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOriginal tweet\u003c/span\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"the-5th-annual-embodied-ai-workshop\"\u003e\u003cstrong\u003eThe 5th Annual Embodied AI Workshop \u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe \u003ca href=\"https://cvpr.thecvf.com/virtual/2024/workshop/23598?ref=labelbox.ghost.io\"\u003eworkshop on Embodied AI\u003c/a\u003e focused on developing intelligent agents, such as robots, that interact with their environment to solve challenging tasks. It aimed to address the integration of perception, action, and reasoning in dynamic, unstructured settings. Discussions included topics like embodied mobile manipulation, generative AI for data and policy generation, and language model planning for task execution. The session featured speakers from leading institutions and companies, including AI2, Microsoft, Meta, Google, DeepMind, Stanford University, and 1X Technologies.\u003c/p\u003e\u003ch2 id=\"additional-tracks\"\u003eAdditional tracks\u003c/h2\u003e\u003cp\u003eOutside of the workshops, CVPR 2024 offered an extensive array of tracks and workshops, catering to various interests such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHuman Understanding\u003c/strong\u003e: Tracks such as New Challenges in 3D Human Understanding and the Workshop on Human Motion Generation explored the latest research in understanding and modeling human actions and interactions.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedical Vision:\u003c/strong\u003e The 9th Workshop on Computer Vision for Microscopy Image Analysis and the Foundation Models for Medical Vision workshop highlighted the integration of AI in medical imaging and diagnostics.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMobile and Embedded Vision:\u003c/strong\u003e The 4th Mobile AI Workshop and Challenges and the Embedded Vision Workshop addressed the unique challenges and innovations in mobile and embedded AI applications.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNeural Rendering\u003c/strong\u003e: Tracks such as the 1st Workshop on Neural Volumetric Video and Neural Rendering Intelligence explored the cutting-edge developments in neural rendering technologies.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOpen World Learning\u003c/strong\u003e: The VAND 2.0: Visual Anomaly and Novelty Detection workshop and the Visual Perception via Learning in an Open World workshop examined the challenges and methodologies for learning in open-world environments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePhysics, Graphics, Geometry, AR/VR/MR\u003c/strong\u003e: Workshops like the 4th Workshop on Physics Based Vision meets Deep Learning and the Computer Vision for Mixed Reality workshop showcased the intersection of computer vision with physical simulations, graphics, and mixed reality applications.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eA significant number of workshops were recorded this year, so be sure to grab a digital pass if you weren’t able to attend.\u003c/p\u003e\u003ch1 id=\"high-quality-data-is-the-secret-ingredient-in-the-race-for-the-next-frontier-models-by-ai-labs\"\u003eHigh-quality data is the secret ingredient in the race for the next frontier models by AI labs\u003c/h1\u003e\u003cp\u003eAlongside our samples of High Brew coffee, common discussions overhead at the booth included:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe importance of synthetic data in overcoming the hurdles and expenses of annotating large datasets.\u0026nbsp;\u003c/li\u003e\u003cli\u003ePalpable excitement about new video models and the combined modeling of video and audio, reflecting a keen interest in multimodal techniques.\u0026nbsp;\u003c/li\u003e\u003cli\u003eAs well as a resurging sense of optimism about progress in robotics.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXdWuir0UKDQCdvg4-pkEvCNgHA2HgNQo1yPbFPLddWFJ_8j5KxVpNJBNYht-mOESRuLEJnsn1GizGZ65_Ilr_OVLT9IGJrd8pv7FjuYOoQLpWcq0E9jnc7dhQGUqAMTsjOE3SsWzYr33HhJceGJW2HQ35E?key=zIKMTHkzae1s7CVvY4HBYQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"900\"\u003e\u003c/figure\u003e\u003cp\u003eAttendees were also avidly discussing using both supervised and unsupervised data to mitigate data scarcity, revealing innovative approaches within the field (as well as an appetite for high-quality data).\u003c/p\u003e\u003cp\u003eLabelbox is at the forefront of supporting these trends with visitors highly interested in our \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox.ghost.io\"\u003enewest multimodal chat solution\u003c/a\u003e, which integrates advanced tooling and managed services to facilitate the creation, evaluation, and optimization of model responses across diverse data modalities. By offering robust tools and services with cross-modal support (such as text, images, videos, audio, and documents), AI researchers and enterprises are enabled to accelerate model building and refinement.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/pa5u4riktm\" title=\"Clip 1: Robot + Car Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLabelbox's new services model provides access to a network of highly-skilled experts, ensuring high-quality, human-evaluated data essential for Reinforcement Learning with Human Feedback (RLHF) and Supervised Fine Tuning (SFT).\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.alignerr.com/?ref=labelbox.ghost.io\"\u003eAlignerr\u003c/a\u003e, a community of subject matter experts, further complements these efforts by generating high-quality data to enhance AI models' safety, reliability, and accuracy. This collaboration between advanced platforms and expert networks underscores the transformative potential of integrating multimodal support and human expertise in the development of next-generation generative AI systems.\u003c/p\u003e\u003ch1 id=\"what-needs-to-happen-over-the-next-6-9-months-based-on-cvpr-2024\"\u003eWhat needs to happen over the next 6-9 months based on CVPR 2024\u003c/h1\u003e\u003cp\u003eThe future of AI is undeniably multimodal, extending to the data layer itself.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor foundation model developers and early innovators, each wave of generative AI has emphasized the critical importance of training data quality and availability. Initially, it was unstructured text data for LLMs, then image data for generative vision models, and now prompt-response data for visual language models.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcHAmVWFcdUtEmsUtijcIQMQtjZDw6plHrXt6R1OneISd-FAOUdK_zuq5s9CA8cH7vIEkPfCB2T2LyQFofo87TVgVRPt8qPiAxWwo0OdSskKgM-SesMlXrD21G05oCbapncpPASRxLmsr8h3NY8QqAleds?key=zIKMTHkzae1s7CVvY4HBYQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1600\"\u003e\u003c/figure\u003e\u003cp\u003eAs new foundation models are integrated into user-facing products, measuring and evaluating quality has become a key challenge. Developers must address questions like what constitutes good quality, how to train models to evaluate prompts and responses, and how to efficiently incorporate human expertise to ensure high-quality labels and responses. Systematically enforcing and improving quality across multiple modalities without wasting resources on fragmented tools and services is essential for progress in this field.\u003c/p\u003e\u003cp\u003eLabelbox plays a crucial role in supporting AI labs with high-quality data, enabling the development of advanced multimodal generative AI models. \u003c/p\u003e\u003cp\u003eWith a network of global expert labelers and a comprehensive suite of tools, Quantumworks Lab offers a fully managed service tailored to specific AI use cases. Our live \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox.ghost.io\"\u003emultimodal chat editor\u003c/a\u003e allows users to compare model outputs side-by-side, while \u003ca href=\"https://docs.labelbox.com/docs/prompt-and-response-generation?ref=labelbox.ghost.io\"\u003eprompt and response generation tools\u003c/a\u003e create datasets for fine-tuning tasks. \u003c/p\u003e\u003cp\u003eAdditionally, preference, selection, and ranking features incorporate human-in-the-loop (HITL) processes to ensure nuanced, high-quality labels for \u003ca href=\"https://labelbox.com/product/annotate/rlhf/?ref=labelbox.ghost.io\"\u003ereinforcement learning with human feedback (RLHF)\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eThis integrated approach helps AI labs systematically enforce and enhance data quality across various modalities, accelerating model development and deployment. \u003c/p\u003e\u003cp\u003eWhile current multimodal AI solutions often fall short of integrating multiple data types effectively, Quantumworks Lab addresses these challenges by providing seamless tools and expert support, ensuring robust and reliable multimodal AI systems.\u003c/p\u003e\u003ch2 id=\"ready-to-be-a-part-of-the-future-of-generative-ai\"\u003eReady to be a part of the future of generative-AI?\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eSign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more and we’d love to hear from you.\u003c/p\u003e\u003cp\u003eIf you're interested in joining Quantumworks Lab, check out our \u003ca href=\"https://labelbox.com/company/jobs/?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003ecareers page\u003c/a\u003e.\u003c/p\u003e","comment_id":"6683870bb813720001afba9e","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/07/YT-Thumbnails--31-.jpg","featured":false,"visibility":"public","created_at":"2024-07-01T21:50:19.000-07:00","updated_at":"2024-07-02T12:44:33.000-07:00","published_at":"2024-07-02T09:34:24.000-07:00","custom_excerpt":"CVPR 2024 recap: Exploring breakthroughs in multimodal, generative, and 3D AI, and their transformative implications for the future of artificial intelligence.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},{"id":"65302fd64e99900001fc0529","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-computer-vision/"},{"id":"65302fc14e99900001fc0527","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-llms/"}],"authors":[{"id":"65aab69e454cd800010e1a79","name":"Mikiko Bazeley","slug":"mikiko","profile_image":"https://labelbox.ghost.io/blog/content/images/2024/01/230829_0520.png","cover_image":null,"bio":null,"website":null,"location":"San Francisco","facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/mikiko/"},{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"}],"primary_author":{"id":"65aab69e454cd800010e1a79","name":"Mikiko Bazeley","slug":"mikiko","profile_image":"https://labelbox.ghost.io/blog/content/images/2024/01/230829_0520.png","cover_image":null,"bio":null,"website":null,"location":"San Francisco","facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/mikiko/"},"primary_tag":{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},"url":"https://labelbox.ghost.io/blog/what-the-focus-on-multimodal-generative-and-3d-ai-mean-for-the-future-at-cvpr-2024/","excerpt":"CVPR 2024 recap: Exploring breakthroughs in multimodal, generative, and 3D AI, and their transformative implications for the future of artificial intelligence.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}]},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"use-automation-to-reduce-your-labeling-time-and-spend"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/blog/use-automation-to-reduce-your-labeling-time-and-spend/?ref=labelbox.ghost.io by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:53:46 GMT -->
</html>