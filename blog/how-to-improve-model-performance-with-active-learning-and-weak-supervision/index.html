<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/blog/how-to-improve-model-performance-with-active-learning-and-weak-supervision/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:36:39 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">How to improve model performance with active learning and weak supervision</title><meta name="description" content="Learn how leading AI teams set up their model training pipelines with active learning and weak supervision — and how it&#x27;s boosted performance." data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="How to improve model performance with active learning and weak supervision" data-next-head=""/><meta property="og:description" content="Learn how leading AI teams set up their model training pipelines with active learning and weak supervision — and how it&#x27;s boosted performance." data-next-head=""/><meta property="og:url" content="https://labelbox.ghost.io/blog/how-to-improve-model-performance-with-active-learning-and-weak-supervision/" data-next-head=""/><meta property="og:image" content="https://labelbox.ghost.io/blog/content/images/2022/12/Active-learning-and-weak-supervision-blog-header.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="How to improve model performance with active learning and weak supervision" data-next-head=""/><meta name="twitter:description" content="Learn how leading AI teams set up their model training pipelines with active learning and weak supervision — and how it&#x27;s boosted performance." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.ghost.io/blog/how-to-improve-model-performance-with-active-learning-and-weak-supervision/" data-next-head=""/><meta property="twitter:image" content="https://labelbox.ghost.io/blog/content/images/2022/12/Active-learning-and-weak-supervision-blog-header.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g48[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.eivcj #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.eivcj .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.eivcj .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.eivcj #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.eivcj #image-viewer .close:hover,.eivcj #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.eivcj .modal-content{width:100%;}}/*!sc*/
data-styled.g105[id="ImageModal__ImageModalWrapper-sc-1ey7m7r-0"]{content:"eivcj,"}/*!sc*/
.QsqTL .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.QsqTL .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.QsqTL .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:20px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h2{padding-top:10px;}}/*!sc*/
.QsqTL .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h3{padding-top:10px;}}/*!sc*/
.QsqTL .content h4{font-size:20px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 16px;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content h4{padding-top:8px;}}/*!sc*/
.QsqTL .content h5{font-size:18px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 14px;}/*!sc*/
.QsqTL .content h6{font-size:16px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 12px;}/*!sc*/
.QsqTL .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.QsqTL .content a:hover{color:#1e40af;}/*!sc*/
.QsqTL .content li{margin-bottom:20px;}/*!sc*/
.QsqTL .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.QsqTL .content ol{list-style:decimal;padding-left:20px;}/*!sc*/
.QsqTL .content .table-container{overflow-x:auto;margin:40px 0;-webkit-overflow-scrolling:touch;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-container{margin:30px -20px;padding:0 20px;}}/*!sc*/
.QsqTL .content table{width:100%;border-collapse:collapse;font-size:16px;background:white;border:1px solid #e5e7eb;border-radius:8px;overflow:hidden;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content table{font-size:14px;}}/*!sc*/
.QsqTL .content .table-container table{margin:0;min-width:600px;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-container table{min-width:700px;border-radius:0;border-left:none;border-right:none;}}/*!sc*/
.QsqTL .content .content table:not(.table-container table){margin:40px 0;min-width:auto;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .content table:not(.table-container table){margin:30px 0;min-width:auto;border-radius:8px;border:1px solid #e5e7eb;}}/*!sc*/
.QsqTL .content thead{background:#fafbfc;border-bottom:1px solid #d1d5db;}/*!sc*/
.QsqTL .content th{padding:16px 20px;text-align:left;font-weight:600;color:#374151;font-size:14px;-webkit-letter-spacing:0.025em;-moz-letter-spacing:0.025em;-ms-letter-spacing:0.025em;letter-spacing:0.025em;border-right:1px solid #f3f4f6;}/*!sc*/
.QsqTL .content th:last-child{border-right:none;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content th{padding:12px 16px;font-size:13px;}}/*!sc*/
.QsqTL .content td{padding:16px 20px;border-bottom:1px solid #f3f4f6;border-right:1px solid #f9fafb;color:#374151;line-height:1.5;}/*!sc*/
.QsqTL .content td:last-child{border-right:none;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content td{padding:12px 16px;}}/*!sc*/
.QsqTL .content tbody tr{-webkit-transition:background-color 0.2s ease;transition:background-color 0.2s ease;}/*!sc*/
.QsqTL .content tbody tr:hover{background-color:#f8fafc;}/*!sc*/
.QsqTL .content tbody tr:last-child td{border-bottom:none;}/*!sc*/
.QsqTL .content .table-wrapper{overflow-x:auto;margin:40px 0;border:1px solid #e5e7eb;border-radius:8px;-webkit-overflow-scrolling:touch;}/*!sc*/
.QsqTL .content .table-wrapper table{margin:0;border:none;border-radius:0;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content .table-wrapper{margin:30px -20px;border-radius:0;border-left:none;border-right:none;}}/*!sc*/
.QsqTL .content code{background:#f1f5f9;padding:2px 6px;border-radius:4px;font-family:'Monaco','Menlo','Ubuntu Mono',monospace;font-size:14px;color:#e11d48;}/*!sc*/
.QsqTL .content pre{background:#1e293b;color:#e2e8f0;padding:20px;border-radius:8px;overflow-x:auto;margin:30px 0;}/*!sc*/
.QsqTL .content pre code{background:transparent;padding:0;color:inherit;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content pre{margin:20px -20px;border-radius:0;padding:16px 20px;}}/*!sc*/
.QsqTL .content blockquote{border-left:4px solid #2563eb;padding:20px 24px;margin:30px 0;background:#f8fafc;border-radius:0 8px 8px 0;font-style:italic;color:#475569;}/*!sc*/
.QsqTL .content blockquote p{margin-bottom:0;}/*!sc*/
.QsqTL .content blockquote p:last-child{margin-bottom:0;}/*!sc*/
@media only screen and (max-width:48rem){.QsqTL .content blockquote{margin:20px 0;padding:16px 20px;}}/*!sc*/
.QsqTL .content hr{border:none;height:1px;background:linear-gradient(to right,transparent,#e5e7eb,transparent);margin:50px 0;}/*!sc*/
.QsqTL .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.QsqTL .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;color:#6b7280;font-style:italic;}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.QsqTL .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.QsqTL .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;border-radius:8px;}/*!sc*/
.QsqTL .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.QsqTL .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100%;height:100%;margin:0 auto;border-radius:8px;}/*!sc*/
.QsqTL .content .kg-bookmark-card{background:white;border-radius:10px;margin-top:60px !important;border:1px solid #e5e7eb;-webkit-transition:border-color 0.3s ease;transition:border-color 0.3s ease;}/*!sc*/
.QsqTL .content .kg-bookmark-card:hover{border-color:#d1d5db;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;color:#262626 !important;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-thumbnail{position:relative;min-width:30%;max-height:100%;overflow:hidden;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-thumbnail img{position:absolute;top:0;left:0;width:100% !important;height:100% !important;-o-object-fit:cover;object-position:left;object-fit:cover;border-radius:0 10px 10px 0;border-left:1px solid #f5f5f5;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;padding:20px;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-title{font-size:1.125rem;line-height:1.3;font-weight:600;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-description{font-size:0.875rem;font-weight:400;line-height:1.4;margin-top:12px;overflow-y:hidden;color:#6b7280;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;font-size:0.9rem;font-weight:400;margin-top:14px;color:#6b7280;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata img{width:22px !important;height:22px !important;margin-right:8px !important;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata .kg-bookmark-author{margin:4px;}/*!sc*/
.QsqTL .content .kg-bookmark-card .kg-bookmark-container .kg-bookmark-content .kg-bookmark-metadata .kg-bookmark-publisher{margin:4px;}/*!sc*/
.QsqTL .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;margin:40px 0;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;margin-bottom:12px;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 6px;border-radius:6px;overflow:hidden;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;object-fit:cover;-webkit-transition:-webkit-transform 0.3s ease;-webkit-transition:transform 0.3s ease;transition:transform 0.3s ease;}/*!sc*/
.QsqTL .kg-gallery-container .kg-gallery-row img:hover{-webkit-transform:scale(1.02);-ms-transform:scale(1.02);transform:scale(1.02);}/*!sc*/
data-styled.g112[id="id__PostContentWrapper-sc-1hduup0-0"]{content:"QsqTL,"}/*!sc*/
@media (max-width:767px){.bwsQop.toc-container{display:none;}}/*!sc*/
.bwsQop.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.bwsQop.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g113[id="id__TocContainer-sc-1hduup0-1"]{content:"bwsQop,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/blog/%5bid%5d-b80b73d0fd88ad55.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="ImageModal__ImageModalWrapper-sc-1ey7m7r-0 eivcj"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All blog posts</a><main class="id__TocContainer-sc-1hduup0-1 bwsQop toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><p class="my-4 text-sm font-medium">Quantumworks Lab<span class="mx-2">•</span>December 19, 2022</p><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">How to improve model performance with active learning and weak supervision</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox.ghost.io/blog/content/images/2022/12/Active-learning-and-weak-supervision-blog-header.png"/></div><main class="id__PostContentWrapper-sc-1hduup0-0 QsqTL md:px-24"><div class="content js-toc-content"><p>AI teams of every maturity level often need to navigate similar sets of hurdles when it comes to improving model performance quickly: the problem of data and the problem of labeling said data. </p><p>Data issues are manifold — many teams have oceans of unstructured, inconsistent, diverse data from myriad sources, and mindfully curating a dataset that represents a realistic mix of necessary classes can be a laborious chore. And once the team moves on to getting their data labeled and verified via QA workflows to ensure quality, the time and costs required tends to climb. Even relatively small datasets can take weeks to label, depending on the complexity of the task.</p><p>That’s why leading AI teams, such as those at Edelman DxI, Deque Systems, and Advent Health Partners, are tackling these challenges with <a href="../../guides/the-guide-to-getting-started-with-active-learning/indexc625.html?ref=labelbox.ghost.io">active learning</a> and <a href="../how-to-improve-model-performance-with-less-data/indexc625.html?ref=labelbox.ghost.io">weak supervision</a>. Read on to see how these teams have set up their AI development pipelines — and how much they’ve accelerated model performance as a result.</p><h2 id="improving-model-performance-with-active-learning">Improving model performance with active learning</h2><h3 id="intelligent-sampling-via-embeddings">Intelligent sampling via embeddings</h3><p>When the AI team at Edelman DxI, a leading PR firm's research, analytics and data consultancy arm, set out to build an AI-powered platform to help their clients gauge the level of trust their audience had in their brand, figuring out how to pull a representative sample of data quickly became a challenge. Their data sources and types were varied, with articles on multiple blogs and new platforms, social media posts, survey answers, and more. They needed to make sure that both the content itself and its sources were distributed in a way realistic to what the AI model would encounter in production. </p><p>To get the right balance of content in their dataset, Edelman DxI built an intelligent sampling workflow, using <a href="../how-to-use-embeddings-to-create-high-quality-training-data/indexc625.html?ref=labelbox.ghost.io">embeddings</a> taken from an off-the shelf-transformer model to map their data by semantics — text that expressed positive, negative, or neutral sentiments. Though the first version of their model was trained on data sampled from the off-the-shelf model, later iterations were sampled using the model itself, a strategy that refined their sampling and enabled the team to tune the data more specifically to the machine learning model requirements. Later, the team used the same method to sample data from a spread of sources.</p><p>When the AI team at Deque set out to build an AI solution to identify accessibility issues on websites and applications, the data they collected was also widely varied. To sort through their data for specific examples to train their object detection model on radio buttons or tables, they used Quantumworks Lab <a href="../../product/catalog/indexc625.html?ref=labelbox.ghost.io">Catalog</a>’s <a href="../../guides/how-to-find-similar-data-in-one-click/indexc625.html?ref=labelbox.ghost.io">similarity search</a> feature, also based on embedding data.</p><p>“The Catalog feature in Quantumworks Lab has helped us to cluster data points based on design patterns. So one pretty good example of this that we found is lists and forms and tables with embeddings. In the Catalog feature, we can very easily find additional unlabeled data points that are coming in from our production data stream and select those for labeling. So we don't have to label or sort through the thousands of images that we get coming in,” says Noé Barrell, Machine Learning Engineer at Deque Systems.</p><h3 id="hybrid-uncertainty-sampling-and-semi-supervised-learning">Hybrid uncertainty sampling and semi-supervised learning</h3><p>The team at Advent Health Partners took a different active learning approach when faced with the challenge of utilizing data from varied sources and formats. As they set out to build a platform that extracted information from medical records to help hospitals, insurance companies, and other organizations process claims, appeals, and payments faster and more efficiently, they had to train their AI on new classes. </p><p>To sample data for the project, they evaluated their existing model over unlabeled samples and calculated the entropy of the output classification vector. The team then used these calculations to group data into two buckets: one with low entropy (meaning that the model was confident on this data), and one with high entropy, where the model had low confidence. The team could then sample data from these two groups, taking only 5-10% of their data from the low entropy group and the rest from the high entropy group, to train the model on new areas.</p><p>Once they trained the model on this dataset, the team realized that they had a serious class imbalance on their hands: one class was 25% of the dataset, while the least represented classes were only about 7% of the dataset. To correct this, they used the early version of their model to create a semi-supervised model that would balance the classes within their unlabeled data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2022/12/AHP_Class-Balance-Correction.png" class="kg-image" alt="" loading="lazy" width="1119" height="631" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2022/12/AHP_Class-Balance-Correction.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2022/12/AHP_Class-Balance-Correction.png 1000w, https://labelbox.ghost.io/blog/content/images/2022/12/AHP_Class-Balance-Correction.png 1119w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">By using a semi-supervised model to ensure that data sampled for labeling was evenly chosen among classes, the AI team at Advent Health Partners saw that the class balance of training data was significantly improved.</span></figcaption></figure><h2 id="improving-model-performance-with-weak-supervision">Improving model performance with weak supervision</h2><p>While using active learning techniques to sample data more effectively can reduce the amount of data that needs to be labeled at a time for a specific AI project, actually labeling the data can still require a large amount of time, talent, and resources. This is especially true for data from disparate sources, of various formats, and other added complexity such as fixing data drift and finding rare edge cases. </p><p>At Advent Health Partners, the medical records processed by their AI-driven platform are regularly over five hundred pages long, containing image and text files of varying formats. Classifying each page typically takes labelers around thirteen seconds. To lower the time and costs of labeling, the team used <a href="https://learn.labelbox.com/blue-river-mal/?ref=labelbox.ghost.io">model-assisted labeling</a> with Labelbox. They used the semi-supervised model from their active learning workflow to create weak labels, reducing the average time per label to eight seconds. </p><p>“This cut a full twenty-five hours off of the amount of time required for labeling tasks, and we found that the labelers had an easier time,” said Robert Coop, Chief AI Officer at Advent Health Partners.</p><p>Edelman DxI’s AI team also leveraged weak supervision to integrate domain expertise in the process without requiring experts to do the labeling work. “Labeling is expensive. You can't scale the process if your domain experts are doing the labeling. The challenge is incorporating domain expertise into our ML solutions,” says David Bartram-Shaw, SVP and Global Head of Data Science at Edelman DxI. </p><p>To accelerate and scale their labeling process, the Edelman DxI team set up a weak supervision loop that brought taxonomies and survey statements from clients and other domain experts, boolean queries, and other inputs into a labeling model, whose output was then used as weak labels for the model in training. As the model improved, these weak labels improved.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox.ghost.io/blog/content/images/2022/12/Edelman_WeakSupervision.png" class="kg-image" alt="" loading="lazy" width="2000" height="971" srcset="https://labelbox.ghost.io/blog/content/images/size/w600/2022/12/Edelman_WeakSupervision.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2022/12/Edelman_WeakSupervision.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2022/12/Edelman_WeakSupervision.png 1600w, https://labelbox.ghost.io/blog/content/images/2022/12/Edelman_WeakSupervision.png 2238w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">This weak supervision workflow enables Edelman DxI’s AI team to fold domain expertise into their AI solution without having experts perform all their labeling.</span></figcaption></figure><p>To learn more about how these leading AI teams leverage active learning and weak supervision strategies in their data engines, watch this <a href="https://event.on24.com/wcc/r/4016856/F8DE903937D5CF4EE82FB9D8573EAA32?ref=labelbox.ghost.io">session from Quantumworks Lab Accelerate 2022</a> on demand, featuring David Bartram-Shaw of Edelman DxI, Noé Barrell of Deque Systems, and Robert Coop of Advent Health Partners.</p></div></main></div></div></div><div class="mt-5 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="my-20 w-full h-[1px] bg-neutral-200"></div><div class="grid grid-cols-12 gap-2"><div class="col-span-12"><h2 class="mb-12 text-center text-3xl md:text-4xl font-medium">Continue reading</h2></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../the-power-of-human-expertise-transforming-audio-and-multimodal-stem-models-with-labelbox-services/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index632b.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2025%2F03%2FBlog_CustomerStories-1.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Esther Na<span class="mx-2">•</span>March 6, 2025</p></div><a href="../the-power-of-human-expertise-transforming-audio-and-multimodal-stem-models-with-labelbox-services/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">The power of human expertise: Transforming audio and multimodal STEM models with Quantumworks Lab Services</p><p class="text-base max-w-2xl undefined line-clamp-3">In this blog, learn about two AI lab customers who utilized Quantumworks Lab&#x27;s top-tier AI trainers to drive innovation in their audio and multimodal STEM models. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../code-runner-secure-scalable-code-execution-for-model-evaluation-2/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index4144.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FLabelbox-code-runner--1-.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Dmytro Apollonin<span class="mx-2">•</span>December 20, 2024</p></div><a href="../code-runner-secure-scalable-code-execution-for-model-evaluation-2/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Code Runner: Secure, scalable code execution for model evaluation</p><p class="text-base max-w-2xl undefined line-clamp-3">Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 xl:col-span-4"><div class="h-100"><div class="bg-white rounded-lg h-100 flex flex-col"><a href="../bringing-ai-to-the-browser-sam2-for-interactive-image-segmentation/index.html" target="_self" class="relative aspect-video  undefined undefined"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-t-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:left;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index447c.html?url=https%3A%2F%2Flabelbox.ghost.io%2Fblog%2Fcontent%2Fimages%2F2024%2F12%2FInteractive-Image-Segmentation-in-the-Browser-with-SAM2.png&amp;w=3840&amp;q=70"/></a><div class="p-6 flex flex-col flex-grow justify-content-between"><div><div><p class="my-4 text-sm font-medium">Stanislav Issayenko<span class="mx-2">•</span>December 19, 2024</p></div><a href="../bringing-ai-to-the-browser-sam2-for-interactive-image-segmentation/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Bringing AI to the browser: SAM2 for interactive image segmentation</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage the power of the SAM2 in a web browser, enabling interactive image segmentation without the need for powerful servers or specialized hardware.</p></a></div></div></div></div></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"how-to-improve-model-performance-with-active-learning-and-weak-supervision","id":"63a0df06a40626003d3256ae","uuid":"edcebfea-a6dd-4d52-9ca1-dc9404acf166","title":"How to improve model performance with active learning and weak supervision","html":"\u003cp\u003eAI teams of every maturity level often need to navigate similar sets of hurdles when it comes to improving model performance quickly: the problem of data and the problem of labeling said data. \u003c/p\u003e\u003cp\u003eData issues are manifold — many teams have oceans of unstructured, inconsistent, diverse data from myriad sources, and mindfully curating a dataset that represents a realistic mix of necessary classes can be a laborious chore. And once the team moves on to getting their data labeled and verified via QA workflows to ensure quality, the time and costs required tends to climb. Even relatively small datasets can take weeks to label, depending on the complexity of the task.\u003c/p\u003e\u003cp\u003eThat’s why leading AI teams, such as those at Edelman DxI, Deque Systems, and Advent Health Partners, are tackling these challenges with \u003ca href=\"https://labelbox.com/guides/the-guide-to-getting-started-with-active-learning/?ref=labelbox.ghost.io\"\u003eactive learning\u003c/a\u003e and \u003ca href=\"https://labelbox.com/blog/how-to-improve-model-performance-with-less-data/?ref=labelbox.ghost.io\"\u003eweak supervision\u003c/a\u003e. Read on to see how these teams have set up their AI development pipelines — and how much they’ve accelerated model performance as a result.\u003c/p\u003e\u003ch2 id=\"improving-model-performance-with-active-learning\"\u003eImproving model performance with active learning\u003c/h2\u003e\u003ch3 id=\"intelligent-sampling-via-embeddings\"\u003eIntelligent sampling via embeddings\u003c/h3\u003e\u003cp\u003eWhen the AI team at Edelman DxI, a leading PR firm's research, analytics and data consultancy arm, set out to build an AI-powered platform to help their clients gauge the level of trust their audience had in their brand, figuring out how to pull a representative sample of data quickly became a challenge. Their data sources and types were varied, with articles on multiple blogs and new platforms, social media posts, survey answers, and more. They needed to make sure that both the content itself and its sources were distributed in a way realistic to what the AI model would encounter in production. \u003c/p\u003e\u003cp\u003eTo get the right balance of content in their dataset, Edelman DxI built an intelligent sampling workflow, using \u003ca href=\"https://labelbox.com/blog/how-to-use-embeddings-to-create-high-quality-training-data/?ref=labelbox.ghost.io\"\u003eembeddings\u003c/a\u003e taken from an off-the shelf-transformer model to map their data by semantics — text that expressed positive, negative, or neutral sentiments. Though the first version of their model was trained on data sampled from the off-the-shelf model, later iterations were sampled using the model itself, a strategy that refined their sampling and enabled the team to tune the data more specifically to the machine learning model requirements. Later, the team used the same method to sample data from a spread of sources.\u003c/p\u003e\u003cp\u003eWhen the AI team at Deque set out to build an AI solution to identify accessibility issues on websites and applications, the data they collected was also widely varied. To sort through their data for specific examples to train their object detection model on radio buttons or tables, they used Quantumworks Lab \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox.ghost.io\"\u003eCatalog\u003c/a\u003e’s \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox.ghost.io\"\u003esimilarity search\u003c/a\u003e feature, also based on embedding data.\u003c/p\u003e\u003cp\u003e“The Catalog feature in Quantumworks Lab has helped us to cluster data points based on design patterns. So one pretty good example of this that we found is lists and forms and tables with embeddings. In the Catalog feature, we can very easily find additional unlabeled data points that are coming in from our production data stream and select those for labeling. So we don't have to label or sort through the thousands of images that we get coming in,” says Noé Barrell, Machine Learning Engineer at Deque Systems.\u003c/p\u003e\u003ch3 id=\"hybrid-uncertainty-sampling-and-semi-supervised-learning\"\u003eHybrid uncertainty sampling and semi-supervised learning\u003c/h3\u003e\u003cp\u003eThe team at Advent Health Partners took a different active learning approach when faced with the challenge of utilizing data from varied sources and formats. As they set out to build a platform that extracted information from medical records to help hospitals, insurance companies, and other organizations process claims, appeals, and payments faster and more efficiently, they had to train their AI on new classes. \u003c/p\u003e\u003cp\u003eTo sample data for the project, they evaluated their existing model over unlabeled samples and calculated the entropy of the output classification vector. The team then used these calculations to group data into two buckets: one with low entropy (meaning that the model was confident on this data), and one with high entropy, where the model had low confidence. The team could then sample data from these two groups, taking only 5-10% of their data from the low entropy group and the rest from the high entropy group, to train the model on new areas.\u003c/p\u003e\u003cp\u003eOnce they trained the model on this dataset, the team realized that they had a serious class imbalance on their hands: one class was 25% of the dataset, while the least represented classes were only about 7% of the dataset. To correct this, they used the early version of their model to create a semi-supervised model that would balance the classes within their unlabeled data.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/12/AHP_Class-Balance-Correction.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1119\" height=\"631\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2022/12/AHP_Class-Balance-Correction.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2022/12/AHP_Class-Balance-Correction.png 1000w, https://labelbox.ghost.io/blog/content/images/2022/12/AHP_Class-Balance-Correction.png 1119w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBy using a semi-supervised model to ensure that data sampled for labeling was evenly chosen among classes, the AI team at Advent Health Partners saw that the class balance of training data was significantly improved.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"improving-model-performance-with-weak-supervision\"\u003eImproving model performance with weak supervision\u003c/h2\u003e\u003cp\u003eWhile using active learning techniques to sample data more effectively can reduce the amount of data that needs to be labeled at a time for a specific AI project, actually labeling the data can still require a large amount of time, talent, and resources. This is especially true for data from disparate sources, of various formats, and other added complexity such as fixing data drift and finding rare edge cases. \u003c/p\u003e\u003cp\u003eAt Advent Health Partners, the medical records processed by their AI-driven platform are regularly over five hundred pages long, containing image and text files of varying formats. Classifying each page typically takes labelers around thirteen seconds. To lower the time and costs of labeling, the team used \u003ca href=\"https://learn.labelbox.com/blue-river-mal/?ref=labelbox.ghost.io\"\u003emodel-assisted labeling\u003c/a\u003e with Labelbox. They used the semi-supervised model from their active learning workflow to create weak labels, reducing the average time per label to eight seconds. \u003c/p\u003e\u003cp\u003e“This cut a full twenty-five hours off of the amount of time required for labeling tasks, and we found that the labelers had an easier time,” said Robert Coop, Chief AI Officer at Advent Health Partners.\u003c/p\u003e\u003cp\u003eEdelman DxI’s AI team also leveraged weak supervision to integrate domain expertise in the process without requiring experts to do the labeling work. “Labeling is expensive. You can't scale the process if your domain experts are doing the labeling. The challenge is incorporating domain expertise into our ML solutions,” says David Bartram-Shaw, SVP and Global Head of Data Science at Edelman DxI. \u003c/p\u003e\u003cp\u003eTo accelerate and scale their labeling process, the Edelman DxI team set up a weak supervision loop that brought taxonomies and survey statements from clients and other domain experts, boolean queries, and other inputs into a labeling model, whose output was then used as weak labels for the model in training. As the model improved, these weak labels improved.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/12/Edelman_WeakSupervision.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"971\" srcset=\"https://labelbox.ghost.io/blog/content/images/size/w600/2022/12/Edelman_WeakSupervision.png 600w, https://labelbox.ghost.io/blog/content/images/size/w1000/2022/12/Edelman_WeakSupervision.png 1000w, https://labelbox.ghost.io/blog/content/images/size/w1600/2022/12/Edelman_WeakSupervision.png 1600w, https://labelbox.ghost.io/blog/content/images/2022/12/Edelman_WeakSupervision.png 2238w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThis weak supervision workflow enables Edelman DxI’s AI team to fold domain expertise into their AI solution without having experts perform all their labeling.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eTo learn more about how these leading AI teams leverage active learning and weak supervision strategies in their data engines, watch this \u003ca href=\"https://event.on24.com/wcc/r/4016856/F8DE903937D5CF4EE82FB9D8573EAA32?ref=labelbox.ghost.io\"\u003esession from Quantumworks Lab Accelerate 2022\u003c/a\u003e on demand, featuring David Bartram-Shaw of Edelman DxI, Noé Barrell of Deque Systems, and Robert Coop of Advent Health Partners.\u003c/p\u003e","comment_id":"63a0df06a40626003d3256ae","feature_image":"https://labelbox.ghost.io/blog/content/images/2022/12/Active-learning-and-weak-supervision-blog-header.png","featured":false,"visibility":"public","created_at":"2022-12-19T14:00:38.000-08:00","updated_at":"2023-10-26T10:55:35.000-07:00","published_at":"2022-12-19T14:34:49.000-08:00","custom_excerpt":"Learn how leading AI teams set up their model training pipelines with active learning and weak supervision — and how it's boosted performance.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/blog/how-to-improve-model-performance-with-active-learning-and-weak-supervision/","tags":[{"id":"65303cb64e99900001fc05a5","name":"Labeling automation","slug":"labeling-automation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/labeling-automation/"},{"id":"65302fd64e99900001fc0529","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-computer-vision/"},{"id":"65302f4b4e99900001fc0521","name":"Industry: Healthcare \u0026 life science","slug":"industry-healthcare-life-science","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox.ghost.io/blog/tag/industry-healthcare-life-science/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"}],"authors":[{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"}],"primary_author":{"id":"5c48ee4aa3b65500ccb6ae25","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":"https://labelbox.ghost.io/blog/content/images/2021/12/Frame-2227-1.svg","cover_image":null,"bio":"Quantumworks Lab is a collaborative training data platform empowering teams to rapidly build artificial intelligence applications.","website":null,"location":"San Francisco","facebook":"getlabelbox/","twitter":"@Quantumworks Lab","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/Quantumworks Lab/"},"primary_tag":{"id":"65303cb64e99900001fc05a5","name":"Labeling automation","slug":"labeling-automation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/labeling-automation/"},"url":"https://labelbox.ghost.io/blog/how-to-improve-model-performance-with-active-learning-and-weak-supervision/","excerpt":"Learn how leading AI teams set up their model training pipelines with active learning and weak supervision — and how it's boosted performance.","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":"How to improve model performance with active learning and weak supervision","og_description":"Learn how leading AI teams set up their model training pipelines with active learning and weak supervision — and how it's boosted performance.","twitter_image":null,"twitter_title":"How to improve model performance with active learning and weak supervision","twitter_description":"Learn how leading AI teams set up their model training pipelines with active learning and weak supervision — and how it's boosted performance.","meta_title":"How to improve model performance with active learning and weak supervision","meta_description":"Learn how leading AI teams set up their model training pipelines with active learning and weak supervision — and how it's boosted performance.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},"recommended":[{"id":"67ca1d6cb890cf0001e11bef","uuid":"16aa9964-ac17-468a-bef9-f6dddf71c1a7","title":"The power of human expertise: Transforming audio and multimodal STEM models with Quantumworks Lab Services","slug":"the-power-of-human-expertise-transforming-audio-and-multimodal-stem-models-with-labelbox-services","html":"\u003cp\u003eLeading frontier AI builders leverage domain-and language-specific expertise to differentiate their models across data modalities—including audio, multimodal, image, text, and video—and to train them for more complex tasks. As the capabilities of AI expands, the need for post-training processes like SFT, RLHF, and human evaluation remain strong. These tasks depend on expert human knowledge to guide models toward higher performance.This is where we come in. Quantumworks Lab is the AI data factory that delivers high-quality data across all modalities, including specialized domains like STEM, finance, coding, and law—across a wide range of languages for each.\u0026nbsp; \u003c/p\u003e\u003cp\u003eWith \u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eLabelbox Labeling Services\u003c/u\u003e\u003c/a\u003e, we harness our skilled talent network, \u003ca href=\"https://www.alignerr.com/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eAlignerr\u003c/u\u003e\u003c/a\u003e, to source, vet, and onboard custom teams of human experts who can align models and generate new training data with domain-specific knowledge. Quantumworks Lab can operate and fully-manage a project with Alignerrs and the Quantumworks Lab Platform that generates new training data for our customers, or through \u003ca href=\"https://labelbox.com/services/alignerr-connect/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eAlignerr Connect\u003c/u\u003e\u003c/a\u003e, customers can browse and select top-tier experts to staff their existing projects and utilize their in-house tools and processes. In this blog, we highlight two recent customers who utilized our top-tier human experts to drive innovation in their cutting-edge audio and multimodal STEM models.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"driving-breakthroughs-in-the-ai-audio-landscape\"\u003eDriving breakthroughs in the AI audio landscape\u0026nbsp;\u003c/h2\u003e\u003cp\u003eA growing AI audio startup aimed to enhance its voice, speech, and sound models by training with expert-labeled data. They faced challenges due to the subjectivity of labeling large volumes of complex audio. Quantumworks Lab addressed this through our platform’s custom audio editor and building a team of trainers with expertise in voice acting and speech. Their background enabled them to label nuanced audio segments with greater accuracy than generalists.\u003c/p\u003e\u003cp\u003eOne of the Alignerrs on the project, Jeff K., has a PhD in Theater and Performance Studies. He shared this about his experience on the project:\u003c/p\u003e\u003cp\u003e\u003cem\u003e“Through years of performing and teaching the arts, I've developed a deep understanding of voice dynamics. I have mental checklists for how and where voices change, which makes it natural for me to identify the various emotions in speech and understand their impact on the listener.\" \u003c/em\u003e\u003cbr\u003e\u003cbr\u003eThis specialized level of human expertise was critical in enabling the startup to create high-quality audio datasets and enhance their AI models, driving the adoption of their advanced audio technology. \u003cbr\u003e\u003cbr\u003eWant to explore the full story behind this success? Read more \u003ca href=\"https://labelbox.com/customers/cutting-edge-audio-models-customer-story/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"improving-multimodal-reasoning-capabilities-with-stem-experts\"\u003eImproving multimodal reasoning capabilities with STEM experts\u003c/h2\u003e\u003cp\u003eA leading AI lab aimed to enhance its large language model (LLM) by identifying weaknesses in K-12 STEM education responses, but they needed a diverse team of STEM experts to create new training data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox’s Labeling Services brought together a team of highly skilled experts with PhDs and Masters in STEM, who were tasked with reviewing multimodal prompts and responses spanning various domains, including natural science, physics, earth science, and language comprehension. Each task included reviewing the model’s response to a question that contained metadata such as question format, subject category, and an associated image URL.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe AI trainers then had to rate the answers across a number of different areas, including examining the model’s ability to read text, analyze image, and respond correctly. This collaboration helped the lab pinpoint areas for improvement and boost performance with high-quality, domain-specific STEM data.\u003c/p\u003e\u003cp\u003eIf you are interested in learning more about this work, read more \u003ca href=\"https://labelbox.com/customers/multimodal-STEM-customer-story/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"looking-to-leverage-labelbox%E2%80%99s-expert-ai-trainers\"\u003eLooking to leverage Quantumworks Lab’s expert AI trainers?\u003c/h2\u003e\u003cp\u003eThese two customer stories highlight a few examples of the groundbreaking work Quantumworks Lab is performing with frontier model builders.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you’d like to learn more about the expert teams we offer and are ready to discuss your data needs, \u003ca href=\"https://labelbox.com/sales/?ref=labelbox.ghost.io\"\u003e\u003cu\u003econtact our team\u003c/u\u003e\u003c/a\u003e anytime on how we can help. You can also directly explore profiles of some of our AI trainers \u003ca href=\"https://labelbox.com/services/alignerr-connect/trainers/?ref=labelbox.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"67ca1d6cb890cf0001e11bef","feature_image":"https://labelbox.ghost.io/blog/content/images/2025/03/Blog_CustomerStories-1.png","featured":false,"visibility":"public","created_at":"2025-03-06T14:10:52.000-08:00","updated_at":"2025-03-31T14:44:29.000-07:00","published_at":"2025-03-06T14:19:30.000-08:00","custom_excerpt":"In this blog, learn about two AI lab customers who utilized Quantumworks Lab's top-tier AI trainers to drive innovation in their audio and multimodal STEM models. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"671a9e7504d48b00016d39a3","name":"Esther Na","slug":"esther","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/esther/"}],"tags":[{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"},{"id":"66e9be59d0584c0001886b42","name":"Services","slug":"services","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/services/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"},{"id":"670018ec863cb90001f263e9","name":"Customers","slug":"customers","description":"Quantumworks Lab customer stories","feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/customers/"}],"primary_author":{"id":"671a9e7504d48b00016d39a3","name":"Esther Na","slug":"esther","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/esther/"},"primary_tag":{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},"url":"https://labelbox.ghost.io/blog/the-power-of-human-expertise-transforming-audio-and-multimodal-stem-models-with-labelbox-services/","excerpt":"In this blog, learn about two AI lab customers who utilized Quantumworks Lab's top-tier AI trainers to drive innovation in their audio and multimodal STEM models. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Customers transform audio and multimodal AI with Quantumworks Lab Services","meta_description":"In this blog, learn about two AI lab customers who utilized Quantumworks Lab's top-tier AI trainers to drive innovation in their audio and multimodal STEM models. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6765b8c06f63bf0001f1ca72","uuid":"9f912bc0-54da-4ac6-ab5f-78d8f926463c","title":"Code Runner: Secure, scalable code execution for model evaluation","slug":"code-runner-secure-scalable-code-execution-for-model-evaluation-2","html":"\u003cp\u003eIn the world of large language models (LLMs), evaluating their responses effectively is a fundamental aspect of improving model performance. We’re excited to announce the latest addition to the Quantumworks Lab platform: Code Runner.\u003cstrong\u003e \u003c/strong\u003eThis new capability pushes the boundaries of interactivity by allowing users to execute written code directly within the evaluation workflow.\u003c/p\u003e\u003cp\u003eCode Runner helps eliminate errors, optimizes functionality, and validates outputs, leading to higher-quality datasets. Today, we’ll introduce this new feature and then dive into the technical details of the infrastructure powering this feature, highlighting how it was designed with security, scalability, and\u003cstrong\u003e \u003c/strong\u003erobustness at its core.\u003c/p\u003e\u003ch2 id=\"what-is-code-runner\"\u003e\u003cstrong\u003eWhat is Code Runner?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCode Runner is a new built-in feature of the Quantumworks Lab platform designed to improve the quality of responses and labels generated in any coding-related projects. The new features enables users to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eDirectly execute code found in either model responses or user-written responses \u003c/li\u003e\u003cli\u003eReceive precise outputs including:\u003cul\u003e\u003cli\u003eStandard output (stdout)\u003c/li\u003e\u003cli\u003eStandard error (stderr)\u003c/li\u003e\u003cli\u003eExecution time\u003c/li\u003e\u003cli\u003eWarnings or runtime errors\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy integrating Code Runner into the evaluation pipeline, we aim to simplify the process of verifying the accuracy, efficiency, and functionality of code responses, all without users needing to leave the platform.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeBDN_1bnU_bTrPrWS59SWalVqw22Gxq3AIxNnbsOJmZGPap3weXHYFEgzrlPnEyhVK1GOjzCVClvQycomfMfhQsulqPk4wdQGqniZv8aIaHGP69wzgcFjdDdr5FgooITwNJCsp?key=GRyWmie9kDWaUfN6osDAF8J7\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"389\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOur system automatically detects the language in the text area and suggests the appropriate environment for execution, whether Python or JavaScript (and more to come).\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBut what makes this feature stand out is the sophisticated infrastructure behind it, designed to ensure seamless execution while maintaining strict security and privacy standards.\u003c/p\u003e\u003ch2 id=\"code-runner-infrastructure-a-deep-dive\"\u003e\u003cstrong\u003eCode Runner infrastructure: A deep dive\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAt the heart of Code Runner’s infrastructure lies Google Cloud Run, a fully managed compute platform that runs containerized applications in a secure, scalable manner. Here are the key components and principles driving the system:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Cloud Run for language-specific environments\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eEvery code execution happens in a dedicated Cloud Run instance. Each instance is tailored to a specific programming language environment (e.g., Python, JavaScript, etc.) and is spun up dynamically based on the code type detected in the user response.\u003c/p\u003e\u003cp\u003eThis design includes the following characteristics to ensure security and speed:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIsolation\u003c/strong\u003e: Each execution is fully containerized, completely isolating the runtime environment from others.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTemporary directories\u003c/strong\u003e: Code is executed in a temporary directory within the container, and it is deleted immediately after execution, leaving no trace behind.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLanguage-specific tools\u003c/strong\u003e: Each environment comes preloaded with the necessary packages and libraries to ensure compatibility and speed.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2. Enhanced security with separate GCP projects \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe Cloud Run service is hosted in a separate Google Cloud Platform (GCP) project, distinct from our main infrastructure. This segmentation provides an additional layer of security by isolating code execution from our core services. Even in the unlikely event of a compromise, the blast radius is contained.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Communication via private service connect\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo ensure secure and controlled communication, all interactions between the main evaluation system and the Cloud Run service occur over Private Service Connect, which provides the following advantages: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNo public exposure\u003c/strong\u003e: The Cloud Run endpoint is never exposed to the public internet, reducing the risk of unauthorized access.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOne-way communication\u003c/strong\u003e: The Private Service Connect setup restricts outbound networking from the Cloud Run service, ensuring that executed code cannot make arbitrary network requests. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGranular networking controls\u003c/strong\u003e: The private network allows for precise control over what resources the Cloud Run service can access.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4. Automatic cleanup\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo maintain a lightweight and secure runtime, the system delivers:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEphemeral execution\u003c/strong\u003e: Each execution request is handled in a stateless, temporary environment.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAutomatic deletion\u003c/strong\u003e: Files, logs, and temporary directories are wiped as soon as execution completes, leaving no residual data.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"how-code-runner-works-a-step-by-step-overview\"\u003e\u003cstrong\u003eHow Code Runner works: A step-by-step overview\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow that you have an understanding of the powerful infrastructure underneath Code Runner, here is a summary of how the feature works from start to finish:\u0026nbsp;\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eCode submission\u003c/strong\u003e: A user requests code execution from the evaluation interface.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLanguage detection\u003c/strong\u003e: The system detects the programming language and forwards the request to the corresponding Cloud Run service.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eExecution\u003c/strong\u003e: The Cloud Run instance spins up a container, executes the code in a sandboxed environment, and collects the results.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResult delivery\u003c/strong\u003e: The system returns the output (stdout, stderr, execution time, and any warnings) to the user for analysis.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCleanup\u003c/strong\u003e: The container and all related resources are terminated and deleted.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"advantages-of-labelbox%E2%80%99s-built-in-code-execution\"\u003e\u003cstrong\u003eAdvantages of Quantumworks Lab’s built-in code execution\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCode Runner’s infrastructure was designed specifically to provide the previously discussed benefits and to address several key challenges that other solutions may face:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSecurity\u003c/strong\u003e: By isolating execution environments and ensuring no public exposure, we eliminate a significant attack surface.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability\u003c/strong\u003e: Cloud Run’s serverless nature allows us to scale dynamically with demand, handling thousands of requests efficiently.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eReliability\u003c/strong\u003e: The use of ephemeral containers ensures that each execution starts in a clean slate, avoiding cross-contamination or resource conflicts.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"explore-it-yourself\"\u003e\u003cstrong\u003eExplore it yourself\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWith Code Runner, we’re empowering users to go beyond static evaluations, enabling dynamic, interactive testing that’s as secure as it is scalable. As always, we’re excited to hear your feedback and explore how we can push this feature even further.\u003c/p\u003e\u003cp\u003e If you want to explore Code Runner and other LLM evaluation tools, \u003ca href=\"https://app.labelbox.com/signup?_r=https://landing-page-git-uv-homepage-refresh-dec-24-labelbox.vercel.app/?utm_keyword=Quantumworks Lab\u0026utm_source=house\u0026utm_medium=email\u0026utm_campaign=1224%2520\u0026gclid=CjwKCAiA34S7BhAtEiwACZzv4a9veoKXnMnMvo2rWJvXkH46oHs4Lb5VFQi2ERBN_sQ5kgypV_zfBxoC0yMQAvD_BwE\u0026landingPageAnonymousId=%22e3f2f82f-be24-4045-b2b9-50a49cb801e8%22\u0026referrer_url=https://landing-page-git-uv-homepage-refresh-dec-24-labelbox.vercel.app/\"\u003e\u003cu\u003esign up\u003c/u\u003e\u003c/a\u003e for our platform today.\u0026nbsp;\u003c/p\u003e\u003cp\u003eStay tuned for updates, and happy coding!\u003c/p\u003e","comment_id":"6765b8c06f63bf0001f1ca72","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/12/Labelbox-code-runner--1-.png","featured":false,"visibility":"public","created_at":"2024-12-20T10:34:40.000-08:00","updated_at":"2025-03-12T12:01:43.000-07:00","published_at":"2024-12-20T12:44:45.000-08:00","custom_excerpt":"Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6765bb156f63bf0001f1ca8d","name":"Dmytro Apollonin","slug":"dmytro","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/dmytro/"}],"tags":[{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},{"id":"5f9af89dd8118b0039136165","name":"Engineering","slug":"engineering","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/engineering/"},{"id":"65302fc14e99900001fc0527","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/building-llms/"},{"id":"65302fb54e99900001fc0525","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox.ghost.io/blog/tag/build-ai/"},{"id":"66e35bd2f39c8800019d9143","name":"Generative AI","slug":"generative-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/generative-ai/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"},{"id":"65303cb64e99900001fc05a5","name":"Labeling automation","slug":"labeling-automation","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/labeling-automation/"},{"id":"6530313c4e99900001fc0537","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6765bb156f63bf0001f1ca8d","name":"Dmytro Apollonin","slug":"dmytro","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/dmytro/"},"primary_tag":{"id":"5fcebc2a9903c40039e8b87e","name":"Announcement","slug":"announcement","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#3B82F6","url":"https://labelbox.ghost.io/blog/tag/announcement/"},"url":"https://labelbox.ghost.io/blog/code-runner-secure-scalable-code-execution-for-model-evaluation-2/","excerpt":"Meet Code Runner, the new in-platform code execution engine designed to simplify coding-related tasks and deliver higher-quality datasets for coding-related projects.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6759fc88a9b5bd0001989d30","uuid":"58632854-1031-4ac5-8fe3-87324337cbe7","title":"Bringing AI to the browser: SAM2 for interactive image segmentation","slug":"bringing-ai-to-the-browser-sam2-for-interactive-image-segmentation","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eIn computer vision, image segmentation is crucial for object recognition, image editing, autonomous driving, and other common applications. The Segment Anything Model 2 (SAM2) pushes the boundaries of interactive image segmentation by allowing users to segment objects in images with minimal input.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTraditionally, running such models required powerful servers or specialized hardware. However, with advancements in web technologies and machine learning libraries, it's now possible to run complex models like SAM2 directly in a browser.\u003c/p\u003e\u003cp\u003eIn this article, we explore how to implement the SAM2 model in a web browser using ONNX Runtime Web (ort). We delve into the architecture of SAM2, how to load and run the model in a browser, and how to create an interactive user interface for real-time image segmentation.\u003c/p\u003e\u003ch2 id=\"understanding-the-sam2-architecture\"\u003eUnderstanding the SAM2 architecture\u003c/h2\u003e\u003ch3 id=\"encoder-decoder-framework\"\u003eEncoder-decoder framework\u003c/h3\u003e\u003cp\u003eSAM2 uses an encoder-decoder architecture:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEncoder:\u003c/strong\u003e Processes the input image to generate a high-dimensional embedding that captures essential features.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDecoder:\u003c/strong\u003e Takes the embedding and user-provided points (positive and negative) to generate segmentation masks.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis architecture allows for interactive and highly accurate segmentation, while also allowing users to\u0026nbsp; iteratively refine the segmentation by adding more points.\u003c/p\u003e\u003ch3 id=\"why-run-sam2-in-the-browser\"\u003eWhy Run SAM2 in the Browser?\u003c/h3\u003e\u003cp\u003eRunning SAM2 in the browser offers several key benefits:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrivacy:\u003c/strong\u003e Images are processed locally, ensuring user data isn't sent to external servers.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAccessibility:\u003c/strong\u003e Users can access the segmentation tool without installing specialized software.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInteractivity:\u003c/strong\u003e Real-time feedback enhances the user experience, allowing for quick iterations.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"implementing-the-encoder-encoderjs\"\u003eImplementing the encoder (encoder.js)\u003c/h2\u003e\u003ch3 id=\"initialization\"\u003eInitialization\u003c/h3\u003e\u003cp\u003eThe encoder loads the ONNX model and prepares it for inference:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003econst ENCODER_MODEL_URL = 'https://storage.googleapis.com/lb-artifacts-testing-public/sam2/sam2_hiera_tiny.encoder.ort';\n\nclass SAM2Encoder {\n  constructor() {\n    this.session = null;\n  }\n\n  async initialize() {\n    this.session = await ort.InferenceSession.create(ENCODER_MODEL_URL);\n    console.log('Encoder model loaded successfully');\n  }\n}\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"image-preprocessing\"\u003eImage preprocessing\u003c/h3\u003e\u003cp\u003eBefore passing the image to the encoder, it must be resized and normalized:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003e Resize:\u003c/strong\u003e Adjust the image to 1024x1024 pixels.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNormalize:\u003c/strong\u003e Scale pixel values to the [-1, 1] range.\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003eimageDataToTensor(image) {\n  const canvas = document.createElement('canvas');\n  const ctx = canvas.getContext('2d');\n  canvas.width = canvas.height = 1024;\n\n  ctx.drawImage(image, 0, 0, 1024, 1024);\n  const imageData = ctx.getImageData(0, 0, 1024, 1024).data;\n  const inputArray = new Float32Array(3 * 1024 * 1024);\n\n  for (let i = 0; i \u0026lt; 1024 * 1024; i++) {\n    inputArray[i] = (imageData[i * 4] / 255.0) * 2 - 1; // R channel\n    inputArray[i + 1024 * 1024] = (imageData[i * 4 + 1] / 255.0) * 2 - 1; // G channel\n    inputArray[i + 2 * 1024 * 1024] = (imageData[i * 4 + 2] / 255.0) * 2 - 1; // B channel\n  }\n\n  return new ort.Tensor('float32', inputArray, [1, 3, 1024, 1024]);\n}\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"encoding-process\"\u003eEncoding process\u003c/h3\u003e\u003cp\u003eThe encode method runs the model and generates the image embedding:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003easync encode(image) {\n  const tensor = this.imageDataToTensor(image);\n  const feeds = { image: tensor };\n  const results = await this.session.run(feeds);\n  this.lastEmbeddings = results.image_embed;\n  return this.lastEmbeddings;\n}\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"implementing-the-decoder-decoderjs\"\u003eImplementing the decoder (decoder.js)\u003c/h2\u003e\u003ch3 id=\"initialization-1\"\u003eInitialization\u003c/h3\u003e\u003cp\u003eSimilar to the encoder, the decoder loads its ONNX model:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003econst DECODER_MODEL_URL = 'https://storage.googleapis.com/lb-artifacts-testing-public/sam2/sam2_hiera_tiny.decoder.onnx';\n\nclass SAM2Predictor {\n  constructor() {\n    this.session = null;\n  }\n\n  async initialize() {\n    this.session = await ort.InferenceSession.create(DECODER_MODEL_URL);\n    console.log('Decoder model loaded successfully');\n  }\n}\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"preparing-inputs\"\u003ePreparing inputs\u003c/h3\u003e\u003cp\u003eThe decoder requires several inputs, including the image embedding and user interaction points:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePoint coordinates (point_coords):\u003c/strong\u003e The (x, y) positions of user clicks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePoint labels (point_labels):\u003c/strong\u003e Indicates positive (foreground) or negative (background) points.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMask input (mask_input):\u003c/strong\u003e An initial mask, set to zeros if not using a previous mask.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOther tensors:\u003c/strong\u003e Additional required inputs like has_mask_input, high_res_feats_0, and high_res_feats_1.\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003eprepareInputs(embedding, points) {\n  const numLabels = 1;\n  const numPoints = points.length;\n  const pointCoordsData = [];\n  const pointLabelsData = [];\n\n  for (let point of points) {\n    pointCoordsData.push([point.x, point.y]);\n    pointLabelsData.push(point.type);\n  }\n\n  return {\n    image_embed: embedding,\n    point_coords: new ort.Tensor('float32', Float32Array.from(pointCoordsData.flat()), [numLabels, numPoints, 2]),\n    point_labels: new ort.Tensor('float32', Float32Array.from(pointLabelsData), [numLabels, numPoints]),\n    mask_input: new ort.Tensor('float32', new Float32Array(numLabels * 1 * 256 * 256), [numLabels, 1, 256, 256]),\n    has_mask_input: new ort.Tensor('float32', new Float32Array([0.0]), [numLabels]),\n    high_res_feats_0: new ort.Tensor('float32', new Float32Array(1 * 32 * 256 * 256), [1, 32, 256, 256]),\n    high_res_feats_1: new ort.Tensor('float32', new Float32Array(1 * 64 * 128 * 128), [1, 64, 128, 128]),\n  };\n}\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"prediction-process\"\u003ePrediction Process\u003c/h3\u003e\u003cp\u003eThe predict method runs the decoder model to generate the segmentation mask:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003easync predict(embedding, inputPoints) {\n  const inputs = this.prepareInputs(embedding, inputPoints);\n  const results = await this.session.run(inputs);\n  return results;\n}\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"building-the-user-interface-appjs\"\u003eBuilding the user interface (app.js)\u003c/h2\u003e\u003ch3 id=\"setting-up-the-canvas\"\u003eSetting up the canvas\u003c/h3\u003e\u003cp\u003eWe use two HTML canvas elements:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSource canvas (sourceCanvas):\u003c/strong\u003e Displays the uploaded image and interaction points.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMask canvas (maskCanvas):\u003c/strong\u003e Overlays the segmentation mask.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"handling-user-interactions\"\u003eHandling user interactions\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eImage upload\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhen a user uploads an image two things happen:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe image is drawn on the sourceCanvas.\u003c/li\u003e\u003cli\u003eThe encoder generates embeddings from the image.\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003eimageInput.addEventListener('change', async (e) =\u0026gt; {\n  // Load image and draw on canvas\n  // Encode the image\n  embedding = await encoder.encode(img);\n});\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eAdding interaction points\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eUsers can click on the image to add positive or negative points:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePositive points:\u003c/strong\u003e Indicate areas to include in the segmentation.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNegative points:\u003c/strong\u003e Indicate areas to exclude.\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003esourceCanvas.addEventListener('click', async (e) =\u0026gt; {\n  const x = e.clientX - rect.left;\n  const y = e.clientY - rect.top;\n  const point = { x: x, y: y, type: isNegative ? 0 : 1 };\n  points.push(point);\n\n  // Draw point on canvas\n  drawPoint(sourceCtx, point);\n\n  // Run prediction\n  const results = await predictor.predict(embedding, points);\n\n  // Draw mask\n  drawMaskOnCanvas(maskCanvas, results['masks'], imageWidth, imageHeight);\n});\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eToggling point types\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eA button allows users to switch between positive and negative points:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003enegativeBtn.addEventListener('click', () =\u0026gt; {\n  isNegative = !isNegative;\n  negativeBtn.textContent = `Negative Points: ${isNegative ? 'ON' : 'OFF'}`;\n});\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"drawing-points-and-masks\"\u003eDrawing points and masks\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eDrawing points\u003c/strong\u003e\u003c/p\u003e\u003cp\u003ePoints are drawn on the sourceCanvas to provide visual feedback:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003efunction drawPoint(ctx, point) {\n  ctx.fillStyle = point.type === 1 ? 'green' : 'red';\n  ctx.beginPath();\n  ctx.arc(point.x, point.y, 5, 0, 2 * Math.PI);\n  ctx.fill();\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eDrawing masks\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eMasks are drawn on the maskCanvas to display the segmentation result:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Javascript\"\u003efunction drawMaskOnCanvas(maskCanvas, maskData, imageWidth, imageHeight) {\n  // Process the mask tensor and draw it over the image\n}\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"running-the-application\"\u003eRunning the application\u003c/h2\u003e\u003ch3 id=\"step-by-step-guide\"\u003eStep-by-step guide\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eSet up the server:\u003c/strong\u003e Start your local server to host the model files.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInclude scripts:\u003c/strong\u003e In your HTML file, include ort and your JavaScript modules.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOpen the application:\u003c/strong\u003e Access the HTML file through your browser.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eUpload an image:\u003c/strong\u003e Use the file input to select an image.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInteract with the image:\u003c/strong\u003e Click on the image to add points and see the segmentation mask update in real-time.\u003c/li\u003e\u003c/ol\u003e\u003ch3 id=\"demonstration\"\u003eDemonstration\u003c/h3\u003e\u003cp\u003eCheck out and play around with an interactive demo \u003ca href=\"https://storage.googleapis.com/lb-artifacts-testing-public/sam2/index.html?ref=labelbox.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eIn this article, we've demonstrated how to run the Segment Anything Model 2 (SAM2) directly in the web browser. By leveraging ONNX Runtime Web and thoughtful implementation of the encoder and decoder, we've created an interactive image segmentation tool that runs entirely on the user-side.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis approach opens doors to privacy-preserving applications and makes advanced machine learning models more accessible. As web technologies continue to evolve, we can expect even more sophisticated models to run efficiently in the browser.\u003c/p\u003e\u003cp\u003eAt Quantumworks Lab, we’ve adopted a hybrid strategy, running SAM’s encoder on the server while executing its decoder right in the browser. This approach ensures real-time image segmentation, preserves user privacy, and expands access to cutting-edge machine learning. As web technologies advance, we look forward to delivering even more powerful models efficiently and securely in the browser.\u003c/p\u003e\u003ch2 id=\"check-out-these-additional-resources\"\u003eCheck out these additional resources:\u0026nbsp;\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eONNX Runtime Web Documentation:\u003c/strong\u003e \u003ca href=\"https://onnxruntime.ai/docs/api/javascript/index.html?ref=labelbox.ghost.io\"\u003e\u003cu\u003eONNX Runtime Web\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSAM2 Model Details:\u003c/strong\u003e \u003ca href=\"https://segment-anything.com/?ref=labelbox.ghost.io\"\u003e\u003cu\u003eSegment Anything Model (SAM)\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/Quantumworks Lab/sam2-web?ref=labelbox.ghost.io\"\u003e\u003cu\u003elink\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDemo:\u003c/strong\u003e \u003ca href=\"https://storage.googleapis.com/lb-artifacts-testing-public/sam2/index.html?ref=labelbox.ghost.io\"\u003e\u003cu\u003elink\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"6759fc88a9b5bd0001989d30","feature_image":"https://labelbox.ghost.io/blog/content/images/2024/12/Interactive-Image-Segmentation-in-the-Browser-with-SAM2.png","featured":false,"visibility":"public","created_at":"2024-12-11T12:56:40.000-08:00","updated_at":"2024-12-20T08:52:19.000-08:00","published_at":"2024-12-19T12:18:40.000-08:00","custom_excerpt":"Learn how to leverage the power of the SAM2 in a web browser, enabling interactive image segmentation without the need for powerful servers or specialized hardware.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6764713c6f63bf0001f1ca64","name":"Stanislav Issayenko","slug":"stanislav","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/stanislav/"}],"tags":[{"id":"5f9af89dd8118b0039136165","name":"Engineering","slug":"engineering","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/engineering/"},{"id":"653030c34e99900001fc0531","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/using-computer-vision/"},{"id":"6700193d863cb90001f263ed","name":"Computer vision","slug":"computer-vision","description":"CV related features, best practices, and information","feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/computer-vision/"},{"id":"653031134e99900001fc0533","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox.ghost.io/blog/tag/label-data-for-ai/"},{"id":"653030aa4e99900001fc052d","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox.ghost.io/blog/tag/use-ai/"}],"primary_author":{"id":"6764713c6f63bf0001f1ca64","name":"Stanislav Issayenko","slug":"stanislav","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox.ghost.io/blog/author/stanislav/"},"primary_tag":{"id":"5f9af89dd8118b0039136165","name":"Engineering","slug":"engineering","description":null,"feature_image":null,"visibility":"public","meta_title":null,"meta_description":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox.ghost.io/blog/tag/engineering/"},"url":"https://labelbox.ghost.io/blog/bringing-ai-to-the-browser-sam2-for-interactive-image-segmentation/","excerpt":"Learn how to leverage the power of the SAM2 in a web browser, enabling interactive image segmentation without the need for powerful servers or specialized hardware.","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}]},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"how-to-improve-model-performance-with-active-learning-and-weak-supervision"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/blog/how-to-improve-model-performance-with-active-learning-and-weak-supervision/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:38:58 GMT -->
</html>