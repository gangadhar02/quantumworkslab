<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/research/tag/computer-vision/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 10:46:31 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Research | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Research | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/research/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Research | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/research/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../static/scripts/munchkin.js"></script><script src="../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../_next/static/chunks/pages/research/tag/%5bid%5d-cbbeaae11d3b3154.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../index.html"><img width="106" height="24" alt="logo" src="../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Research</h1><p class="text-base max-w-xs text-neutral-500  pr-6"></p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../academic-papers/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Academic papers</a><a href="../coursework/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Coursework</a><a href="index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Computer vision</a><a href="../natural-language/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Natural language</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="mb-10"><h2 class="text-3xl md:text-4xl font-medium mb-4">computer-vision</h2><p class="text-base max-w-2xl font-medium text-neutral-500"></p></div><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../automatic-recognition-of-emotional-subgroups-in-images/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index6b3d.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../automatic-recognition-of-emotional-subgroups-in-images/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Automatic recognition of emotional subgroups in images</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index328d.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Evaluating terrain-dependent performance for martian frost detection in visible satellite observations</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index591c.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">The impact of artificial intelligence assessment on diabetic retinopathy</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the University of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../deep-learning-for-live-cell-shape-detection/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index1ddd.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../deep-learning-for-live-cell-shape-detection/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Deep learning for live cell shape detection</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../solar-panel-detection-from-aerial-images/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexd094.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../solar-panel-detection-from-aerial-images/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">HyperionSolarNet: Solar panel detection from aerial images</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../multi-view-deep-learning-for-reliable-post-disaster-damage-classification/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.11.08-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.11.08-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.11.08-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.11.08-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.11.08-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.11.08-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.11.08-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.11.08-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index8d79.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.11.08-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../multi-view-deep-learning-for-reliable-post-disaster-damage-classification/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Multi-view deep learning for reliable post-disaster damage classification</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Texas A&amp;M University, recently researched ways to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. </p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8">Page 1 of 1</div></div></div></div></div></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"635b459399ae26003d07ae7a","uuid":"59073aae-19d2-4752-8438-3eca7000fac4","title":"Automatic recognition of emotional subgroups in images","slug":"automatic-recognition-of-emotional-subgroups-in-images","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eTracking individuals is not always the most efficient way of sensing emotions, especially in large crowds, where outputs would get cluttered. Additionally, an individual’s emotion can be better predicted when incorporating emotions from others in their social group, while at the same time people tend to be part of social groups that feel and act in a similar manner. Recognizing emotional subgroups is therefore a more efficient way of detecting emotion or behavior within a crowd. Simply combining the tasks of group and emotion recognition is not likely to suffice, since emotional subgroups can either split up or combine social groups complicating the task.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eImages that show agreement among annotators, are most often those that elicit the use of the summation strategy, while images with partial agreement more often elicit the use of the emotion-based fusion (putting more emphasis on emotion than social groups) or the group-based fusion (putting more emphasis on social groups than on emotion) strategy. Experimenting with different additional features suggests, with a modest performance improvement, that face size and gaze direction contain meaningful information. This shows that the task of emotional subgroup recognition is a complex one, but also that a relatively small feature vector is already able to reasonably represent human perception.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers used Quantumworks Lab to have human annotators label a set of 171 images, and their recognition strategies were analyzed. Three main strategies for labeling images are identified, with each strategy assigning either 1) more weight to emotions (emotion-based fusion), 2) more weight to spatial structures (groupbased fusion), or 3) equal weight to both (summation strategy). Based on these strategies, algorithms are developed to automatically recognize emotional subgroups. In particular, K-means and hierarchical clustering are used with location and emotion features derived from a fne-tuned VGG network. Additionally, they experimented with face size and gaze direction as extra input features and found that the best performance came from hierarchical clustering with emotion, location and gaze direction as input.\u003c/p\u003e\u003cp\u003e\u003cbr\u003eRead the full PDF \u003ca href=\"https://www.ijcai.org/proceedings/2022/0190.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635b459399ae26003d07ae7a","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-8.07.21-PM.png","featured":false,"visibility":"public","created_at":"2022-10-28T02:59:31.000+00:00","updated_at":"2022-12-06T21:31:21.000+00:00","published_at":"2022-10-28T03:07:50.000+00:00","custom_excerpt":"Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automatic-recognition-of-emotional-subgroups-in-images/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/automatic-recognition-of-emotional-subgroups-in-images/","excerpt":"Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635ab9d199ae26003d07adea","uuid":"139ae20c-fa83-4721-9a27-147e14820cf8","title":"Evaluating terrain-dependent performance for martian frost detection in visible satellite observations","slug":"evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Past studies have focused on manually analyzing the behavior of the frost cycle in the northern mid-latitude region of Mars using high-resolution visible observations from orbit. Extending these studies globally requires automating the detection of frost using data science techniques such as convolutional neural networks. However, visible indications of frost presence can vary significantly depending on the geologic context on which the frost is superimposed. \u003c/p\u003e\u003cp\u003eUnlike the Earth, the atmosphere of Mars is comprised primarily of carbon dioxide (CO2) and this volatile constitutes most of the frost, falling as snow or condensing at the surface due to surface temperatures falling to the CO2 frost point. A small amount of water frost will also form when temperatures are below the water (H2O) frost point, but only if the local concentration of H2O vapor in the atmosphere is high enough.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The research team from JPL presented a novel approach for spatially partitioning data to reduce biases in model performance estimation, illustrate how geologic context affects automated frost detection, and propose future work to further mitigate observed biases in automated frost detection work. \u003c/p\u003e\u003cp\u003eThey found that geologic context bias is present and significant for this model’s performance on the test set, specifically for dune fields often found in northern mid-latitude craters. Interestingly, for human annotators, dunes often provide strong evidence of frost due to the striking visual appearance of defrosting marks which expose dark basalt sand beneath light-color frost.\u003c/p\u003e\u003cp\u003eThey also saw a large degree of diversity in frost appearance on the underrepresented terrain type, both inherently and due to differing illumination and observational conditions, making the concept challenging for the classification model to learn.\u003c/p\u003e\u003cp\u003eFuture improvements will permit the training of models better suited for full-planet frost detection, thereby facilitating the creation of global frost maps\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eLabelbox was used to annotate polygonal boundaries around regions with visible evidence of frost. For each polygon, they collected additional information from the labeler including the applicable visible indicators as well as geologic context, which is either “dunes,” “gullies,” “crater rim/wall,” or “other.” The geologic context categories was mutually exclusive, so labelers could only pick one geologic context per frost polygon. \u003c/p\u003e\u003cp\u003eIn addition, they used the geologic context information to investigate terrain-dependent bias in classifier performance. To document the labeling process, they performed an iterative series of labeling sessions with both data science and science domain experts. \u003c/p\u003e\u003cp\u003eDomain expert labeling guidance and clarifications at each iteration were captured in a labeling guide, included with the publicly available dataset2. A total of 6 subframes, detailed in the released dataset, were excluded due to contamination with excess instrument noise or cloud cover. Each subframe was labeled by three different annotators.\u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://www.cs.emory.edu/~sgu33/workshop/DeepSpatial2022/papers/p4.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635ab9d199ae26003d07adea","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-10.09.34-AM.png","featured":false,"visibility":"public","created_at":"2022-10-27T17:03:13.000+00:00","updated_at":"2022-12-06T21:31:55.000+00:00","published_at":"2022-10-27T17:10:10.000+00:00","custom_excerpt":"Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/","excerpt":"Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635ab62c99ae26003d07adb0","uuid":"8426f3d6-2b81-4dbe-963a-866ba213f73b","title":"The impact of artificial intelligence assessment on diabetic retinopathy","slug":"impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the\u003cstrong\u003e \u003c/strong\u003eUniversity of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. Using validated AI algorithms instead of scarce trained specialists, this could potentially increase the efficiency and accessibility of screening programs. Systematic reviews of deep learning-based algorithms in DR screening have highlighted such advantages as reduction in demands for manpower, cost of screening, and intragrader and intergrader variability. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch: \u003c/strong\u003eThe growing burden of diabetes and its associated complications is increasing the demands on health care systems, particularly in low-resource countries. Their hypothesis was that adherence to referral services would be higher among patients randomized to receive AI-supported screening with immediate feedback compared with those randomized to receive delayed communication of results until after human grading was completed. Evidence from real-life screening programs is limited, and no evidence exists on community acceptance of the use of AI-supported DR screening.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: Immediate feedback on referral status based on AI-supported screening was associated with statistically significantly higher referral adherence compared with delayed communications of results from human graders. These results provide evidence for an important benefit of AI screening in promoting adherence to prescribed treatment for diabetic eye care in sub-Saharan Africa.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eAll images, anonymized with a unique patient registration number, were uploaded to Orbis International’s Cybersight AI. A mobile device or laptop and an internet connection are required to access Cybersight AI, which generates a response regarding the presence or absence of referable DR based on a macula-centered image from each available eye of a participant within 60 seconds. Afterwards, all images were uploaded to Quantumworks Lab for annotation and grading by a United Kingdom National Health System formally trained retinal specialist. \u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://reader.elsevier.com/reader/sd/pii/S2666914522000574?token=871F5D7CE9870F35BC04F4954510A8796D2A713DA63A98B7B2FF383765C5642101B2C77F003218EC780639144DEF7323\u0026originRegion=us-east-1\u0026originCreation=20221027165201\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635ab62c99ae26003d07adb0","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-9.51.43-AM.png","featured":false,"visibility":"public","created_at":"2022-10-27T16:47:40.000+00:00","updated_at":"2022-12-06T21:32:07.000+00:00","published_at":"2022-10-27T16:54:48.000+00:00","custom_excerpt":"Researchers from the University of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/","excerpt":"Researchers from the University of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635a067799ae26003d07ad35","uuid":"80084265-efe2-45ec-8ca2-a854ad139075","title":"Deep learning for live cell shape detection","slug":"deep-learning-for-live-cell-shape-detection","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging. AFM is used as mechanical characterization in a wide range of samples, including live cells, proteins, and other biomolecules. It is also instrumental for measuring interaction forces and binding kinetics for protein–protein or receptor–ligand interactions on live cells at a single-molecule level. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eThe difficulty lies in performing force measurements and high-resolution imaging with AFM and data analytics because it is time-consuming and require special skill sets and continuous human supervision. Recently, researchers have explored the applications of using artificial intelligence (AI) and deep learning (DL) in the bioimaging field. However, the applications of AI to AFM operations for live-cell characterization are little-known until now. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The researchers implemented a deep learning framework to perform automatic sample selection based on the cell shape for AFM probe navigation during AFM biomechanical mapping. They established a closed-loop scanner trajectory control for measuring multiple cell samples at high speed for automated navigation. With this, they achieved a 60× speed-up in AFM navigation and reduced the time involved in searching for the particular cell shape in a large sample. Their innovation directly applies to many bio-AFM applications with AI-guided intelligent automation through image data analysis together with smart navigation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1328\" height=\"412\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1328w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers leveraged Quantumworks Lab to label their data and enabled experts to annotate the cell shape by drawing bounding boxes around it and labeling it with an accurate shape. Collecting these images was time-consuming and tedious as the user had to manually scan the cell samples and capture the images. In addition, performing the annotations, especially on low-quality images, was a painstaking task, leading to a smaller dataset with fewer annotated images. To address this challenge, they implemented data augmentation techniques on the fly (during training), which involved rotating the original images by 90◦ clockwise or counter-clockwise, by 180◦ , flipping them upside down, and by left-right mirroring. This enhanced the original dataset with more data samples with different orientations, which further made the DL network robust to the variety of cell shape orientations encountered during inference.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1246\" height=\"478\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1246w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eVisualizing the predictions on low-quality images. Target/ground truth images are shown in the top row and the corresponding predictions in the bottom row.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://www.mdpi.com/2306-5354/9/10/522/pdf?version=1665735868\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635a067799ae26003d07ad35","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.35.32-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T04:17:59.000+00:00","updated_at":"2022-12-06T21:32:36.000+00:00","published_at":"2022-10-27T04:35:55.000+00:00","custom_excerpt":"Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/deep-learning-for-live-cell-shape-detection/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/deep-learning-for-live-cell-shape-detection/","excerpt":"Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6359fb7199ae26003d07acfe","uuid":"5bdfd1b2-714c-4783-9060-d7a77c6cc248","title":"HyperionSolarNet: Solar panel detection from aerial images","slug":"solar-panel-detection-from-aerial-images","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003e With the effects of global climate change impacting the world, collective efforts are needed to reduce greenhouse gas emissions. The energy sector continues to be the single largest contributor to climate change and many efforts are focused on reducing dependence on carbon-emitting power plants and moving to renewable energy sources, such as solar power.\u003c/p\u003e\u003cp\u003eTheir work focused on creating a world map of solar panels, identifying locations and total surface area of solar panels within a given geographic area. The researchers used deep learning methods for automated detection of solar panel locations and their surface area using aerial imagery. The framework, which consisted of a two-branch model using an image classifier in tandem with a semantic segmentation model, was trained on a created dataset of satellite images.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: Their work provided an efficient and scalable method for detecting solar panels, achieving an accuracy of 0.96 for classification and an IoU score of 0.82 for segmentation performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used\u003c/strong\u003e: The research team annotated 836 images containing solar panels using the Quantumworks Lab platform, and produced corresponding segmentation masks, resizing all images to a size of 512x512 pixels for training and testing. The researchers manually annotated these images using Quantumworks Lab and created mask labels for them, afterwards, evaluating the HyperionSolarNet segmentation model against these test set images and finding  an IoU score of 0.82. \u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://s3.us-east-1.amazonaws.com/climate-change-ai/papers/neurips2021/41/paper.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6359fb7199ae26003d07acfe","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-8.37.39-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T03:30:57.000+00:00","updated_at":"2022-12-06T21:32:57.000+00:00","published_at":"2022-10-27T03:38:47.000+00:00","custom_excerpt":"Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/solar-panel-detection-from-aerial-images/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/solar-panel-detection-from-aerial-images/","excerpt":"Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6359f4bf99ae26003d07acb9","uuid":"5f42e2ff-655d-430a-abc1-b64636796925","title":"Multi-view deep learning for reliable post-disaster damage classification","slug":"multi-view-deep-learning-for-reliable-post-disaster-damage-classification","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from Texas A\u0026amp;M University, recently researched ways to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch challenge\u003c/strong\u003e: In their research, the current practices and research efforts in adopting AI for post-disaster damage assessment are generally (a) qualitative, lacking refined classification of building damage levels based on standard damage scales, and (b) trained based on aerial or satellite imagery with limited views, which, although indicative, are not completely descriptive of the damage scale. \u003c/p\u003e\u003cp\u003eTo enable more accurate and reliable automated quantification of damage levels, the present study proposes the use of more comprehensive visual data in the form of multiple ground and aerial views of the buildings. To have such a spatially-aware damage prediction model, a Multi-view Convolution Neural Network (MV-CNN) architecture, the researchers used a novel approach that combined the information from different views of a damaged building. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eThis spatial 3D context damage information will result in more accurate identification of damages and reliable quantification of damage levels. The proposed model was trained and validated on reconnaissance visual dataset containing expert labeled, geotagged images of the inspected buildings following hurricane Harvey. \u003c/p\u003e\u003cp\u003eTheir research team developed a model demonstrates reasonably good accuracy in predicting the damage levels and can be used to support more informed and reliable AI-assisted disaster management practices.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used:\u003c/strong\u003e The model was trained using pixel level annotations of the buildings using the Quantumworks Lab platform. The idea was to recognize the pixels belonging to buildings in an image and filter out the visual information unnecessary for determining the damage state of the building like trees, sky, roads, etc. Data annotation was also performed using Quantumworks Lab, and in total, images corresponding to 400 buildings (2000 images) were annotated and assigned labels representing their damage state. The damage labels (0 to 5) were based on the expert assessment reported in the original database. The entire annotated dataset was split into training (80%), validation (10%), and testing sets (10%).\u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://arxiv.org/pdf/2208.03419.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6359f4bf99ae26003d07acb9","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-8.11.08-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T03:02:23.000+00:00","updated_at":"2022-12-06T21:33:10.000+00:00","published_at":"2022-10-27T03:14:23.000+00:00","custom_excerpt":"Researchers from Texas A\u0026M University, recently researched ways to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/multi-view-deep-learning-for-reliable-post-disaster-damage-classification/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/multi-view-deep-learning-for-reliable-post-disaster-damage-classification/","excerpt":"Researchers from Texas A\u0026M University, recently researched ways to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":6,"allPosts":[{"id":"635b459399ae26003d07ae7a","uuid":"59073aae-19d2-4752-8438-3eca7000fac4","title":"Automatic recognition of emotional subgroups in images","slug":"automatic-recognition-of-emotional-subgroups-in-images","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eTracking individuals is not always the most efficient way of sensing emotions, especially in large crowds, where outputs would get cluttered. Additionally, an individual’s emotion can be better predicted when incorporating emotions from others in their social group, while at the same time people tend to be part of social groups that feel and act in a similar manner. Recognizing emotional subgroups is therefore a more efficient way of detecting emotion or behavior within a crowd. Simply combining the tasks of group and emotion recognition is not likely to suffice, since emotional subgroups can either split up or combine social groups complicating the task.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eImages that show agreement among annotators, are most often those that elicit the use of the summation strategy, while images with partial agreement more often elicit the use of the emotion-based fusion (putting more emphasis on emotion than social groups) or the group-based fusion (putting more emphasis on social groups than on emotion) strategy. Experimenting with different additional features suggests, with a modest performance improvement, that face size and gaze direction contain meaningful information. This shows that the task of emotional subgroup recognition is a complex one, but also that a relatively small feature vector is already able to reasonably represent human perception.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers used Quantumworks Lab to have human annotators label a set of 171 images, and their recognition strategies were analyzed. Three main strategies for labeling images are identified, with each strategy assigning either 1) more weight to emotions (emotion-based fusion), 2) more weight to spatial structures (groupbased fusion), or 3) equal weight to both (summation strategy). Based on these strategies, algorithms are developed to automatically recognize emotional subgroups. In particular, K-means and hierarchical clustering are used with location and emotion features derived from a fne-tuned VGG network. Additionally, they experimented with face size and gaze direction as extra input features and found that the best performance came from hierarchical clustering with emotion, location and gaze direction as input.\u003c/p\u003e\u003cp\u003e\u003cbr\u003eRead the full PDF \u003ca href=\"https://www.ijcai.org/proceedings/2022/0190.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635b459399ae26003d07ae7a","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-8.07.21-PM.png","featured":false,"visibility":"public","created_at":"2022-10-28T02:59:31.000+00:00","updated_at":"2022-12-06T21:31:21.000+00:00","published_at":"2022-10-28T03:07:50.000+00:00","custom_excerpt":"Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automatic-recognition-of-emotional-subgroups-in-images/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/automatic-recognition-of-emotional-subgroups-in-images/","excerpt":"Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635ab9d199ae26003d07adea","uuid":"139ae20c-fa83-4721-9a27-147e14820cf8","title":"Evaluating terrain-dependent performance for martian frost detection in visible satellite observations","slug":"evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Past studies have focused on manually analyzing the behavior of the frost cycle in the northern mid-latitude region of Mars using high-resolution visible observations from orbit. Extending these studies globally requires automating the detection of frost using data science techniques such as convolutional neural networks. However, visible indications of frost presence can vary significantly depending on the geologic context on which the frost is superimposed. \u003c/p\u003e\u003cp\u003eUnlike the Earth, the atmosphere of Mars is comprised primarily of carbon dioxide (CO2) and this volatile constitutes most of the frost, falling as snow or condensing at the surface due to surface temperatures falling to the CO2 frost point. A small amount of water frost will also form when temperatures are below the water (H2O) frost point, but only if the local concentration of H2O vapor in the atmosphere is high enough.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The research team from JPL presented a novel approach for spatially partitioning data to reduce biases in model performance estimation, illustrate how geologic context affects automated frost detection, and propose future work to further mitigate observed biases in automated frost detection work. \u003c/p\u003e\u003cp\u003eThey found that geologic context bias is present and significant for this model’s performance on the test set, specifically for dune fields often found in northern mid-latitude craters. Interestingly, for human annotators, dunes often provide strong evidence of frost due to the striking visual appearance of defrosting marks which expose dark basalt sand beneath light-color frost.\u003c/p\u003e\u003cp\u003eThey also saw a large degree of diversity in frost appearance on the underrepresented terrain type, both inherently and due to differing illumination and observational conditions, making the concept challenging for the classification model to learn.\u003c/p\u003e\u003cp\u003eFuture improvements will permit the training of models better suited for full-planet frost detection, thereby facilitating the creation of global frost maps\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eLabelbox was used to annotate polygonal boundaries around regions with visible evidence of frost. For each polygon, they collected additional information from the labeler including the applicable visible indicators as well as geologic context, which is either “dunes,” “gullies,” “crater rim/wall,” or “other.” The geologic context categories was mutually exclusive, so labelers could only pick one geologic context per frost polygon. \u003c/p\u003e\u003cp\u003eIn addition, they used the geologic context information to investigate terrain-dependent bias in classifier performance. To document the labeling process, they performed an iterative series of labeling sessions with both data science and science domain experts. \u003c/p\u003e\u003cp\u003eDomain expert labeling guidance and clarifications at each iteration were captured in a labeling guide, included with the publicly available dataset2. A total of 6 subframes, detailed in the released dataset, were excluded due to contamination with excess instrument noise or cloud cover. Each subframe was labeled by three different annotators.\u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://www.cs.emory.edu/~sgu33/workshop/DeepSpatial2022/papers/p4.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635ab9d199ae26003d07adea","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-10.09.34-AM.png","featured":false,"visibility":"public","created_at":"2022-10-27T17:03:13.000+00:00","updated_at":"2022-12-06T21:31:55.000+00:00","published_at":"2022-10-27T17:10:10.000+00:00","custom_excerpt":"Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/","excerpt":"Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635ab62c99ae26003d07adb0","uuid":"8426f3d6-2b81-4dbe-963a-866ba213f73b","title":"The impact of artificial intelligence assessment on diabetic retinopathy","slug":"impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the\u003cstrong\u003e \u003c/strong\u003eUniversity of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. Using validated AI algorithms instead of scarce trained specialists, this could potentially increase the efficiency and accessibility of screening programs. Systematic reviews of deep learning-based algorithms in DR screening have highlighted such advantages as reduction in demands for manpower, cost of screening, and intragrader and intergrader variability. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch: \u003c/strong\u003eThe growing burden of diabetes and its associated complications is increasing the demands on health care systems, particularly in low-resource countries. Their hypothesis was that adherence to referral services would be higher among patients randomized to receive AI-supported screening with immediate feedback compared with those randomized to receive delayed communication of results until after human grading was completed. Evidence from real-life screening programs is limited, and no evidence exists on community acceptance of the use of AI-supported DR screening.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: Immediate feedback on referral status based on AI-supported screening was associated with statistically significantly higher referral adherence compared with delayed communications of results from human graders. These results provide evidence for an important benefit of AI screening in promoting adherence to prescribed treatment for diabetic eye care in sub-Saharan Africa.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eAll images, anonymized with a unique patient registration number, were uploaded to Orbis International’s Cybersight AI. A mobile device or laptop and an internet connection are required to access Cybersight AI, which generates a response regarding the presence or absence of referable DR based on a macula-centered image from each available eye of a participant within 60 seconds. Afterwards, all images were uploaded to Quantumworks Lab for annotation and grading by a United Kingdom National Health System formally trained retinal specialist. \u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://reader.elsevier.com/reader/sd/pii/S2666914522000574?token=871F5D7CE9870F35BC04F4954510A8796D2A713DA63A98B7B2FF383765C5642101B2C77F003218EC780639144DEF7323\u0026originRegion=us-east-1\u0026originCreation=20221027165201\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635ab62c99ae26003d07adb0","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-9.51.43-AM.png","featured":false,"visibility":"public","created_at":"2022-10-27T16:47:40.000+00:00","updated_at":"2022-12-06T21:32:07.000+00:00","published_at":"2022-10-27T16:54:48.000+00:00","custom_excerpt":"Researchers from the University of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/","excerpt":"Researchers from the University of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635a067799ae26003d07ad35","uuid":"80084265-efe2-45ec-8ca2-a854ad139075","title":"Deep learning for live cell shape detection","slug":"deep-learning-for-live-cell-shape-detection","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging. AFM is used as mechanical characterization in a wide range of samples, including live cells, proteins, and other biomolecules. It is also instrumental for measuring interaction forces and binding kinetics for protein–protein or receptor–ligand interactions on live cells at a single-molecule level. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eThe difficulty lies in performing force measurements and high-resolution imaging with AFM and data analytics because it is time-consuming and require special skill sets and continuous human supervision. Recently, researchers have explored the applications of using artificial intelligence (AI) and deep learning (DL) in the bioimaging field. However, the applications of AI to AFM operations for live-cell characterization are little-known until now. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The researchers implemented a deep learning framework to perform automatic sample selection based on the cell shape for AFM probe navigation during AFM biomechanical mapping. They established a closed-loop scanner trajectory control for measuring multiple cell samples at high speed for automated navigation. With this, they achieved a 60× speed-up in AFM navigation and reduced the time involved in searching for the particular cell shape in a large sample. Their innovation directly applies to many bio-AFM applications with AI-guided intelligent automation through image data analysis together with smart navigation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1328\" height=\"412\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1328w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers leveraged Quantumworks Lab to label their data and enabled experts to annotate the cell shape by drawing bounding boxes around it and labeling it with an accurate shape. Collecting these images was time-consuming and tedious as the user had to manually scan the cell samples and capture the images. In addition, performing the annotations, especially on low-quality images, was a painstaking task, leading to a smaller dataset with fewer annotated images. To address this challenge, they implemented data augmentation techniques on the fly (during training), which involved rotating the original images by 90◦ clockwise or counter-clockwise, by 180◦ , flipping them upside down, and by left-right mirroring. This enhanced the original dataset with more data samples with different orientations, which further made the DL network robust to the variety of cell shape orientations encountered during inference.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1246\" height=\"478\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1246w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eVisualizing the predictions on low-quality images. Target/ground truth images are shown in the top row and the corresponding predictions in the bottom row.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://www.mdpi.com/2306-5354/9/10/522/pdf?version=1665735868\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635a067799ae26003d07ad35","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.35.32-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T04:17:59.000+00:00","updated_at":"2022-12-06T21:32:36.000+00:00","published_at":"2022-10-27T04:35:55.000+00:00","custom_excerpt":"Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/deep-learning-for-live-cell-shape-detection/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/deep-learning-for-live-cell-shape-detection/","excerpt":"Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6359fb7199ae26003d07acfe","uuid":"5bdfd1b2-714c-4783-9060-d7a77c6cc248","title":"HyperionSolarNet: Solar panel detection from aerial images","slug":"solar-panel-detection-from-aerial-images","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003e With the effects of global climate change impacting the world, collective efforts are needed to reduce greenhouse gas emissions. The energy sector continues to be the single largest contributor to climate change and many efforts are focused on reducing dependence on carbon-emitting power plants and moving to renewable energy sources, such as solar power.\u003c/p\u003e\u003cp\u003eTheir work focused on creating a world map of solar panels, identifying locations and total surface area of solar panels within a given geographic area. The researchers used deep learning methods for automated detection of solar panel locations and their surface area using aerial imagery. The framework, which consisted of a two-branch model using an image classifier in tandem with a semantic segmentation model, was trained on a created dataset of satellite images.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: Their work provided an efficient and scalable method for detecting solar panels, achieving an accuracy of 0.96 for classification and an IoU score of 0.82 for segmentation performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used\u003c/strong\u003e: The research team annotated 836 images containing solar panels using the Quantumworks Lab platform, and produced corresponding segmentation masks, resizing all images to a size of 512x512 pixels for training and testing. The researchers manually annotated these images using Quantumworks Lab and created mask labels for them, afterwards, evaluating the HyperionSolarNet segmentation model against these test set images and finding  an IoU score of 0.82. \u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://s3.us-east-1.amazonaws.com/climate-change-ai/papers/neurips2021/41/paper.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6359fb7199ae26003d07acfe","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-8.37.39-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T03:30:57.000+00:00","updated_at":"2022-12-06T21:32:57.000+00:00","published_at":"2022-10-27T03:38:47.000+00:00","custom_excerpt":"Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/solar-panel-detection-from-aerial-images/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/solar-panel-detection-from-aerial-images/","excerpt":"Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6359f4bf99ae26003d07acb9","uuid":"5f42e2ff-655d-430a-abc1-b64636796925","title":"Multi-view deep learning for reliable post-disaster damage classification","slug":"multi-view-deep-learning-for-reliable-post-disaster-damage-classification","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from Texas A\u0026amp;M University, recently researched ways to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch challenge\u003c/strong\u003e: In their research, the current practices and research efforts in adopting AI for post-disaster damage assessment are generally (a) qualitative, lacking refined classification of building damage levels based on standard damage scales, and (b) trained based on aerial or satellite imagery with limited views, which, although indicative, are not completely descriptive of the damage scale. \u003c/p\u003e\u003cp\u003eTo enable more accurate and reliable automated quantification of damage levels, the present study proposes the use of more comprehensive visual data in the form of multiple ground and aerial views of the buildings. To have such a spatially-aware damage prediction model, a Multi-view Convolution Neural Network (MV-CNN) architecture, the researchers used a novel approach that combined the information from different views of a damaged building. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eThis spatial 3D context damage information will result in more accurate identification of damages and reliable quantification of damage levels. The proposed model was trained and validated on reconnaissance visual dataset containing expert labeled, geotagged images of the inspected buildings following hurricane Harvey. \u003c/p\u003e\u003cp\u003eTheir research team developed a model demonstrates reasonably good accuracy in predicting the damage levels and can be used to support more informed and reliable AI-assisted disaster management practices.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used:\u003c/strong\u003e The model was trained using pixel level annotations of the buildings using the Quantumworks Lab platform. The idea was to recognize the pixels belonging to buildings in an image and filter out the visual information unnecessary for determining the damage state of the building like trees, sky, roads, etc. Data annotation was also performed using Quantumworks Lab, and in total, images corresponding to 400 buildings (2000 images) were annotated and assigned labels representing their damage state. The damage labels (0 to 5) were based on the expert assessment reported in the original database. The entire annotated dataset was split into training (80%), validation (10%), and testing sets (10%).\u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://arxiv.org/pdf/2208.03419.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6359f4bf99ae26003d07acb9","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-8.11.08-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T03:02:23.000+00:00","updated_at":"2022-12-06T21:33:10.000+00:00","published_at":"2022-10-27T03:14:23.000+00:00","custom_excerpt":"Researchers from Texas A\u0026M University, recently researched ways to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/multi-view-deep-learning-for-reliable-post-disaster-damage-classification/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/multi-view-deep-learning-for-reliable-post-disaster-damage-classification/","excerpt":"Researchers from Texas A\u0026M University, recently researched ways to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"tag":{"slug":"computer-vision","id":"635ac78099ae26003d07ae30","name":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"count":{"posts":6},"url":"https://labelbox-research.ghost.io/tag/computer-vision/"},"slug":"computer-vision","currentPage":1},"__N_SSG":true},"page":"/research/tag/[id]","query":{"id":"computer-vision"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/research/tag/computer-vision/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 10:46:38 GMT -->
</html>