<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/research/tag/academic-papers/page/2/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:36:31 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Research | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Research | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/research/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Research | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/research/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../../../static/scripts/munchkin.js"></script><script src="../../../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/research/tag/%5bid%5d/page/%5bpagenum%5d-9fdde7f3e3e1fe47.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../../../index.html"><img width="106" height="24" alt="logo" src="../../../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Research</h1><p class="text-base max-w-xs text-neutral-500  pr-6"></p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../../index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Academic papers</a><a href="../../../coursework/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Coursework</a><a href="../../../computer-vision/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Computer vision</a><a href="../../../natural-language/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Natural language</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../an-ai-based-computer-aided-system-for-knee-osteoarthritis-assessment/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FiStock-1142758724-1-e1590763901978.jpeg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FiStock-1142758724-1-e1590763901978.jpeg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FiStock-1142758724-1-e1590763901978.jpeg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FiStock-1142758724-1-e1590763901978.jpeg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FiStock-1142758724-1-e1590763901978.jpeg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FiStock-1142758724-1-e1590763901978.jpeg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FiStock-1142758724-1-e1590763901978.jpeg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FiStock-1142758724-1-e1590763901978.jpeg&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexa909.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FiStock-1142758724-1-e1590763901978.jpeg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../an-ai-based-computer-aided-system-for-knee-osteoarthritis-assessment/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">An AI-based computer-aided system for better knee osteoarthritis assessment</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the Medical University of Graz recently collaborated to analyze the impact of an artificial intelligence (AI)-based computer system on the accuracy and agreement rate of board-certified orthopedic surgeons to detect X-ray features indicative of knee OA (osteoarthritis assessment).</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../lightweight-multi-drone-detection-and-3d-localization-via-yolo/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-03-at-12.54.00-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-03-at-12.54.00-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-03-at-12.54.00-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-03-at-12.54.00-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-03-at-12.54.00-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-03-at-12.54.00-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-03-at-12.54.00-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-03-at-12.54.00-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexffa7.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-03-at-12.54.00-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../lightweight-multi-drone-detection-and-3d-localization-via-yolo/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Lightweight multi-drone detection and 3D-localization via YOLO</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Indian Institute of Technology Kanpur recently evaluated a method to perform real-time multiple drone detection and three-dimensional localization using state-of-the-art tiny-YOLOv4 object detection algorithm and stereo triangulation.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../automated-detection-of-malaria-parasites-using-convolutional-neural-networks/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2F0179MarionsMalaria5.jpeg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2F0179MarionsMalaria5.jpeg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2F0179MarionsMalaria5.jpeg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2F0179MarionsMalaria5.jpeg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2F0179MarionsMalaria5.jpeg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2F0179MarionsMalaria5.jpeg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2F0179MarionsMalaria5.jpeg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2F0179MarionsMalaria5.jpeg&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index87ce.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2F0179MarionsMalaria5.jpeg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../automated-detection-of-malaria-parasites-using-convolutional-neural-networks/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Automated detection of malaria parasites using convolutional neural networks</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Imperial College London recently focused on developing an automated image analysis method to improve accuracy and standardization of smear inspection that retains capacity for expert confirmation and image archiving. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../automatic-recognition-of-emotional-subgroups-in-images/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index6b3d.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-8.07.21-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../automatic-recognition-of-emotional-subgroups-in-images/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Automatic recognition of emotional subgroups in images</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../a-hierarchical-optical-coherence-tomography-annotation-workflow-with-crowds-and-medical-experts/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-7.57.46-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-7.57.46-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-7.57.46-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-7.57.46-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-7.57.46-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-7.57.46-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-7.57.46-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-7.57.46-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexc574.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-7.57.46-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../a-hierarchical-optical-coherence-tomography-annotation-workflow-with-crowds-and-medical-experts/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">A hierarchical optical coherence tomography annotation workflow with crowds and medical experts</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Genentech are researching solutions to the heavy burden it takes to annotate clinical images due to the cost and availability of medical experts.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index328d.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-10.09.34-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Evaluating terrain-dependent performance for martian frost detection in visible satellite observations</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index591c.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.51.43-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">The impact of artificial intelligence assessment on diabetic retinopathy</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the University of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../baby-physical-safety-monitoring-in-smart-home-using-action-recognition-system/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.44.00-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.44.00-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.44.00-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.44.00-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.44.00-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.44.00-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.44.00-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.44.00-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexd9d4.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-27-at-9.44.00-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../baby-physical-safety-monitoring-in-smart-home-using-action-recognition-system/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Baby physical safety monitoring in smart home using action recognition system</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the University of Cincinnati recently studied advances in action recognition, as humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../deep-learning-for-live-cell-shape-detection/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index1ddd.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-9.35.32-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../deep-learning-for-live-cell-shape-detection/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Deep learning for live cell shape detection</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../solar-panel-detection-from-aerial-images/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexd094.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FScreen-Shot-2022-10-26-at-8.37.39-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../solar-panel-detection-from-aerial-images/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">HyperionSolarNet: Solar panel detection from aerial images</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. </p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8"><a class="mr-9 text-neutral-700 mb-1" href="../1/index.html">&lt;</a>Page 2 of 3<a class="ml-9 text-neutral-700 mb-1" href="../3/index.html">&gt;</a></div></div></div></div></div></div>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    Â© Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"636fb7750d444a003deb98a2","uuid":"f571d6de-5904-4156-8e19-6e12dec7cb3a","title":"An AI-based computer-aided system for better knee osteoarthritis assessment","slug":"an-ai-based-computer-aided-system-for-knee-osteoarthritis-assessment","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from the Medical University of Graz recently studied the impact of using an artificial intelligence (AI)-based computer system on the accuracy and agreement rate of board-certified orthopedic surgeons in order to detect X-ray features that can indicate knee OA (osteoarthritis assessment). The researchers focused on building a framework to compare the efficacy of unaided assessments versus the results to those of senior residents.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eEarly-stage OA signs are invisible on plain X-rays, as cartilage degeneration cannot be directly assessed, and OA constitutes a three-dimensional problem. This is reflected by fair to moderate interobserver reliability for knee OA assessment using X-rays alone. To overcome these issues, different solutions, including novel quantitative grading methods as well as automatic knee X-ray assessment tools, have been proposed. Artificial intelligence (AI) and deep learning have been used in medical image classification related to the musculoskeletal system. The researchers analyzed the intra- and interobserver reliability of board-certified orthopedic surgeons (also known as senior readers) for knee OA grade assessment using either AI-annotated or plain X-rays. Afterwards, they compared the outcome of senior readers to that of senior residents (termed junior readers) with aided analysis in terms of agreement rate and overall performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eThe use of AI-based software leads to improvement in the radiological judgement of senior orthopedic surgeons with regard to X-ray features indicative of knee OA and KL grade, as measured by the agreement rate and overall accuracy in comparison to the ground truth. The agreement and accuracy rates of senior readers were comparable to those of junior readers with aided analysis. Consequently, standard of care may be improved by the additional application of AI-based software in the radiological evaluation of knee OA.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe entire labelling process was divided into 3 steps. First, three senior readers were trained on the structure of the AI software report, OARSI grading system labelling process and the Quantumworks Lab platform was used. Second, readers assessedâunaided (i.e. without AI annotations)â124 plain knee X-rays and defined KL grade, osteophytes, sclerosis, and JSN by completing a list. The readers were able to work remotely at their preferred time and allowed to interrupt and resume labelling at any time, without time restrictions for labelling individual images or the entire dataset. However, the time it took readers to label each image, as well as the time of the entire labelling process. Third, after a minimum of 2 weeks after the second step had been completed, the same 124 knee X-rays were relabelled by the readers, with images provided at random order (to avoid creating observer bias. At this point, however, each image was supplemented with the AI softwareâs report together with a binary score of whether OA was present on the X-ray.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"685\" height=\"436\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image.png 600w, https://labelbox-research.ghost.io/content/images/2022/11/image.png 685w\"\u003e\u003cfigcaption\u003eDiagram of the studyâs labelling process\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://link.springer.com/content/pdf/10.1007/s00167-022-07220-y.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"636fb7750d444a003deb98a2","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/iStock-1142758724-1-e1590763901978.jpeg","featured":false,"visibility":"public","created_at":"2022-11-12T15:10:45.000+00:00","updated_at":"2022-12-06T21:30:18.000+00:00","published_at":"2022-11-12T15:24:31.000+00:00","custom_excerpt":"Researchers from the Medical University of Graz recently collaborated to analyze the impact of an artificial intelligence (AI)-based computer system on the accuracy and agreement rate of board-certified orthopedic surgeons to detect X-ray features indicative of knee OA (osteoarthritis assessment).","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/an-ai-based-computer-aided-system-for-knee-osteoarthritis-assessment/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/an-ai-based-computer-aided-system-for-knee-osteoarthritis-assessment/","excerpt":"Researchers from the Medical University of Graz recently collaborated to analyze the impact of an artificial intelligence (AI)-based computer system on the accuracy and agreement rate of board-certified orthopedic surgeons to detect X-ray features indicative of knee OA (osteoarthritis assessment).","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63641a4f0b3b41003d9e0bd1","uuid":"9a9ced15-5dae-4db6-9b66-8d7aa6e56ca1","title":"Lightweight multi-drone detection and 3D-localization via YOLO","slug":"lightweight-multi-drone-detection-and-3d-localization-via-yolo","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from Indian Institute of Technology Kanpur recently evaluated a method to perform real-time multiple drone detection and three-dimensional localization using state-of-the-art tiny-YOLOv4 object detection algorithm and stereo triangulation. They also released the source code for the project, with pre-trained models and the curated synthetic stereo dataset to further advance this research across the community.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eUnmanned Aerial Vehicles (UAVs) have gained massive popularity in recent years, owing to the advancements in technology and surge in the number of use cases for UAVs which include traffic management, security and surveillance, supply of essentials, disaster management, warehouse operations etc. Drones were initially a military, surveillance and security tool. Early versions of the drone were much larger, but as time progressed, they got smaller and smarter. \u003c/p\u003e\u003cp\u003eConsequently with the development of small and agile drones, their applications have time and again raised security concerns. Their increasing use in swarm systems have also sparked another research direction in dynamic detection and localization of multiple drones in such systems, especially for counter-drone systems. \u003c/p\u003e\u003cp\u003eProgressively, deep learning based solutions for detecting drones have improved at the task of object detection, but have also grown bulkier and have relied heavily on bulky computing power.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFinding: \u003c/strong\u003eTheir computer vision approach was able to eliminate the need for computationally expensive stereo matching algorithms, thereby significantly reducing the memory footprint and making it deployable on embedded systems. \u003c/p\u003e\u003cp\u003eTheir drone detection system was highly modular (with support for various detection algorithms) and capable of identifying multiple drones in a system, with real-time detection accuracy of up to 77% with an average FPS of 332 (on Nvidia Titan Xp). They also tested the complete pipeline in AirSim environment, detecting drones at a maximum distance of 8 meters, with a mean error of 23% of the distance. \u003c/p\u003e\u003cp\u003eThe researchers found that the modern, neural net based tiny-YOLO v4 algorithm attained higher frame rates and detection accuracy results than leading CPU based algorithms, and coupled with their classical stereo triangulation based depth estimation module, can be used for 3D localization.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used:\u003c/strong\u003e The Quantumworks Lab platform was used as the primary annotation tool of choice, enriching the researchers' dataset with images containing multiple drones. Other than the images of drones, the dataset also contains images of non-drone, drone-like ânegativeâ objects, as to avoid their model from overfitting. The dataset contains 5529 images along with annotated files corresponding to each image, containing parameters of bounding box such as height, width, center x, y coordinates, and object class.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://arxiv.org/pdf/2202.09097.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63641a4f0b3b41003d9e0bd1","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-03-at-12.54.00-PM.png","featured":false,"visibility":"public","created_at":"2022-11-03T19:45:19.000+00:00","updated_at":"2022-12-06T21:30:30.000+00:00","published_at":"2022-11-03T20:04:31.000+00:00","custom_excerpt":"Researchers from Indian Institute of Technology Kanpur recently evaluated a method to perform real-time multiple drone detection and three-dimensional localization using state-of-the-art tiny-YOLOv4 object detection algorithm and stereo triangulation.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/lightweight-multi-drone-detection-and-3d-localization-via-yolo/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/lightweight-multi-drone-detection-and-3d-localization-via-yolo/","excerpt":"Researchers from Indian Institute of Technology Kanpur recently evaluated a method to perform real-time multiple drone detection and three-dimensional localization using state-of-the-art tiny-YOLOv4 object detection algorithm and stereo triangulation.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"636418130b3b41003d9e0b8c","uuid":"28a1c8a7-8c71-4de7-a7e2-6980b362db5f","title":"Automated detection of malaria parasites using convolutional neural networks","slug":"automated-detection-of-malaria-parasites-using-convolutional-neural-networks","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from Imperial College London recently focused on developing an automated image analysis method to improve accuracy and standardization of smear inspection that retains capacity for expert confirmation and image archiving. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Microscopic examination of blood smears is known to be the gold standard for laboratory inspection and diagnosis of malaria. Smear inspection is, however, time-consuming and dependent on trained microscopists with results varying in accuracy. To advance this technique using convolution neural networks (CNNs), a machine learning method was developed to hone in on red blood cell (RBC) detection, differentiation between infected/uninfected cells, and parasite life stage categorization from unprocessed, heterogeneous smear images. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: Based on a pretrained Faster Region-Based Convolutional Neural Networks (R-CNN) model for RBC detection, their model performed accurately, with an average precision of 0.99 at an intersection-over-union threshold of 0.5. Application of a residual neural network-50 model to infected cells also performed accurately, with an area under the receiver operating characteristic curve of 0.98. \u003c/p\u003e\u003cp\u003eCombined with a mobile-friendly web-based interface that they built, called PlasmoCount, their ML method permits rapid navigation through and review of results for quality assurance. By standardizing assessment of Giemsa smears, their method markedly improves inspection reproducibility and presents a realistic route to both routine lab and future field-based automated malaria diagnosis. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers leveraged a \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-research.ghost.io\"\u003emodel-assisted approach\u003c/a\u003e for labeling using the Quantumworks Lab platform which greatly improved labeling speed and performance. \u003c/p\u003e\u003cp\u003eEach labeling round contained ~100 raw images of Giemsa smears. To aid the first labeling round, they trained their object detection model on a dataset of Plasmodium vivax-infected blood smears from the Broad Bioimage Benchmark Collection. Predictions on their P. falciparum dataset were then uploaded as prelabels using the Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/getting-started-with-the-python-sdk?ref=labelbox-research.ghost.io\"\u003ePython SDK\u003c/a\u003e. For each of the following labeling rounds, the RBC detection model and malaria identification classifier were trained on all the previous labeled datasets to generate new labels. \u003c/p\u003e\u003cp\u003eAnnotators could then correct, add, and delete bounding boxes around each RBC, and choose from three labels: infected, uninfected, and unsure. All images were labeled five times by five designated annotators from three different research centers for the test set.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8724263/?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"636418130b3b41003d9e0b8c","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/0179MarionsMalaria5.jpeg","featured":false,"visibility":"public","created_at":"2022-11-03T19:35:47.000+00:00","updated_at":"2022-12-06T21:30:43.000+00:00","published_at":"2022-11-03T19:42:17.000+00:00","custom_excerpt":"Researchers from Imperial College London recently focused on developing an automated image analysis method to improve accuracy and standardization of smear inspection that retains capacity for expert confirmation and image archiving. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automated-detection-of-malaria-parasites-using-convolutional-neural-networks/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/automated-detection-of-malaria-parasites-using-convolutional-neural-networks/","excerpt":"Researchers from Imperial College London recently focused on developing an automated image analysis method to improve accuracy and standardization of smear inspection that retains capacity for expert confirmation and image archiving. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635b459399ae26003d07ae7a","uuid":"59073aae-19d2-4752-8438-3eca7000fac4","title":"Automatic recognition of emotional subgroups in images","slug":"automatic-recognition-of-emotional-subgroups-in-images","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eTracking individuals is not always the most efficient way of sensing emotions, especially in large crowds, where outputs would get cluttered. Additionally, an individualâs emotion can be better predicted when incorporating emotions from others in their social group, while at the same time people tend to be part of social groups that feel and act in a similar manner. Recognizing emotional subgroups is therefore a more efficient way of detecting emotion or behavior within a crowd. Simply combining the tasks of group and emotion recognition is not likely to suffice, since emotional subgroups can either split up or combine social groups complicating the task.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eImages that show agreement among annotators, are most often those that elicit the use of the summation strategy, while images with partial agreement more often elicit the use of the emotion-based fusion (putting more emphasis on emotion than social groups) or the group-based fusion (putting more emphasis on social groups than on emotion) strategy. Experimenting with different additional features suggests, with a modest performance improvement, that face size and gaze direction contain meaningful information. This shows that the task of emotional subgroup recognition is a complex one, but also that a relatively small feature vector is already able to reasonably represent human perception.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers used Quantumworks Lab to have human annotators label a set of 171 images, and their recognition strategies were analyzed. Three main strategies for labeling images are identified, with each strategy assigning either 1) more weight to emotions (emotion-based fusion), 2) more weight to spatial structures (groupbased fusion), or 3) equal weight to both (summation strategy). Based on these strategies, algorithms are developed to automatically recognize emotional subgroups. In particular, K-means and hierarchical clustering are used with location and emotion features derived from a fne-tuned VGG network. Additionally, they experimented with face size and gaze direction as extra input features and found that the best performance came from hierarchical clustering with emotion, location and gaze direction as input.\u003c/p\u003e\u003cp\u003e\u003cbr\u003eRead the full PDF \u003ca href=\"https://www.ijcai.org/proceedings/2022/0190.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635b459399ae26003d07ae7a","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-8.07.21-PM.png","featured":false,"visibility":"public","created_at":"2022-10-28T02:59:31.000+00:00","updated_at":"2022-12-06T21:31:21.000+00:00","published_at":"2022-10-28T03:07:50.000+00:00","custom_excerpt":"Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automatic-recognition-of-emotional-subgroups-in-images/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/automatic-recognition-of-emotional-subgroups-in-images/","excerpt":"Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635b43bc99ae26003d07ae57","uuid":"1f4c6d83-b8f5-4c44-ac98-388d26b4e143","title":"A hierarchical optical coherence tomography annotation workflow with crowds and medical experts","slug":"a-hierarchical-optical-coherence-tomography-annotation-workflow-with-crowds-and-medical-experts","html":"\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eResearchers from Genentech are researching solutions to the heavy burden it takes to annotate clinical images due to the cost and availability of medical experts. To address this problem, they proposed a hierarchical annotation workflow in which medical experts review aggregated crowdsourced annotations, using dense annotation of optical coherence tomography (OCT) images from age-related macular degeneration (AMD) patients as an example.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eThe proposed hierarchical annotation workflow with crowds and medical experts could reduce the burden on medical experts in extensive clinical annotation tasks.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eAll annotation was performed on Quantumworks Lab which distributed images to remote crowds and medical experts. OCT B-scans with â¥ 9 averages from 20x20Â° volume scans of AMD patients were randomly selected. Two medical experts annotated 25 representative B-scans with rich pathology. In a training session, 27 labelers read through an annotation guideline and practiced on 15 of the B-scans. B-scans with color-coded agreements and disagreements were presented to the crowd, visualizing their discrepancy to the expert annotations.\u003c/p\u003e\u003cp\u003eRead the full paper \u003ca href=\"https://iovs.arvojournals.org/article.aspx?articleid=2783125\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635b43bc99ae26003d07ae57","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-7.57.46-PM.png","featured":false,"visibility":"public","created_at":"2022-10-28T02:51:40.000+00:00","updated_at":"2022-12-06T21:31:40.000+00:00","published_at":"2022-10-28T02:58:26.000+00:00","custom_excerpt":"Researchers from Genentech are researching solutions to the heavy burden it takes to annotate clinical images due to the cost and availability of medical experts.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/a-hierarchical-optical-coherence-tomography-annotation-workflow-with-crowds-and-medical-experts/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/a-hierarchical-optical-coherence-tomography-annotation-workflow-with-crowds-and-medical-experts/","excerpt":"Researchers from Genentech are researching solutions to the heavy burden it takes to annotate clinical images due to the cost and availability of medical experts.","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635ab9d199ae26003d07adea","uuid":"139ae20c-fa83-4721-9a27-147e14820cf8","title":"Evaluating terrain-dependent performance for martian frost detection in visible satellite observations","slug":"evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Past studies have focused on manually analyzing the behavior of the frost cycle in the northern mid-latitude region of Mars using high-resolution visible observations from orbit. Extending these studies globally requires automating the detection of frost using data science techniques such as convolutional neural networks. However, visible indications of frost presence can vary significantly depending on the geologic context on which the frost is superimposed. \u003c/p\u003e\u003cp\u003eUnlike the Earth, the atmosphere of Mars is comprised primarily of carbon dioxide (CO2) and this volatile constitutes most of the frost, falling as snow or condensing at the surface due to surface temperatures falling to the CO2 frost point. A small amount of water frost will also form when temperatures are below the water (H2O) frost point, but only if the local concentration of H2O vapor in the atmosphere is high enough.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The research team from JPL presented a novel approach for spatially partitioning data to reduce biases in model performance estimation, illustrate how geologic context affects automated frost detection, and propose future work to further mitigate observed biases in automated frost detection work. \u003c/p\u003e\u003cp\u003eThey found that geologic context bias is present and significant for this modelâs performance on the test set, specifically for dune fields often found in northern mid-latitude craters. Interestingly, for human annotators, dunes often provide strong evidence of frost due to the striking visual appearance of defrosting marks which expose dark basalt sand beneath light-color frost.\u003c/p\u003e\u003cp\u003eThey also saw a large degree of diversity in frost appearance on the underrepresented terrain type, both inherently and due to differing illumination and observational conditions, making the concept challenging for the classification model to learn.\u003c/p\u003e\u003cp\u003eFuture improvements will permit the training of models better suited for full-planet frost detection, thereby facilitating the creation of global frost maps\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eLabelbox was used to annotate polygonal boundaries around regions with visible evidence of frost. For each polygon, they collected additional information from the labeler including the applicable visible indicators as well as geologic context, which is either âdunes,â âgullies,â âcrater rim/wall,â or âother.â The geologic context categories was mutually exclusive, so labelers could only pick one geologic context per frost polygon. \u003c/p\u003e\u003cp\u003eIn addition, they used the geologic context information to investigate terrain-dependent bias in classifier performance. To document the labeling process, they performed an iterative series of labeling sessions with both data science and science domain experts. \u003c/p\u003e\u003cp\u003eDomain expert labeling guidance and clarifications at each iteration were captured in a labeling guide, included with the publicly available dataset2. A total of 6 subframes, detailed in the released dataset, were excluded due to contamination with excess instrument noise or cloud cover. Each subframe was labeled by three different annotators.\u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://www.cs.emory.edu/~sgu33/workshop/DeepSpatial2022/papers/p4.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635ab9d199ae26003d07adea","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-10.09.34-AM.png","featured":false,"visibility":"public","created_at":"2022-10-27T17:03:13.000+00:00","updated_at":"2022-12-06T21:31:55.000+00:00","published_at":"2022-10-27T17:10:10.000+00:00","custom_excerpt":"Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/","excerpt":"Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635ab62c99ae26003d07adb0","uuid":"8426f3d6-2b81-4dbe-963a-866ba213f73b","title":"The impact of artificial intelligence assessment on diabetic retinopathy","slug":"impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the\u003cstrong\u003e \u003c/strong\u003eUniversity of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. Using validated AI algorithms instead of scarce trained specialists, this could potentially increase the efficiency and accessibility of screening programs. Systematic reviews of deep learning-based algorithms in DR screening have highlighted such advantages as reduction in demands for manpower, cost of screening, and intragrader and intergrader variability. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch: \u003c/strong\u003eThe growing burden of diabetes and its associated complications is increasing the demands on health care systems, particularly in low-resource countries. Their hypothesis was that adherence to referral services would be higher among patients randomized to receive AI-supported screening with immediate feedback compared with those randomized to receive delayed communication of results until after human grading was completed. Evidence from real-life screening programs is limited, and no evidence exists on community acceptance of the use of AI-supported DR screening.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: Immediate feedback on referral status based on AI-supported screening was associated with statistically significantly higher referral adherence compared with delayed communications of results from human graders. These results provide evidence for an important benefit of AI screening in promoting adherence to prescribed treatment for diabetic eye care in sub-Saharan Africa.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eAll images, anonymized with a unique patient registration number, were uploaded to Orbis Internationalâs Cybersight AI. A mobile device or laptop and an internet connection are required to access Cybersight AI, which generates a response regarding the presence or absence of referable DR based on a macula-centered image from each available eye of a participant within 60 seconds. Afterwards, all images were uploaded to Quantumworks Lab for annotation and grading by a United Kingdom National Health System formally trained retinal specialist. \u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://reader.elsevier.com/reader/sd/pii/S2666914522000574?token=871F5D7CE9870F35BC04F4954510A8796D2A713DA63A98B7B2FF383765C5642101B2C77F003218EC780639144DEF7323\u0026originRegion=us-east-1\u0026originCreation=20221027165201\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635ab62c99ae26003d07adb0","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-9.51.43-AM.png","featured":false,"visibility":"public","created_at":"2022-10-27T16:47:40.000+00:00","updated_at":"2022-12-06T21:32:07.000+00:00","published_at":"2022-10-27T16:54:48.000+00:00","custom_excerpt":"Researchers from the University of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/","excerpt":"Researchers from the University of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635ab40899ae26003d07ad80","uuid":"12c85e7e-2368-4da3-ac77-8ceae25c1e7c","title":"Baby physical safety monitoring in smart home using action recognition system","slug":"baby-physical-safety-monitoring-in-smart-home-using-action-recognition-system","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the University of Cincinnati recently studied advances in action recognition, as humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. This is because the brain operates on a bidirectional communication model, which has radically improved the accuracy of recognition and prediction based on features connected to previous experiences. During the past decade, deep learning models for action recognition have significantly improved. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch Challenge\u003c/strong\u003e: Deep neural networks typically struggle with these tasks on a smaller dataset for specific Action Recognition (AR) tasks. As with most action recognition tasks, the ambiguity of accurately describing activities in spatial-temporal data is a drawback that can be overcome by curating suitable datasets, including careful annotations and preprocessing of video data for analyzing various recognition tasks. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: The researchers presented a novel lightweight framework combining transfer learning techniques with a Conv2D LSTM layer to extract features from the pre-trained I3D model on the Kinetics dataset for a new AR task (Smart Baby Care) that requires a smaller dataset and less computational resources. Furthermore, they developed a benchmark dataset and an automated model that uses LSTM convolution with I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in a smart baby room. By implementing video augmentation to improve model performance on the smart baby care task and comparing it to to other benchmark models, their experimental framework achieved better performance with less computational resources.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eLabelbox was used to create a curated dataset from scratch for this project to develop an AR model for a specific Action Recognition task (Smart Baby Care). Our data source came from an open-source video from social media platforms such as (YouTube, Instagram, Pexels, etc.). The videos were manually downloaded and then trimmed with a python script using the inbuilt FF-MPEG library and annotated frame-wise using Labelbox. \u003c/p\u003e","comment_id":"635ab40899ae26003d07ad80","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-9.44.00-AM.png","featured":false,"visibility":"public","created_at":"2022-10-27T16:38:32.000+00:00","updated_at":"2022-12-06T21:32:23.000+00:00","published_at":"2022-10-27T16:44:41.000+00:00","custom_excerpt":"Researchers from the University of Cincinnati recently studied advances in action recognition, as humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/baby-physical-safety-monitoring-in-smart-home-using-action-recognition-system/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/baby-physical-safety-monitoring-in-smart-home-using-action-recognition-system/","excerpt":"Researchers from the University of Cincinnati recently studied advances in action recognition, as humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635a067799ae26003d07ad35","uuid":"80084265-efe2-45ec-8ca2-a854ad139075","title":"Deep learning for live cell shape detection","slug":"deep-learning-for-live-cell-shape-detection","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging. AFM is used as mechanical characterization in a wide range of samples, including live cells, proteins, and other biomolecules. It is also instrumental for measuring interaction forces and binding kinetics for proteinâprotein or receptorâligand interactions on live cells at a single-molecule level. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eThe difficulty lies in performing force measurements and high-resolution imaging with AFM and data analytics because it is time-consuming and require special skill sets and continuous human supervision. Recently, researchers have explored the applications of using artificial intelligence (AI) and deep learning (DL) in the bioimaging field. However, the applications of AI to AFM operations for live-cell characterization are little-known until now. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The researchers implemented a deep learning framework to perform automatic sample selection based on the cell shape for AFM probe navigation during AFM biomechanical mapping. They established a closed-loop scanner trajectory control for measuring multiple cell samples at high speed for automated navigation. With this, they achieved a 60Ã speed-up in AFM navigation and reduced the time involved in searching for the particular cell shape in a large sample. Their innovation directly applies to many bio-AFM applications with AI-guided intelligent automation through image data analysis together with smart navigation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1328\" height=\"412\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1328w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers leveraged Quantumworks Lab to label their data and enabled experts to annotate the cell shape by drawing bounding boxes around it and labeling it with an accurate shape. Collecting these images was time-consuming and tedious as the user had to manually scan the cell samples and capture the images. In addition, performing the annotations, especially on low-quality images, was a painstaking task, leading to a smaller dataset with fewer annotated images. To address this challenge, they implemented data augmentation techniques on the fly (during training), which involved rotating the original images by 90â¦ clockwise or counter-clockwise, by 180â¦ , flipping them upside down, and by left-right mirroring. This enhanced the original dataset with more data samples with different orientations, which further made the DL network robust to the variety of cell shape orientations encountered during inference.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1246\" height=\"478\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1246w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eVisualizing the predictions on low-quality images. Target/ground truth images are shown in the top row and the corresponding predictions in the bottom row.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://www.mdpi.com/2306-5354/9/10/522/pdf?version=1665735868\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635a067799ae26003d07ad35","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.35.32-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T04:17:59.000+00:00","updated_at":"2022-12-06T21:32:36.000+00:00","published_at":"2022-10-27T04:35:55.000+00:00","custom_excerpt":"Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/deep-learning-for-live-cell-shape-detection/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/deep-learning-for-live-cell-shape-detection/","excerpt":"Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6359fb7199ae26003d07acfe","uuid":"5bdfd1b2-714c-4783-9060-d7a77c6cc248","title":"HyperionSolarNet: Solar panel detection from aerial images","slug":"solar-panel-detection-from-aerial-images","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003e With the effects of global climate change impacting the world, collective efforts are needed to reduce greenhouse gas emissions. The energy sector continues to be the single largest contributor to climate change and many efforts are focused on reducing dependence on carbon-emitting power plants and moving to renewable energy sources, such as solar power.\u003c/p\u003e\u003cp\u003eTheir work focused on creating a world map of solar panels, identifying locations and total surface area of solar panels within a given geographic area. The researchers used deep learning methods for automated detection of solar panel locations and their surface area using aerial imagery. The framework, which consisted of a two-branch model using an image classifier in tandem with a semantic segmentation model, was trained on a created dataset of satellite images.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: Their work provided an efficient and scalable method for detecting solar panels, achieving an accuracy of 0.96 for classification and an IoU score of 0.82 for segmentation performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used\u003c/strong\u003e: The research team annotated 836 images containing solar panels using the Quantumworks Lab platform, and produced corresponding segmentation masks, resizing all images to a size of 512x512 pixels for training and testing. The researchers manually annotated these images using Quantumworks Lab and created mask labels for them, afterwards, evaluating the HyperionSolarNet segmentation model against these test set images and finding Â an IoU score of 0.82. \u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://s3.us-east-1.amazonaws.com/climate-change-ai/papers/neurips2021/41/paper.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6359fb7199ae26003d07acfe","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-8.37.39-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T03:30:57.000+00:00","updated_at":"2022-12-06T21:32:57.000+00:00","published_at":"2022-10-27T03:38:47.000+00:00","custom_excerpt":"Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/solar-panel-detection-from-aerial-images/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/solar-panel-detection-from-aerial-images/","excerpt":"Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":21,"tag":{"slug":"academic-papers","id":"635953ac4af05f0031c98e9f","name":"Academic papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"count":{"posts":21},"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"slug":"academic-papers","currentPage":"2"},"__N_SSG":true},"page":"/research/tag/[id]/page/[pagenum]","query":{"id":"academic-papers","pagenum":"2"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script>
    

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>

                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="/static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    Â© Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer>
</body>
<!-- Mirrored from labelbox.com/research/tag/academic-papers/page/2/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:36:35 GMT -->
</html>