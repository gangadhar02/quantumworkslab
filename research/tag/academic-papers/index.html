<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/research/tag/academic-papers/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 10:46:25 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Research | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Research | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/research/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Research | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/research/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../static/scripts/munchkin.js"></script><script src="../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../_next/static/chunks/pages/research/tag/%5bid%5d-cbbeaae11d3b3154.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../index.html"><img width="106" height="24" alt="logo" src="../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Research</h1><p class="text-base max-w-xs text-neutral-500  pr-6"></p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Academic papers</a><a href="../coursework/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Coursework</a><a href="../computer-vision/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Computer vision</a><a href="../natural-language/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Natural language</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="mb-10"><h2 class="text-3xl md:text-4xl font-medium mb-4">Academic papers</h2><p class="text-base max-w-2xl font-medium text-neutral-500"></p></div><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../ai-guided-defect-detection/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexc0cb.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../ai-guided-defect-detection/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Quantumworks Lab services helps research team explore AI-guided defect detection techniques</p><p class="text-base max-w-2xl undefined line-clamp-3">At Quantumworks Lab, we&#x27;re passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we&#x27;re excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.

These posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we&#x27;re exploring AI-Guided </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../a-benchmark-for-long-form-medical-question-answering/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index9fe7.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../a-benchmark-for-long-form-medical-question-answering/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">A benchmark for long-form medical question answering</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexcd10.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Identifying and counting avian blood cells in whole slide images via deep learning</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index0a39.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">NASA/JPL: Onboard instruments for the detection of microscopy biosignatures</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how researchers from NASA/JPL are  advancing biosignature detection which offers insights and lessons for future mission concepts exploring life in the outer solar system.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexc553.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Detecting feature requests of 3rd-party developers via machine learning: A case study of the SAP Community</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexac0f.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Real-time segmentation of desiccation cracks onboard UAVs for planetary exploration</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures).</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../fruit-flower-detection-in-apple-orchards-using-ml/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexab24.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../fruit-flower-detection-in-apple-orchards-using-ml/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Fruit flower detection in apple orchards using ML</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the University of Guleph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index38cf.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Automated recognition of  cricket batting techniques in videos using deep learning</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the University of Johannesburg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-8.46.45-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-8.46.45-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-8.46.45-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-8.46.45-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-8.46.45-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-8.46.45-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-8.46.45-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-8.46.45-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index697c.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-8.46.45-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Better gait and posture classification using sensors in individuals with mobility impairment after a stroke</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the University of Zurich recently developed and validated a solution for better monitoring the gait and posture of individuals who have suffered a stroke via sensors.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-12-at-2.30.08-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-12-at-2.30.08-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-12-at-2.30.08-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-12-at-2.30.08-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-12-at-2.30.08-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-12-at-2.30.08-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-12-at-2.30.08-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-12-at-2.30.08-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexe742.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-12-at-2.30.08-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Generating integrated bill of materials using mask R-CNN models for construction</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Pusan National University studied how to utilize AI to identify and generate concrete formworks.</p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8">Page 1 of 3<a class="ml-9 text-neutral-700 mb-1" href="page/2/index.html">&gt;</a></div></div></div></div></div></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"67be4062651e6b0001cd2bdf","uuid":"ef0d7946-0433-4386-bc84-c788c313d47e","title":"Quantumworks Lab services helps research team explore AI-guided defect detection techniques","slug":"ai-guided-defect-detection","html":"\u003cp\u003eAt Quantumworks Lab, we're passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we're excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.\u003c/p\u003e\u003cp\u003eThese posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we're exploring \u003ca href=\"https://arxiv.org/pdf/2404.07306?ref=labelbox-research.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAI-Guided Defect Detection Techniques to Model Single Crystal Diamond Growth\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003ea study by Rohan Reddy Mekala, Elias Garratt, Matthias Muehle, Arjun Srinivasan, Adam Porter, and Mikael Lindvall.\u003c/p\u003e\u003ch2 id=\"research-introduction\"\u003e\u003cstrong\u003eResearch introduction\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eResearchers recently identified several challenges in producing high-quality single crystal diamonds (SCDs) at scale. Despite extensive development efforts, current manufacturing relies on trial-and-error experimentation, leading to inconsistent results and defects. In order to address these issues, researchers across the United States collaborated to propose new methodologies using machine learning and AI models to predict future diamond growth states for accelerated material development, improved quality, and larger sizes.\u003c/p\u003e\u003cp\u003eThe research team focused on these challenges:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLack of predictability:\u003c/strong\u003e Existing techniques currently lack the ability to predict diamond growth states, hindering process control and the potential for correction.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIssues with scale:\u003c/strong\u003e Diamond is an essential material for tools in power electronics, health sciences, and engineering, but each field has different requirements in terms of quality, purity, and size. The inability to predict growth subsequently makes it difficult to scale production.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIneffective research methodology:\u003c/strong\u003e Current research methods on developing a method to sustainably and reliably produce high quality diamonds rely on a trial and error method, leading to inconsistent results and defects from each methodology.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom these challenges, researchers identified an opportunity in using machine learning and deep learning algorithms to predict future diamond growth states to shorten the growth cycle, improve prediction accuracy, and enhance crystalline quality.\u003c/p\u003e\u003ch2 id=\"how-labelbox-was-used\"\u003e\u003cstrong\u003eHow Quantumworks Lab was used\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eLabelbox services was used to build the dataset for object detection of the diamond growth-run images. The process began with an initial batch of 100 images, which were reviewed by a team of three material scientists and 15 Alignerrs that were vetted and onboarded through Quantumworks Lab Labeling Services.. Alignerrs provide expert, on-demand annotation services and are selected from a network of trained professionals across diverse data domains.\u003c/p\u003e\u003cp\u003eThe scientists provided detailed instructions, including explanatory videos and meetings, to guide the labeling process. The labels were then reviewed by the Alignerrs, where predominant occurrences were identified based on a consensus score. Afterward, the material scientists conducted a final review to ensure the accuracy, consistency, and integrity of the data.\u003c/p\u003e\u003ch2 id=\"analysis-and-model-assisted-labeling\"\u003e\u003cstrong\u003eAnalysis and model-assisted labeling\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-research.ghost.io\"\u003e\u003cu\u003eModel-assisted labeling\u003c/u\u003e\u003c/a\u003e (MAL) was implemented in order to improve the consistency of data labeling and address potential variability in interpretation of instructions amongst labelers. This involved training a baseline model incrementally using iterative image-annotation pairs from the initial batch. This baseline model was then overlaid on subsequent batches to assist Alignerrs, significantly reducing labeling time and improving accuracy.\u003c/p\u003e\u003cp\u003eWith MAL, the time to label each image decreased from 15 minutes to just 2 minutes. Alignerrs were able to correct and refine annotation based on the baseline model’s predictions until a segmentation accuracy threshold of 80% was achieved. Once this threshold was achieved, the model’s overlays were used as a starting point for future batches.\u003c/p\u003e\u003cp\u003eAfter the minimum number of images were processed, the final set of image-label pairs was passed on for further research and development of the final semantic segmentation and object detection models. Using MAL and this human-in-the-loop workflow, this streamlined annotation process leads to improved efficiency and label quality.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeGAy55bE9C_7cOm9nsW-H0IraNnL1CMua-RCDhe88CKCtuOqn0BTPC4wr0gSgZ5HKrxTsNRkbrOtuMF7yQ5QFYvXGaexfSPvfZJFjHT5C4IpKxXHYkaI-YiUNq0z4eP8yyd2Pg?key=cmC32LkeleH-01xeZ1gdpicE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"424\"\u003e\u003c/figure\u003e\u003ch2 id=\"key-findings\"\u003e\u003cstrong\u003eKey findings\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eUsing object detection and image segmentation algorithms along with the support of a highly-skilled team of Quantumworks Lab Alignerrs, researchers were able to create a defect detection pipeline for diamond growth data. This pipeline achieved a high defect classification accuracy with 93.35% for Center-Defects, 92.83% for Poly-crystalline-Defects, and 91.98% for Edge-Defects.\u003c/p\u003e\u003cp\u003eThe authors were able to use this pipeline to accurately detect defects, reduce time and costs, and predict future diamond growth.\u003c/p\u003e\u003cp\u003eWorking on your own research? Reach out to the team at research@labelbox.com to request a research license or to share your AI research with us.\u0026nbsp;\u003c/p\u003e","comment_id":"67be4062651e6b0001cd2bdf","feature_image":"https://labelbox-research.ghost.io/content/images/2025/02/AI-Guided-Defect-Detection-Techniques.png","featured":false,"visibility":"public","created_at":"2025-02-25T22:12:50.000+00:00","updated_at":"2025-02-25T22:45:46.000+00:00","published_at":"2025-02-25T22:45:46.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"67be114424e4bd0001a29369","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/lisa/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"67be114424e4bd0001a29369","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/lisa/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/ai-guided-defect-detection/","excerpt":"At Quantumworks Lab, we're passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we're excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.\n\nThese posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we're exploring AI-Guided ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How Quantumworks Lab services helps explore AI-guided defect detection techniques ","meta_description":"Learn how researchers use Quantumworks Lab services and software to accurately detect defects, reduce time and costs, and predict future diamond growth.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"674656bbd6cd0800017443d1","uuid":"a0cd1b3b-3473-473e-a076-9c9f2ede5adb","title":"A benchmark for long-form medical question answering","slug":"a-benchmark-for-long-form-medical-question-answering","html":"\u003cp\u003eResearchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. Existing benchmarks often rely on automatic metrics and multiple-choice questions, which do not fully capture the complexities of real-world clinical applications. To bridge this gap, the authors introduced a publicly available benchmark comprising real-world consumer medical questions, with long-form answers evaluated by medical professionals. This resource aims to facilitate more accurate assessments of LLMs' performance in medical question answering.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch area and challenges \u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLack of comprehensive benchmarks\u003c/strong\u003e: Existing evaluations focus on automatic metrics and multiple-choice formats, which do not adequately reflect the nuances of clinical scenarios.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eClosed-source studies\u003c/strong\u003e: Many studies on long-form medical QA are not publicly accessible, limiting reproducibility and the enhancement of existing baselines.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAbsence of human annotations\u003c/strong\u003e: There's a scarcity of datasets with human medical expert annotations, hindering the development of reliable evaluation metrics.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1114\" height=\"784\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 1114w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eScheme of the difficulty levels of medical questions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eTo build a dataset of real-world medical questions, 4,271 queries were collected from the Lavita Medical AI Assist platform between  spanning 1,693 conversations (single-turn and multi-turn). After deduplication, removal of sample questions, and filtering non-English entries using Lingua, the dataset was refined to 2,698 unique queries. \u003c/p\u003e\u003cp\u003ePairwise annotation tasks were completed using Quantumworks Lab's annotation platform and human evaluations were conducted with a group of 3 medical doctors, with two doctors assigned per batch, specializing in radiology and pathology. \u003c/p\u003e\u003cp\u003eBefore starting the main round of annotations, the researchers shared the annotation scheme with the doctors and conducted a trial round with each on a small sample of questions. They then gathered the doctors’ feedback to ensure that all annotation criteria were clear and that there was no ambiguity regarding the instructions. After confirming clarity and receiving approval from the doctors, they proceeded with the main batches of annotations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eLLM-as-a-Judge\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe researchers designed their LLM-as-a-judge prompt by combining templates from Zheng et al. and WildBench. The prompt, shown below, enables pairwise comparison of responses across multiple criteria, and used GPT-4o-2024-08-06 and Claude-3-5-Sonnet-20241022 as judges.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1142\" height=\"638\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eKey findings\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpen LLMs' potential\u003c/strong\u003e: Preliminary results indicated that open-source LLMs exhibit strong performance in medical QA, comparable to leading closed models.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAlignment with human judgments\u003c/strong\u003e: The research included a comprehensive analysis of LLMs acting as judges, revealing significant alignment between human evaluations and LLM assessments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePublicly available benchmark\u003c/strong\u003e: The authors provided a new benchmark featuring real-world medical questions and expert-annotated long-form answers, promoting transparency and reproducibility in future research.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1146\" height=\"386\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 1146w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThis paper has been submitted to \u003cem\u003eAIM-FM: Advancements in Medical Foundation Models Workshop\u003c/em\u003e at NeurIPS 2024. You can read the full paper \u003ca href=\"https://arxiv.org/pdf/2411.09834?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"674656bbd6cd0800017443d1","feature_image":"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.53.21-PM.png","featured":false,"visibility":"public","created_at":"2024-11-26T23:16:11.000+00:00","updated_at":"2025-07-03T18:07:05.000+00:00","published_at":"2024-11-26T23:43:04.000+00:00","custom_excerpt":"Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/a-benchmark-for-long-form-medical-question-answering/","excerpt":"Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65bbe41d21ca940001df755f","uuid":"eb9e0733-ed5d-4b7c-9061-6ba30c57b26e","title":"Identifying and counting avian blood cells in whole slide images via deep learning","slug":"identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eAvian blood analysis is a fundamental method for investigating a wide range of topics concerning individual birds and populations of birds. Determining precise blood cell counts helps researchers gain insights into the health condition of birds. For example, the ratio of heterophils to lymphocytes (H/L ratio) is a well-established index for comparing relative stress load. However, such measurements are currently often obtained manually by human experts. The researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models. The first model determined image regions that are suitable for counting blood cells, and the second model is an instance segmentation model that detected the cells in the determined image regions. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eAutomated visual and acoustic monitoring methods for birds can provide information about the presence and the number of bird species or individuals in certain areas, but analyzing the physiological conditions of individual birds allows researchers to understand potential causes of negative population trends. \u003c/p\u003e\u003cp\u003eFor example, measuring the physiological stress of birds can serve as a valuable early warning indicator for conservation efforts. The physiological conditions and the stress of birds can be determined in several ways, e.g., by assessing the body weight or the fat and muscle scores in migratory birds. Other frequently used methods are investigating the parasite loads, measuring the heart rates, and measuring the levels of circulating stress hormones, such as corticosterone. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The region selection model achieves up to 97.3% in terms of F1 score (i.e., the harmonic mean of precision and recall), and the instance segmentation model achieves up to 90.7% in terms of mean average precision. The approach can  help ornithologists acquire hematological data from avian blood smears more precisely and efficiently.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003e The researchers used the Quantumworks Lab platform annotating images of our datasets with model-assisted labeling. The instance segmentation dataset was annotated in an iterative, model-assisted manner, using the tile selection network to propose regions to be annotated and eventually selected them based on how many rare cells had been detected by an intermediate instance segmentation model. \u003c/p\u003e\u003cp\u003eIn the very first iteration, they used a superpixel algorithm to generate simple instance masks. In each iteration, they uploaded the corresponding instance segmentation masks to Quantumworks Lab to be refined by our human expert. This procedure significantly reduced the time needed to fully annotate an image file with masks and class labels compared to annotating from scratch. Overall, they went through four iterations of labeling. For the annotated cell instances, they established two primary categories: erythrocyte, with only the nucleus annotated, and leukocyte. The latter was further split into five subtypes, namely, lymphocyte, eosinophil, heterophil, basophil, and monocyte. Thrombocytes were not explicitly annotated; they were considered to be part of the background during training. \u003c/p\u003e\u003cp\u003eThe trained neural network model was able to distinguish between non-relevant thrombocytes and other annotated cell types, e.g., erythrocytes. By annotating only the nucleus of each erythrocyte rather than the entire cell including the cytoplasm, they maintained the option to label parasite-infected instances individually in future work. Cells infected with parasites may be annotated by masking the entire cell including the cytoplasm. One erythrocyte can be simultaneously counted as both an erythrocyte and a cell with blood parasite because of the distinct annotation regions.\u003c/p\u003e\u003cp\u003eYou can read the full paper \u003ca href=\"https://www.mdpi.com/2673-6004/5/1/4?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"65bbe41d21ca940001df755f","feature_image":"https://labelbox-research.ghost.io/content/images/2024/02/download.jpeg","featured":false,"visibility":"public","created_at":"2024-02-01T18:34:05.000+00:00","updated_at":"2024-02-01T18:39:06.000+00:00","published_at":"2024-02-01T18:39:06.000+00:00","custom_excerpt":"Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/","excerpt":"Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65b2e290c909bf0001caf529","uuid":"0a31c74d-75d8-4e37-840c-92b6ba27034f","title":"NASA/JPL: Onboard instruments for the detection of microscopy biosignatures","slug":"onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eThe quest for extraterrestrial life represents a critical scientific endeavor with civilization-level implications. Promising targets for exploration include icy moons in the solar system, identified for their potential as habitats due to liquid oceans. However, the lack of a precise definition of life presents a fundamental challenge to formulating detection strategies. In response to the bandwidth limitations in transmitting data from distant ocean worlds like Enceladus or Europa, an emerging discipline called Onboard Science Instrument Autonomy (OSIA) evaluates, summarizes, and prioritizes observational instrument data within flight systems. Two OSIA implementations, identifying life-like motion in digital holographic microscopy videos and cellular structure and composition through fluorescence, were developed as part of the Ocean World Life Surveyor (OWLS) prototype instrument suite at the Jet Propulsion Laboratory. Flight-like requirements and computational constraints, akin to those on the Mars helicopter, \"Ingenuity,\" were employed to lower barriers to infusion. The study, which included evaluation using simulated and laboratory data and a live field test at the Mono Lake planetary analog site, demonstrates OSIA's potential for biosignature detection and offers insights and lessons for future mission concepts exploring the outer solar system.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1878\" height=\"940\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1000w, https://labelbox-research.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1600w, https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1878w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eLife detection is a uniquely challenging scientific objective for two reasons. First, there remains considerable disagreement on the fundamental definition of life on Earth. Second, for any single proposed biosignature, there are  processes that can generate similar, misleading signals. This is exacerbated on other planetary bodies where dominant physical processes may substantially differ from those studied on Earth. To address this, life detection missions should include the capability to detect conceptually orthogonal biosignatures that together reduce the likelihood of misinterpretation of biotic and abiotic phenomena. \u003c/p\u003e\u003cp\u003eWhile such data volumes are routinely accommodated in a laboratory setting, the need for space missions to communicate all findings across vast interplanetary distances and through over-subscribed resources like the Deep Space Network makes communication bandwidth a primary bottleneck for planetary exploration. Put simply, the compelling detection of extraterrestrial life may require over 10,000 times more raw data than is transmissible by a space mission.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eTo evaluate the performance of the particle tracker and motility classifier, salient particles were manually tracked throughout each observation and annotated as motile or nonmotile. Labels were generated by external labelers from Labelbox. To ensure annotation consistency and quality, the researchers provided the labelers with a labeling guide document and video with a specific annotation protocol. All labels were then reviewed for quality by our research team. In total, 778 and 199 tracks were labeled in DHM and FLFM data, respectively. All labeled data including raw observations, labeled tracks, and the labeling guide are published in the \u003ca href=\"https://dataverse.jpl.nasa.gov/dataset.xhtml?persistentId=doi%3A10.48577%2Fjpl.2KTVW5\u0026ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003eJPL Open Repository\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eYou can read the full research paper \u003ca href=\"https://iopscience.iop.org/article/10.3847/PSJ/ad0227?ref=labelbox-research.ghost.io#psjad0227s5\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"65b2e290c909bf0001caf529","feature_image":"https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.51.40-PM.png","featured":false,"visibility":"public","created_at":"2024-01-25T22:37:04.000+00:00","updated_at":"2024-01-25T22:54:18.000+00:00","published_at":"2024-01-25T22:54:18.000+00:00","custom_excerpt":"Learn how researchers from NASA/JPL are  advancing biosignature detection which offers insights and lessons for future mission concepts exploring life in the outer solar system.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor/","excerpt":"Learn how researchers from NASA/JPL are  advancing biosignature detection which offers insights and lessons for future mission concepts exploring life in the outer solar system.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b79b8bfcccab003d9cc62e","uuid":"f5659846-7e71-467e-ac06-142456f90ac9","title":"Detecting feature requests of 3rd-party developers via machine learning: A case study of the SAP Community","slug":"detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eThe elicitation of requirements is central for the development of successful software products. While traditional requirement elicitation techniques such as user interviews are highly labor-intensive, data-driven elicitation techniques promise enhanced scalability through the exploitation of new data sources like app store reviews or social media posts. For enterprise software vendors, requirements elicitation remains challenging because app store reviews are scarce and vendors have no direct access to users.  The researchers investigated whether enterprise software vendors can elicit requirements from their sponsored developer communities through data-driven techniques. \u003c/p\u003e\u003cp\u003eThe researchers wanted to analyze whether it is possible to automatically detect feature requests in the questions of community members through a binary machine learning classifier. The motivation for such a classifier is that sponsored developer communities typically contain millions of posts, but only a few are relevant for the elicitation of requirements.\u003c/p\u003e\u003cp\u003eThe potential of sponsored developer communities for data-driven requirements elicitation is a promising source of information. While developers outside of the enterprise software domain often rely on autonomous developer communities such as Stack Overflow or Stack Exchange, enterprise software vendors typically nurture their own, self-hosted developer communities such as the SAP Community, Salesforce’s Trailblazer Community or ServiceNow’s Now Community\u003c/p\u003e\u003cp\u003eTo answer their research question, they collected data from the SAP Community and generated a manually labeled data set of 1,500 questions. Following the design science paradigm, they developed a supervised and binary machine learning classifier. They observed the highest prediction accuracy (0.8187) for the classifier when they extracted features with the pre-trained SBERT-Model and classified them with the Naïve Bayes algorithm. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eFollowing the design science methodology, the researchers collected data from the SAP Community and developed a supervised machine learning classifier, which automatically detected feature requests of third-party developers. Based on a manually labeled data set of 1,500 questions, their classifier reached a high accuracy of 0.819 and revealed that supervised machine learning models are an effective means for the identification of feature requests.\u003c/p\u003e\u003cp\u003eTheir study opens up two major avenues for future research. First, while they used sponsored developer communities to mine enterprise software requirements, future research can explore the elicitation of bugs to improve the maintenance of software products. Second, future research can also explore different types of feature requests in sponsored developer communities.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers used Quantumworks Lab to collect the assessments from their labelers. They used a managed-labeler approach to label our final sample of 1,500 questions. A key reason for this approach was that the assessment of whether a question contains a feature request requires solid knowledge about SAP’s products and their functionality. Labelers needed to be SAP experts to make an accurate assessment. Additionally, they wanted to have three labelers so that they can rely on the majority label when the assessments are discordant.  Labeler one is part of the researchers and has worked for their case company SAP for multiple years and in varying roles. Labeler two and three were recruited via the freelancer platform Fiverr. Both have a university degree and several years of experience with SAP’s technology.\u003c/p\u003e\u003cp\u003eMoreover, the labeling task was a binary evaluation, meaning that our labelers were presented with a question and they had to assess whether the question contains a feature request or not. Existing answers and comments were not shown to the labelers as our goals was to train the classifier on the questions only. The inclusion of answers and comments\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1378\" height=\"596\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 1378w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.researchgate.net/profile/Martin-Kauschinger/publication/364165571_Detecting_Feature_Requests_of_Third-Party_Developers_through_Machine_Learning_A_Case_Study_of_the_SAP_Community/links/633d3f1676e39959d69f8513/Detecting-Feature-Requests-of-Third-Party-Developers-through-Machine-Learning-A-Case-Study-of-the-SAP-Community.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63b79b8bfcccab003d9cc62e","feature_image":"https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.03.34-PM.png","featured":false,"visibility":"public","created_at":"2023-01-06T03:54:51.000+00:00","updated_at":"2023-01-06T04:04:39.000+00:00","published_at":"2023-01-06T04:04:15.000+00:00","custom_excerpt":"Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community/","excerpt":"Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63966b25136f7d003d87b29d","uuid":"eb20cc4a-5d82-447e-a492-e54912b2ca4a","title":"Real-time segmentation of desiccation cracks onboard UAVs for planetary exploration","slug":"real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures). \u003c/p\u003e\u003cp\u003eThe search for biosignatures on other planets focuses on identifying evidence of habitable environments, characterized by the presence or former-presence of water, and other factors that may have allowed organisms to grow and be preserved in the rock record. Mud cracks, or desiccations cracks, are a type of sedimentary structure associated with muddy environments where water has been present, such as dry lake beds.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Planetary surfaces are a primary focus of space exploration. Some of the most challenging current efforts in planetary exploration relate to the search for life, or biosignatures, in these environments. Detecting water-related textures, and thus evidence for potentially habitable environments, has the potential to focus and accelerate the search for biosignatures on other planets. Desiccation cracks are sedimentary features providing evidence of sediment-water interaction. \u003c/p\u003e\u003cp\u003eThey are known from both Earth and Mars, and are likely to be found via aerial exploration approaches of ancient lakes, rivers, or shallow marine environments where biosignatures may be found. Current approaches using image processing to detect desiccation cracks focus on segmenting just the cracks and prove somewhat successful. \u003c/p\u003e\u003cp\u003eHowever, the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection over much larger surface areas has not yet been explored.  The researchers advanced novel ways to develop and deploy of a desiccation crack detection system using UAVs and AI, leveraging new data collection techniques at varying heights above ground level and data-augmentation with a range of pixel-level and spatial transforms. Three state-of-the-art CNN segmentation networks are trained and evaluated using PyTorch. The networks are deployed on an edge-AI device integrated with a companion computer onboard a sub-2kg quadrotor UAV.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e Results indicate that the models can segment desiccation cracks on airborne collected images at various locations at heights ranging from 5 to 20m. Deployment of the models for real-time inference onboard small UAVs shows potential for application in the field. This research shows the feasibility of a low-volume data training to UAV deployment pipeline while highlighting potential hurdles in the processing pipeline for future efforts. They presented a system and architecture for onboard UAV detectors of sedimentary features, which can speed up the search for life on other planets.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1482\" height=\"678\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 1482w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe images were all labeled for segmentation using the Labelbox. The image database was uploaded to Quantumworks Lab using the datasets creation feature. Labeling was conducted following a standardized labelling procedure developed during an initial exploratory labelling step to manage artifacts, ensure consistency and accelerate the process. L\u003c/p\u003e\u003cp\u003eLabeling a dataset can take a considerable amount of time. The process is prone to suffer from bias from human labelers. Labeling desiccation cracks requires the definition of clear boundaries. A single label was used (desiccation) for time optimization and simplification and a standardized labelling procedure was developed. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"676\" height=\"398\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png 600w, https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png 676w\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of a labeled desiccation crack texture using Quantumworks Lab overlaid in light blue\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.researchgate.net/publication/358402708_Real-time_Segmentation_of_Desiccation_Cracks_onboard_UAVs_for_Planetary_Exploration?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63966b25136f7d003d87b29d","feature_image":"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.51.52-PM.png","featured":false,"visibility":"public","created_at":"2022-12-11T23:43:33.000+00:00","updated_at":"2024-01-25T22:36:35.000+00:00","published_at":"2022-12-11T23:52:12.000+00:00","custom_excerpt":"Researchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures).","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/","excerpt":"Researchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures).","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6384ec65a3ae7a003dfd4db2","uuid":"723c1b43-17d8-4ffa-b0c2-c54d13f4b7b9","title":"Fruit flower detection in apple orchards using ML","slug":"fruit-flower-detection-in-apple-orchards-using-ml","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the University of Guelph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Apples are typically a tree fruit crops grown worldwide with an estimated annual production of 124 million. To increase economic sustainability and compete globally, apple growers are strategically improving returns by adopting new cultivars, rootstocks and orchard management practices that improve fruit quality. \u003c/p\u003e\u003cp\u003eAdditionally, there is also an urgent need to reduce labour, which accounts for over 60% of production costs. The researchers presented the application of machine vision and learning techniques to detect and identify the number of flower clusters on apple trees leading to the ability to predict the potential yield of apples. The current issue in the agriculture field is that most of the machinery currently being used requires worker supervision. The inclusion of automated machinery can greatly increase the yield produced compared to manual labour and lessen the load required as it is a very labour-intensive job. The inclusion of automation can drastically increase the efficiency of the operation as well as the quality.\u003c/p\u003e\u003cp\u003eThe most challenging problem is dealing with occlusion as there are usually many objects in a dense location in an agricultural setting. Another important challenge that must be considered when designing an object detection system in agriculture is illumination. Due to most operations occurring in an outside environment, variance in natural lighting can greatly affect the performance of the object detection algorithm. Other challenges consist of similar appearance, such as shape and color, as well as multi-fruit detection cases. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1071\" height=\"1593\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image-3.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/11/image-3.png 1000w, https://labelbox-research.ghost.io/content/images/2022/11/image-3.png 1071w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eA mobile robot platform that was used for the data collection.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e A new field robot was designed and built to collect and build a dataset of 1500 images of apples trees. The trained model produced a cluster precision of 0.88 or 88% and a percentage error of 14% over 106 trees running the mobile vehicle on both sides of the trees. The detection model was predicting less than the actual amount but the fruit flower count is still significant in that it can give researchers information on the estimated growth and production of each tree with respect to the actions applied to each fruit tree. Their research helps lay the foundation for future application of machine vision and learning techniques within apple orchards or other fruit tree settings.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers collected and properly filtered the images and transferred them from a storage device to OneDrive for easy access on multiple platforms. The entire labelling process used Quantumworks Lab, which allowed the process to operate collaboratively working on labeling the dataset as there is no need in transferring and downloading large number of photos. The platform was also chosen because it keeps track of how many photos still need labelling and the estimated time it took per photo. The platform was also very easy to use as there were no other external setup needed other than an account to the service. \u003c/p\u003e\u003cp\u003eWith the help of hired undergraduate students at the University of Guelph, it took about a month to completely label 1499 photos. The photos consisted of a mixture of close-up and zoomed-out photos which involved classes, labelled as “Fruit Flower Single”, “Fruit Flower Cluster” and “Tree”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"550\" height=\"320\"\u003e\u003cfigcaption\u003eAn example of the training dataset.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.mdpi.com/2076-3417/12/22/11420?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6384ec65a3ae7a003dfd4db2","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-28-at-9.25.30-AM.png","featured":false,"visibility":"public","created_at":"2022-11-28T17:14:13.000+00:00","updated_at":"2022-12-06T21:29:44.000+00:00","published_at":"2022-11-28T17:26:19.000+00:00","custom_excerpt":"Researchers from the University of Guleph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/fruit-flower-detection-in-apple-orchards-using-ml/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/fruit-flower-detection-in-apple-orchards-using-ml/","excerpt":"Researchers from the University of Guleph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63731b8cf1a5bd003d56b1ee","uuid":"4b21fd7f-058b-408b-b990-b962c56c009c","title":"Automated recognition of  cricket batting techniques in videos using deep learning","slug":"automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the University of Johannesberg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge\u003c/strong\u003e: There have been limited studies demonstrating the validation of batting techniques in cricket using machine learning. Cricket batting technique is intricate because it involves a series of complex gestures needed to perform a stroke, one of these gestures performed by the batsman is referred to as the batting backlift technique (BBT).\u003c/p\u003e\u003cp\u003ePrevious research has indicated that the BBT can be seen as a contributing factor to successful batsmanship. There are two backlifts investigated in this study, namely the lateral batting backlift technique (LBBT), and the straight batting backlift technique (SBBT). The LBBT is a technique present where the toe and face of the bat are lifted laterally in the direction of second slip. The SBBT is represented whenever the toe and face of the bat are pointed toward the stumps and ground.\u003c/p\u003e\u003cp\u003eThe study demonstrated how the batting backlift technique in cricket can be automatically recognized in video footage and compares the performance of popular deep learning architectures, namely, AlexNet, Inception V3, Inception Resnet V2, and Xception. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eBuilding a unique dataset showing the lateral and straight backlift classes and assessed according to standard machine learning metrics, the researchers found that the architectures was comparable with similar performance with one false positive in the lateral class and a precision score of 100%, along with a recall score of 95%, and an f1-score of 98% for each architecture. \u003c/p\u003e\u003cp\u003eThe AlexNet architecture performed the worst out of the four architectures as it incorrectly classified four images that were supposed to be in the straight class. The architecture that is best suited for the problem domain was the Xception architecture with a loss of 0.03 and 98.2.5% accuracy, thus demonstrating its capability in differentiating between lateral and straight backlifts. \u003c/p\u003e\u003cp\u003eThe study provides a way forward in the automatic recognition of player patterns and motion capture, making it less challenging for sports scientists, biomechanists and video analysts working in the field.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eFor the construction of the dataset, the process went through a comprehensive YouTube search of First-Class International Cricket Test Match highlights, where the match’s environment has fewer variations to consider.\u003c/p\u003e\u003cp\u003eUsing the Quantumworks Lab platform, each object within a cricket scene is labeled, allowing for easier isolation and extraction of the batsman in each frame. The frame used for constructing the dataset was when the bowler was about to release the ball towards the batsman. \u003c/p\u003e\u003cp\u003eThe frame was identified as the ideal time period for the position of the batsman at the instant of delivery. Using an 80:20 data split, the training class had 160 images, and the testing class had 40 images, resulting in a total of 200 images, which served as a baseline to draw comparisons of the proposed architectures. The image aspect ratio chosen through testing and validation is 128×128, which is chosen to avoid distorting the original image.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.nature.com/articles/s41598-022-05966-6?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"63731b8cf1a5bd003d56b1ee","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-9.03.28-PM.png","featured":false,"visibility":"public","created_at":"2022-11-15T04:54:36.000+00:00","updated_at":"2022-12-06T21:29:33.000+00:00","published_at":"2022-11-15T05:04:13.000+00:00","custom_excerpt":"Researchers from the University of Johannesburg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/","excerpt":"Researchers from the University of Johannesburg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"637314caf1a5bd003d56b18b","uuid":"22517009-3c11-42c1-bb77-8a829d81aa80","title":"Better gait and posture classification using sensors in individuals with mobility impairment after a stroke","slug":"gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from the University of Zurich recently developed and validated a solution for better monitoring the gait and posture of individuals who have suffered a stroke via sensors. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge\u003c/strong\u003e: Stroke leads to motor impairment which reduces physical activity, negatively affecting social participation, and increasing the risk of secondary cardiovascular events. Continuous monitoring of physical activity with motion sensors is promising because it allows the prescription of tailored treatments in a timely manner. Accurate classification of gait activities and body posture is necessary to extract actionable information for outcome measures from unstructured motion data. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1320\" height=\"480\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png 1320w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eFive movement sensors were developed for research purposes and were attached with elastic straps, one on the dorsal side of the wrists, at the lateral malleolus of the ankles, and on the chest below the sternum.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eTheir method achieved enhanced performance when predicting real-life gait versus non-gait (Gait classification) with an accuracy between 85% and 93% across sensor configurations, using SVM and LR modeling. \u003c/p\u003e\u003cp\u003eOn the much more challenging task of discriminating between the body postures lying, sitting, and standing as well as walking, and stair ascent/descent (Gait and postures classification), their method achieved accuracies between 80% and 86% with at least one ankle and wrist sensor attached unilaterally. \u003c/p\u003e\u003cp\u003eThis research will hopefully prove useful resource to other researchers and clinicians in the increasingly important field of digital health in the form of remote movement monitoring using motion sensors.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eVideo and movement sensor data (locations: wrists, ankles, and chest) were collected from fourteen stroke survivors with motor impairment who performed real-life activities in their home environment. All video data was then labeled using the Quantumworks Lab platform for five classes of gait and body postures and three classes of transitions that served as ground truth. Afterwards, they trained support vector machine (SVM), logistic regression (LR), and k-nearest neighbor (kNN) models to identify gait bouts only or gait and posture. Model performance was assessed by and compared across five different sensor placement configurations.\u003c/p\u003e\u003cp\u003eVideo data was recorded with a frame-rate of 30 fps, whereas time-series from the IMU was collected with 50 fps. A single experimenter labeled videos in a frame-by-frame manner, and the labels were subsequently resampled to match the frequency of the synchronized IMUs. For quality assessment, a random sample of 33.3% of data was labeled by a second experimenter.\u003c/p\u003e\u003cp\u003eLabeling criteria were defined for start-to-end conditions of three body postures (lying down, sitting, standing) and two gait types (walking and stair ascent/descent). Additionally, they annotated three transition labels between two corresponding posture or gait types (lying down/sit, sit/stand, stand/walk) without specifying directionality (e.g., sit-to-stand or stand-tosit). This labeling framework resulted in a discrete label for every frame of the recording. \u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/576132/fphys-13-933987.pdf?sequence=3\u0026isAllowed=y\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"637314caf1a5bd003d56b18b","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-8.46.45-PM.png","featured":false,"visibility":"public","created_at":"2022-11-15T04:25:46.000+00:00","updated_at":"2022-12-06T21:29:54.000+00:00","published_at":"2022-11-15T04:39:59.000+00:00","custom_excerpt":"Researchers from the University of Zurich recently developed and validated a solution for better monitoring the gait and posture of individuals who have suffered a stroke via sensors.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke/","excerpt":"Researchers from the University of Zurich recently developed and validated a solution for better monitoring the gait and posture of individuals who have suffered a stroke via sensors.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63701b560d444a003deb9903","uuid":"ad11e726-fd5b-4e26-a5f0-8d2f08fe6d8a","title":"Generating integrated bill of materials using mask R-CNN models for construction","slug":"generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Pusan National University recently studied ways to utilize AI to identify concrete formworks. Concrete formworks are temporary structures that are used as a mold for concrete placement during construction. AI can be a practical solution that can overcome the difficulties associated with traditional bill of materials preparation and help automatically generate concrete formwork bill of materials (BoMs).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Concrete formwork is a crucial temporary structure for concrete placement not only in structural design but also in budget planning. Artificial intelligence (AI) possesses a great potential in planning and managing this temporary structure and can automate quantity take-off and cost estimation.\u003c/p\u003e\u003cp\u003eUsually, concrete formworks take a large portion of the concrete budget and needs correct quantity take-off and cost estimation. Formwork cost takes up to 15% of the total budget for the construction project and up to 33% of the total budget for the concrete structure. Miscalculation of quantity take-off and cost estimation could result in time delays and cost overruns. Therefore, the concrete formwork should be planned and managed well because it has a significant impact on the construction process.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"622\" height=\"632\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image-1.png 600w, https://labelbox-research.ghost.io/content/images/2022/11/image-1.png 622w\"\u003e\u003cfigcaption\u003eMask R-CNN performing image segmentation\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eThis study showed that integrating AI with a cost database for automatic concrete formwork BoM generation showed high potential. The model they built extracted formwork component objects including numerical dimensions from 2D formwork drawing images automatically. The Mask R-CNN technique was used for object recognition and extraction, while the OCR technique was used to quantify these objects' information to be used for automated BoM generation. A cost database was created based on the market price of formwork components. Then, the model generated a formwork integrated with the cost database, showing high precision in both object recognition and extraction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe labeling pipeline was performed using the Quantumworks Lab platform. Polygonal object masks were created around pipe supports, and rectangular object masks were created around sheathing, joists, struts, and dimensions. The labeling process provided boundaries of the object of interest in a 2D formwork drawing image. The labeled data was then received in JSON file format, with object masks and XML files for each of the labelled images. For each labeled image, separate image masks were created for the objects in the drawing images.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"800\" height=\"331\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image-2.png 600w, https://labelbox-research.ghost.io/content/images/2022/11/image-2.png 800w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eCreating object labels for formwork components inside of Quantumworks Lab\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eManual object segmentation was then done on the side view of a formwork, while object masks were used as binary images of 0 and 1, where 0 indicates background pixels and 1 object pixels. The objects masks include those for sheathing, joist, pipe support, strut and dimension. In total, 3353 labels are created for 186 training images, so total 3353 object masks were generated.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://reader.elsevier.com/reader/sd/pii/S0926580522005143?token=BE99CD73C129B70CE3DAF2720111CBC727801999A617C0D6C164B9DD894F20D4F3C737D23539474F2E84D71FB0E59B35\u0026originRegion=us-east-1\u0026originCreation=20221112222621\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63701b560d444a003deb9903","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-12-at-2.30.08-PM.png","featured":false,"visibility":"public","created_at":"2022-11-12T22:16:54.000+00:00","updated_at":"2022-12-06T21:30:08.000+00:00","published_at":"2022-11-12T22:31:50.000+00:00","custom_excerpt":"Researchers from Pusan National University studied how to utilize AI to identify and generate concrete formworks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model/","excerpt":"Researchers from Pusan National University studied how to utilize AI to identify and generate concrete formworks.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":21,"allPosts":[{"id":"67be4062651e6b0001cd2bdf","uuid":"ef0d7946-0433-4386-bc84-c788c313d47e","title":"Quantumworks Lab services helps research team explore AI-guided defect detection techniques","slug":"ai-guided-defect-detection","html":"\u003cp\u003eAt Quantumworks Lab, we're passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we're excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.\u003c/p\u003e\u003cp\u003eThese posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we're exploring \u003ca href=\"https://arxiv.org/pdf/2404.07306?ref=labelbox-research.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAI-Guided Defect Detection Techniques to Model Single Crystal Diamond Growth\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003ea study by Rohan Reddy Mekala, Elias Garratt, Matthias Muehle, Arjun Srinivasan, Adam Porter, and Mikael Lindvall.\u003c/p\u003e\u003ch2 id=\"research-introduction\"\u003e\u003cstrong\u003eResearch introduction\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eResearchers recently identified several challenges in producing high-quality single crystal diamonds (SCDs) at scale. Despite extensive development efforts, current manufacturing relies on trial-and-error experimentation, leading to inconsistent results and defects. In order to address these issues, researchers across the United States collaborated to propose new methodologies using machine learning and AI models to predict future diamond growth states for accelerated material development, improved quality, and larger sizes.\u003c/p\u003e\u003cp\u003eThe research team focused on these challenges:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLack of predictability:\u003c/strong\u003e Existing techniques currently lack the ability to predict diamond growth states, hindering process control and the potential for correction.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIssues with scale:\u003c/strong\u003e Diamond is an essential material for tools in power electronics, health sciences, and engineering, but each field has different requirements in terms of quality, purity, and size. The inability to predict growth subsequently makes it difficult to scale production.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIneffective research methodology:\u003c/strong\u003e Current research methods on developing a method to sustainably and reliably produce high quality diamonds rely on a trial and error method, leading to inconsistent results and defects from each methodology.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom these challenges, researchers identified an opportunity in using machine learning and deep learning algorithms to predict future diamond growth states to shorten the growth cycle, improve prediction accuracy, and enhance crystalline quality.\u003c/p\u003e\u003ch2 id=\"how-labelbox-was-used\"\u003e\u003cstrong\u003eHow Quantumworks Lab was used\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eLabelbox services was used to build the dataset for object detection of the diamond growth-run images. The process began with an initial batch of 100 images, which were reviewed by a team of three material scientists and 15 Alignerrs that were vetted and onboarded through Quantumworks Lab Labeling Services.. Alignerrs provide expert, on-demand annotation services and are selected from a network of trained professionals across diverse data domains.\u003c/p\u003e\u003cp\u003eThe scientists provided detailed instructions, including explanatory videos and meetings, to guide the labeling process. The labels were then reviewed by the Alignerrs, where predominant occurrences were identified based on a consensus score. Afterward, the material scientists conducted a final review to ensure the accuracy, consistency, and integrity of the data.\u003c/p\u003e\u003ch2 id=\"analysis-and-model-assisted-labeling\"\u003e\u003cstrong\u003eAnalysis and model-assisted labeling\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-research.ghost.io\"\u003e\u003cu\u003eModel-assisted labeling\u003c/u\u003e\u003c/a\u003e (MAL) was implemented in order to improve the consistency of data labeling and address potential variability in interpretation of instructions amongst labelers. This involved training a baseline model incrementally using iterative image-annotation pairs from the initial batch. This baseline model was then overlaid on subsequent batches to assist Alignerrs, significantly reducing labeling time and improving accuracy.\u003c/p\u003e\u003cp\u003eWith MAL, the time to label each image decreased from 15 minutes to just 2 minutes. Alignerrs were able to correct and refine annotation based on the baseline model’s predictions until a segmentation accuracy threshold of 80% was achieved. Once this threshold was achieved, the model’s overlays were used as a starting point for future batches.\u003c/p\u003e\u003cp\u003eAfter the minimum number of images were processed, the final set of image-label pairs was passed on for further research and development of the final semantic segmentation and object detection models. Using MAL and this human-in-the-loop workflow, this streamlined annotation process leads to improved efficiency and label quality.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeGAy55bE9C_7cOm9nsW-H0IraNnL1CMua-RCDhe88CKCtuOqn0BTPC4wr0gSgZ5HKrxTsNRkbrOtuMF7yQ5QFYvXGaexfSPvfZJFjHT5C4IpKxXHYkaI-YiUNq0z4eP8yyd2Pg?key=cmC32LkeleH-01xeZ1gdpicE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"424\"\u003e\u003c/figure\u003e\u003ch2 id=\"key-findings\"\u003e\u003cstrong\u003eKey findings\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eUsing object detection and image segmentation algorithms along with the support of a highly-skilled team of Quantumworks Lab Alignerrs, researchers were able to create a defect detection pipeline for diamond growth data. This pipeline achieved a high defect classification accuracy with 93.35% for Center-Defects, 92.83% for Poly-crystalline-Defects, and 91.98% for Edge-Defects.\u003c/p\u003e\u003cp\u003eThe authors were able to use this pipeline to accurately detect defects, reduce time and costs, and predict future diamond growth.\u003c/p\u003e\u003cp\u003eWorking on your own research? Reach out to the team at research@labelbox.com to request a research license or to share your AI research with us.\u0026nbsp;\u003c/p\u003e","comment_id":"67be4062651e6b0001cd2bdf","feature_image":"https://labelbox-research.ghost.io/content/images/2025/02/AI-Guided-Defect-Detection-Techniques.png","featured":false,"visibility":"public","created_at":"2025-02-25T22:12:50.000+00:00","updated_at":"2025-02-25T22:45:46.000+00:00","published_at":"2025-02-25T22:45:46.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"67be114424e4bd0001a29369","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/lisa/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"67be114424e4bd0001a29369","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/lisa/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/ai-guided-defect-detection/","excerpt":"At Quantumworks Lab, we're passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we're excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.\n\nThese posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we're exploring AI-Guided ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How Quantumworks Lab services helps explore AI-guided defect detection techniques ","meta_description":"Learn how researchers use Quantumworks Lab services and software to accurately detect defects, reduce time and costs, and predict future diamond growth.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"674656bbd6cd0800017443d1","uuid":"a0cd1b3b-3473-473e-a076-9c9f2ede5adb","title":"A benchmark for long-form medical question answering","slug":"a-benchmark-for-long-form-medical-question-answering","html":"\u003cp\u003eResearchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. Existing benchmarks often rely on automatic metrics and multiple-choice questions, which do not fully capture the complexities of real-world clinical applications. To bridge this gap, the authors introduced a publicly available benchmark comprising real-world consumer medical questions, with long-form answers evaluated by medical professionals. This resource aims to facilitate more accurate assessments of LLMs' performance in medical question answering.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch area and challenges \u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLack of comprehensive benchmarks\u003c/strong\u003e: Existing evaluations focus on automatic metrics and multiple-choice formats, which do not adequately reflect the nuances of clinical scenarios.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eClosed-source studies\u003c/strong\u003e: Many studies on long-form medical QA are not publicly accessible, limiting reproducibility and the enhancement of existing baselines.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAbsence of human annotations\u003c/strong\u003e: There's a scarcity of datasets with human medical expert annotations, hindering the development of reliable evaluation metrics.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1114\" height=\"784\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 1114w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eScheme of the difficulty levels of medical questions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eTo build a dataset of real-world medical questions, 4,271 queries were collected from the Lavita Medical AI Assist platform between  spanning 1,693 conversations (single-turn and multi-turn). After deduplication, removal of sample questions, and filtering non-English entries using Lingua, the dataset was refined to 2,698 unique queries. \u003c/p\u003e\u003cp\u003ePairwise annotation tasks were completed using Quantumworks Lab's annotation platform and human evaluations were conducted with a group of 3 medical doctors, with two doctors assigned per batch, specializing in radiology and pathology. \u003c/p\u003e\u003cp\u003eBefore starting the main round of annotations, the researchers shared the annotation scheme with the doctors and conducted a trial round with each on a small sample of questions. They then gathered the doctors’ feedback to ensure that all annotation criteria were clear and that there was no ambiguity regarding the instructions. After confirming clarity and receiving approval from the doctors, they proceeded with the main batches of annotations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eLLM-as-a-Judge\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe researchers designed their LLM-as-a-judge prompt by combining templates from Zheng et al. and WildBench. The prompt, shown below, enables pairwise comparison of responses across multiple criteria, and used GPT-4o-2024-08-06 and Claude-3-5-Sonnet-20241022 as judges.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1142\" height=\"638\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eKey findings\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpen LLMs' potential\u003c/strong\u003e: Preliminary results indicated that open-source LLMs exhibit strong performance in medical QA, comparable to leading closed models.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAlignment with human judgments\u003c/strong\u003e: The research included a comprehensive analysis of LLMs acting as judges, revealing significant alignment between human evaluations and LLM assessments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePublicly available benchmark\u003c/strong\u003e: The authors provided a new benchmark featuring real-world medical questions and expert-annotated long-form answers, promoting transparency and reproducibility in future research.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1146\" height=\"386\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 1146w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThis paper has been submitted to \u003cem\u003eAIM-FM: Advancements in Medical Foundation Models Workshop\u003c/em\u003e at NeurIPS 2024. You can read the full paper \u003ca href=\"https://arxiv.org/pdf/2411.09834?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"674656bbd6cd0800017443d1","feature_image":"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.53.21-PM.png","featured":false,"visibility":"public","created_at":"2024-11-26T23:16:11.000+00:00","updated_at":"2025-07-03T18:07:05.000+00:00","published_at":"2024-11-26T23:43:04.000+00:00","custom_excerpt":"Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/a-benchmark-for-long-form-medical-question-answering/","excerpt":"Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65bbe41d21ca940001df755f","uuid":"eb9e0733-ed5d-4b7c-9061-6ba30c57b26e","title":"Identifying and counting avian blood cells in whole slide images via deep learning","slug":"identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eAvian blood analysis is a fundamental method for investigating a wide range of topics concerning individual birds and populations of birds. Determining precise blood cell counts helps researchers gain insights into the health condition of birds. For example, the ratio of heterophils to lymphocytes (H/L ratio) is a well-established index for comparing relative stress load. However, such measurements are currently often obtained manually by human experts. The researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models. The first model determined image regions that are suitable for counting blood cells, and the second model is an instance segmentation model that detected the cells in the determined image regions. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eAutomated visual and acoustic monitoring methods for birds can provide information about the presence and the number of bird species or individuals in certain areas, but analyzing the physiological conditions of individual birds allows researchers to understand potential causes of negative population trends. \u003c/p\u003e\u003cp\u003eFor example, measuring the physiological stress of birds can serve as a valuable early warning indicator for conservation efforts. The physiological conditions and the stress of birds can be determined in several ways, e.g., by assessing the body weight or the fat and muscle scores in migratory birds. Other frequently used methods are investigating the parasite loads, measuring the heart rates, and measuring the levels of circulating stress hormones, such as corticosterone. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The region selection model achieves up to 97.3% in terms of F1 score (i.e., the harmonic mean of precision and recall), and the instance segmentation model achieves up to 90.7% in terms of mean average precision. The approach can  help ornithologists acquire hematological data from avian blood smears more precisely and efficiently.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003e The researchers used the Quantumworks Lab platform annotating images of our datasets with model-assisted labeling. The instance segmentation dataset was annotated in an iterative, model-assisted manner, using the tile selection network to propose regions to be annotated and eventually selected them based on how many rare cells had been detected by an intermediate instance segmentation model. \u003c/p\u003e\u003cp\u003eIn the very first iteration, they used a superpixel algorithm to generate simple instance masks. In each iteration, they uploaded the corresponding instance segmentation masks to Quantumworks Lab to be refined by our human expert. This procedure significantly reduced the time needed to fully annotate an image file with masks and class labels compared to annotating from scratch. Overall, they went through four iterations of labeling. For the annotated cell instances, they established two primary categories: erythrocyte, with only the nucleus annotated, and leukocyte. The latter was further split into five subtypes, namely, lymphocyte, eosinophil, heterophil, basophil, and monocyte. Thrombocytes were not explicitly annotated; they were considered to be part of the background during training. \u003c/p\u003e\u003cp\u003eThe trained neural network model was able to distinguish between non-relevant thrombocytes and other annotated cell types, e.g., erythrocytes. By annotating only the nucleus of each erythrocyte rather than the entire cell including the cytoplasm, they maintained the option to label parasite-infected instances individually in future work. Cells infected with parasites may be annotated by masking the entire cell including the cytoplasm. One erythrocyte can be simultaneously counted as both an erythrocyte and a cell with blood parasite because of the distinct annotation regions.\u003c/p\u003e\u003cp\u003eYou can read the full paper \u003ca href=\"https://www.mdpi.com/2673-6004/5/1/4?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"65bbe41d21ca940001df755f","feature_image":"https://labelbox-research.ghost.io/content/images/2024/02/download.jpeg","featured":false,"visibility":"public","created_at":"2024-02-01T18:34:05.000+00:00","updated_at":"2024-02-01T18:39:06.000+00:00","published_at":"2024-02-01T18:39:06.000+00:00","custom_excerpt":"Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/","excerpt":"Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65b2e290c909bf0001caf529","uuid":"0a31c74d-75d8-4e37-840c-92b6ba27034f","title":"NASA/JPL: Onboard instruments for the detection of microscopy biosignatures","slug":"onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eThe quest for extraterrestrial life represents a critical scientific endeavor with civilization-level implications. Promising targets for exploration include icy moons in the solar system, identified for their potential as habitats due to liquid oceans. However, the lack of a precise definition of life presents a fundamental challenge to formulating detection strategies. In response to the bandwidth limitations in transmitting data from distant ocean worlds like Enceladus or Europa, an emerging discipline called Onboard Science Instrument Autonomy (OSIA) evaluates, summarizes, and prioritizes observational instrument data within flight systems. Two OSIA implementations, identifying life-like motion in digital holographic microscopy videos and cellular structure and composition through fluorescence, were developed as part of the Ocean World Life Surveyor (OWLS) prototype instrument suite at the Jet Propulsion Laboratory. Flight-like requirements and computational constraints, akin to those on the Mars helicopter, \"Ingenuity,\" were employed to lower barriers to infusion. The study, which included evaluation using simulated and laboratory data and a live field test at the Mono Lake planetary analog site, demonstrates OSIA's potential for biosignature detection and offers insights and lessons for future mission concepts exploring the outer solar system.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1878\" height=\"940\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1000w, https://labelbox-research.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1600w, https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1878w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eLife detection is a uniquely challenging scientific objective for two reasons. First, there remains considerable disagreement on the fundamental definition of life on Earth. Second, for any single proposed biosignature, there are  processes that can generate similar, misleading signals. This is exacerbated on other planetary bodies where dominant physical processes may substantially differ from those studied on Earth. To address this, life detection missions should include the capability to detect conceptually orthogonal biosignatures that together reduce the likelihood of misinterpretation of biotic and abiotic phenomena. \u003c/p\u003e\u003cp\u003eWhile such data volumes are routinely accommodated in a laboratory setting, the need for space missions to communicate all findings across vast interplanetary distances and through over-subscribed resources like the Deep Space Network makes communication bandwidth a primary bottleneck for planetary exploration. Put simply, the compelling detection of extraterrestrial life may require over 10,000 times more raw data than is transmissible by a space mission.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eTo evaluate the performance of the particle tracker and motility classifier, salient particles were manually tracked throughout each observation and annotated as motile or nonmotile. Labels were generated by external labelers from Labelbox. To ensure annotation consistency and quality, the researchers provided the labelers with a labeling guide document and video with a specific annotation protocol. All labels were then reviewed for quality by our research team. In total, 778 and 199 tracks were labeled in DHM and FLFM data, respectively. All labeled data including raw observations, labeled tracks, and the labeling guide are published in the \u003ca href=\"https://dataverse.jpl.nasa.gov/dataset.xhtml?persistentId=doi%3A10.48577%2Fjpl.2KTVW5\u0026ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003eJPL Open Repository\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eYou can read the full research paper \u003ca href=\"https://iopscience.iop.org/article/10.3847/PSJ/ad0227?ref=labelbox-research.ghost.io#psjad0227s5\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"65b2e290c909bf0001caf529","feature_image":"https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.51.40-PM.png","featured":false,"visibility":"public","created_at":"2024-01-25T22:37:04.000+00:00","updated_at":"2024-01-25T22:54:18.000+00:00","published_at":"2024-01-25T22:54:18.000+00:00","custom_excerpt":"Learn how researchers from NASA/JPL are  advancing biosignature detection which offers insights and lessons for future mission concepts exploring life in the outer solar system.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor/","excerpt":"Learn how researchers from NASA/JPL are  advancing biosignature detection which offers insights and lessons for future mission concepts exploring life in the outer solar system.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b79b8bfcccab003d9cc62e","uuid":"f5659846-7e71-467e-ac06-142456f90ac9","title":"Detecting feature requests of 3rd-party developers via machine learning: A case study of the SAP Community","slug":"detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eThe elicitation of requirements is central for the development of successful software products. While traditional requirement elicitation techniques such as user interviews are highly labor-intensive, data-driven elicitation techniques promise enhanced scalability through the exploitation of new data sources like app store reviews or social media posts. For enterprise software vendors, requirements elicitation remains challenging because app store reviews are scarce and vendors have no direct access to users.  The researchers investigated whether enterprise software vendors can elicit requirements from their sponsored developer communities through data-driven techniques. \u003c/p\u003e\u003cp\u003eThe researchers wanted to analyze whether it is possible to automatically detect feature requests in the questions of community members through a binary machine learning classifier. The motivation for such a classifier is that sponsored developer communities typically contain millions of posts, but only a few are relevant for the elicitation of requirements.\u003c/p\u003e\u003cp\u003eThe potential of sponsored developer communities for data-driven requirements elicitation is a promising source of information. While developers outside of the enterprise software domain often rely on autonomous developer communities such as Stack Overflow or Stack Exchange, enterprise software vendors typically nurture their own, self-hosted developer communities such as the SAP Community, Salesforce’s Trailblazer Community or ServiceNow’s Now Community\u003c/p\u003e\u003cp\u003eTo answer their research question, they collected data from the SAP Community and generated a manually labeled data set of 1,500 questions. Following the design science paradigm, they developed a supervised and binary machine learning classifier. They observed the highest prediction accuracy (0.8187) for the classifier when they extracted features with the pre-trained SBERT-Model and classified them with the Naïve Bayes algorithm. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eFollowing the design science methodology, the researchers collected data from the SAP Community and developed a supervised machine learning classifier, which automatically detected feature requests of third-party developers. Based on a manually labeled data set of 1,500 questions, their classifier reached a high accuracy of 0.819 and revealed that supervised machine learning models are an effective means for the identification of feature requests.\u003c/p\u003e\u003cp\u003eTheir study opens up two major avenues for future research. First, while they used sponsored developer communities to mine enterprise software requirements, future research can explore the elicitation of bugs to improve the maintenance of software products. Second, future research can also explore different types of feature requests in sponsored developer communities.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers used Quantumworks Lab to collect the assessments from their labelers. They used a managed-labeler approach to label our final sample of 1,500 questions. A key reason for this approach was that the assessment of whether a question contains a feature request requires solid knowledge about SAP’s products and their functionality. Labelers needed to be SAP experts to make an accurate assessment. Additionally, they wanted to have three labelers so that they can rely on the majority label when the assessments are discordant.  Labeler one is part of the researchers and has worked for their case company SAP for multiple years and in varying roles. Labeler two and three were recruited via the freelancer platform Fiverr. Both have a university degree and several years of experience with SAP’s technology.\u003c/p\u003e\u003cp\u003eMoreover, the labeling task was a binary evaluation, meaning that our labelers were presented with a question and they had to assess whether the question contains a feature request or not. Existing answers and comments were not shown to the labelers as our goals was to train the classifier on the questions only. The inclusion of answers and comments\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1378\" height=\"596\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 1378w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.researchgate.net/profile/Martin-Kauschinger/publication/364165571_Detecting_Feature_Requests_of_Third-Party_Developers_through_Machine_Learning_A_Case_Study_of_the_SAP_Community/links/633d3f1676e39959d69f8513/Detecting-Feature-Requests-of-Third-Party-Developers-through-Machine-Learning-A-Case-Study-of-the-SAP-Community.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63b79b8bfcccab003d9cc62e","feature_image":"https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.03.34-PM.png","featured":false,"visibility":"public","created_at":"2023-01-06T03:54:51.000+00:00","updated_at":"2023-01-06T04:04:39.000+00:00","published_at":"2023-01-06T04:04:15.000+00:00","custom_excerpt":"Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community/","excerpt":"Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63966b25136f7d003d87b29d","uuid":"eb20cc4a-5d82-447e-a492-e54912b2ca4a","title":"Real-time segmentation of desiccation cracks onboard UAVs for planetary exploration","slug":"real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures). \u003c/p\u003e\u003cp\u003eThe search for biosignatures on other planets focuses on identifying evidence of habitable environments, characterized by the presence or former-presence of water, and other factors that may have allowed organisms to grow and be preserved in the rock record. Mud cracks, or desiccations cracks, are a type of sedimentary structure associated with muddy environments where water has been present, such as dry lake beds.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Planetary surfaces are a primary focus of space exploration. Some of the most challenging current efforts in planetary exploration relate to the search for life, or biosignatures, in these environments. Detecting water-related textures, and thus evidence for potentially habitable environments, has the potential to focus and accelerate the search for biosignatures on other planets. Desiccation cracks are sedimentary features providing evidence of sediment-water interaction. \u003c/p\u003e\u003cp\u003eThey are known from both Earth and Mars, and are likely to be found via aerial exploration approaches of ancient lakes, rivers, or shallow marine environments where biosignatures may be found. Current approaches using image processing to detect desiccation cracks focus on segmenting just the cracks and prove somewhat successful. \u003c/p\u003e\u003cp\u003eHowever, the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection over much larger surface areas has not yet been explored.  The researchers advanced novel ways to develop and deploy of a desiccation crack detection system using UAVs and AI, leveraging new data collection techniques at varying heights above ground level and data-augmentation with a range of pixel-level and spatial transforms. Three state-of-the-art CNN segmentation networks are trained and evaluated using PyTorch. The networks are deployed on an edge-AI device integrated with a companion computer onboard a sub-2kg quadrotor UAV.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e Results indicate that the models can segment desiccation cracks on airborne collected images at various locations at heights ranging from 5 to 20m. Deployment of the models for real-time inference onboard small UAVs shows potential for application in the field. This research shows the feasibility of a low-volume data training to UAV deployment pipeline while highlighting potential hurdles in the processing pipeline for future efforts. They presented a system and architecture for onboard UAV detectors of sedimentary features, which can speed up the search for life on other planets.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1482\" height=\"678\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 1482w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe images were all labeled for segmentation using the Labelbox. The image database was uploaded to Quantumworks Lab using the datasets creation feature. Labeling was conducted following a standardized labelling procedure developed during an initial exploratory labelling step to manage artifacts, ensure consistency and accelerate the process. L\u003c/p\u003e\u003cp\u003eLabeling a dataset can take a considerable amount of time. The process is prone to suffer from bias from human labelers. Labeling desiccation cracks requires the definition of clear boundaries. A single label was used (desiccation) for time optimization and simplification and a standardized labelling procedure was developed. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"676\" height=\"398\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png 600w, https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png 676w\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of a labeled desiccation crack texture using Quantumworks Lab overlaid in light blue\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.researchgate.net/publication/358402708_Real-time_Segmentation_of_Desiccation_Cracks_onboard_UAVs_for_Planetary_Exploration?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63966b25136f7d003d87b29d","feature_image":"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.51.52-PM.png","featured":false,"visibility":"public","created_at":"2022-12-11T23:43:33.000+00:00","updated_at":"2024-01-25T22:36:35.000+00:00","published_at":"2022-12-11T23:52:12.000+00:00","custom_excerpt":"Researchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures).","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/","excerpt":"Researchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures).","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6384ec65a3ae7a003dfd4db2","uuid":"723c1b43-17d8-4ffa-b0c2-c54d13f4b7b9","title":"Fruit flower detection in apple orchards using ML","slug":"fruit-flower-detection-in-apple-orchards-using-ml","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the University of Guelph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Apples are typically a tree fruit crops grown worldwide with an estimated annual production of 124 million. To increase economic sustainability and compete globally, apple growers are strategically improving returns by adopting new cultivars, rootstocks and orchard management practices that improve fruit quality. \u003c/p\u003e\u003cp\u003eAdditionally, there is also an urgent need to reduce labour, which accounts for over 60% of production costs. The researchers presented the application of machine vision and learning techniques to detect and identify the number of flower clusters on apple trees leading to the ability to predict the potential yield of apples. The current issue in the agriculture field is that most of the machinery currently being used requires worker supervision. The inclusion of automated machinery can greatly increase the yield produced compared to manual labour and lessen the load required as it is a very labour-intensive job. The inclusion of automation can drastically increase the efficiency of the operation as well as the quality.\u003c/p\u003e\u003cp\u003eThe most challenging problem is dealing with occlusion as there are usually many objects in a dense location in an agricultural setting. Another important challenge that must be considered when designing an object detection system in agriculture is illumination. Due to most operations occurring in an outside environment, variance in natural lighting can greatly affect the performance of the object detection algorithm. Other challenges consist of similar appearance, such as shape and color, as well as multi-fruit detection cases. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1071\" height=\"1593\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image-3.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/11/image-3.png 1000w, https://labelbox-research.ghost.io/content/images/2022/11/image-3.png 1071w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eA mobile robot platform that was used for the data collection.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e A new field robot was designed and built to collect and build a dataset of 1500 images of apples trees. The trained model produced a cluster precision of 0.88 or 88% and a percentage error of 14% over 106 trees running the mobile vehicle on both sides of the trees. The detection model was predicting less than the actual amount but the fruit flower count is still significant in that it can give researchers information on the estimated growth and production of each tree with respect to the actions applied to each fruit tree. Their research helps lay the foundation for future application of machine vision and learning techniques within apple orchards or other fruit tree settings.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers collected and properly filtered the images and transferred them from a storage device to OneDrive for easy access on multiple platforms. The entire labelling process used Quantumworks Lab, which allowed the process to operate collaboratively working on labeling the dataset as there is no need in transferring and downloading large number of photos. The platform was also chosen because it keeps track of how many photos still need labelling and the estimated time it took per photo. The platform was also very easy to use as there were no other external setup needed other than an account to the service. \u003c/p\u003e\u003cp\u003eWith the help of hired undergraduate students at the University of Guelph, it took about a month to completely label 1499 photos. The photos consisted of a mixture of close-up and zoomed-out photos which involved classes, labelled as “Fruit Flower Single”, “Fruit Flower Cluster” and “Tree”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"550\" height=\"320\"\u003e\u003cfigcaption\u003eAn example of the training dataset.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.mdpi.com/2076-3417/12/22/11420?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6384ec65a3ae7a003dfd4db2","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-28-at-9.25.30-AM.png","featured":false,"visibility":"public","created_at":"2022-11-28T17:14:13.000+00:00","updated_at":"2022-12-06T21:29:44.000+00:00","published_at":"2022-11-28T17:26:19.000+00:00","custom_excerpt":"Researchers from the University of Guleph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/fruit-flower-detection-in-apple-orchards-using-ml/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/fruit-flower-detection-in-apple-orchards-using-ml/","excerpt":"Researchers from the University of Guleph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63731b8cf1a5bd003d56b1ee","uuid":"4b21fd7f-058b-408b-b990-b962c56c009c","title":"Automated recognition of  cricket batting techniques in videos using deep learning","slug":"automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the University of Johannesberg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge\u003c/strong\u003e: There have been limited studies demonstrating the validation of batting techniques in cricket using machine learning. Cricket batting technique is intricate because it involves a series of complex gestures needed to perform a stroke, one of these gestures performed by the batsman is referred to as the batting backlift technique (BBT).\u003c/p\u003e\u003cp\u003ePrevious research has indicated that the BBT can be seen as a contributing factor to successful batsmanship. There are two backlifts investigated in this study, namely the lateral batting backlift technique (LBBT), and the straight batting backlift technique (SBBT). The LBBT is a technique present where the toe and face of the bat are lifted laterally in the direction of second slip. The SBBT is represented whenever the toe and face of the bat are pointed toward the stumps and ground.\u003c/p\u003e\u003cp\u003eThe study demonstrated how the batting backlift technique in cricket can be automatically recognized in video footage and compares the performance of popular deep learning architectures, namely, AlexNet, Inception V3, Inception Resnet V2, and Xception. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eBuilding a unique dataset showing the lateral and straight backlift classes and assessed according to standard machine learning metrics, the researchers found that the architectures was comparable with similar performance with one false positive in the lateral class and a precision score of 100%, along with a recall score of 95%, and an f1-score of 98% for each architecture. \u003c/p\u003e\u003cp\u003eThe AlexNet architecture performed the worst out of the four architectures as it incorrectly classified four images that were supposed to be in the straight class. The architecture that is best suited for the problem domain was the Xception architecture with a loss of 0.03 and 98.2.5% accuracy, thus demonstrating its capability in differentiating between lateral and straight backlifts. \u003c/p\u003e\u003cp\u003eThe study provides a way forward in the automatic recognition of player patterns and motion capture, making it less challenging for sports scientists, biomechanists and video analysts working in the field.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eFor the construction of the dataset, the process went through a comprehensive YouTube search of First-Class International Cricket Test Match highlights, where the match’s environment has fewer variations to consider.\u003c/p\u003e\u003cp\u003eUsing the Quantumworks Lab platform, each object within a cricket scene is labeled, allowing for easier isolation and extraction of the batsman in each frame. The frame used for constructing the dataset was when the bowler was about to release the ball towards the batsman. \u003c/p\u003e\u003cp\u003eThe frame was identified as the ideal time period for the position of the batsman at the instant of delivery. Using an 80:20 data split, the training class had 160 images, and the testing class had 40 images, resulting in a total of 200 images, which served as a baseline to draw comparisons of the proposed architectures. The image aspect ratio chosen through testing and validation is 128×128, which is chosen to avoid distorting the original image.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.nature.com/articles/s41598-022-05966-6?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"63731b8cf1a5bd003d56b1ee","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-9.03.28-PM.png","featured":false,"visibility":"public","created_at":"2022-11-15T04:54:36.000+00:00","updated_at":"2022-12-06T21:29:33.000+00:00","published_at":"2022-11-15T05:04:13.000+00:00","custom_excerpt":"Researchers from the University of Johannesburg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/","excerpt":"Researchers from the University of Johannesburg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"637314caf1a5bd003d56b18b","uuid":"22517009-3c11-42c1-bb77-8a829d81aa80","title":"Better gait and posture classification using sensors in individuals with mobility impairment after a stroke","slug":"gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from the University of Zurich recently developed and validated a solution for better monitoring the gait and posture of individuals who have suffered a stroke via sensors. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge\u003c/strong\u003e: Stroke leads to motor impairment which reduces physical activity, negatively affecting social participation, and increasing the risk of secondary cardiovascular events. Continuous monitoring of physical activity with motion sensors is promising because it allows the prescription of tailored treatments in a timely manner. Accurate classification of gait activities and body posture is necessary to extract actionable information for outcome measures from unstructured motion data. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1320\" height=\"480\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png 1320w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eFive movement sensors were developed for research purposes and were attached with elastic straps, one on the dorsal side of the wrists, at the lateral malleolus of the ankles, and on the chest below the sternum.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eTheir method achieved enhanced performance when predicting real-life gait versus non-gait (Gait classification) with an accuracy between 85% and 93% across sensor configurations, using SVM and LR modeling. \u003c/p\u003e\u003cp\u003eOn the much more challenging task of discriminating between the body postures lying, sitting, and standing as well as walking, and stair ascent/descent (Gait and postures classification), their method achieved accuracies between 80% and 86% with at least one ankle and wrist sensor attached unilaterally. \u003c/p\u003e\u003cp\u003eThis research will hopefully prove useful resource to other researchers and clinicians in the increasingly important field of digital health in the form of remote movement monitoring using motion sensors.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eVideo and movement sensor data (locations: wrists, ankles, and chest) were collected from fourteen stroke survivors with motor impairment who performed real-life activities in their home environment. All video data was then labeled using the Quantumworks Lab platform for five classes of gait and body postures and three classes of transitions that served as ground truth. Afterwards, they trained support vector machine (SVM), logistic regression (LR), and k-nearest neighbor (kNN) models to identify gait bouts only or gait and posture. Model performance was assessed by and compared across five different sensor placement configurations.\u003c/p\u003e\u003cp\u003eVideo data was recorded with a frame-rate of 30 fps, whereas time-series from the IMU was collected with 50 fps. A single experimenter labeled videos in a frame-by-frame manner, and the labels were subsequently resampled to match the frequency of the synchronized IMUs. For quality assessment, a random sample of 33.3% of data was labeled by a second experimenter.\u003c/p\u003e\u003cp\u003eLabeling criteria were defined for start-to-end conditions of three body postures (lying down, sitting, standing) and two gait types (walking and stair ascent/descent). Additionally, they annotated three transition labels between two corresponding posture or gait types (lying down/sit, sit/stand, stand/walk) without specifying directionality (e.g., sit-to-stand or stand-tosit). This labeling framework resulted in a discrete label for every frame of the recording. \u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/576132/fphys-13-933987.pdf?sequence=3\u0026isAllowed=y\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"637314caf1a5bd003d56b18b","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-8.46.45-PM.png","featured":false,"visibility":"public","created_at":"2022-11-15T04:25:46.000+00:00","updated_at":"2022-12-06T21:29:54.000+00:00","published_at":"2022-11-15T04:39:59.000+00:00","custom_excerpt":"Researchers from the University of Zurich recently developed and validated a solution for better monitoring the gait and posture of individuals who have suffered a stroke via sensors.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke/","excerpt":"Researchers from the University of Zurich recently developed and validated a solution for better monitoring the gait and posture of individuals who have suffered a stroke via sensors.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63701b560d444a003deb9903","uuid":"ad11e726-fd5b-4e26-a5f0-8d2f08fe6d8a","title":"Generating integrated bill of materials using mask R-CNN models for construction","slug":"generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Pusan National University recently studied ways to utilize AI to identify concrete formworks. Concrete formworks are temporary structures that are used as a mold for concrete placement during construction. AI can be a practical solution that can overcome the difficulties associated with traditional bill of materials preparation and help automatically generate concrete formwork bill of materials (BoMs).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Concrete formwork is a crucial temporary structure for concrete placement not only in structural design but also in budget planning. Artificial intelligence (AI) possesses a great potential in planning and managing this temporary structure and can automate quantity take-off and cost estimation.\u003c/p\u003e\u003cp\u003eUsually, concrete formworks take a large portion of the concrete budget and needs correct quantity take-off and cost estimation. Formwork cost takes up to 15% of the total budget for the construction project and up to 33% of the total budget for the concrete structure. Miscalculation of quantity take-off and cost estimation could result in time delays and cost overruns. Therefore, the concrete formwork should be planned and managed well because it has a significant impact on the construction process.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"622\" height=\"632\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image-1.png 600w, https://labelbox-research.ghost.io/content/images/2022/11/image-1.png 622w\"\u003e\u003cfigcaption\u003eMask R-CNN performing image segmentation\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eThis study showed that integrating AI with a cost database for automatic concrete formwork BoM generation showed high potential. The model they built extracted formwork component objects including numerical dimensions from 2D formwork drawing images automatically. The Mask R-CNN technique was used for object recognition and extraction, while the OCR technique was used to quantify these objects' information to be used for automated BoM generation. A cost database was created based on the market price of formwork components. Then, the model generated a formwork integrated with the cost database, showing high precision in both object recognition and extraction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe labeling pipeline was performed using the Quantumworks Lab platform. Polygonal object masks were created around pipe supports, and rectangular object masks were created around sheathing, joists, struts, and dimensions. The labeling process provided boundaries of the object of interest in a 2D formwork drawing image. The labeled data was then received in JSON file format, with object masks and XML files for each of the labelled images. For each labeled image, separate image masks were created for the objects in the drawing images.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"800\" height=\"331\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image-2.png 600w, https://labelbox-research.ghost.io/content/images/2022/11/image-2.png 800w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eCreating object labels for formwork components inside of Quantumworks Lab\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eManual object segmentation was then done on the side view of a formwork, while object masks were used as binary images of 0 and 1, where 0 indicates background pixels and 1 object pixels. The objects masks include those for sheathing, joist, pipe support, strut and dimension. In total, 3353 labels are created for 186 training images, so total 3353 object masks were generated.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://reader.elsevier.com/reader/sd/pii/S0926580522005143?token=BE99CD73C129B70CE3DAF2720111CBC727801999A617C0D6C164B9DD894F20D4F3C737D23539474F2E84D71FB0E59B35\u0026originRegion=us-east-1\u0026originCreation=20221112222621\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63701b560d444a003deb9903","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-12-at-2.30.08-PM.png","featured":false,"visibility":"public","created_at":"2022-11-12T22:16:54.000+00:00","updated_at":"2022-12-06T21:30:08.000+00:00","published_at":"2022-11-12T22:31:50.000+00:00","custom_excerpt":"Researchers from Pusan National University studied how to utilize AI to identify and generate concrete formworks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model/","excerpt":"Researchers from Pusan National University studied how to utilize AI to identify and generate concrete formworks.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"tag":{"slug":"academic-papers","id":"635953ac4af05f0031c98e9f","name":"Academic papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"count":{"posts":21},"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"slug":"academic-papers","currentPage":1},"__N_SSG":true},"page":"/research/tag/[id]","query":{"id":"academic-papers"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/research/tag/academic-papers/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 10:46:29 GMT -->
</html>