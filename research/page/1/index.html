<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/research/page/1/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:37:08 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Research | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Research | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/Research/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Research | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/Research/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../static/scripts/munchkin.js"></script><script src="../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g34[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../_next/static/chunks/pages/research/page/%5bid%5d-9624d3ab595969d3.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../index.html"><img width="106" height="24" alt="logo" src="../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Research</h1><p class="text-base max-w-xs text-neutral-500  pr-6"></p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Latest</a><a href="../../tag/academic-papers/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Academic papers</a><a href="../../tag/coursework/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Coursework</a><a href="../../tag/computer-vision/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Computer vision</a><a href="../../tag/natural-language/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Natural language</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../trins-towards-multimodal-language-models-that-can-read/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.12.03-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.12.03-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.12.03-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.12.03-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.12.03-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.12.03-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.12.03-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.12.03-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index6614.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.12.03-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../trins-towards-multimodal-language-models-that-can-read/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">TRINS: Towards Multimodal Language Models that Can Read</p><p class="text-base max-w-2xl undefined line-clamp-3">TRINS is a dataset of 39K+ text-rich images with detailed captions and QA pairs, built using Quantumworks Lab for high-quality annotation—enabling LaRA, a compact vision-language model that outperforms larger systems on reading and reasoning tasks.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../hum-card-a-human-crowded-annotated-real-dataset/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.03.51-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.03.51-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.03.51-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.03.51-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.03.51-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.03.51-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.03.51-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.03.51-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexd128.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2FScreenshot-2025-07-02-at-3.03.51-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../hum-card-a-human-crowded-annotated-real-dataset/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">HUM-CARD: A human crowded annotated real dataset</p><p class="text-base max-w-2xl undefined line-clamp-3">The HUM-CARD study introduces a richly annotated dataset of densely crowded urban scenes to advance crowd analysis and environmental impact research, using Quantumworks Lab to ensure high-quality, scalable human annotations.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../ai-guided-defect-detection/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexc0cb.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../ai-guided-defect-detection/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Quantumworks Lab services helps research team explore AI-guided defect detection techniques</p><p class="text-base max-w-2xl undefined line-clamp-3">At Quantumworks Lab, we&#x27;re passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we&#x27;re excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.

These posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we&#x27;re exploring AI-Guided </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../a-benchmark-for-long-form-medical-question-answering/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index9fe7.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../a-benchmark-for-long-form-medical-question-answering/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">A benchmark for long-form medical question answering</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexcd10.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Identifying and counting avian blood cells in whole slide images via deep learning</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index0a39.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-25-at-2.51.40-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">NASA/JPL: Onboard instruments for the detection of microscopy biosignatures</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how researchers from NASA/JPL are  advancing biosignature detection which offers insights and lessons for future mission concepts exploring life in the outer solar system.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexc553.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FScreen-Shot-2023-01-05-at-8.03.34-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Detecting feature requests of 3rd-party developers via machine learning: A case study of the SAP Community</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexac0f.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FScreen-Shot-2022-12-11-at-3.51.52-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Real-time segmentation of desiccation cracks onboard UAVs for planetary exploration</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures).</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../fruit-flower-detection-in-apple-orchards-using-ml/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexab24.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-28-at-9.25.30-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../fruit-flower-detection-in-apple-orchards-using-ml/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Fruit flower detection in apple orchards using ML</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the University of Guleph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index38cf.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2022%2F11%2FScreen-Shot-2022-11-14-at-9.03.28-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Automated recognition of  cricket batting techniques in videos using deep learning</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from the University of Johannesburg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.</p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8">Page 1 of 3<a class="ml-9 text-neutral-700 mb-1" href="../2/index.html">&gt;</a></div></div></div></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"6865adbad35f6c00012c1ccb","uuid":"c13f592c-e428-4733-a5fd-83ee017618ad","title":"TRINS: Towards Multimodal Language Models that Can Read","slug":"trins-towards-multimodal-language-models-that-can-read","html":"\u003cp\u003eThis paper introduces TRINS, a large-scale dataset specifically designed to improve multimodal models' ability to \"read\" — that is, to interpret, reason about, and generate language grounded in both visual scenes and embedded text (like signage, documents, and labels). The dataset contains 39,153 carefully annotated, text-rich images, each paired with long-form captions (averaging 65 words) and over 102,000 question–answer pairs designed to evaluate reading comprehension within images.\u003c/p\u003e\u003cp\u003eMost existing multimodal datasets (e.g., COCO, TextVQA) provide relatively short captions or sparse annotations. TRINS stands out by densely capturing both visual context and embedded text, making it especially useful for training models that need to perform OCR-like reasoning or multimodal understanding where text is a critical part of the image content.\u003c/p\u003e\u003cp\u003eTo showcase the power of this dataset, the authors also introduce LaRA (Language–vision Reading Assistant), a compact and efficient multimodal model trained on TRINS. Despite being much smaller than models like GPT-4V or Gemini, LaRA achieves state-of-the-art performance on multiple benchmarks involving text-rich image understanding, including both captioning and visual question answering.\u003c/p\u003e\u003chr\u003e\u003ch3 id=\"how-labelbox-was-used\"\u003e\u003cstrong\u003eHow Quantumworks Lab Was Used\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe success of TRINS hinges on the quality and density of its annotations — and Quantumworks Lab played a central role in enabling this at scale.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHuman-in-the-loop annotation at scale\u003c/strong\u003e:\u003cbr\u003eThe authors used Quantumworks Lab to annotate 40,576 images, involving 2,079 hours of human labeling and 159 hours of human review. Annotators were instructed to describe both the visual content and the embedded textual content, with attention to attributes like font style, color, position, and meaning.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eStructured annotation guidelines\u003c/strong\u003e:\u003cbr\u003eLabelbox was used to create and enforce detailed ontologies and annotation policies, helping annotators capture high-quality data that included:\u003cul\u003e\u003cli\u003eWhat is happening in the image.\u003c/li\u003e\u003cli\u003eWhat the text says.\u003c/li\u003e\u003cli\u003eWhy the text is important.\u003c/li\u003e\u003cli\u003eOptional deeper reasoning or interpretation.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eQuality assurance workflows\u003c/strong\u003e:\u003cbr\u003eAfter initial annotation, TRINS employed automated checks — including OCR vs. human text comparisons — and manual reviews via Quantumworks Lab to flag low-quality or incomplete examples. These entries were either rejected or routed for re-annotation, ensuring consistency and accuracy throughout the dataset.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnabling long-form, expressive annotations\u003c/strong\u003e:\u003cbr\u003eLabelbox’s flexible interface made it easier to collect detailed, multi-sentence captions and diverse QA pairs, a sharp contrast to most datasets that rely on much shorter labels.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch3 id=\"further-research\"\u003eFurther research\u003c/h3\u003e\u003cp\u003eTRINS represents a major leap forward in multimodal data quality and scale, especially in domains that require deep understanding of images with embedded text (e.g., street scenes, product images, forms). With Quantumworks Lab, the researchers were able to create a dataset that is not only large, but also rich, expressive, and verified, enabling the training of smaller yet more capable models like LaRA.\u003c/p\u003e\u003cp\u003eThis combination of advanced human annotation and machine-efficient modeling signals a promising direction for building language–vision models that can truly read and reason. You can read the full paper \u003ca href=\"https://arxiv.org/pdf/2406.06730?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6865adbad35f6c00012c1ccb","feature_image":"https://labelbox-research.ghost.io/content/images/2025/07/Screenshot-2025-07-02-at-3.12.03-PM.png","featured":false,"visibility":"public","created_at":"2025-07-02T22:07:54.000+00:00","updated_at":"2025-07-02T22:13:21.000+00:00","published_at":"2025-07-02T22:13:21.000+00:00","custom_excerpt":"TRINS is a dataset of 39K+ text-rich images with detailed captions and QA pairs, built using Quantumworks Lab for high-quality annotation—enabling LaRA, a compact vision-language model that outperforms larger systems on reading and reasoning tasks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":null,"url":"https://labelbox-research.ghost.io/trins-towards-multimodal-language-models-that-can-read/","excerpt":"TRINS is a dataset of 39K+ text-rich images with detailed captions and QA pairs, built using Quantumworks Lab for high-quality annotation—enabling LaRA, a compact vision-language model that outperforms larger systems on reading and reasoning tasks.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6865ab35d35f6c00012c1cbb","uuid":"1e7f92e9-6f2c-4b9e-a9e2-4e9b56120df1","title":"HUM-CARD: A human crowded annotated real dataset","slug":"hum-card-a-human-crowded-annotated-real-dataset","html":"\u003col\u003e\u003cli\u003e\u003cstrong\u003eObjective \u0026amp; Motivation\u003c/strong\u003e\u003cul\u003e\u003cli\u003eThe researchers collected and annotated images portraying dense crowds in urban spaces to analyze crowd-related environmental impacts and aid related computer vision tasks.\u003c/li\u003e\u003cli\u003eA major goal was to create a dataset that supports researchers developing algorithms for crowd analysis and environmental monitoring.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDataset Composition\u003c/strong\u003e\u003cul\u003e\u003cli\u003eIncludes thousands of real-world urban images, capturing a range of densities and scenes (e.g., streets, transportation hubs).\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAnnotation Process \u0026amp; Quality Control\u003c/strong\u003e\u003cul\u003e\u003cli\u003eMultiple annotators labeled humans in the scenes to ensure high accuracy.\u003c/li\u003e\u003cli\u003eThe dataset was refined through consensus mechanisms to improve label consistency.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eApplications \u0026amp; Benchmarking\u003c/strong\u003e\u003cul\u003e\u003cli\u003eBeyond a standalone dataset, it is used to benchmark crowd-counting and density estimation models.\u003c/li\u003e\u003cli\u003eDemonstrates improvements over existing datasets due to greater complexity and realism.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnvironmental Insights\u003c/strong\u003e\u003cul\u003e\u003cli\u003eAnalysis explores relationships between crowd density, urban space usage, and environmental stressors, offering a richer context for environmental-management applications.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ol\u003e\u003chr\u003e\u003ch3 id=\"how-labelbox-was-used\"\u003e\u003cstrong\u003eHow Quantumworks Lab Was Used\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe authors employed \u003cstrong\u003eLabelbox\u003c/strong\u003e, a collaborative data‑labeling platform, to efficiently and accurately annotate their dataset:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThey used Quantumworks Lab to \u003cstrong\u003ebuild the annotation ontology (AO)\u003c/strong\u003e: defining labeling categories (e.g., \"main object,\" sub‑classifications, contextual options) to structure the annotation task)\u003c/li\u003e\u003cli\u003eThe platform enabled sophisticated \u003cstrong\u003eeditor environments\u003c/strong\u003e, including image segmentation workflows for video frames. This streamlined annotator work and ensured consistent labels across the dataset.\u003c/li\u003e\u003cli\u003eKey Quantumworks Lab features used included:\u003cul\u003e\u003cli\u003eCustom ontologies for complex scene interpretation.\u003c/li\u003e\u003cli\u003eSupport for bounding boxes, segmentation masks, and multi-label annotations, tailored to dense crowd imagery.\u003c/li\u003e\u003cli\u003eBuilt-in quality control and consensus-based workflows to validate annotator agreement.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003eThe HUM‑CARD dataset provides a rich resource for understanding urban crowding and its environmental dynamics. By leveraging Quantumworks Lab’s ontology creation tools, annotation workflows, and quality‑control mechanisms, the authors delivered a high‑quality, densely annotated dataset that supports both environmental studies and crowd‑analysis computer vision research.\u003c/p\u003e","comment_id":"6865ab35d35f6c00012c1cbb","feature_image":"https://labelbox-research.ghost.io/content/images/2025/07/Screenshot-2025-07-02-at-3.03.51-PM.png","featured":false,"visibility":"public","created_at":"2025-07-02T21:57:09.000+00:00","updated_at":"2025-07-02T22:04:26.000+00:00","published_at":"2025-07-02T22:04:26.000+00:00","custom_excerpt":"The HUM-CARD study introduces a richly annotated dataset of densely crowded urban scenes to advance crowd analysis and environmental impact research, using Quantumworks Lab to ensure high-quality, scalable human annotations.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":null,"url":"https://labelbox-research.ghost.io/hum-card-a-human-crowded-annotated-real-dataset/","excerpt":"The HUM-CARD study introduces a richly annotated dataset of densely crowded urban scenes to advance crowd analysis and environmental impact research, using Quantumworks Lab to ensure high-quality, scalable human annotations.","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"67be4062651e6b0001cd2bdf","uuid":"ef0d7946-0433-4386-bc84-c788c313d47e","title":"Quantumworks Lab services helps research team explore AI-guided defect detection techniques","slug":"ai-guided-defect-detection","html":"\u003cp\u003eAt Quantumworks Lab, we're passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we're excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.\u003c/p\u003e\u003cp\u003eThese posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we're exploring \u003ca href=\"https://arxiv.org/pdf/2404.07306?ref=labelbox-research.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAI-Guided Defect Detection Techniques to Model Single Crystal Diamond Growth\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003ea study by Rohan Reddy Mekala, Elias Garratt, Matthias Muehle, Arjun Srinivasan, Adam Porter, and Mikael Lindvall.\u003c/p\u003e\u003ch2 id=\"research-introduction\"\u003e\u003cstrong\u003eResearch introduction\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eResearchers recently identified several challenges in producing high-quality single crystal diamonds (SCDs) at scale. Despite extensive development efforts, current manufacturing relies on trial-and-error experimentation, leading to inconsistent results and defects. In order to address these issues, researchers across the United States collaborated to propose new methodologies using machine learning and AI models to predict future diamond growth states for accelerated material development, improved quality, and larger sizes.\u003c/p\u003e\u003cp\u003eThe research team focused on these challenges:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLack of predictability:\u003c/strong\u003e Existing techniques currently lack the ability to predict diamond growth states, hindering process control and the potential for correction.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIssues with scale:\u003c/strong\u003e Diamond is an essential material for tools in power electronics, health sciences, and engineering, but each field has different requirements in terms of quality, purity, and size. The inability to predict growth subsequently makes it difficult to scale production.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIneffective research methodology:\u003c/strong\u003e Current research methods on developing a method to sustainably and reliably produce high quality diamonds rely on a trial and error method, leading to inconsistent results and defects from each methodology.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom these challenges, researchers identified an opportunity in using machine learning and deep learning algorithms to predict future diamond growth states to shorten the growth cycle, improve prediction accuracy, and enhance crystalline quality.\u003c/p\u003e\u003ch2 id=\"how-labelbox-was-used\"\u003e\u003cstrong\u003eHow Quantumworks Lab was used\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eLabelbox services was used to build the dataset for object detection of the diamond growth-run images. The process began with an initial batch of 100 images, which were reviewed by a team of three material scientists and 15 Alignerrs that were vetted and onboarded through Quantumworks Lab Labeling Services.. Alignerrs provide expert, on-demand annotation services and are selected from a network of trained professionals across diverse data domains.\u003c/p\u003e\u003cp\u003eThe scientists provided detailed instructions, including explanatory videos and meetings, to guide the labeling process. The labels were then reviewed by the Alignerrs, where predominant occurrences were identified based on a consensus score. Afterward, the material scientists conducted a final review to ensure the accuracy, consistency, and integrity of the data.\u003c/p\u003e\u003ch2 id=\"analysis-and-model-assisted-labeling\"\u003e\u003cstrong\u003eAnalysis and model-assisted labeling\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-research.ghost.io\"\u003e\u003cu\u003eModel-assisted labeling\u003c/u\u003e\u003c/a\u003e (MAL) was implemented in order to improve the consistency of data labeling and address potential variability in interpretation of instructions amongst labelers. This involved training a baseline model incrementally using iterative image-annotation pairs from the initial batch. This baseline model was then overlaid on subsequent batches to assist Alignerrs, significantly reducing labeling time and improving accuracy.\u003c/p\u003e\u003cp\u003eWith MAL, the time to label each image decreased from 15 minutes to just 2 minutes. Alignerrs were able to correct and refine annotation based on the baseline model’s predictions until a segmentation accuracy threshold of 80% was achieved. Once this threshold was achieved, the model’s overlays were used as a starting point for future batches.\u003c/p\u003e\u003cp\u003eAfter the minimum number of images were processed, the final set of image-label pairs was passed on for further research and development of the final semantic segmentation and object detection models. Using MAL and this human-in-the-loop workflow, this streamlined annotation process leads to improved efficiency and label quality.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeGAy55bE9C_7cOm9nsW-H0IraNnL1CMua-RCDhe88CKCtuOqn0BTPC4wr0gSgZ5HKrxTsNRkbrOtuMF7yQ5QFYvXGaexfSPvfZJFjHT5C4IpKxXHYkaI-YiUNq0z4eP8yyd2Pg?key=cmC32LkeleH-01xeZ1gdpicE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"424\"\u003e\u003c/figure\u003e\u003ch2 id=\"key-findings\"\u003e\u003cstrong\u003eKey findings\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eUsing object detection and image segmentation algorithms along with the support of a highly-skilled team of Quantumworks Lab Alignerrs, researchers were able to create a defect detection pipeline for diamond growth data. This pipeline achieved a high defect classification accuracy with 93.35% for Center-Defects, 92.83% for Poly-crystalline-Defects, and 91.98% for Edge-Defects.\u003c/p\u003e\u003cp\u003eThe authors were able to use this pipeline to accurately detect defects, reduce time and costs, and predict future diamond growth.\u003c/p\u003e\u003cp\u003eWorking on your own research? Reach out to the team at research@labelbox.com to request a research license or to share your AI research with us.\u0026nbsp;\u003c/p\u003e","comment_id":"67be4062651e6b0001cd2bdf","feature_image":"https://labelbox-research.ghost.io/content/images/2025/02/AI-Guided-Defect-Detection-Techniques.png","featured":false,"visibility":"public","created_at":"2025-02-25T22:12:50.000+00:00","updated_at":"2025-02-25T22:45:46.000+00:00","published_at":"2025-02-25T22:45:46.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"67be114424e4bd0001a29369","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/lisa/"}],"primary_author":{"id":"67be114424e4bd0001a29369","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/lisa/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/ai-guided-defect-detection/","excerpt":"At Quantumworks Lab, we're passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we're excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.\n\nThese posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we're exploring AI-Guided ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How Quantumworks Lab services helps explore AI-guided defect detection techniques ","meta_description":"Learn how researchers use Quantumworks Lab services and software to accurately detect defects, reduce time and costs, and predict future diamond growth.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"674656bbd6cd0800017443d1","uuid":"a0cd1b3b-3473-473e-a076-9c9f2ede5adb","title":"A benchmark for long-form medical question answering","slug":"a-benchmark-for-long-form-medical-question-answering","html":"\u003cp\u003eResearchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. Existing benchmarks often rely on automatic metrics and multiple-choice questions, which do not fully capture the complexities of real-world clinical applications. To bridge this gap, the authors introduced a publicly available benchmark comprising real-world consumer medical questions, with long-form answers evaluated by medical professionals. This resource aims to facilitate more accurate assessments of LLMs' performance in medical question answering.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch area and challenges \u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLack of comprehensive benchmarks\u003c/strong\u003e: Existing evaluations focus on automatic metrics and multiple-choice formats, which do not adequately reflect the nuances of clinical scenarios.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eClosed-source studies\u003c/strong\u003e: Many studies on long-form medical QA are not publicly accessible, limiting reproducibility and the enhancement of existing baselines.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAbsence of human annotations\u003c/strong\u003e: There's a scarcity of datasets with human medical expert annotations, hindering the development of reliable evaluation metrics.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1114\" height=\"784\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 1114w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eScheme of the difficulty levels of medical questions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eTo build a dataset of real-world medical questions, 4,271 queries were collected from the Lavita Medical AI Assist platform between  spanning 1,693 conversations (single-turn and multi-turn). After deduplication, removal of sample questions, and filtering non-English entries using Lingua, the dataset was refined to 2,698 unique queries. \u003c/p\u003e\u003cp\u003ePairwise annotation tasks were completed using Quantumworks Lab's annotation platform and human evaluations were conducted with a group of 3 medical doctors, with two doctors assigned per batch, specializing in radiology and pathology. \u003c/p\u003e\u003cp\u003eBefore starting the main round of annotations, the researchers shared the annotation scheme with the doctors and conducted a trial round with each on a small sample of questions. They then gathered the doctors’ feedback to ensure that all annotation criteria were clear and that there was no ambiguity regarding the instructions. After confirming clarity and receiving approval from the doctors, they proceeded with the main batches of annotations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eLLM-as-a-Judge\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe researchers designed their LLM-as-a-judge prompt by combining templates from Zheng et al. and WildBench. The prompt, shown below, enables pairwise comparison of responses across multiple criteria, and used GPT-4o-2024-08-06 and Claude-3-5-Sonnet-20241022 as judges.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1142\" height=\"638\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eKey findings\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpen LLMs' potential\u003c/strong\u003e: Preliminary results indicated that open-source LLMs exhibit strong performance in medical QA, comparable to leading closed models.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAlignment with human judgments\u003c/strong\u003e: The research included a comprehensive analysis of LLMs acting as judges, revealing significant alignment between human evaluations and LLM assessments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePublicly available benchmark\u003c/strong\u003e: The authors provided a new benchmark featuring real-world medical questions and expert-annotated long-form answers, promoting transparency and reproducibility in future research.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1146\" height=\"386\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 1146w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThis paper has been submitted to \u003cem\u003eAIM-FM: Advancements in Medical Foundation Models Workshop\u003c/em\u003e at NeurIPS 2024. You can read the full paper \u003ca href=\"https://arxiv.org/pdf/2411.09834?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"674656bbd6cd0800017443d1","feature_image":"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.53.21-PM.png","featured":false,"visibility":"public","created_at":"2024-11-26T23:16:11.000+00:00","updated_at":"2025-07-03T18:07:05.000+00:00","published_at":"2024-11-26T23:43:04.000+00:00","custom_excerpt":"Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/a-benchmark-for-long-form-medical-question-answering/","excerpt":"Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65bbe41d21ca940001df755f","uuid":"eb9e0733-ed5d-4b7c-9061-6ba30c57b26e","title":"Identifying and counting avian blood cells in whole slide images via deep learning","slug":"identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eAvian blood analysis is a fundamental method for investigating a wide range of topics concerning individual birds and populations of birds. Determining precise blood cell counts helps researchers gain insights into the health condition of birds. For example, the ratio of heterophils to lymphocytes (H/L ratio) is a well-established index for comparing relative stress load. However, such measurements are currently often obtained manually by human experts. The researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models. The first model determined image regions that are suitable for counting blood cells, and the second model is an instance segmentation model that detected the cells in the determined image regions. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eAutomated visual and acoustic monitoring methods for birds can provide information about the presence and the number of bird species or individuals in certain areas, but analyzing the physiological conditions of individual birds allows researchers to understand potential causes of negative population trends. \u003c/p\u003e\u003cp\u003eFor example, measuring the physiological stress of birds can serve as a valuable early warning indicator for conservation efforts. The physiological conditions and the stress of birds can be determined in several ways, e.g., by assessing the body weight or the fat and muscle scores in migratory birds. Other frequently used methods are investigating the parasite loads, measuring the heart rates, and measuring the levels of circulating stress hormones, such as corticosterone. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The region selection model achieves up to 97.3% in terms of F1 score (i.e., the harmonic mean of precision and recall), and the instance segmentation model achieves up to 90.7% in terms of mean average precision. The approach can  help ornithologists acquire hematological data from avian blood smears more precisely and efficiently.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003e The researchers used the Quantumworks Lab platform annotating images of our datasets with model-assisted labeling. The instance segmentation dataset was annotated in an iterative, model-assisted manner, using the tile selection network to propose regions to be annotated and eventually selected them based on how many rare cells had been detected by an intermediate instance segmentation model. \u003c/p\u003e\u003cp\u003eIn the very first iteration, they used a superpixel algorithm to generate simple instance masks. In each iteration, they uploaded the corresponding instance segmentation masks to Quantumworks Lab to be refined by our human expert. This procedure significantly reduced the time needed to fully annotate an image file with masks and class labels compared to annotating from scratch. Overall, they went through four iterations of labeling. For the annotated cell instances, they established two primary categories: erythrocyte, with only the nucleus annotated, and leukocyte. The latter was further split into five subtypes, namely, lymphocyte, eosinophil, heterophil, basophil, and monocyte. Thrombocytes were not explicitly annotated; they were considered to be part of the background during training. \u003c/p\u003e\u003cp\u003eThe trained neural network model was able to distinguish between non-relevant thrombocytes and other annotated cell types, e.g., erythrocytes. By annotating only the nucleus of each erythrocyte rather than the entire cell including the cytoplasm, they maintained the option to label parasite-infected instances individually in future work. Cells infected with parasites may be annotated by masking the entire cell including the cytoplasm. One erythrocyte can be simultaneously counted as both an erythrocyte and a cell with blood parasite because of the distinct annotation regions.\u003c/p\u003e\u003cp\u003eYou can read the full paper \u003ca href=\"https://www.mdpi.com/2673-6004/5/1/4?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"65bbe41d21ca940001df755f","feature_image":"https://labelbox-research.ghost.io/content/images/2024/02/download.jpeg","featured":false,"visibility":"public","created_at":"2024-02-01T18:34:05.000+00:00","updated_at":"2024-02-01T18:39:06.000+00:00","published_at":"2024-02-01T18:39:06.000+00:00","custom_excerpt":"Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/","excerpt":"Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65b2e290c909bf0001caf529","uuid":"0a31c74d-75d8-4e37-840c-92b6ba27034f","title":"NASA/JPL: Onboard instruments for the detection of microscopy biosignatures","slug":"onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eThe quest for extraterrestrial life represents a critical scientific endeavor with civilization-level implications. Promising targets for exploration include icy moons in the solar system, identified for their potential as habitats due to liquid oceans. However, the lack of a precise definition of life presents a fundamental challenge to formulating detection strategies. In response to the bandwidth limitations in transmitting data from distant ocean worlds like Enceladus or Europa, an emerging discipline called Onboard Science Instrument Autonomy (OSIA) evaluates, summarizes, and prioritizes observational instrument data within flight systems. Two OSIA implementations, identifying life-like motion in digital holographic microscopy videos and cellular structure and composition through fluorescence, were developed as part of the Ocean World Life Surveyor (OWLS) prototype instrument suite at the Jet Propulsion Laboratory. Flight-like requirements and computational constraints, akin to those on the Mars helicopter, \"Ingenuity,\" were employed to lower barriers to infusion. The study, which included evaluation using simulated and laboratory data and a live field test at the Mono Lake planetary analog site, demonstrates OSIA's potential for biosignature detection and offers insights and lessons for future mission concepts exploring the outer solar system.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1878\" height=\"940\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1000w, https://labelbox-research.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1600w, https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1878w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eLife detection is a uniquely challenging scientific objective for two reasons. First, there remains considerable disagreement on the fundamental definition of life on Earth. Second, for any single proposed biosignature, there are  processes that can generate similar, misleading signals. This is exacerbated on other planetary bodies where dominant physical processes may substantially differ from those studied on Earth. To address this, life detection missions should include the capability to detect conceptually orthogonal biosignatures that together reduce the likelihood of misinterpretation of biotic and abiotic phenomena. \u003c/p\u003e\u003cp\u003eWhile such data volumes are routinely accommodated in a laboratory setting, the need for space missions to communicate all findings across vast interplanetary distances and through over-subscribed resources like the Deep Space Network makes communication bandwidth a primary bottleneck for planetary exploration. Put simply, the compelling detection of extraterrestrial life may require over 10,000 times more raw data than is transmissible by a space mission.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eTo evaluate the performance of the particle tracker and motility classifier, salient particles were manually tracked throughout each observation and annotated as motile or nonmotile. Labels were generated by external labelers from Labelbox. To ensure annotation consistency and quality, the researchers provided the labelers with a labeling guide document and video with a specific annotation protocol. All labels were then reviewed for quality by our research team. In total, 778 and 199 tracks were labeled in DHM and FLFM data, respectively. All labeled data including raw observations, labeled tracks, and the labeling guide are published in the \u003ca href=\"https://dataverse.jpl.nasa.gov/dataset.xhtml?persistentId=doi%3A10.48577%2Fjpl.2KTVW5\u0026ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003eJPL Open Repository\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eYou can read the full research paper \u003ca href=\"https://iopscience.iop.org/article/10.3847/PSJ/ad0227?ref=labelbox-research.ghost.io#psjad0227s5\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"65b2e290c909bf0001caf529","feature_image":"https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.51.40-PM.png","featured":false,"visibility":"public","created_at":"2024-01-25T22:37:04.000+00:00","updated_at":"2024-01-25T22:54:18.000+00:00","published_at":"2024-01-25T22:54:18.000+00:00","custom_excerpt":"Learn how researchers from NASA/JPL are  advancing biosignature detection which offers insights and lessons for future mission concepts exploring life in the outer solar system.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor/","excerpt":"Learn how researchers from NASA/JPL are  advancing biosignature detection which offers insights and lessons for future mission concepts exploring life in the outer solar system.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b79b8bfcccab003d9cc62e","uuid":"f5659846-7e71-467e-ac06-142456f90ac9","title":"Detecting feature requests of 3rd-party developers via machine learning: A case study of the SAP Community","slug":"detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eThe elicitation of requirements is central for the development of successful software products. While traditional requirement elicitation techniques such as user interviews are highly labor-intensive, data-driven elicitation techniques promise enhanced scalability through the exploitation of new data sources like app store reviews or social media posts. For enterprise software vendors, requirements elicitation remains challenging because app store reviews are scarce and vendors have no direct access to users.  The researchers investigated whether enterprise software vendors can elicit requirements from their sponsored developer communities through data-driven techniques. \u003c/p\u003e\u003cp\u003eThe researchers wanted to analyze whether it is possible to automatically detect feature requests in the questions of community members through a binary machine learning classifier. The motivation for such a classifier is that sponsored developer communities typically contain millions of posts, but only a few are relevant for the elicitation of requirements.\u003c/p\u003e\u003cp\u003eThe potential of sponsored developer communities for data-driven requirements elicitation is a promising source of information. While developers outside of the enterprise software domain often rely on autonomous developer communities such as Stack Overflow or Stack Exchange, enterprise software vendors typically nurture their own, self-hosted developer communities such as the SAP Community, Salesforce’s Trailblazer Community or ServiceNow’s Now Community\u003c/p\u003e\u003cp\u003eTo answer their research question, they collected data from the SAP Community and generated a manually labeled data set of 1,500 questions. Following the design science paradigm, they developed a supervised and binary machine learning classifier. They observed the highest prediction accuracy (0.8187) for the classifier when they extracted features with the pre-trained SBERT-Model and classified them with the Naïve Bayes algorithm. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eFollowing the design science methodology, the researchers collected data from the SAP Community and developed a supervised machine learning classifier, which automatically detected feature requests of third-party developers. Based on a manually labeled data set of 1,500 questions, their classifier reached a high accuracy of 0.819 and revealed that supervised machine learning models are an effective means for the identification of feature requests.\u003c/p\u003e\u003cp\u003eTheir study opens up two major avenues for future research. First, while they used sponsored developer communities to mine enterprise software requirements, future research can explore the elicitation of bugs to improve the maintenance of software products. Second, future research can also explore different types of feature requests in sponsored developer communities.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers used Quantumworks Lab to collect the assessments from their labelers. They used a managed-labeler approach to label our final sample of 1,500 questions. A key reason for this approach was that the assessment of whether a question contains a feature request requires solid knowledge about SAP’s products and their functionality. Labelers needed to be SAP experts to make an accurate assessment. Additionally, they wanted to have three labelers so that they can rely on the majority label when the assessments are discordant.  Labeler one is part of the researchers and has worked for their case company SAP for multiple years and in varying roles. Labeler two and three were recruited via the freelancer platform Fiverr. Both have a university degree and several years of experience with SAP’s technology.\u003c/p\u003e\u003cp\u003eMoreover, the labeling task was a binary evaluation, meaning that our labelers were presented with a question and they had to assess whether the question contains a feature request or not. Existing answers and comments were not shown to the labelers as our goals was to train the classifier on the questions only. The inclusion of answers and comments\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1378\" height=\"596\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 1378w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.researchgate.net/profile/Martin-Kauschinger/publication/364165571_Detecting_Feature_Requests_of_Third-Party_Developers_through_Machine_Learning_A_Case_Study_of_the_SAP_Community/links/633d3f1676e39959d69f8513/Detecting-Feature-Requests-of-Third-Party-Developers-through-Machine-Learning-A-Case-Study-of-the-SAP-Community.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63b79b8bfcccab003d9cc62e","feature_image":"https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.03.34-PM.png","featured":false,"visibility":"public","created_at":"2023-01-06T03:54:51.000+00:00","updated_at":"2023-01-06T04:04:39.000+00:00","published_at":"2023-01-06T04:04:15.000+00:00","custom_excerpt":"Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community/","excerpt":"Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63966b25136f7d003d87b29d","uuid":"eb20cc4a-5d82-447e-a492-e54912b2ca4a","title":"Real-time segmentation of desiccation cracks onboard UAVs for planetary exploration","slug":"real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures). \u003c/p\u003e\u003cp\u003eThe search for biosignatures on other planets focuses on identifying evidence of habitable environments, characterized by the presence or former-presence of water, and other factors that may have allowed organisms to grow and be preserved in the rock record. Mud cracks, or desiccations cracks, are a type of sedimentary structure associated with muddy environments where water has been present, such as dry lake beds.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Planetary surfaces are a primary focus of space exploration. Some of the most challenging current efforts in planetary exploration relate to the search for life, or biosignatures, in these environments. Detecting water-related textures, and thus evidence for potentially habitable environments, has the potential to focus and accelerate the search for biosignatures on other planets. Desiccation cracks are sedimentary features providing evidence of sediment-water interaction. \u003c/p\u003e\u003cp\u003eThey are known from both Earth and Mars, and are likely to be found via aerial exploration approaches of ancient lakes, rivers, or shallow marine environments where biosignatures may be found. Current approaches using image processing to detect desiccation cracks focus on segmenting just the cracks and prove somewhat successful. \u003c/p\u003e\u003cp\u003eHowever, the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection over much larger surface areas has not yet been explored.  The researchers advanced novel ways to develop and deploy of a desiccation crack detection system using UAVs and AI, leveraging new data collection techniques at varying heights above ground level and data-augmentation with a range of pixel-level and spatial transforms. Three state-of-the-art CNN segmentation networks are trained and evaluated using PyTorch. The networks are deployed on an edge-AI device integrated with a companion computer onboard a sub-2kg quadrotor UAV.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e Results indicate that the models can segment desiccation cracks on airborne collected images at various locations at heights ranging from 5 to 20m. Deployment of the models for real-time inference onboard small UAVs shows potential for application in the field. This research shows the feasibility of a low-volume data training to UAV deployment pipeline while highlighting potential hurdles in the processing pipeline for future efforts. They presented a system and architecture for onboard UAV detectors of sedimentary features, which can speed up the search for life on other planets.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1482\" height=\"678\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 1482w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe images were all labeled for segmentation using the Labelbox. The image database was uploaded to Quantumworks Lab using the datasets creation feature. Labeling was conducted following a standardized labelling procedure developed during an initial exploratory labelling step to manage artifacts, ensure consistency and accelerate the process. L\u003c/p\u003e\u003cp\u003eLabeling a dataset can take a considerable amount of time. The process is prone to suffer from bias from human labelers. Labeling desiccation cracks requires the definition of clear boundaries. A single label was used (desiccation) for time optimization and simplification and a standardized labelling procedure was developed. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"676\" height=\"398\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png 600w, https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png 676w\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of a labeled desiccation crack texture using Quantumworks Lab overlaid in light blue\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.researchgate.net/publication/358402708_Real-time_Segmentation_of_Desiccation_Cracks_onboard_UAVs_for_Planetary_Exploration?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63966b25136f7d003d87b29d","feature_image":"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.51.52-PM.png","featured":false,"visibility":"public","created_at":"2022-12-11T23:43:33.000+00:00","updated_at":"2024-01-25T22:36:35.000+00:00","published_at":"2022-12-11T23:52:12.000+00:00","custom_excerpt":"Researchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures).","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/","tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/","excerpt":"Researchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures).","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6384ec65a3ae7a003dfd4db2","uuid":"723c1b43-17d8-4ffa-b0c2-c54d13f4b7b9","title":"Fruit flower detection in apple orchards using ML","slug":"fruit-flower-detection-in-apple-orchards-using-ml","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the University of Guelph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Apples are typically a tree fruit crops grown worldwide with an estimated annual production of 124 million. To increase economic sustainability and compete globally, apple growers are strategically improving returns by adopting new cultivars, rootstocks and orchard management practices that improve fruit quality. \u003c/p\u003e\u003cp\u003eAdditionally, there is also an urgent need to reduce labour, which accounts for over 60% of production costs. The researchers presented the application of machine vision and learning techniques to detect and identify the number of flower clusters on apple trees leading to the ability to predict the potential yield of apples. The current issue in the agriculture field is that most of the machinery currently being used requires worker supervision. The inclusion of automated machinery can greatly increase the yield produced compared to manual labour and lessen the load required as it is a very labour-intensive job. The inclusion of automation can drastically increase the efficiency of the operation as well as the quality.\u003c/p\u003e\u003cp\u003eThe most challenging problem is dealing with occlusion as there are usually many objects in a dense location in an agricultural setting. Another important challenge that must be considered when designing an object detection system in agriculture is illumination. Due to most operations occurring in an outside environment, variance in natural lighting can greatly affect the performance of the object detection algorithm. Other challenges consist of similar appearance, such as shape and color, as well as multi-fruit detection cases. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1071\" height=\"1593\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image-3.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/11/image-3.png 1000w, https://labelbox-research.ghost.io/content/images/2022/11/image-3.png 1071w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eA mobile robot platform that was used for the data collection.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e A new field robot was designed and built to collect and build a dataset of 1500 images of apples trees. The trained model produced a cluster precision of 0.88 or 88% and a percentage error of 14% over 106 trees running the mobile vehicle on both sides of the trees. The detection model was predicting less than the actual amount but the fruit flower count is still significant in that it can give researchers information on the estimated growth and production of each tree with respect to the actions applied to each fruit tree. Their research helps lay the foundation for future application of machine vision and learning techniques within apple orchards or other fruit tree settings.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers collected and properly filtered the images and transferred them from a storage device to OneDrive for easy access on multiple platforms. The entire labelling process used Quantumworks Lab, which allowed the process to operate collaboratively working on labeling the dataset as there is no need in transferring and downloading large number of photos. The platform was also chosen because it keeps track of how many photos still need labelling and the estimated time it took per photo. The platform was also very easy to use as there were no other external setup needed other than an account to the service. \u003c/p\u003e\u003cp\u003eWith the help of hired undergraduate students at the University of Guelph, it took about a month to completely label 1499 photos. The photos consisted of a mixture of close-up and zoomed-out photos which involved classes, labelled as “Fruit Flower Single”, “Fruit Flower Cluster” and “Tree”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"550\" height=\"320\"\u003e\u003cfigcaption\u003eAn example of the training dataset.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.mdpi.com/2076-3417/12/22/11420?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6384ec65a3ae7a003dfd4db2","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-28-at-9.25.30-AM.png","featured":false,"visibility":"public","created_at":"2022-11-28T17:14:13.000+00:00","updated_at":"2022-12-06T21:29:44.000+00:00","published_at":"2022-11-28T17:26:19.000+00:00","custom_excerpt":"Researchers from the University of Guleph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/fruit-flower-detection-in-apple-orchards-using-ml/","tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/fruit-flower-detection-in-apple-orchards-using-ml/","excerpt":"Researchers from the University of Guleph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63731b8cf1a5bd003d56b1ee","uuid":"4b21fd7f-058b-408b-b990-b962c56c009c","title":"Automated recognition of  cricket batting techniques in videos using deep learning","slug":"automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the University of Johannesberg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge\u003c/strong\u003e: There have been limited studies demonstrating the validation of batting techniques in cricket using machine learning. Cricket batting technique is intricate because it involves a series of complex gestures needed to perform a stroke, one of these gestures performed by the batsman is referred to as the batting backlift technique (BBT).\u003c/p\u003e\u003cp\u003ePrevious research has indicated that the BBT can be seen as a contributing factor to successful batsmanship. There are two backlifts investigated in this study, namely the lateral batting backlift technique (LBBT), and the straight batting backlift technique (SBBT). The LBBT is a technique present where the toe and face of the bat are lifted laterally in the direction of second slip. The SBBT is represented whenever the toe and face of the bat are pointed toward the stumps and ground.\u003c/p\u003e\u003cp\u003eThe study demonstrated how the batting backlift technique in cricket can be automatically recognized in video footage and compares the performance of popular deep learning architectures, namely, AlexNet, Inception V3, Inception Resnet V2, and Xception. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eBuilding a unique dataset showing the lateral and straight backlift classes and assessed according to standard machine learning metrics, the researchers found that the architectures was comparable with similar performance with one false positive in the lateral class and a precision score of 100%, along with a recall score of 95%, and an f1-score of 98% for each architecture. \u003c/p\u003e\u003cp\u003eThe AlexNet architecture performed the worst out of the four architectures as it incorrectly classified four images that were supposed to be in the straight class. The architecture that is best suited for the problem domain was the Xception architecture with a loss of 0.03 and 98.2.5% accuracy, thus demonstrating its capability in differentiating between lateral and straight backlifts. \u003c/p\u003e\u003cp\u003eThe study provides a way forward in the automatic recognition of player patterns and motion capture, making it less challenging for sports scientists, biomechanists and video analysts working in the field.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eFor the construction of the dataset, the process went through a comprehensive YouTube search of First-Class International Cricket Test Match highlights, where the match’s environment has fewer variations to consider.\u003c/p\u003e\u003cp\u003eUsing the Quantumworks Lab platform, each object within a cricket scene is labeled, allowing for easier isolation and extraction of the batsman in each frame. The frame used for constructing the dataset was when the bowler was about to release the ball towards the batsman. \u003c/p\u003e\u003cp\u003eThe frame was identified as the ideal time period for the position of the batsman at the instant of delivery. Using an 80:20 data split, the training class had 160 images, and the testing class had 40 images, resulting in a total of 200 images, which served as a baseline to draw comparisons of the proposed architectures. The image aspect ratio chosen through testing and validation is 128×128, which is chosen to avoid distorting the original image.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.nature.com/articles/s41598-022-05966-6?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"63731b8cf1a5bd003d56b1ee","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-9.03.28-PM.png","featured":false,"visibility":"public","created_at":"2022-11-15T04:54:36.000+00:00","updated_at":"2022-12-06T21:29:33.000+00:00","published_at":"2022-11-15T05:04:13.000+00:00","custom_excerpt":"Researchers from the University of Johannesburg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/","tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/","excerpt":"Researchers from the University of Johannesburg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":25,"allPosts":[{"id":"6865adbad35f6c00012c1ccb","uuid":"c13f592c-e428-4733-a5fd-83ee017618ad","title":"TRINS: Towards Multimodal Language Models that Can Read","slug":"trins-towards-multimodal-language-models-that-can-read","html":"\u003cp\u003eThis paper introduces TRINS, a large-scale dataset specifically designed to improve multimodal models' ability to \"read\" — that is, to interpret, reason about, and generate language grounded in both visual scenes and embedded text (like signage, documents, and labels). The dataset contains 39,153 carefully annotated, text-rich images, each paired with long-form captions (averaging 65 words) and over 102,000 question–answer pairs designed to evaluate reading comprehension within images.\u003c/p\u003e\u003cp\u003eMost existing multimodal datasets (e.g., COCO, TextVQA) provide relatively short captions or sparse annotations. TRINS stands out by densely capturing both visual context and embedded text, making it especially useful for training models that need to perform OCR-like reasoning or multimodal understanding where text is a critical part of the image content.\u003c/p\u003e\u003cp\u003eTo showcase the power of this dataset, the authors also introduce LaRA (Language–vision Reading Assistant), a compact and efficient multimodal model trained on TRINS. Despite being much smaller than models like GPT-4V or Gemini, LaRA achieves state-of-the-art performance on multiple benchmarks involving text-rich image understanding, including both captioning and visual question answering.\u003c/p\u003e\u003chr\u003e\u003ch3 id=\"how-labelbox-was-used\"\u003e\u003cstrong\u003eHow Quantumworks Lab Was Used\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe success of TRINS hinges on the quality and density of its annotations — and Quantumworks Lab played a central role in enabling this at scale.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHuman-in-the-loop annotation at scale\u003c/strong\u003e:\u003cbr\u003eThe authors used Quantumworks Lab to annotate 40,576 images, involving 2,079 hours of human labeling and 159 hours of human review. Annotators were instructed to describe both the visual content and the embedded textual content, with attention to attributes like font style, color, position, and meaning.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eStructured annotation guidelines\u003c/strong\u003e:\u003cbr\u003eLabelbox was used to create and enforce detailed ontologies and annotation policies, helping annotators capture high-quality data that included:\u003cul\u003e\u003cli\u003eWhat is happening in the image.\u003c/li\u003e\u003cli\u003eWhat the text says.\u003c/li\u003e\u003cli\u003eWhy the text is important.\u003c/li\u003e\u003cli\u003eOptional deeper reasoning or interpretation.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eQuality assurance workflows\u003c/strong\u003e:\u003cbr\u003eAfter initial annotation, TRINS employed automated checks — including OCR vs. human text comparisons — and manual reviews via Quantumworks Lab to flag low-quality or incomplete examples. These entries were either rejected or routed for re-annotation, ensuring consistency and accuracy throughout the dataset.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnabling long-form, expressive annotations\u003c/strong\u003e:\u003cbr\u003eLabelbox’s flexible interface made it easier to collect detailed, multi-sentence captions and diverse QA pairs, a sharp contrast to most datasets that rely on much shorter labels.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch3 id=\"further-research\"\u003eFurther research\u003c/h3\u003e\u003cp\u003eTRINS represents a major leap forward in multimodal data quality and scale, especially in domains that require deep understanding of images with embedded text (e.g., street scenes, product images, forms). With Quantumworks Lab, the researchers were able to create a dataset that is not only large, but also rich, expressive, and verified, enabling the training of smaller yet more capable models like LaRA.\u003c/p\u003e\u003cp\u003eThis combination of advanced human annotation and machine-efficient modeling signals a promising direction for building language–vision models that can truly read and reason. You can read the full paper \u003ca href=\"https://arxiv.org/pdf/2406.06730?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6865adbad35f6c00012c1ccb","feature_image":"https://labelbox-research.ghost.io/content/images/2025/07/Screenshot-2025-07-02-at-3.12.03-PM.png","featured":false,"visibility":"public","created_at":"2025-07-02T22:07:54.000+00:00","updated_at":"2025-07-02T22:13:21.000+00:00","published_at":"2025-07-02T22:13:21.000+00:00","custom_excerpt":"TRINS is a dataset of 39K+ text-rich images with detailed captions and QA pairs, built using Quantumworks Lab for high-quality annotation—enabling LaRA, a compact vision-language model that outperforms larger systems on reading and reasoning tasks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":null,"url":"https://labelbox-research.ghost.io/trins-towards-multimodal-language-models-that-can-read/","excerpt":"TRINS is a dataset of 39K+ text-rich images with detailed captions and QA pairs, built using Quantumworks Lab for high-quality annotation—enabling LaRA, a compact vision-language model that outperforms larger systems on reading and reasoning tasks.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6865ab35d35f6c00012c1cbb","uuid":"1e7f92e9-6f2c-4b9e-a9e2-4e9b56120df1","title":"HUM-CARD: A human crowded annotated real dataset","slug":"hum-card-a-human-crowded-annotated-real-dataset","html":"\u003col\u003e\u003cli\u003e\u003cstrong\u003eObjective \u0026amp; Motivation\u003c/strong\u003e\u003cul\u003e\u003cli\u003eThe researchers collected and annotated images portraying dense crowds in urban spaces to analyze crowd-related environmental impacts and aid related computer vision tasks.\u003c/li\u003e\u003cli\u003eA major goal was to create a dataset that supports researchers developing algorithms for crowd analysis and environmental monitoring.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDataset Composition\u003c/strong\u003e\u003cul\u003e\u003cli\u003eIncludes thousands of real-world urban images, capturing a range of densities and scenes (e.g., streets, transportation hubs).\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAnnotation Process \u0026amp; Quality Control\u003c/strong\u003e\u003cul\u003e\u003cli\u003eMultiple annotators labeled humans in the scenes to ensure high accuracy.\u003c/li\u003e\u003cli\u003eThe dataset was refined through consensus mechanisms to improve label consistency.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eApplications \u0026amp; Benchmarking\u003c/strong\u003e\u003cul\u003e\u003cli\u003eBeyond a standalone dataset, it is used to benchmark crowd-counting and density estimation models.\u003c/li\u003e\u003cli\u003eDemonstrates improvements over existing datasets due to greater complexity and realism.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnvironmental Insights\u003c/strong\u003e\u003cul\u003e\u003cli\u003eAnalysis explores relationships between crowd density, urban space usage, and environmental stressors, offering a richer context for environmental-management applications.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ol\u003e\u003chr\u003e\u003ch3 id=\"how-labelbox-was-used\"\u003e\u003cstrong\u003eHow Quantumworks Lab Was Used\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe authors employed \u003cstrong\u003eLabelbox\u003c/strong\u003e, a collaborative data‑labeling platform, to efficiently and accurately annotate their dataset:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThey used Quantumworks Lab to \u003cstrong\u003ebuild the annotation ontology (AO)\u003c/strong\u003e: defining labeling categories (e.g., \"main object,\" sub‑classifications, contextual options) to structure the annotation task)\u003c/li\u003e\u003cli\u003eThe platform enabled sophisticated \u003cstrong\u003eeditor environments\u003c/strong\u003e, including image segmentation workflows for video frames. This streamlined annotator work and ensured consistent labels across the dataset.\u003c/li\u003e\u003cli\u003eKey Quantumworks Lab features used included:\u003cul\u003e\u003cli\u003eCustom ontologies for complex scene interpretation.\u003c/li\u003e\u003cli\u003eSupport for bounding boxes, segmentation masks, and multi-label annotations, tailored to dense crowd imagery.\u003c/li\u003e\u003cli\u003eBuilt-in quality control and consensus-based workflows to validate annotator agreement.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003eThe HUM‑CARD dataset provides a rich resource for understanding urban crowding and its environmental dynamics. By leveraging Quantumworks Lab’s ontology creation tools, annotation workflows, and quality‑control mechanisms, the authors delivered a high‑quality, densely annotated dataset that supports both environmental studies and crowd‑analysis computer vision research.\u003c/p\u003e","comment_id":"6865ab35d35f6c00012c1cbb","feature_image":"https://labelbox-research.ghost.io/content/images/2025/07/Screenshot-2025-07-02-at-3.03.51-PM.png","featured":false,"visibility":"public","created_at":"2025-07-02T21:57:09.000+00:00","updated_at":"2025-07-02T22:04:26.000+00:00","published_at":"2025-07-02T22:04:26.000+00:00","custom_excerpt":"The HUM-CARD study introduces a richly annotated dataset of densely crowded urban scenes to advance crowd analysis and environmental impact research, using Quantumworks Lab to ensure high-quality, scalable human annotations.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":null,"url":"https://labelbox-research.ghost.io/hum-card-a-human-crowded-annotated-real-dataset/","excerpt":"The HUM-CARD study introduces a richly annotated dataset of densely crowded urban scenes to advance crowd analysis and environmental impact research, using Quantumworks Lab to ensure high-quality, scalable human annotations.","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"67be4062651e6b0001cd2bdf","uuid":"ef0d7946-0433-4386-bc84-c788c313d47e","title":"Quantumworks Lab services helps research team explore AI-guided defect detection techniques","slug":"ai-guided-defect-detection","html":"\u003cp\u003eAt Quantumworks Lab, we're passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we're excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.\u003c/p\u003e\u003cp\u003eThese posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we're exploring \u003ca href=\"https://arxiv.org/pdf/2404.07306?ref=labelbox-research.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAI-Guided Defect Detection Techniques to Model Single Crystal Diamond Growth\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003ea study by Rohan Reddy Mekala, Elias Garratt, Matthias Muehle, Arjun Srinivasan, Adam Porter, and Mikael Lindvall.\u003c/p\u003e\u003ch2 id=\"research-introduction\"\u003e\u003cstrong\u003eResearch introduction\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eResearchers recently identified several challenges in producing high-quality single crystal diamonds (SCDs) at scale. Despite extensive development efforts, current manufacturing relies on trial-and-error experimentation, leading to inconsistent results and defects. In order to address these issues, researchers across the United States collaborated to propose new methodologies using machine learning and AI models to predict future diamond growth states for accelerated material development, improved quality, and larger sizes.\u003c/p\u003e\u003cp\u003eThe research team focused on these challenges:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLack of predictability:\u003c/strong\u003e Existing techniques currently lack the ability to predict diamond growth states, hindering process control and the potential for correction.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIssues with scale:\u003c/strong\u003e Diamond is an essential material for tools in power electronics, health sciences, and engineering, but each field has different requirements in terms of quality, purity, and size. The inability to predict growth subsequently makes it difficult to scale production.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIneffective research methodology:\u003c/strong\u003e Current research methods on developing a method to sustainably and reliably produce high quality diamonds rely on a trial and error method, leading to inconsistent results and defects from each methodology.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom these challenges, researchers identified an opportunity in using machine learning and deep learning algorithms to predict future diamond growth states to shorten the growth cycle, improve prediction accuracy, and enhance crystalline quality.\u003c/p\u003e\u003ch2 id=\"how-labelbox-was-used\"\u003e\u003cstrong\u003eHow Quantumworks Lab was used\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eLabelbox services was used to build the dataset for object detection of the diamond growth-run images. The process began with an initial batch of 100 images, which were reviewed by a team of three material scientists and 15 Alignerrs that were vetted and onboarded through Quantumworks Lab Labeling Services.. Alignerrs provide expert, on-demand annotation services and are selected from a network of trained professionals across diverse data domains.\u003c/p\u003e\u003cp\u003eThe scientists provided detailed instructions, including explanatory videos and meetings, to guide the labeling process. The labels were then reviewed by the Alignerrs, where predominant occurrences were identified based on a consensus score. Afterward, the material scientists conducted a final review to ensure the accuracy, consistency, and integrity of the data.\u003c/p\u003e\u003ch2 id=\"analysis-and-model-assisted-labeling\"\u003e\u003cstrong\u003eAnalysis and model-assisted labeling\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-research.ghost.io\"\u003e\u003cu\u003eModel-assisted labeling\u003c/u\u003e\u003c/a\u003e (MAL) was implemented in order to improve the consistency of data labeling and address potential variability in interpretation of instructions amongst labelers. This involved training a baseline model incrementally using iterative image-annotation pairs from the initial batch. This baseline model was then overlaid on subsequent batches to assist Alignerrs, significantly reducing labeling time and improving accuracy.\u003c/p\u003e\u003cp\u003eWith MAL, the time to label each image decreased from 15 minutes to just 2 minutes. Alignerrs were able to correct and refine annotation based on the baseline model’s predictions until a segmentation accuracy threshold of 80% was achieved. Once this threshold was achieved, the model’s overlays were used as a starting point for future batches.\u003c/p\u003e\u003cp\u003eAfter the minimum number of images were processed, the final set of image-label pairs was passed on for further research and development of the final semantic segmentation and object detection models. Using MAL and this human-in-the-loop workflow, this streamlined annotation process leads to improved efficiency and label quality.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeGAy55bE9C_7cOm9nsW-H0IraNnL1CMua-RCDhe88CKCtuOqn0BTPC4wr0gSgZ5HKrxTsNRkbrOtuMF7yQ5QFYvXGaexfSPvfZJFjHT5C4IpKxXHYkaI-YiUNq0z4eP8yyd2Pg?key=cmC32LkeleH-01xeZ1gdpicE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"424\"\u003e\u003c/figure\u003e\u003ch2 id=\"key-findings\"\u003e\u003cstrong\u003eKey findings\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eUsing object detection and image segmentation algorithms along with the support of a highly-skilled team of Quantumworks Lab Alignerrs, researchers were able to create a defect detection pipeline for diamond growth data. This pipeline achieved a high defect classification accuracy with 93.35% for Center-Defects, 92.83% for Poly-crystalline-Defects, and 91.98% for Edge-Defects.\u003c/p\u003e\u003cp\u003eThe authors were able to use this pipeline to accurately detect defects, reduce time and costs, and predict future diamond growth.\u003c/p\u003e\u003cp\u003eWorking on your own research? Reach out to the team at research@labelbox.com to request a research license or to share your AI research with us.\u0026nbsp;\u003c/p\u003e","comment_id":"67be4062651e6b0001cd2bdf","feature_image":"https://labelbox-research.ghost.io/content/images/2025/02/AI-Guided-Defect-Detection-Techniques.png","featured":false,"visibility":"public","created_at":"2025-02-25T22:12:50.000+00:00","updated_at":"2025-02-25T22:45:46.000+00:00","published_at":"2025-02-25T22:45:46.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"67be114424e4bd0001a29369","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/lisa/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"67be114424e4bd0001a29369","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/lisa/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/ai-guided-defect-detection/","excerpt":"At Quantumworks Lab, we're passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we're excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.\n\nThese posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we're exploring AI-Guided ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How Quantumworks Lab services helps explore AI-guided defect detection techniques ","meta_description":"Learn how researchers use Quantumworks Lab services and software to accurately detect defects, reduce time and costs, and predict future diamond growth.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"674656bbd6cd0800017443d1","uuid":"a0cd1b3b-3473-473e-a076-9c9f2ede5adb","title":"A benchmark for long-form medical question answering","slug":"a-benchmark-for-long-form-medical-question-answering","html":"\u003cp\u003eResearchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. Existing benchmarks often rely on automatic metrics and multiple-choice questions, which do not fully capture the complexities of real-world clinical applications. To bridge this gap, the authors introduced a publicly available benchmark comprising real-world consumer medical questions, with long-form answers evaluated by medical professionals. This resource aims to facilitate more accurate assessments of LLMs' performance in medical question answering.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch area and challenges \u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLack of comprehensive benchmarks\u003c/strong\u003e: Existing evaluations focus on automatic metrics and multiple-choice formats, which do not adequately reflect the nuances of clinical scenarios.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eClosed-source studies\u003c/strong\u003e: Many studies on long-form medical QA are not publicly accessible, limiting reproducibility and the enhancement of existing baselines.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAbsence of human annotations\u003c/strong\u003e: There's a scarcity of datasets with human medical expert annotations, hindering the development of reliable evaluation metrics.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1114\" height=\"784\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 1114w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eScheme of the difficulty levels of medical questions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eTo build a dataset of real-world medical questions, 4,271 queries were collected from the Lavita Medical AI Assist platform between  spanning 1,693 conversations (single-turn and multi-turn). After deduplication, removal of sample questions, and filtering non-English entries using Lingua, the dataset was refined to 2,698 unique queries. \u003c/p\u003e\u003cp\u003ePairwise annotation tasks were completed using Quantumworks Lab's annotation platform and human evaluations were conducted with a group of 3 medical doctors, with two doctors assigned per batch, specializing in radiology and pathology. \u003c/p\u003e\u003cp\u003eBefore starting the main round of annotations, the researchers shared the annotation scheme with the doctors and conducted a trial round with each on a small sample of questions. They then gathered the doctors’ feedback to ensure that all annotation criteria were clear and that there was no ambiguity regarding the instructions. After confirming clarity and receiving approval from the doctors, they proceeded with the main batches of annotations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eLLM-as-a-Judge\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe researchers designed their LLM-as-a-judge prompt by combining templates from Zheng et al. and WildBench. The prompt, shown below, enables pairwise comparison of responses across multiple criteria, and used GPT-4o-2024-08-06 and Claude-3-5-Sonnet-20241022 as judges.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1142\" height=\"638\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eKey findings\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpen LLMs' potential\u003c/strong\u003e: Preliminary results indicated that open-source LLMs exhibit strong performance in medical QA, comparable to leading closed models.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAlignment with human judgments\u003c/strong\u003e: The research included a comprehensive analysis of LLMs acting as judges, revealing significant alignment between human evaluations and LLM assessments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePublicly available benchmark\u003c/strong\u003e: The authors provided a new benchmark featuring real-world medical questions and expert-annotated long-form answers, promoting transparency and reproducibility in future research.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1146\" height=\"386\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 1146w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThis paper has been submitted to \u003cem\u003eAIM-FM: Advancements in Medical Foundation Models Workshop\u003c/em\u003e at NeurIPS 2024. You can read the full paper \u003ca href=\"https://arxiv.org/pdf/2411.09834?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"674656bbd6cd0800017443d1","feature_image":"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.53.21-PM.png","featured":false,"visibility":"public","created_at":"2024-11-26T23:16:11.000+00:00","updated_at":"2025-07-03T18:07:05.000+00:00","published_at":"2024-11-26T23:43:04.000+00:00","custom_excerpt":"Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/a-benchmark-for-long-form-medical-question-answering/","excerpt":"Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65bbe41d21ca940001df755f","uuid":"eb9e0733-ed5d-4b7c-9061-6ba30c57b26e","title":"Identifying and counting avian blood cells in whole slide images via deep learning","slug":"identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eAvian blood analysis is a fundamental method for investigating a wide range of topics concerning individual birds and populations of birds. Determining precise blood cell counts helps researchers gain insights into the health condition of birds. For example, the ratio of heterophils to lymphocytes (H/L ratio) is a well-established index for comparing relative stress load. However, such measurements are currently often obtained manually by human experts. The researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models. The first model determined image regions that are suitable for counting blood cells, and the second model is an instance segmentation model that detected the cells in the determined image regions. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eAutomated visual and acoustic monitoring methods for birds can provide information about the presence and the number of bird species or individuals in certain areas, but analyzing the physiological conditions of individual birds allows researchers to understand potential causes of negative population trends. \u003c/p\u003e\u003cp\u003eFor example, measuring the physiological stress of birds can serve as a valuable early warning indicator for conservation efforts. The physiological conditions and the stress of birds can be determined in several ways, e.g., by assessing the body weight or the fat and muscle scores in migratory birds. Other frequently used methods are investigating the parasite loads, measuring the heart rates, and measuring the levels of circulating stress hormones, such as corticosterone. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The region selection model achieves up to 97.3% in terms of F1 score (i.e., the harmonic mean of precision and recall), and the instance segmentation model achieves up to 90.7% in terms of mean average precision. The approach can  help ornithologists acquire hematological data from avian blood smears more precisely and efficiently.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003e The researchers used the Quantumworks Lab platform annotating images of our datasets with model-assisted labeling. The instance segmentation dataset was annotated in an iterative, model-assisted manner, using the tile selection network to propose regions to be annotated and eventually selected them based on how many rare cells had been detected by an intermediate instance segmentation model. \u003c/p\u003e\u003cp\u003eIn the very first iteration, they used a superpixel algorithm to generate simple instance masks. In each iteration, they uploaded the corresponding instance segmentation masks to Quantumworks Lab to be refined by our human expert. This procedure significantly reduced the time needed to fully annotate an image file with masks and class labels compared to annotating from scratch. Overall, they went through four iterations of labeling. For the annotated cell instances, they established two primary categories: erythrocyte, with only the nucleus annotated, and leukocyte. The latter was further split into five subtypes, namely, lymphocyte, eosinophil, heterophil, basophil, and monocyte. Thrombocytes were not explicitly annotated; they were considered to be part of the background during training. \u003c/p\u003e\u003cp\u003eThe trained neural network model was able to distinguish between non-relevant thrombocytes and other annotated cell types, e.g., erythrocytes. By annotating only the nucleus of each erythrocyte rather than the entire cell including the cytoplasm, they maintained the option to label parasite-infected instances individually in future work. Cells infected with parasites may be annotated by masking the entire cell including the cytoplasm. One erythrocyte can be simultaneously counted as both an erythrocyte and a cell with blood parasite because of the distinct annotation regions.\u003c/p\u003e\u003cp\u003eYou can read the full paper \u003ca href=\"https://www.mdpi.com/2673-6004/5/1/4?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"65bbe41d21ca940001df755f","feature_image":"https://labelbox-research.ghost.io/content/images/2024/02/download.jpeg","featured":false,"visibility":"public","created_at":"2024-02-01T18:34:05.000+00:00","updated_at":"2024-02-01T18:39:06.000+00:00","published_at":"2024-02-01T18:39:06.000+00:00","custom_excerpt":"Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/","excerpt":"Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65b2e290c909bf0001caf529","uuid":"0a31c74d-75d8-4e37-840c-92b6ba27034f","title":"NASA/JPL: Onboard instruments for the detection of microscopy biosignatures","slug":"onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eThe quest for extraterrestrial life represents a critical scientific endeavor with civilization-level implications. Promising targets for exploration include icy moons in the solar system, identified for their potential as habitats due to liquid oceans. However, the lack of a precise definition of life presents a fundamental challenge to formulating detection strategies. In response to the bandwidth limitations in transmitting data from distant ocean worlds like Enceladus or Europa, an emerging discipline called Onboard Science Instrument Autonomy (OSIA) evaluates, summarizes, and prioritizes observational instrument data within flight systems. Two OSIA implementations, identifying life-like motion in digital holographic microscopy videos and cellular structure and composition through fluorescence, were developed as part of the Ocean World Life Surveyor (OWLS) prototype instrument suite at the Jet Propulsion Laboratory. Flight-like requirements and computational constraints, akin to those on the Mars helicopter, \"Ingenuity,\" were employed to lower barriers to infusion. The study, which included evaluation using simulated and laboratory data and a live field test at the Mono Lake planetary analog site, demonstrates OSIA's potential for biosignature detection and offers insights and lessons for future mission concepts exploring the outer solar system.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1878\" height=\"940\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1000w, https://labelbox-research.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1600w, https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.49.18-PM.png 1878w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eLife detection is a uniquely challenging scientific objective for two reasons. First, there remains considerable disagreement on the fundamental definition of life on Earth. Second, for any single proposed biosignature, there are  processes that can generate similar, misleading signals. This is exacerbated on other planetary bodies where dominant physical processes may substantially differ from those studied on Earth. To address this, life detection missions should include the capability to detect conceptually orthogonal biosignatures that together reduce the likelihood of misinterpretation of biotic and abiotic phenomena. \u003c/p\u003e\u003cp\u003eWhile such data volumes are routinely accommodated in a laboratory setting, the need for space missions to communicate all findings across vast interplanetary distances and through over-subscribed resources like the Deep Space Network makes communication bandwidth a primary bottleneck for planetary exploration. Put simply, the compelling detection of extraterrestrial life may require over 10,000 times more raw data than is transmissible by a space mission.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eTo evaluate the performance of the particle tracker and motility classifier, salient particles were manually tracked throughout each observation and annotated as motile or nonmotile. Labels were generated by external labelers from Labelbox. To ensure annotation consistency and quality, the researchers provided the labelers with a labeling guide document and video with a specific annotation protocol. All labels were then reviewed for quality by our research team. In total, 778 and 199 tracks were labeled in DHM and FLFM data, respectively. All labeled data including raw observations, labeled tracks, and the labeling guide are published in the \u003ca href=\"https://dataverse.jpl.nasa.gov/dataset.xhtml?persistentId=doi%3A10.48577%2Fjpl.2KTVW5\u0026ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003eJPL Open Repository\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eYou can read the full research paper \u003ca href=\"https://iopscience.iop.org/article/10.3847/PSJ/ad0227?ref=labelbox-research.ghost.io#psjad0227s5\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"65b2e290c909bf0001caf529","feature_image":"https://labelbox-research.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-2.51.40-PM.png","featured":false,"visibility":"public","created_at":"2024-01-25T22:37:04.000+00:00","updated_at":"2024-01-25T22:54:18.000+00:00","published_at":"2024-01-25T22:54:18.000+00:00","custom_excerpt":"Learn how researchers from NASA/JPL are  advancing biosignature detection which offers insights and lessons for future mission concepts exploring life in the outer solar system.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/onboard-science-instrument-autonomy-for-the-detection-of-microscopy-biosignatures-on-the-ocean-worlds-life-surveyor/","excerpt":"Learn how researchers from NASA/JPL are  advancing biosignature detection which offers insights and lessons for future mission concepts exploring life in the outer solar system.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b79b8bfcccab003d9cc62e","uuid":"f5659846-7e71-467e-ac06-142456f90ac9","title":"Detecting feature requests of 3rd-party developers via machine learning: A case study of the SAP Community","slug":"detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eThe elicitation of requirements is central for the development of successful software products. While traditional requirement elicitation techniques such as user interviews are highly labor-intensive, data-driven elicitation techniques promise enhanced scalability through the exploitation of new data sources like app store reviews or social media posts. For enterprise software vendors, requirements elicitation remains challenging because app store reviews are scarce and vendors have no direct access to users.  The researchers investigated whether enterprise software vendors can elicit requirements from their sponsored developer communities through data-driven techniques. \u003c/p\u003e\u003cp\u003eThe researchers wanted to analyze whether it is possible to automatically detect feature requests in the questions of community members through a binary machine learning classifier. The motivation for such a classifier is that sponsored developer communities typically contain millions of posts, but only a few are relevant for the elicitation of requirements.\u003c/p\u003e\u003cp\u003eThe potential of sponsored developer communities for data-driven requirements elicitation is a promising source of information. While developers outside of the enterprise software domain often rely on autonomous developer communities such as Stack Overflow or Stack Exchange, enterprise software vendors typically nurture their own, self-hosted developer communities such as the SAP Community, Salesforce’s Trailblazer Community or ServiceNow’s Now Community\u003c/p\u003e\u003cp\u003eTo answer their research question, they collected data from the SAP Community and generated a manually labeled data set of 1,500 questions. Following the design science paradigm, they developed a supervised and binary machine learning classifier. They observed the highest prediction accuracy (0.8187) for the classifier when they extracted features with the pre-trained SBERT-Model and classified them with the Naïve Bayes algorithm. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eFollowing the design science methodology, the researchers collected data from the SAP Community and developed a supervised machine learning classifier, which automatically detected feature requests of third-party developers. Based on a manually labeled data set of 1,500 questions, their classifier reached a high accuracy of 0.819 and revealed that supervised machine learning models are an effective means for the identification of feature requests.\u003c/p\u003e\u003cp\u003eTheir study opens up two major avenues for future research. First, while they used sponsored developer communities to mine enterprise software requirements, future research can explore the elicitation of bugs to improve the maintenance of software products. Second, future research can also explore different types of feature requests in sponsored developer communities.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers used Quantumworks Lab to collect the assessments from their labelers. They used a managed-labeler approach to label our final sample of 1,500 questions. A key reason for this approach was that the assessment of whether a question contains a feature request requires solid knowledge about SAP’s products and their functionality. Labelers needed to be SAP experts to make an accurate assessment. Additionally, they wanted to have three labelers so that they can rely on the majority label when the assessments are discordant.  Labeler one is part of the researchers and has worked for their case company SAP for multiple years and in varying roles. Labeler two and three were recruited via the freelancer platform Fiverr. Both have a university degree and several years of experience with SAP’s technology.\u003c/p\u003e\u003cp\u003eMoreover, the labeling task was a binary evaluation, meaning that our labelers were presented with a question and they had to assess whether the question contains a feature request or not. Existing answers and comments were not shown to the labelers as our goals was to train the classifier on the questions only. The inclusion of answers and comments\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1378\" height=\"596\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.01.24-PM.png 1378w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.researchgate.net/profile/Martin-Kauschinger/publication/364165571_Detecting_Feature_Requests_of_Third-Party_Developers_through_Machine_Learning_A_Case_Study_of_the_SAP_Community/links/633d3f1676e39959d69f8513/Detecting-Feature-Requests-of-Third-Party-Developers-through-Machine-Learning-A-Case-Study-of-the-SAP-Community.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63b79b8bfcccab003d9cc62e","feature_image":"https://labelbox-research.ghost.io/content/images/2023/01/Screen-Shot-2023-01-05-at-8.03.34-PM.png","featured":false,"visibility":"public","created_at":"2023-01-06T03:54:51.000+00:00","updated_at":"2023-01-06T04:04:39.000+00:00","published_at":"2023-01-06T04:04:15.000+00:00","custom_excerpt":"Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/detecting-feature-requests-of-third-party-developers-through-machine-learning-a-case-study-of-the-sap-community/","excerpt":"Researchers from the Technical University of Munich, University of Innsbruck and SAP Deutschland set out to test whether the use of supervised machine learning models can be an effective means for the identification of feature requests.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63966b25136f7d003d87b29d","uuid":"eb20cc4a-5d82-447e-a492-e54912b2ca4a","title":"Real-time segmentation of desiccation cracks onboard UAVs for planetary exploration","slug":"real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures). \u003c/p\u003e\u003cp\u003eThe search for biosignatures on other planets focuses on identifying evidence of habitable environments, characterized by the presence or former-presence of water, and other factors that may have allowed organisms to grow and be preserved in the rock record. Mud cracks, or desiccations cracks, are a type of sedimentary structure associated with muddy environments where water has been present, such as dry lake beds.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Planetary surfaces are a primary focus of space exploration. Some of the most challenging current efforts in planetary exploration relate to the search for life, or biosignatures, in these environments. Detecting water-related textures, and thus evidence for potentially habitable environments, has the potential to focus and accelerate the search for biosignatures on other planets. Desiccation cracks are sedimentary features providing evidence of sediment-water interaction. \u003c/p\u003e\u003cp\u003eThey are known from both Earth and Mars, and are likely to be found via aerial exploration approaches of ancient lakes, rivers, or shallow marine environments where biosignatures may be found. Current approaches using image processing to detect desiccation cracks focus on segmenting just the cracks and prove somewhat successful. \u003c/p\u003e\u003cp\u003eHowever, the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection over much larger surface areas has not yet been explored.  The researchers advanced novel ways to develop and deploy of a desiccation crack detection system using UAVs and AI, leveraging new data collection techniques at varying heights above ground level and data-augmentation with a range of pixel-level and spatial transforms. Three state-of-the-art CNN segmentation networks are trained and evaluated using PyTorch. The networks are deployed on an edge-AI device integrated with a companion computer onboard a sub-2kg quadrotor UAV.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e Results indicate that the models can segment desiccation cracks on airborne collected images at various locations at heights ranging from 5 to 20m. Deployment of the models for real-time inference onboard small UAVs shows potential for application in the field. This research shows the feasibility of a low-volume data training to UAV deployment pipeline while highlighting potential hurdles in the processing pipeline for future efforts. They presented a system and architecture for onboard UAV detectors of sedimentary features, which can speed up the search for life on other planets.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1482\" height=\"678\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.49.15-PM.png 1482w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe images were all labeled for segmentation using the Labelbox. The image database was uploaded to Quantumworks Lab using the datasets creation feature. Labeling was conducted following a standardized labelling procedure developed during an initial exploratory labelling step to manage artifacts, ensure consistency and accelerate the process. L\u003c/p\u003e\u003cp\u003eLabeling a dataset can take a considerable amount of time. The process is prone to suffer from bias from human labelers. Labeling desiccation cracks requires the definition of clear boundaries. A single label was used (desiccation) for time optimization and simplification and a standardized labelling procedure was developed. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"676\" height=\"398\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png 600w, https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.48.44-PM.png 676w\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of a labeled desiccation crack texture using Quantumworks Lab overlaid in light blue\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.researchgate.net/publication/358402708_Real-time_Segmentation_of_Desiccation_Cracks_onboard_UAVs_for_Planetary_Exploration?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63966b25136f7d003d87b29d","feature_image":"https://labelbox-research.ghost.io/content/images/2022/12/Screen-Shot-2022-12-11-at-3.51.52-PM.png","featured":false,"visibility":"public","created_at":"2022-12-11T23:43:33.000+00:00","updated_at":"2024-01-25T22:36:35.000+00:00","published_at":"2022-12-11T23:52:12.000+00:00","custom_excerpt":"Researchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures).","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/real-time-segmentation-of-n-cracks-onboard-uavs-for-planetary-exploration/","excerpt":"Researchers from Queensland University of Technology studied the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection to look for habitable environments and traces of life (biosignatures).","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6384ec65a3ae7a003dfd4db2","uuid":"723c1b43-17d8-4ffa-b0c2-c54d13f4b7b9","title":"Fruit flower detection in apple orchards using ML","slug":"fruit-flower-detection-in-apple-orchards-using-ml","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the University of Guelph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Apples are typically a tree fruit crops grown worldwide with an estimated annual production of 124 million. To increase economic sustainability and compete globally, apple growers are strategically improving returns by adopting new cultivars, rootstocks and orchard management practices that improve fruit quality. \u003c/p\u003e\u003cp\u003eAdditionally, there is also an urgent need to reduce labour, which accounts for over 60% of production costs. The researchers presented the application of machine vision and learning techniques to detect and identify the number of flower clusters on apple trees leading to the ability to predict the potential yield of apples. The current issue in the agriculture field is that most of the machinery currently being used requires worker supervision. The inclusion of automated machinery can greatly increase the yield produced compared to manual labour and lessen the load required as it is a very labour-intensive job. The inclusion of automation can drastically increase the efficiency of the operation as well as the quality.\u003c/p\u003e\u003cp\u003eThe most challenging problem is dealing with occlusion as there are usually many objects in a dense location in an agricultural setting. Another important challenge that must be considered when designing an object detection system in agriculture is illumination. Due to most operations occurring in an outside environment, variance in natural lighting can greatly affect the performance of the object detection algorithm. Other challenges consist of similar appearance, such as shape and color, as well as multi-fruit detection cases. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1071\" height=\"1593\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image-3.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/11/image-3.png 1000w, https://labelbox-research.ghost.io/content/images/2022/11/image-3.png 1071w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eA mobile robot platform that was used for the data collection.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e A new field robot was designed and built to collect and build a dataset of 1500 images of apples trees. The trained model produced a cluster precision of 0.88 or 88% and a percentage error of 14% over 106 trees running the mobile vehicle on both sides of the trees. The detection model was predicting less than the actual amount but the fruit flower count is still significant in that it can give researchers information on the estimated growth and production of each tree with respect to the actions applied to each fruit tree. Their research helps lay the foundation for future application of machine vision and learning techniques within apple orchards or other fruit tree settings.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers collected and properly filtered the images and transferred them from a storage device to OneDrive for easy access on multiple platforms. The entire labelling process used Quantumworks Lab, which allowed the process to operate collaboratively working on labeling the dataset as there is no need in transferring and downloading large number of photos. The platform was also chosen because it keeps track of how many photos still need labelling and the estimated time it took per photo. The platform was also very easy to use as there were no other external setup needed other than an account to the service. \u003c/p\u003e\u003cp\u003eWith the help of hired undergraduate students at the University of Guelph, it took about a month to completely label 1499 photos. The photos consisted of a mixture of close-up and zoomed-out photos which involved classes, labelled as “Fruit Flower Single”, “Fruit Flower Cluster” and “Tree”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"550\" height=\"320\"\u003e\u003cfigcaption\u003eAn example of the training dataset.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.mdpi.com/2076-3417/12/22/11420?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6384ec65a3ae7a003dfd4db2","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-28-at-9.25.30-AM.png","featured":false,"visibility":"public","created_at":"2022-11-28T17:14:13.000+00:00","updated_at":"2022-12-06T21:29:44.000+00:00","published_at":"2022-11-28T17:26:19.000+00:00","custom_excerpt":"Researchers from the University of Guleph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/fruit-flower-detection-in-apple-orchards-using-ml/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/fruit-flower-detection-in-apple-orchards-using-ml/","excerpt":"Researchers from the University of Guleph recently demonstrated the effective use of machine vision and learning technologies to support the development of smart agriculture.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63731b8cf1a5bd003d56b1ee","uuid":"4b21fd7f-058b-408b-b990-b962c56c009c","title":"Automated recognition of  cricket batting techniques in videos using deep learning","slug":"automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the University of Johannesberg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge\u003c/strong\u003e: There have been limited studies demonstrating the validation of batting techniques in cricket using machine learning. Cricket batting technique is intricate because it involves a series of complex gestures needed to perform a stroke, one of these gestures performed by the batsman is referred to as the batting backlift technique (BBT).\u003c/p\u003e\u003cp\u003ePrevious research has indicated that the BBT can be seen as a contributing factor to successful batsmanship. There are two backlifts investigated in this study, namely the lateral batting backlift technique (LBBT), and the straight batting backlift technique (SBBT). The LBBT is a technique present where the toe and face of the bat are lifted laterally in the direction of second slip. The SBBT is represented whenever the toe and face of the bat are pointed toward the stumps and ground.\u003c/p\u003e\u003cp\u003eThe study demonstrated how the batting backlift technique in cricket can be automatically recognized in video footage and compares the performance of popular deep learning architectures, namely, AlexNet, Inception V3, Inception Resnet V2, and Xception. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eBuilding a unique dataset showing the lateral and straight backlift classes and assessed according to standard machine learning metrics, the researchers found that the architectures was comparable with similar performance with one false positive in the lateral class and a precision score of 100%, along with a recall score of 95%, and an f1-score of 98% for each architecture. \u003c/p\u003e\u003cp\u003eThe AlexNet architecture performed the worst out of the four architectures as it incorrectly classified four images that were supposed to be in the straight class. The architecture that is best suited for the problem domain was the Xception architecture with a loss of 0.03 and 98.2.5% accuracy, thus demonstrating its capability in differentiating between lateral and straight backlifts. \u003c/p\u003e\u003cp\u003eThe study provides a way forward in the automatic recognition of player patterns and motion capture, making it less challenging for sports scientists, biomechanists and video analysts working in the field.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eFor the construction of the dataset, the process went through a comprehensive YouTube search of First-Class International Cricket Test Match highlights, where the match’s environment has fewer variations to consider.\u003c/p\u003e\u003cp\u003eUsing the Quantumworks Lab platform, each object within a cricket scene is labeled, allowing for easier isolation and extraction of the batsman in each frame. The frame used for constructing the dataset was when the bowler was about to release the ball towards the batsman. \u003c/p\u003e\u003cp\u003eThe frame was identified as the ideal time period for the position of the batsman at the instant of delivery. Using an 80:20 data split, the training class had 160 images, and the testing class had 40 images, resulting in a total of 200 images, which served as a baseline to draw comparisons of the proposed architectures. The image aspect ratio chosen through testing and validation is 128×128, which is chosen to avoid distorting the original image.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.nature.com/articles/s41598-022-05966-6?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"63731b8cf1a5bd003d56b1ee","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-9.03.28-PM.png","featured":false,"visibility":"public","created_at":"2022-11-15T04:54:36.000+00:00","updated_at":"2022-12-06T21:29:33.000+00:00","published_at":"2022-11-15T05:04:13.000+00:00","custom_excerpt":"Researchers from the University of Johannesburg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/automated-recognition-of-cricket-batting-in-video-footage-using-deep-learning-architectures/","excerpt":"Researchers from the University of Johannesburg recently studied how complex batting techniques in cricket can be more efficiently analyzed using machine learning.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"637314caf1a5bd003d56b18b","uuid":"22517009-3c11-42c1-bb77-8a829d81aa80","title":"Better gait and posture classification using sensors in individuals with mobility impairment after a stroke","slug":"gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from the University of Zurich recently developed and validated a solution for better monitoring the gait and posture of individuals who have suffered a stroke via sensors. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge\u003c/strong\u003e: Stroke leads to motor impairment which reduces physical activity, negatively affecting social participation, and increasing the risk of secondary cardiovascular events. Continuous monitoring of physical activity with motion sensors is promising because it allows the prescription of tailored treatments in a timely manner. Accurate classification of gait activities and body posture is necessary to extract actionable information for outcome measures from unstructured motion data. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1320\" height=\"480\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-8.35.07-PM.png 1320w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eFive movement sensors were developed for research purposes and were attached with elastic straps, one on the dorsal side of the wrists, at the lateral malleolus of the ankles, and on the chest below the sternum.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eTheir method achieved enhanced performance when predicting real-life gait versus non-gait (Gait classification) with an accuracy between 85% and 93% across sensor configurations, using SVM and LR modeling. \u003c/p\u003e\u003cp\u003eOn the much more challenging task of discriminating between the body postures lying, sitting, and standing as well as walking, and stair ascent/descent (Gait and postures classification), their method achieved accuracies between 80% and 86% with at least one ankle and wrist sensor attached unilaterally. \u003c/p\u003e\u003cp\u003eThis research will hopefully prove useful resource to other researchers and clinicians in the increasingly important field of digital health in the form of remote movement monitoring using motion sensors.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eVideo and movement sensor data (locations: wrists, ankles, and chest) were collected from fourteen stroke survivors with motor impairment who performed real-life activities in their home environment. All video data was then labeled using the Quantumworks Lab platform for five classes of gait and body postures and three classes of transitions that served as ground truth. Afterwards, they trained support vector machine (SVM), logistic regression (LR), and k-nearest neighbor (kNN) models to identify gait bouts only or gait and posture. Model performance was assessed by and compared across five different sensor placement configurations.\u003c/p\u003e\u003cp\u003eVideo data was recorded with a frame-rate of 30 fps, whereas time-series from the IMU was collected with 50 fps. A single experimenter labeled videos in a frame-by-frame manner, and the labels were subsequently resampled to match the frequency of the synchronized IMUs. For quality assessment, a random sample of 33.3% of data was labeled by a second experimenter.\u003c/p\u003e\u003cp\u003eLabeling criteria were defined for start-to-end conditions of three body postures (lying down, sitting, standing) and two gait types (walking and stair ascent/descent). Additionally, they annotated three transition labels between two corresponding posture or gait types (lying down/sit, sit/stand, stand/walk) without specifying directionality (e.g., sit-to-stand or stand-tosit). This labeling framework resulted in a discrete label for every frame of the recording. \u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/576132/fphys-13-933987.pdf?sequence=3\u0026isAllowed=y\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"637314caf1a5bd003d56b18b","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-14-at-8.46.45-PM.png","featured":false,"visibility":"public","created_at":"2022-11-15T04:25:46.000+00:00","updated_at":"2022-12-06T21:29:54.000+00:00","published_at":"2022-11-15T04:39:59.000+00:00","custom_excerpt":"Researchers from the University of Zurich recently developed and validated a solution for better monitoring the gait and posture of individuals who have suffered a stroke via sensors.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/gait-and-posture-accuracy-classification-using-movement-sensors-in-individuals-with-mobility-impairment-after-stroke/","excerpt":"Researchers from the University of Zurich recently developed and validated a solution for better monitoring the gait and posture of individuals who have suffered a stroke via sensors.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63701b560d444a003deb9903","uuid":"ad11e726-fd5b-4e26-a5f0-8d2f08fe6d8a","title":"Generating integrated bill of materials using mask R-CNN models for construction","slug":"generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Pusan National University recently studied ways to utilize AI to identify concrete formworks. Concrete formworks are temporary structures that are used as a mold for concrete placement during construction. AI can be a practical solution that can overcome the difficulties associated with traditional bill of materials preparation and help automatically generate concrete formwork bill of materials (BoMs).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Concrete formwork is a crucial temporary structure for concrete placement not only in structural design but also in budget planning. Artificial intelligence (AI) possesses a great potential in planning and managing this temporary structure and can automate quantity take-off and cost estimation.\u003c/p\u003e\u003cp\u003eUsually, concrete formworks take a large portion of the concrete budget and needs correct quantity take-off and cost estimation. Formwork cost takes up to 15% of the total budget for the construction project and up to 33% of the total budget for the concrete structure. Miscalculation of quantity take-off and cost estimation could result in time delays and cost overruns. Therefore, the concrete formwork should be planned and managed well because it has a significant impact on the construction process.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"622\" height=\"632\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image-1.png 600w, https://labelbox-research.ghost.io/content/images/2022/11/image-1.png 622w\"\u003e\u003cfigcaption\u003eMask R-CNN performing image segmentation\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eThis study showed that integrating AI with a cost database for automatic concrete formwork BoM generation showed high potential. The model they built extracted formwork component objects including numerical dimensions from 2D formwork drawing images automatically. The Mask R-CNN technique was used for object recognition and extraction, while the OCR technique was used to quantify these objects' information to be used for automated BoM generation. A cost database was created based on the market price of formwork components. Then, the model generated a formwork integrated with the cost database, showing high precision in both object recognition and extraction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe labeling pipeline was performed using the Quantumworks Lab platform. Polygonal object masks were created around pipe supports, and rectangular object masks were created around sheathing, joists, struts, and dimensions. The labeling process provided boundaries of the object of interest in a 2D formwork drawing image. The labeled data was then received in JSON file format, with object masks and XML files for each of the labelled images. For each labeled image, separate image masks were created for the objects in the drawing images.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"800\" height=\"331\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image-2.png 600w, https://labelbox-research.ghost.io/content/images/2022/11/image-2.png 800w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eCreating object labels for formwork components inside of Quantumworks Lab\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eManual object segmentation was then done on the side view of a formwork, while object masks were used as binary images of 0 and 1, where 0 indicates background pixels and 1 object pixels. The objects masks include those for sheathing, joist, pipe support, strut and dimension. In total, 3353 labels are created for 186 training images, so total 3353 object masks were generated.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://reader.elsevier.com/reader/sd/pii/S0926580522005143?token=BE99CD73C129B70CE3DAF2720111CBC727801999A617C0D6C164B9DD894F20D4F3C737D23539474F2E84D71FB0E59B35\u0026originRegion=us-east-1\u0026originCreation=20221112222621\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63701b560d444a003deb9903","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-12-at-2.30.08-PM.png","featured":false,"visibility":"public","created_at":"2022-11-12T22:16:54.000+00:00","updated_at":"2022-12-06T21:30:08.000+00:00","published_at":"2022-11-12T22:31:50.000+00:00","custom_excerpt":"Researchers from Pusan National University studied how to utilize AI to identify and generate concrete formworks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/generating-integrated-bill-of-materials-using-mask-r-cnn-artificial-intelligence-model/","excerpt":"Researchers from Pusan National University studied how to utilize AI to identify and generate concrete formworks.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"636fb7750d444a003deb98a2","uuid":"f571d6de-5904-4156-8e19-6e12dec7cb3a","title":"An AI-based computer-aided system for better knee osteoarthritis assessment","slug":"an-ai-based-computer-aided-system-for-knee-osteoarthritis-assessment","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from the Medical University of Graz recently studied the impact of using an artificial intelligence (AI)-based computer system on the accuracy and agreement rate of board-certified orthopedic surgeons in order to detect X-ray features that can indicate knee OA (osteoarthritis assessment). The researchers focused on building a framework to compare the efficacy of unaided assessments versus the results to those of senior residents.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eEarly-stage OA signs are invisible on plain X-rays, as cartilage degeneration cannot be directly assessed, and OA constitutes a three-dimensional problem. This is reflected by fair to moderate interobserver reliability for knee OA assessment using X-rays alone. To overcome these issues, different solutions, including novel quantitative grading methods as well as automatic knee X-ray assessment tools, have been proposed. Artificial intelligence (AI) and deep learning have been used in medical image classification related to the musculoskeletal system. The researchers analyzed the intra- and interobserver reliability of board-certified orthopedic surgeons (also known as senior readers) for knee OA grade assessment using either AI-annotated or plain X-rays. Afterwards, they compared the outcome of senior readers to that of senior residents (termed junior readers) with aided analysis in terms of agreement rate and overall performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eThe use of AI-based software leads to improvement in the radiological judgement of senior orthopedic surgeons with regard to X-ray features indicative of knee OA and KL grade, as measured by the agreement rate and overall accuracy in comparison to the ground truth. The agreement and accuracy rates of senior readers were comparable to those of junior readers with aided analysis. Consequently, standard of care may be improved by the additional application of AI-based software in the radiological evaluation of knee OA.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe entire labelling process was divided into 3 steps. First, three senior readers were trained on the structure of the AI software report, OARSI grading system labelling process and the Quantumworks Lab platform was used. Second, readers assessed—unaided (i.e. without AI annotations)—124 plain knee X-rays and defined KL grade, osteophytes, sclerosis, and JSN by completing a list. The readers were able to work remotely at their preferred time and allowed to interrupt and resume labelling at any time, without time restrictions for labelling individual images or the entire dataset. However, the time it took readers to label each image, as well as the time of the entire labelling process. Third, after a minimum of 2 weeks after the second step had been completed, the same 124 knee X-rays were relabelled by the readers, with images provided at random order (to avoid creating observer bias. At this point, however, each image was supplemented with the AI software’s report together with a binary score of whether OA was present on the X-ray.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/11/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"685\" height=\"436\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/11/image.png 600w, https://labelbox-research.ghost.io/content/images/2022/11/image.png 685w\"\u003e\u003cfigcaption\u003eDiagram of the study’s labelling process\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://link.springer.com/content/pdf/10.1007/s00167-022-07220-y.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"636fb7750d444a003deb98a2","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/iStock-1142758724-1-e1590763901978.jpeg","featured":false,"visibility":"public","created_at":"2022-11-12T15:10:45.000+00:00","updated_at":"2022-12-06T21:30:18.000+00:00","published_at":"2022-11-12T15:24:31.000+00:00","custom_excerpt":"Researchers from the Medical University of Graz recently collaborated to analyze the impact of an artificial intelligence (AI)-based computer system on the accuracy and agreement rate of board-certified orthopedic surgeons to detect X-ray features indicative of knee OA (osteoarthritis assessment).","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/an-ai-based-computer-aided-system-for-knee-osteoarthritis-assessment/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/an-ai-based-computer-aided-system-for-knee-osteoarthritis-assessment/","excerpt":"Researchers from the Medical University of Graz recently collaborated to analyze the impact of an artificial intelligence (AI)-based computer system on the accuracy and agreement rate of board-certified orthopedic surgeons to detect X-ray features indicative of knee OA (osteoarthritis assessment).","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63641a4f0b3b41003d9e0bd1","uuid":"9a9ced15-5dae-4db6-9b66-8d7aa6e56ca1","title":"Lightweight multi-drone detection and 3D-localization via YOLO","slug":"lightweight-multi-drone-detection-and-3d-localization-via-yolo","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from Indian Institute of Technology Kanpur recently evaluated a method to perform real-time multiple drone detection and three-dimensional localization using state-of-the-art tiny-YOLOv4 object detection algorithm and stereo triangulation. They also released the source code for the project, with pre-trained models and the curated synthetic stereo dataset to further advance this research across the community.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eUnmanned Aerial Vehicles (UAVs) have gained massive popularity in recent years, owing to the advancements in technology and surge in the number of use cases for UAVs which include traffic management, security and surveillance, supply of essentials, disaster management, warehouse operations etc. Drones were initially a military, surveillance and security tool. Early versions of the drone were much larger, but as time progressed, they got smaller and smarter. \u003c/p\u003e\u003cp\u003eConsequently with the development of small and agile drones, their applications have time and again raised security concerns. Their increasing use in swarm systems have also sparked another research direction in dynamic detection and localization of multiple drones in such systems, especially for counter-drone systems. \u003c/p\u003e\u003cp\u003eProgressively, deep learning based solutions for detecting drones have improved at the task of object detection, but have also grown bulkier and have relied heavily on bulky computing power.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFinding: \u003c/strong\u003eTheir computer vision approach was able to eliminate the need for computationally expensive stereo matching algorithms, thereby significantly reducing the memory footprint and making it deployable on embedded systems. \u003c/p\u003e\u003cp\u003eTheir drone detection system was highly modular (with support for various detection algorithms) and capable of identifying multiple drones in a system, with real-time detection accuracy of up to 77% with an average FPS of 332 (on Nvidia Titan Xp). They also tested the complete pipeline in AirSim environment, detecting drones at a maximum distance of 8 meters, with a mean error of 23% of the distance. \u003c/p\u003e\u003cp\u003eThe researchers found that the modern, neural net based tiny-YOLO v4 algorithm attained higher frame rates and detection accuracy results than leading CPU based algorithms, and coupled with their classical stereo triangulation based depth estimation module, can be used for 3D localization.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used:\u003c/strong\u003e The Quantumworks Lab platform was used as the primary annotation tool of choice, enriching the researchers' dataset with images containing multiple drones. Other than the images of drones, the dataset also contains images of non-drone, drone-like “negative” objects, as to avoid their model from overfitting. The dataset contains 5529 images along with annotated files corresponding to each image, containing parameters of bounding box such as height, width, center x, y coordinates, and object class.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://arxiv.org/pdf/2202.09097.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"63641a4f0b3b41003d9e0bd1","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-03-at-12.54.00-PM.png","featured":false,"visibility":"public","created_at":"2022-11-03T19:45:19.000+00:00","updated_at":"2022-12-06T21:30:30.000+00:00","published_at":"2022-11-03T20:04:31.000+00:00","custom_excerpt":"Researchers from Indian Institute of Technology Kanpur recently evaluated a method to perform real-time multiple drone detection and three-dimensional localization using state-of-the-art tiny-YOLOv4 object detection algorithm and stereo triangulation.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/lightweight-multi-drone-detection-and-3d-localization-via-yolo/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/lightweight-multi-drone-detection-and-3d-localization-via-yolo/","excerpt":"Researchers from Indian Institute of Technology Kanpur recently evaluated a method to perform real-time multiple drone detection and three-dimensional localization using state-of-the-art tiny-YOLOv4 object detection algorithm and stereo triangulation.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"636418130b3b41003d9e0b8c","uuid":"28a1c8a7-8c71-4de7-a7e2-6980b362db5f","title":"Automated detection of malaria parasites using convolutional neural networks","slug":"automated-detection-of-malaria-parasites-using-convolutional-neural-networks","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from Imperial College London recently focused on developing an automated image analysis method to improve accuracy and standardization of smear inspection that retains capacity for expert confirmation and image archiving. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Microscopic examination of blood smears is known to be the gold standard for laboratory inspection and diagnosis of malaria. Smear inspection is, however, time-consuming and dependent on trained microscopists with results varying in accuracy. To advance this technique using convolution neural networks (CNNs), a machine learning method was developed to hone in on red blood cell (RBC) detection, differentiation between infected/uninfected cells, and parasite life stage categorization from unprocessed, heterogeneous smear images. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: Based on a pretrained Faster Region-Based Convolutional Neural Networks (R-CNN) model for RBC detection, their model performed accurately, with an average precision of 0.99 at an intersection-over-union threshold of 0.5. Application of a residual neural network-50 model to infected cells also performed accurately, with an area under the receiver operating characteristic curve of 0.98. \u003c/p\u003e\u003cp\u003eCombined with a mobile-friendly web-based interface that they built, called PlasmoCount, their ML method permits rapid navigation through and review of results for quality assurance. By standardizing assessment of Giemsa smears, their method markedly improves inspection reproducibility and presents a realistic route to both routine lab and future field-based automated malaria diagnosis. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers leveraged a \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-research.ghost.io\"\u003emodel-assisted approach\u003c/a\u003e for labeling using the Quantumworks Lab platform which greatly improved labeling speed and performance. \u003c/p\u003e\u003cp\u003eEach labeling round contained ~100 raw images of Giemsa smears. To aid the first labeling round, they trained their object detection model on a dataset of Plasmodium vivax-infected blood smears from the Broad Bioimage Benchmark Collection. Predictions on their P. falciparum dataset were then uploaded as prelabels using the Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/getting-started-with-the-python-sdk?ref=labelbox-research.ghost.io\"\u003ePython SDK\u003c/a\u003e. For each of the following labeling rounds, the RBC detection model and malaria identification classifier were trained on all the previous labeled datasets to generate new labels. \u003c/p\u003e\u003cp\u003eAnnotators could then correct, add, and delete bounding boxes around each RBC, and choose from three labels: infected, uninfected, and unsure. All images were labeled five times by five designated annotators from three different research centers for the test set.\u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8724263/?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"636418130b3b41003d9e0b8c","feature_image":"https://labelbox-research.ghost.io/content/images/2022/11/0179MarionsMalaria5.jpeg","featured":false,"visibility":"public","created_at":"2022-11-03T19:35:47.000+00:00","updated_at":"2022-12-06T21:30:43.000+00:00","published_at":"2022-11-03T19:42:17.000+00:00","custom_excerpt":"Researchers from Imperial College London recently focused on developing an automated image analysis method to improve accuracy and standardization of smear inspection that retains capacity for expert confirmation and image archiving. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automated-detection-of-malaria-parasites-using-convolutional-neural-networks/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/automated-detection-of-malaria-parasites-using-convolutional-neural-networks/","excerpt":"Researchers from Imperial College London recently focused on developing an automated image analysis method to improve accuracy and standardization of smear inspection that retains capacity for expert confirmation and image archiving. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635b479b99ae26003d07aeb5","uuid":"b24d95b3-cc4f-4e53-98a1-7b078a616511","title":"Automated segmentation of cervical anatomy to interrogate preterm birth","slug":"automated-segmentation-of-cervical-anatomy-to-interrogate-preterm-birth","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Columbia University are finding ways to develop a computational tool developed to better segment the entire cross-sectional area of the cervix tissue from 2D transvaginal ultrasounds, enabling the creation of a generalizable, cervical-features-based prediction model of PTB risk.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e A safe, full-term pregnancy is vital to the health and wellbeing of every child, yet it is far from guaranteed. Preterm birth (PTB) is the leading cause of perinatal death and remains a major global health concern. Due to limited pregnancy-related research, clinicians cannot fully explain what triggers healthy, gestationally-appropriate labor, let alone risky premature labor. This lack of fundamental understanding hinders the ability to predict PTB on an individual patient level. This work focuses on the cervix, a complex biomechanical barrier in pregnancy. Sonographic measurement of cervical length with transvaginal ultrasound (TVUS) is a common clinical test to assess the risk of subsequent PTB.\u003c/p\u003e\u003cp\u003eManual segmentation methods are usually time consuming, not scaleable, and can vary between clinicians and sonographers. Therefore, the researchers chose to adopt an automatic deep learning based multi-class residual UNet architecture segmentation method.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003e  The researchers demonstrated that the standard-of-care TVUS may be used to accurately segment cervical geometry, enabling the study of cervical variations across pregnancies with broader implications in understanding and ultimately preventing PTB. Given that accurate sonographic cervix segmentation followed by multi-dimensional cervical geometric feature extraction is can have a higher PTB predictive capability compared to sonographic cervical length alone. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used:\u003c/strong\u003e All data was annotated using Quantumworks Lab, with the model being trained on data from the Cervical Length Education and Review (CLEAR) program, which includes second and third trimester TVUS images from multiple sites and ultrasound systems across the United States.\u003c/p\u003e\u003cp\u003eRead the full paper \u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-031-17117-8_5?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635b479b99ae26003d07aeb5","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-8.17.12-PM.png","featured":false,"visibility":"public","created_at":"2022-10-28T03:08:11.000+00:00","updated_at":"2022-12-06T21:30:58.000+00:00","published_at":"2022-10-28T03:17:35.000+00:00","custom_excerpt":"Researchers from Columbia University are finding ways to develop a computational tool developed to better segment the entire cross-sectional area of the cervix tissue from 2D transvaginal ultrasounds, enabling the creation of a generalizable, cervical-features-based prediction model of PTB risk.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automated-segmentation-of-cervical-anatomy-to-interrogate-preterm-birth/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":null,"url":"https://labelbox-research.ghost.io/automated-segmentation-of-cervical-anatomy-to-interrogate-preterm-birth/","excerpt":"Researchers from Columbia University are finding ways to develop a computational tool developed to better segment the entire cross-sectional area of the cervix tissue from 2D transvaginal ultrasounds, enabling the creation of a generalizable, cervical-features-based prediction model of PTB risk.","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635b459399ae26003d07ae7a","uuid":"59073aae-19d2-4752-8438-3eca7000fac4","title":"Automatic recognition of emotional subgroups in images","slug":"automatic-recognition-of-emotional-subgroups-in-images","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eTracking individuals is not always the most efficient way of sensing emotions, especially in large crowds, where outputs would get cluttered. Additionally, an individual’s emotion can be better predicted when incorporating emotions from others in their social group, while at the same time people tend to be part of social groups that feel and act in a similar manner. Recognizing emotional subgroups is therefore a more efficient way of detecting emotion or behavior within a crowd. Simply combining the tasks of group and emotion recognition is not likely to suffice, since emotional subgroups can either split up or combine social groups complicating the task.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eImages that show agreement among annotators, are most often those that elicit the use of the summation strategy, while images with partial agreement more often elicit the use of the emotion-based fusion (putting more emphasis on emotion than social groups) or the group-based fusion (putting more emphasis on social groups than on emotion) strategy. Experimenting with different additional features suggests, with a modest performance improvement, that face size and gaze direction contain meaningful information. This shows that the task of emotional subgroup recognition is a complex one, but also that a relatively small feature vector is already able to reasonably represent human perception.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers used Quantumworks Lab to have human annotators label a set of 171 images, and their recognition strategies were analyzed. Three main strategies for labeling images are identified, with each strategy assigning either 1) more weight to emotions (emotion-based fusion), 2) more weight to spatial structures (groupbased fusion), or 3) equal weight to both (summation strategy). Based on these strategies, algorithms are developed to automatically recognize emotional subgroups. In particular, K-means and hierarchical clustering are used with location and emotion features derived from a fne-tuned VGG network. Additionally, they experimented with face size and gaze direction as extra input features and found that the best performance came from hierarchical clustering with emotion, location and gaze direction as input.\u003c/p\u003e\u003cp\u003e\u003cbr\u003eRead the full PDF \u003ca href=\"https://www.ijcai.org/proceedings/2022/0190.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635b459399ae26003d07ae7a","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-8.07.21-PM.png","featured":false,"visibility":"public","created_at":"2022-10-28T02:59:31.000+00:00","updated_at":"2022-12-06T21:31:21.000+00:00","published_at":"2022-10-28T03:07:50.000+00:00","custom_excerpt":"Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/automatic-recognition-of-emotional-subgroups-in-images/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/automatic-recognition-of-emotional-subgroups-in-images/","excerpt":"Researchers from Vrije Universiteit Amsterdam are advancing better ways to combine social group detection and group emotion recognition in images especially for use cases such as crowd surveillance or event analysis. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635b43bc99ae26003d07ae57","uuid":"1f4c6d83-b8f5-4c44-ac98-388d26b4e143","title":"A hierarchical optical coherence tomography annotation workflow with crowds and medical experts","slug":"a-hierarchical-optical-coherence-tomography-annotation-workflow-with-crowds-and-medical-experts","html":"\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eResearchers from Genentech are researching solutions to the heavy burden it takes to annotate clinical images due to the cost and availability of medical experts. To address this problem, they proposed a hierarchical annotation workflow in which medical experts review aggregated crowdsourced annotations, using dense annotation of optical coherence tomography (OCT) images from age-related macular degeneration (AMD) patients as an example.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eThe proposed hierarchical annotation workflow with crowds and medical experts could reduce the burden on medical experts in extensive clinical annotation tasks.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eAll annotation was performed on Quantumworks Lab which distributed images to remote crowds and medical experts. OCT B-scans with ≥ 9 averages from 20x20° volume scans of AMD patients were randomly selected. Two medical experts annotated 25 representative B-scans with rich pathology. In a training session, 27 labelers read through an annotation guideline and practiced on 15 of the B-scans. B-scans with color-coded agreements and disagreements were presented to the crowd, visualizing their discrepancy to the expert annotations.\u003c/p\u003e\u003cp\u003eRead the full paper \u003ca href=\"https://iovs.arvojournals.org/article.aspx?articleid=2783125\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635b43bc99ae26003d07ae57","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-7.57.46-PM.png","featured":false,"visibility":"public","created_at":"2022-10-28T02:51:40.000+00:00","updated_at":"2022-12-06T21:31:40.000+00:00","published_at":"2022-10-28T02:58:26.000+00:00","custom_excerpt":"Researchers from Genentech are researching solutions to the heavy burden it takes to annotate clinical images due to the cost and availability of medical experts.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/a-hierarchical-optical-coherence-tomography-annotation-workflow-with-crowds-and-medical-experts/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/a-hierarchical-optical-coherence-tomography-annotation-workflow-with-crowds-and-medical-experts/","excerpt":"Researchers from Genentech are researching solutions to the heavy burden it takes to annotate clinical images due to the cost and availability of medical experts.","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635ab9d199ae26003d07adea","uuid":"139ae20c-fa83-4721-9a27-147e14820cf8","title":"Evaluating terrain-dependent performance for martian frost detection in visible satellite observations","slug":"evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge:\u003c/strong\u003e Past studies have focused on manually analyzing the behavior of the frost cycle in the northern mid-latitude region of Mars using high-resolution visible observations from orbit. Extending these studies globally requires automating the detection of frost using data science techniques such as convolutional neural networks. However, visible indications of frost presence can vary significantly depending on the geologic context on which the frost is superimposed. \u003c/p\u003e\u003cp\u003eUnlike the Earth, the atmosphere of Mars is comprised primarily of carbon dioxide (CO2) and this volatile constitutes most of the frost, falling as snow or condensing at the surface due to surface temperatures falling to the CO2 frost point. A small amount of water frost will also form when temperatures are below the water (H2O) frost point, but only if the local concentration of H2O vapor in the atmosphere is high enough.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The research team from JPL presented a novel approach for spatially partitioning data to reduce biases in model performance estimation, illustrate how geologic context affects automated frost detection, and propose future work to further mitigate observed biases in automated frost detection work. \u003c/p\u003e\u003cp\u003eThey found that geologic context bias is present and significant for this model’s performance on the test set, specifically for dune fields often found in northern mid-latitude craters. Interestingly, for human annotators, dunes often provide strong evidence of frost due to the striking visual appearance of defrosting marks which expose dark basalt sand beneath light-color frost.\u003c/p\u003e\u003cp\u003eThey also saw a large degree of diversity in frost appearance on the underrepresented terrain type, both inherently and due to differing illumination and observational conditions, making the concept challenging for the classification model to learn.\u003c/p\u003e\u003cp\u003eFuture improvements will permit the training of models better suited for full-planet frost detection, thereby facilitating the creation of global frost maps\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eLabelbox was used to annotate polygonal boundaries around regions with visible evidence of frost. For each polygon, they collected additional information from the labeler including the applicable visible indicators as well as geologic context, which is either “dunes,” “gullies,” “crater rim/wall,” or “other.” The geologic context categories was mutually exclusive, so labelers could only pick one geologic context per frost polygon. \u003c/p\u003e\u003cp\u003eIn addition, they used the geologic context information to investigate terrain-dependent bias in classifier performance. To document the labeling process, they performed an iterative series of labeling sessions with both data science and science domain experts. \u003c/p\u003e\u003cp\u003eDomain expert labeling guidance and clarifications at each iteration were captured in a labeling guide, included with the publicly available dataset2. A total of 6 subframes, detailed in the released dataset, were excluded due to contamination with excess instrument noise or cloud cover. Each subframe was labeled by three different annotators.\u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://www.cs.emory.edu/~sgu33/workshop/DeepSpatial2022/papers/p4.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635ab9d199ae26003d07adea","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-10.09.34-AM.png","featured":false,"visibility":"public","created_at":"2022-10-27T17:03:13.000+00:00","updated_at":"2022-12-06T21:31:55.000+00:00","published_at":"2022-10-27T17:10:10.000+00:00","custom_excerpt":"Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/evaluating-terrain-dependent-performance-for-martian-frost-detection-in-visible-satellite-observations/","excerpt":"Researchers from Jet Propulsion Labs recently studied seasonal frosting and defrosting on the surface of Mars which is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635ab62c99ae26003d07adb0","uuid":"8426f3d6-2b81-4dbe-963a-866ba213f73b","title":"The impact of artificial intelligence assessment on diabetic retinopathy","slug":"impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the\u003cstrong\u003e \u003c/strong\u003eUniversity of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. Using validated AI algorithms instead of scarce trained specialists, this could potentially increase the efficiency and accessibility of screening programs. Systematic reviews of deep learning-based algorithms in DR screening have highlighted such advantages as reduction in demands for manpower, cost of screening, and intragrader and intergrader variability. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch: \u003c/strong\u003eThe growing burden of diabetes and its associated complications is increasing the demands on health care systems, particularly in low-resource countries. Their hypothesis was that adherence to referral services would be higher among patients randomized to receive AI-supported screening with immediate feedback compared with those randomized to receive delayed communication of results until after human grading was completed. Evidence from real-life screening programs is limited, and no evidence exists on community acceptance of the use of AI-supported DR screening.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: Immediate feedback on referral status based on AI-supported screening was associated with statistically significantly higher referral adherence compared with delayed communications of results from human graders. These results provide evidence for an important benefit of AI screening in promoting adherence to prescribed treatment for diabetic eye care in sub-Saharan Africa.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eAll images, anonymized with a unique patient registration number, were uploaded to Orbis International’s Cybersight AI. A mobile device or laptop and an internet connection are required to access Cybersight AI, which generates a response regarding the presence or absence of referable DR based on a macula-centered image from each available eye of a participant within 60 seconds. Afterwards, all images were uploaded to Quantumworks Lab for annotation and grading by a United Kingdom National Health System formally trained retinal specialist. \u003c/p\u003e\u003cp\u003eYou can read the full PDF \u003ca href=\"https://reader.elsevier.com/reader/sd/pii/S2666914522000574?token=871F5D7CE9870F35BC04F4954510A8796D2A713DA63A98B7B2FF383765C5642101B2C77F003218EC780639144DEF7323\u0026originRegion=us-east-1\u0026originCreation=20221027165201\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635ab62c99ae26003d07adb0","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-9.51.43-AM.png","featured":false,"visibility":"public","created_at":"2022-10-27T16:47:40.000+00:00","updated_at":"2022-12-06T21:32:07.000+00:00","published_at":"2022-10-27T16:54:48.000+00:00","custom_excerpt":"Researchers from the University of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/impact-of-artificial-intelligence-assessment-of-diabetic-retinopathy/","excerpt":"Researchers from the University of Rwanda studied the recent advances in computer-based analysis using artificial intelligence (AI), which present a promising opportunity to test and refine automatic grading of diabetic retinal images for screening. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635ab40899ae26003d07ad80","uuid":"12c85e7e-2368-4da3-ac77-8ceae25c1e7c","title":"Baby physical safety monitoring in smart home using action recognition system","slug":"baby-physical-safety-monitoring-in-smart-home-using-action-recognition-system","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from the University of Cincinnati recently studied advances in action recognition, as humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. This is because the brain operates on a bidirectional communication model, which has radically improved the accuracy of recognition and prediction based on features connected to previous experiences. During the past decade, deep learning models for action recognition have significantly improved. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch Challenge\u003c/strong\u003e: Deep neural networks typically struggle with these tasks on a smaller dataset for specific Action Recognition (AR) tasks. As with most action recognition tasks, the ambiguity of accurately describing activities in spatial-temporal data is a drawback that can be overcome by curating suitable datasets, including careful annotations and preprocessing of video data for analyzing various recognition tasks. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: The researchers presented a novel lightweight framework combining transfer learning techniques with a Conv2D LSTM layer to extract features from the pre-trained I3D model on the Kinetics dataset for a new AR task (Smart Baby Care) that requires a smaller dataset and less computational resources. Furthermore, they developed a benchmark dataset and an automated model that uses LSTM convolution with I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in a smart baby room. By implementing video augmentation to improve model performance on the smart baby care task and comparing it to to other benchmark models, their experimental framework achieved better performance with less computational resources.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eLabelbox was used to create a curated dataset from scratch for this project to develop an AR model for a specific Action Recognition task (Smart Baby Care). Our data source came from an open-source video from social media platforms such as (YouTube, Instagram, Pexels, etc.). The videos were manually downloaded and then trimmed with a python script using the inbuilt FF-MPEG library and annotated frame-wise using Labelbox. \u003c/p\u003e","comment_id":"635ab40899ae26003d07ad80","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-27-at-9.44.00-AM.png","featured":false,"visibility":"public","created_at":"2022-10-27T16:38:32.000+00:00","updated_at":"2022-12-06T21:32:23.000+00:00","published_at":"2022-10-27T16:44:41.000+00:00","custom_excerpt":"Researchers from the University of Cincinnati recently studied advances in action recognition, as humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/baby-physical-safety-monitoring-in-smart-home-using-action-recognition-system/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/baby-physical-safety-monitoring-in-smart-home-using-action-recognition-system/","excerpt":"Researchers from the University of Cincinnati recently studied advances in action recognition, as humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635a067799ae26003d07ad35","uuid":"80084265-efe2-45ec-8ca2-a854ad139075","title":"Deep learning for live cell shape detection","slug":"deep-learning-for-live-cell-shape-detection","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging. AFM is used as mechanical characterization in a wide range of samples, including live cells, proteins, and other biomolecules. It is also instrumental for measuring interaction forces and binding kinetics for protein–protein or receptor–ligand interactions on live cells at a single-molecule level. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eThe difficulty lies in performing force measurements and high-resolution imaging with AFM and data analytics because it is time-consuming and require special skill sets and continuous human supervision. Recently, researchers have explored the applications of using artificial intelligence (AI) and deep learning (DL) in the bioimaging field. However, the applications of AI to AFM operations for live-cell characterization are little-known until now. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The researchers implemented a deep learning framework to perform automatic sample selection based on the cell shape for AFM probe navigation during AFM biomechanical mapping. They established a closed-loop scanner trajectory control for measuring multiple cell samples at high speed for automated navigation. With this, they achieved a 60× speed-up in AFM navigation and reduced the time involved in searching for the particular cell shape in a large sample. Their innovation directly applies to many bio-AFM applications with AI-guided intelligent automation through image data analysis together with smart navigation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1328\" height=\"412\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1328w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers leveraged Quantumworks Lab to label their data and enabled experts to annotate the cell shape by drawing bounding boxes around it and labeling it with an accurate shape. Collecting these images was time-consuming and tedious as the user had to manually scan the cell samples and capture the images. In addition, performing the annotations, especially on low-quality images, was a painstaking task, leading to a smaller dataset with fewer annotated images. To address this challenge, they implemented data augmentation techniques on the fly (during training), which involved rotating the original images by 90◦ clockwise or counter-clockwise, by 180◦ , flipping them upside down, and by left-right mirroring. This enhanced the original dataset with more data samples with different orientations, which further made the DL network robust to the variety of cell shape orientations encountered during inference.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1246\" height=\"478\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1246w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eVisualizing the predictions on low-quality images. Target/ground truth images are shown in the top row and the corresponding predictions in the bottom row.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://www.mdpi.com/2306-5354/9/10/522/pdf?version=1665735868\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635a067799ae26003d07ad35","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.35.32-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T04:17:59.000+00:00","updated_at":"2022-12-06T21:32:36.000+00:00","published_at":"2022-10-27T04:35:55.000+00:00","custom_excerpt":"Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/deep-learning-for-live-cell-shape-detection/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/deep-learning-for-live-cell-shape-detection/","excerpt":"Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6359fb7199ae26003d07acfe","uuid":"5bdfd1b2-714c-4783-9060-d7a77c6cc248","title":"HyperionSolarNet: Solar panel detection from aerial images","slug":"solar-panel-detection-from-aerial-images","html":"\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003e With the effects of global climate change impacting the world, collective efforts are needed to reduce greenhouse gas emissions. The energy sector continues to be the single largest contributor to climate change and many efforts are focused on reducing dependence on carbon-emitting power plants and moving to renewable energy sources, such as solar power.\u003c/p\u003e\u003cp\u003eTheir work focused on creating a world map of solar panels, identifying locations and total surface area of solar panels within a given geographic area. The researchers used deep learning methods for automated detection of solar panel locations and their surface area using aerial imagery. The framework, which consisted of a two-branch model using an image classifier in tandem with a semantic segmentation model, was trained on a created dataset of satellite images.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings\u003c/strong\u003e: Their work provided an efficient and scalable method for detecting solar panels, achieving an accuracy of 0.96 for classification and an IoU score of 0.82 for segmentation performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used\u003c/strong\u003e: The research team annotated 836 images containing solar panels using the Quantumworks Lab platform, and produced corresponding segmentation masks, resizing all images to a size of 512x512 pixels for training and testing. The researchers manually annotated these images using Quantumworks Lab and created mask labels for them, afterwards, evaluating the HyperionSolarNet segmentation model against these test set images and finding  an IoU score of 0.82. \u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://s3.us-east-1.amazonaws.com/climate-change-ai/papers/neurips2021/41/paper.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6359fb7199ae26003d07acfe","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-8.37.39-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T03:30:57.000+00:00","updated_at":"2022-12-06T21:32:57.000+00:00","published_at":"2022-10-27T03:38:47.000+00:00","custom_excerpt":"Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/solar-panel-detection-from-aerial-images/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/solar-panel-detection-from-aerial-images/","excerpt":"Researchers from UC Berkeley set out to create a comprehensive database for locating solar panels given the importance of being able to assist analysts and policymakers on defining strategies for further expansion of solar energy. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6359f4bf99ae26003d07acb9","uuid":"5f42e2ff-655d-430a-abc1-b64636796925","title":"Multi-view deep learning for reliable post-disaster damage classification","slug":"multi-view-deep-learning-for-reliable-post-disaster-damage-classification","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eResearchers from Texas A\u0026amp;M University, recently researched ways to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch challenge\u003c/strong\u003e: In their research, the current practices and research efforts in adopting AI for post-disaster damage assessment are generally (a) qualitative, lacking refined classification of building damage levels based on standard damage scales, and (b) trained based on aerial or satellite imagery with limited views, which, although indicative, are not completely descriptive of the damage scale. \u003c/p\u003e\u003cp\u003eTo enable more accurate and reliable automated quantification of damage levels, the present study proposes the use of more comprehensive visual data in the form of multiple ground and aerial views of the buildings. To have such a spatially-aware damage prediction model, a Multi-view Convolution Neural Network (MV-CNN) architecture, the researchers used a novel approach that combined the information from different views of a damaged building. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings: \u003c/strong\u003eThis spatial 3D context damage information will result in more accurate identification of damages and reliable quantification of damage levels. The proposed model was trained and validated on reconnaissance visual dataset containing expert labeled, geotagged images of the inspected buildings following hurricane Harvey. \u003c/p\u003e\u003cp\u003eTheir research team developed a model demonstrates reasonably good accuracy in predicting the damage levels and can be used to support more informed and reliable AI-assisted disaster management practices.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used:\u003c/strong\u003e The model was trained using pixel level annotations of the buildings using the Quantumworks Lab platform. The idea was to recognize the pixels belonging to buildings in an image and filter out the visual information unnecessary for determining the damage state of the building like trees, sky, roads, etc. Data annotation was also performed using Quantumworks Lab, and in total, images corresponding to 400 buildings (2000 images) were annotated and assigned labels representing their damage state. The damage labels (0 to 5) were based on the expert assessment reported in the original database. The entire annotated dataset was split into training (80%), validation (10%), and testing sets (10%).\u003c/p\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://arxiv.org/pdf/2208.03419.pdf?ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6359f4bf99ae26003d07acb9","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-8.11.08-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T03:02:23.000+00:00","updated_at":"2022-12-06T21:33:10.000+00:00","published_at":"2022-10-27T03:14:23.000+00:00","custom_excerpt":"Researchers from Texas A\u0026M University, recently researched ways to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/multi-view-deep-learning-for-reliable-post-disaster-damage-classification/","authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/multi-view-deep-learning-for-reliable-post-disaster-damage-classification/","excerpt":"Researchers from Texas A\u0026M University, recently researched ways to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635b05ac99ae26003d07ae3f","uuid":"2ea4158c-8aff-42c6-af3c-1dfcff5147a1","title":"CS230, Stanford","slug":"cs230-stanford","html":"\u003cp\u003eLabelbox has been heavily used by the students enrolled in CS230\u003c/p\u003e\u003cfigure class=\"kg-card kg-bookmark-card\"\u003e\u003ca class=\"kg-bookmark-container\" href=\"https://cs230.stanford.edu/?ref=labelbox-research.ghost.io\"\u003e\u003cdiv class=\"kg-bookmark-content\"\u003e\u003cdiv class=\"kg-bookmark-title\"\u003eCS230 Deep Learning\u003c/div\u003e\u003cdiv class=\"kg-bookmark-description\"\u003eDeep Learning is one of the most highly sought after skills in AI. In this course, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Drop…\u003c/div\u003e\u003cdiv class=\"kg-bookmark-metadata\"\u003e\u003cimg class=\"kg-bookmark-icon\" src=\"https://cs230.stanford.edu/favicon.ico\" alt=\"\"\u003e\u003cspan class=\"kg-bookmark-publisher\"\u003eAndrew NgInstructor\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"kg-bookmark-thumbnail\"\u003e\u003cimg src=\"https://cs230.stanford.edu/doks-theme/assets/images/layout/logo.png\" alt=\"\"\u003e\u003c/div\u003e\u003c/a\u003e\u003c/figure\u003e","comment_id":"635b05ac99ae26003d07ae3f","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Coursework-1.png","featured":false,"visibility":"public","created_at":"2022-10-27T22:26:52.000+00:00","updated_at":"2022-12-06T21:33:22.000+00:00","published_at":"2022-10-24T22:27:00.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/cs230-stanford/","authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/manu/"}],"tags":[{"id":"635ac77499ae26003d07ae2e","name":"Coursework","slug":"coursework","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/coursework/"},{"id":"635ac79199ae26003d07ae32","name":"natural-language","slug":"natural-language","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/natural-language/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/manu/"},"primary_tag":{"id":"635ac77499ae26003d07ae2e","name":"Coursework","slug":"coursework","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/coursework/"},"url":"https://labelbox-research.ghost.io/cs230-stanford/","excerpt":"Quantumworks Lab has been heavily used by the students enrolled in CS230\n\nCS230 Deep LearningDeep Learning is one of the most highly sought after skills in AI. In this course, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Drop…Andrew NgInstructor","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"featured":null,"currentPage":1,"url":"/research"},"__N_SSG":true},"page":"/research/page/[id]","query":{"id":"1"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/research/page/1/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:37:08 GMT -->
</html>