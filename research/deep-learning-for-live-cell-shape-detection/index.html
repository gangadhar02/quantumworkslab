<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/research/deep-learning-for-live-cell-shape-detection/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:36:53 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Deep learning for live cell shape detection</title><meta name="description" data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Deep learning for live cell shape detection" data-next-head=""/><meta property="og:description" data-next-head=""/><meta property="og:url" content="https://labelbox-research.ghost.io/deep-learning-for-live-cell-shape-detection/" data-next-head=""/><meta property="og:image" content="https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.35.32-PM.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Deep learning for live cell shape detection" data-next-head=""/><meta name="twitter:description" data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox-research.ghost.io/deep-learning-for-live-cell-shape-detection/" data-next-head=""/><meta property="twitter:image" content="https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.35.32-PM.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g34[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.cmbwoS .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.cmbwoS .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.cmbwoS .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:2px;}/*!sc*/
@media only screen and (min-width:48rem){.cmbwoS .content h2{padding-top:10px;}}/*!sc*/
.cmbwoS .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.cmbwoS .content h3{padding-top:10px;}}/*!sc*/
.cmbwoS .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.cmbwoS .content a:hover{color:#1e40af;}/*!sc*/
.cmbwoS .content li{margin-bottom:20px;}/*!sc*/
.cmbwoS .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.cmbwoS .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.cmbwoS .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.cmbwoS .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.cmbwoS .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.cmbwoS .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}/*!sc*/
.cmbwoS .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.cmbwoS .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100% !important;height:100% !important;margin:0 auto;}/*!sc*/
.cmbwoS .content .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;}/*!sc*/
.cmbwoS .content .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;}/*!sc*/
.cmbwoS .content .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 0 0 0.75em;}/*!sc*/
.cmbwoS .content .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;}/*!sc*/
data-styled.g80[id="id__PostContentWrapper-sc-obz6gi-0"]{content:"cmbwoS,"}/*!sc*/
.ekqZbR #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.ekqZbR .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.ekqZbR .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.ekqZbR #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.ekqZbR #image-viewer .close:hover,.ekqZbR #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.ekqZbR .modal-content{width:100%;}}/*!sc*/
data-styled.g81[id="id__ImageModal-sc-obz6gi-1"]{content:"ekqZbR,"}/*!sc*/
@media (max-width:1026px){.iNvWif.toc-container{display:none;}}/*!sc*/
.iNvWif.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.iNvWif.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.iNvWif.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.iNvWif.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.iNvWif.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.iNvWif.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.iNvWif.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.iNvWif.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g82[id="id__TocContainer-sc-obz6gi-2"]{content:"iNvWif,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/research/%5bid%5d-0635c3e5cef2ea5d.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="id__ImageModal-sc-obz6gi-1 ekqZbR"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All posts</a><main class="id__TocContainer-sc-obz6gi-2 iNvWif toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><h1 class="mt-4 text-2xl md:text-5xl font-bold max-w-3xl mb-12">Deep learning for live cell shape detection</h1></div><img class="img-fluid rounded-lg mx-auto" src="../../../labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.35.32-PM.png"/></div><main class="id__PostContentWrapper-sc-obz6gi-0 cmbwoS md:px-24"><div class="content js-toc-content"><p><strong>Summary:</strong> Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging. AFM is used as mechanical characterization in a wide range of samples, including live cells, proteins, and other biomolecules. It is also instrumental for measuring interaction forces and binding kinetics for protein–protein or receptor–ligand interactions on live cells at a single-molecule level. </p><p><strong>Challenge: </strong>The difficulty lies in performing force measurements and high-resolution imaging with AFM and data analytics because it is time-consuming and require special skill sets and continuous human supervision. Recently, researchers have explored the applications of using artificial intelligence (AI) and deep learning (DL) in the bioimaging field. However, the applications of AI to AFM operations for live-cell characterization are little-known until now. </p><p><strong>Findings:</strong> The researchers implemented a deep learning framework to perform automatic sample selection based on the cell shape for AFM probe navigation during AFM biomechanical mapping. They established a closed-loop scanner trajectory control for measuring multiple cell samples at high speed for automated navigation. With this, they achieved a 60× speed-up in AFM navigation and reduced the time involved in searching for the particular cell shape in a large sample. Their innovation directly applies to many bio-AFM applications with AI-guided intelligent automation through image data analysis together with smart navigation.</p><figure class="kg-card kg-image-card"><img src="../../../labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png" class="kg-image" alt loading="lazy" width="1328" height="412" srcset="https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1328w" sizes="(min-width: 720px) 720px"></figure><p><strong>How Quantumworks Lab was used: </strong>The researchers leveraged Quantumworks Lab to label their data and enabled experts to annotate the cell shape by drawing bounding boxes around it and labeling it with an accurate shape. Collecting these images was time-consuming and tedious as the user had to manually scan the cell samples and capture the images. In addition, performing the annotations, especially on low-quality images, was a painstaking task, leading to a smaller dataset with fewer annotated images. To address this challenge, they implemented data augmentation techniques on the fly (during training), which involved rotating the original images by 90◦ clockwise or counter-clockwise, by 180◦ , flipping them upside down, and by left-right mirroring. This enhanced the original dataset with more data samples with different orientations, which further made the DL network robust to the variety of cell shape orientations encountered during inference.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png" class="kg-image" alt loading="lazy" width="1246" height="478" srcset="https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1246w" sizes="(min-width: 720px) 720px"><figcaption>Visualizing the predictions on low-quality images. Target/ground truth images are shown in the top row and the corresponding predictions in the bottom row.</figcaption></figure><p>Read the full PDF <a href="https://www.mdpi.com/2306-5354/9/10/522/pdf?version=1665735868&amp;ref=labelbox-research.ghost.io">here</a>.</p></div></main></div></div></div><div class="mt-5 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="my-20 w-full h-[1px] bg-neutral-200"></div><div class=""><div class=""><h2 class="mb-12 text-center text-3xl md:text-4xl font-medium">Continue reading</h2></div><div class="flex flex-wrap justify-content-center"><div class="max-w-md mx-2 "><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../ai-guided-defect-detection/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/indexc0cb.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2FAI-Guided-Defect-Detection-Techniques.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../ai-guided-defect-detection/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Quantumworks Lab services helps research team explore AI-guided defect detection techniques</p><p class="text-base max-w-2xl undefined line-clamp-3">At Quantumworks Lab, we&#x27;re passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we&#x27;re excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.

These posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we&#x27;re exploring AI-Guided </p></a></div></div></div></div></div><div class="max-w-md mx-2 "><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../a-benchmark-for-long-form-medical-question-answering/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index9fe7.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2FScreenshot-2024-11-26-at-3.53.21-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../a-benchmark-for-long-form-medical-question-answering/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">A benchmark for long-form medical question answering</p><p class="text-base max-w-2xl undefined line-clamp-3">Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. </p></a></div></div></div></div></div><div class="max-w-md mx-2 "><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=3840&amp;q=70 3840w" src="../../_next/image/indexcd10.html?url=https%3A%2F%2Flabelbox-research.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdownload.jpeg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Identifying and counting avian blood cells in whole slide images via deep learning</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.</p></a></div></div></div></div></div></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><span style="color: inherit; cursor: default;">Start for free</span></section></div>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><span style="color: inherit; cursor: default;">Docs</span></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <span style="color: inherit; cursor: default;">Terms of Service</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Privacy Notice</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Copyright Dispute Policy</span>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"deep-learning-for-live-cell-shape-detection","id":"635a067799ae26003d07ad35","uuid":"80084265-efe2-45ec-8ca2-a854ad139075","title":"Deep learning for live cell shape detection","html":"\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging. AFM is used as mechanical characterization in a wide range of samples, including live cells, proteins, and other biomolecules. It is also instrumental for measuring interaction forces and binding kinetics for protein–protein or receptor–ligand interactions on live cells at a single-molecule level. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eThe difficulty lies in performing force measurements and high-resolution imaging with AFM and data analytics because it is time-consuming and require special skill sets and continuous human supervision. Recently, researchers have explored the applications of using artificial intelligence (AI) and deep learning (DL) in the bioimaging field. However, the applications of AI to AFM operations for live-cell characterization are little-known until now. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The researchers implemented a deep learning framework to perform automatic sample selection based on the cell shape for AFM probe navigation during AFM biomechanical mapping. They established a closed-loop scanner trajectory control for measuring multiple cell samples at high speed for automated navigation. With this, they achieved a 60× speed-up in AFM navigation and reduced the time involved in searching for the particular cell shape in a large sample. Their innovation directly applies to many bio-AFM applications with AI-guided intelligent automation through image data analysis together with smart navigation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1328\" height=\"412\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.29.09-PM.png 1328w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eThe researchers leveraged Quantumworks Lab to label their data and enabled experts to annotate the cell shape by drawing bounding boxes around it and labeling it with an accurate shape. Collecting these images was time-consuming and tedious as the user had to manually scan the cell samples and capture the images. In addition, performing the annotations, especially on low-quality images, was a painstaking task, leading to a smaller dataset with fewer annotated images. To address this challenge, they implemented data augmentation techniques on the fly (during training), which involved rotating the original images by 90◦ clockwise or counter-clockwise, by 180◦ , flipping them upside down, and by left-right mirroring. This enhanced the original dataset with more data samples with different orientations, which further made the DL network robust to the variety of cell shape orientations encountered during inference.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1246\" height=\"478\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.30.11-PM.png 1246w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003eVisualizing the predictions on low-quality images. Target/ground truth images are shown in the top row and the corresponding predictions in the bottom row.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRead the full PDF \u003ca href=\"https://www.mdpi.com/2306-5354/9/10/522/pdf?version=1665735868\u0026ref=labelbox-research.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"635a067799ae26003d07ad35","feature_image":"https://labelbox-research.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-9.35.32-PM.png","featured":false,"visibility":"public","created_at":"2022-10-27T04:17:59.000+00:00","updated_at":"2022-12-06T21:32:36.000+00:00","published_at":"2022-10-27T04:35:55.000+00:00","custom_excerpt":"Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/research/deep-learning-for-live-cell-shape-detection/","tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},{"id":"635ac78099ae26003d07ae30","name":"computer-vision","slug":"computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/computer-vision/"}],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/deep-learning-for-live-cell-shape-detection/","excerpt":"Researchers from Iowa State University were looking to advance atomic force microscopy (AFM) in order to provide a platform for high-resolution topographical imaging.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},"recommended":[{"id":"67be4062651e6b0001cd2bdf","uuid":"ef0d7946-0433-4386-bc84-c788c313d47e","title":"Quantumworks Lab services helps research team explore AI-guided defect detection techniques","slug":"ai-guided-defect-detection","html":"\u003cp\u003eAt Quantumworks Lab, we're passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we're excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.\u003c/p\u003e\u003cp\u003eThese posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we're exploring \u003ca href=\"https://arxiv.org/pdf/2404.07306?ref=labelbox-research.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAI-Guided Defect Detection Techniques to Model Single Crystal Diamond Growth\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003ea study by Rohan Reddy Mekala, Elias Garratt, Matthias Muehle, Arjun Srinivasan, Adam Porter, and Mikael Lindvall.\u003c/p\u003e\u003ch2 id=\"research-introduction\"\u003e\u003cstrong\u003eResearch introduction\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eResearchers recently identified several challenges in producing high-quality single crystal diamonds (SCDs) at scale. Despite extensive development efforts, current manufacturing relies on trial-and-error experimentation, leading to inconsistent results and defects. In order to address these issues, researchers across the United States collaborated to propose new methodologies using machine learning and AI models to predict future diamond growth states for accelerated material development, improved quality, and larger sizes.\u003c/p\u003e\u003cp\u003eThe research team focused on these challenges:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLack of predictability:\u003c/strong\u003e Existing techniques currently lack the ability to predict diamond growth states, hindering process control and the potential for correction.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIssues with scale:\u003c/strong\u003e Diamond is an essential material for tools in power electronics, health sciences, and engineering, but each field has different requirements in terms of quality, purity, and size. The inability to predict growth subsequently makes it difficult to scale production.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIneffective research methodology:\u003c/strong\u003e Current research methods on developing a method to sustainably and reliably produce high quality diamonds rely on a trial and error method, leading to inconsistent results and defects from each methodology.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom these challenges, researchers identified an opportunity in using machine learning and deep learning algorithms to predict future diamond growth states to shorten the growth cycle, improve prediction accuracy, and enhance crystalline quality.\u003c/p\u003e\u003ch2 id=\"how-labelbox-was-used\"\u003e\u003cstrong\u003eHow Quantumworks Lab was used\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eLabelbox services was used to build the dataset for object detection of the diamond growth-run images. The process began with an initial batch of 100 images, which were reviewed by a team of three material scientists and 15 Alignerrs that were vetted and onboarded through Quantumworks Lab Labeling Services.. Alignerrs provide expert, on-demand annotation services and are selected from a network of trained professionals across diverse data domains.\u003c/p\u003e\u003cp\u003eThe scientists provided detailed instructions, including explanatory videos and meetings, to guide the labeling process. The labels were then reviewed by the Alignerrs, where predominant occurrences were identified based on a consensus score. Afterward, the material scientists conducted a final review to ensure the accuracy, consistency, and integrity of the data.\u003c/p\u003e\u003ch2 id=\"analysis-and-model-assisted-labeling\"\u003e\u003cstrong\u003eAnalysis and model-assisted labeling\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-research.ghost.io\"\u003e\u003cu\u003eModel-assisted labeling\u003c/u\u003e\u003c/a\u003e (MAL) was implemented in order to improve the consistency of data labeling and address potential variability in interpretation of instructions amongst labelers. This involved training a baseline model incrementally using iterative image-annotation pairs from the initial batch. This baseline model was then overlaid on subsequent batches to assist Alignerrs, significantly reducing labeling time and improving accuracy.\u003c/p\u003e\u003cp\u003eWith MAL, the time to label each image decreased from 15 minutes to just 2 minutes. Alignerrs were able to correct and refine annotation based on the baseline model’s predictions until a segmentation accuracy threshold of 80% was achieved. Once this threshold was achieved, the model’s overlays were used as a starting point for future batches.\u003c/p\u003e\u003cp\u003eAfter the minimum number of images were processed, the final set of image-label pairs was passed on for further research and development of the final semantic segmentation and object detection models. Using MAL and this human-in-the-loop workflow, this streamlined annotation process leads to improved efficiency and label quality.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeGAy55bE9C_7cOm9nsW-H0IraNnL1CMua-RCDhe88CKCtuOqn0BTPC4wr0gSgZ5HKrxTsNRkbrOtuMF7yQ5QFYvXGaexfSPvfZJFjHT5C4IpKxXHYkaI-YiUNq0z4eP8yyd2Pg?key=cmC32LkeleH-01xeZ1gdpicE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"424\"\u003e\u003c/figure\u003e\u003ch2 id=\"key-findings\"\u003e\u003cstrong\u003eKey findings\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eUsing object detection and image segmentation algorithms along with the support of a highly-skilled team of Quantumworks Lab Alignerrs, researchers were able to create a defect detection pipeline for diamond growth data. This pipeline achieved a high defect classification accuracy with 93.35% for Center-Defects, 92.83% for Poly-crystalline-Defects, and 91.98% for Edge-Defects.\u003c/p\u003e\u003cp\u003eThe authors were able to use this pipeline to accurately detect defects, reduce time and costs, and predict future diamond growth.\u003c/p\u003e\u003cp\u003eWorking on your own research? Reach out to the team at research@labelbox.com to request a research license or to share your AI research with us.\u0026nbsp;\u003c/p\u003e","comment_id":"67be4062651e6b0001cd2bdf","feature_image":"https://labelbox-research.ghost.io/content/images/2025/02/AI-Guided-Defect-Detection-Techniques.png","featured":false,"visibility":"public","created_at":"2025-02-25T22:12:50.000+00:00","updated_at":"2025-02-25T22:45:46.000+00:00","published_at":"2025-02-25T22:45:46.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"67be114424e4bd0001a29369","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/lisa/"}],"primary_author":{"id":"67be114424e4bd0001a29369","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/lisa/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/ai-guided-defect-detection/","excerpt":"At Quantumworks Lab, we're passionate about empowering the AI community, especially universities and cutting-edge research teams. While we often share updates on our platform and customer stories, we're excited to introduce a new series of blog posts highlighting fascinating research projects leveraging Labelbox.\n\nThese posts will delve into how researchers are using Quantumworks Lab—both our software and services—to tackle complex data challenges and push the boundaries of AI. Today, we're exploring AI-Guided ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How Quantumworks Lab services helps explore AI-guided defect detection techniques ","meta_description":"Learn how researchers use Quantumworks Lab services and software to accurately detect defects, reduce time and costs, and predict future diamond growth.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"674656bbd6cd0800017443d1","uuid":"a0cd1b3b-3473-473e-a076-9c9f2ede5adb","title":"A benchmark for long-form medical question answering","slug":"a-benchmark-for-long-form-medical-question-answering","html":"\u003cp\u003eResearchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. Existing benchmarks often rely on automatic metrics and multiple-choice questions, which do not fully capture the complexities of real-world clinical applications. To bridge this gap, the authors introduced a publicly available benchmark comprising real-world consumer medical questions, with long-form answers evaluated by medical professionals. This resource aims to facilitate more accurate assessments of LLMs' performance in medical question answering.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResearch area and challenges \u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLack of comprehensive benchmarks\u003c/strong\u003e: Existing evaluations focus on automatic metrics and multiple-choice formats, which do not adequately reflect the nuances of clinical scenarios.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eClosed-source studies\u003c/strong\u003e: Many studies on long-form medical QA are not publicly accessible, limiting reproducibility and the enhancement of existing baselines.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAbsence of human annotations\u003c/strong\u003e: There's a scarcity of datasets with human medical expert annotations, hindering the development of reliable evaluation metrics.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1114\" height=\"784\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.28.23-PM.png 1114w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eScheme of the difficulty levels of medical questions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003eTo build a dataset of real-world medical questions, 4,271 queries were collected from the Lavita Medical AI Assist platform between  spanning 1,693 conversations (single-turn and multi-turn). After deduplication, removal of sample questions, and filtering non-English entries using Lingua, the dataset was refined to 2,698 unique queries. \u003c/p\u003e\u003cp\u003ePairwise annotation tasks were completed using Quantumworks Lab's annotation platform and human evaluations were conducted with a group of 3 medical doctors, with two doctors assigned per batch, specializing in radiology and pathology. \u003c/p\u003e\u003cp\u003eBefore starting the main round of annotations, the researchers shared the annotation scheme with the doctors and conducted a trial round with each on a small sample of questions. They then gathered the doctors’ feedback to ensure that all annotation criteria were clear and that there was no ambiguity regarding the instructions. After confirming clarity and receiving approval from the doctors, they proceeded with the main batches of annotations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eLLM-as-a-Judge\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe researchers designed their LLM-as-a-judge prompt by combining templates from Zheng et al. and WildBench. The prompt, shown below, enables pairwise comparison of responses across multiple criteria, and used GPT-4o-2024-08-06 and Claude-3-5-Sonnet-20241022 as judges.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1142\" height=\"638\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.30.05-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eKey findings\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpen LLMs' potential\u003c/strong\u003e: Preliminary results indicated that open-source LLMs exhibit strong performance in medical QA, comparable to leading closed models.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAlignment with human judgments\u003c/strong\u003e: The research included a comprehensive analysis of LLMs acting as judges, revealing significant alignment between human evaluations and LLM assessments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePublicly available benchmark\u003c/strong\u003e: The authors provided a new benchmark featuring real-world medical questions and expert-annotated long-form answers, promoting transparency and reproducibility in future research.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1146\" height=\"386\" srcset=\"https://labelbox-research.ghost.io/content/images/size/w600/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 600w, https://labelbox-research.ghost.io/content/images/size/w1000/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 1000w, https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.31.38-PM.png 1146w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThis paper has been submitted to \u003cem\u003eAIM-FM: Advancements in Medical Foundation Models Workshop\u003c/em\u003e at NeurIPS 2024. You can read the full paper \u003ca href=\"https://arxiv.org/pdf/2411.09834?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"674656bbd6cd0800017443d1","feature_image":"https://labelbox-research.ghost.io/content/images/2024/11/Screenshot-2024-11-26-at-3.53.21-PM.png","featured":false,"visibility":"public","created_at":"2024-11-26T23:16:11.000+00:00","updated_at":"2025-07-03T18:07:05.000+00:00","published_at":"2024-11-26T23:43:04.000+00:00","custom_excerpt":"Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/a-benchmark-for-long-form-medical-question-answering/","excerpt":"Researchers from Dartmouth University recently advanced the need for comprehensive evaluation benchmarks for large language models (LLMs) in the medical domain. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65bbe41d21ca940001df755f","uuid":"eb9e0733-ed5d-4b7c-9061-6ba30c57b26e","title":"Identifying and counting avian blood cells in whole slide images via deep learning","slug":"identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning","html":"\u003cp\u003e\u003cstrong\u003eSummary: \u003c/strong\u003eAvian blood analysis is a fundamental method for investigating a wide range of topics concerning individual birds and populations of birds. Determining precise blood cell counts helps researchers gain insights into the health condition of birds. For example, the ratio of heterophils to lymphocytes (H/L ratio) is a well-established index for comparing relative stress load. However, such measurements are currently often obtained manually by human experts. The researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models. The first model determined image regions that are suitable for counting blood cells, and the second model is an instance segmentation model that detected the cells in the determined image regions. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eChallenge: \u003c/strong\u003eAutomated visual and acoustic monitoring methods for birds can provide information about the presence and the number of bird species or individuals in certain areas, but analyzing the physiological conditions of individual birds allows researchers to understand potential causes of negative population trends. \u003c/p\u003e\u003cp\u003eFor example, measuring the physiological stress of birds can serve as a valuable early warning indicator for conservation efforts. The physiological conditions and the stress of birds can be determined in several ways, e.g., by assessing the body weight or the fat and muscle scores in migratory birds. Other frequently used methods are investigating the parasite loads, measuring the heart rates, and measuring the levels of circulating stress hormones, such as corticosterone. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFindings:\u003c/strong\u003e The region selection model achieves up to 97.3% in terms of F1 score (i.e., the harmonic mean of precision and recall), and the instance segmentation model achieves up to 90.7% in terms of mean average precision. The approach can  help ornithologists acquire hematological data from avian blood smears more precisely and efficiently.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow Quantumworks Lab was used: \u003c/strong\u003e The researchers used the Quantumworks Lab platform annotating images of our datasets with model-assisted labeling. The instance segmentation dataset was annotated in an iterative, model-assisted manner, using the tile selection network to propose regions to be annotated and eventually selected them based on how many rare cells had been detected by an intermediate instance segmentation model. \u003c/p\u003e\u003cp\u003eIn the very first iteration, they used a superpixel algorithm to generate simple instance masks. In each iteration, they uploaded the corresponding instance segmentation masks to Quantumworks Lab to be refined by our human expert. This procedure significantly reduced the time needed to fully annotate an image file with masks and class labels compared to annotating from scratch. Overall, they went through four iterations of labeling. For the annotated cell instances, they established two primary categories: erythrocyte, with only the nucleus annotated, and leukocyte. The latter was further split into five subtypes, namely, lymphocyte, eosinophil, heterophil, basophil, and monocyte. Thrombocytes were not explicitly annotated; they were considered to be part of the background during training. \u003c/p\u003e\u003cp\u003eThe trained neural network model was able to distinguish between non-relevant thrombocytes and other annotated cell types, e.g., erythrocytes. By annotating only the nucleus of each erythrocyte rather than the entire cell including the cytoplasm, they maintained the option to label parasite-infected instances individually in future work. Cells infected with parasites may be annotated by masking the entire cell including the cytoplasm. One erythrocyte can be simultaneously counted as both an erythrocyte and a cell with blood parasite because of the distinct annotation regions.\u003c/p\u003e\u003cp\u003eYou can read the full paper \u003ca href=\"https://www.mdpi.com/2673-6004/5/1/4?ref=labelbox-research.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"65bbe41d21ca940001df755f","feature_image":"https://labelbox-research.ghost.io/content/images/2024/02/download.jpeg","featured":false,"visibility":"public","created_at":"2024-02-01T18:34:05.000+00:00","updated_at":"2024-02-01T18:39:06.000+00:00","published_at":"2024-02-01T18:39:06.000+00:00","custom_excerpt":"Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"}],"authors":[{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"}],"primary_author":{"id":"6359f21d99ae26003d07acb6","name":"David Mok","slug":"david","profile_image":"https://labelbox-research.ghost.io/content/images/2022/11/Screen-Shot-2022-11-18-at-9.11.05-AM.png","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-research.ghost.io/author/david/"},"primary_tag":{"id":"635953ac4af05f0031c98e9f","name":"Academic papers","slug":"academic-papers","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-research.ghost.io/tag/academic-papers/"},"url":"https://labelbox-research.ghost.io/identifying-and-counting-avian-blood-cells-in-whole-slide-images-via-deep-learning/","excerpt":"Learn how researchers presented a novel approach to automatically quantify avian red and white blood cells in whole slide images, based on two deep neural network models.","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}]},"__N_SSG":true},"page":"/research/[id]","query":{"id":"deep-learning-for-live-cell-shape-detection"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script>
    

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>

                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><span style="color: inherit; cursor: default;">Docs</span></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="/static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <span style="color: inherit; cursor: default;">Terms of Service</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Privacy Notice</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Copyright Dispute Policy</span>
                </div>
            </div>
        </div>
    </footer>
</body>
<!-- Mirrored from labelbox.com/research/deep-learning-for-live-cell-shape-detection/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:36:56 GMT -->
</html>