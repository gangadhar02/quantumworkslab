<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/product/model/foundry-models/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 10:44:49 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Foundry Models | Quantumworks Lab</title><meta name="description" content="Explore Quantumworks Lab Foundry Models for world-class foundations models that enable AI teams to enrich datasets and automate tasks." data-next-head=""/><link rel="preconnect" href="../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Foundry Models | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Explore Quantumworks Lab Foundry Models for world-class foundations models that enable AI teams to enrich datasets and automate tasks." data-next-head=""/><meta property="og:url" content="https://labelbox.com/product/model/foundry-models/" data-next-head=""/><meta property="og:image" content="https://images.ctfassets.net/j20krz61k3rk/2IuU4VBMv46Omo75i9uUzb/dc0ecefe6a995781b2e796fcfd620a49/modelfoundry.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Foundry Models | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Explore Quantumworks Lab Foundry Models for world-class foundations models that enable AI teams to enrich datasets and automate tasks." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/product/model/foundry-models/" data-next-head=""/><meta property="twitter:image" content="https://images.ctfassets.net/j20krz61k3rk/2IuU4VBMv46Omo75i9uUzb/dc0ecefe6a995781b2e796fcfd620a49/modelfoundry.png" data-next-head=""/><link rel="canonical" href="index.html" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../static/scripts/munchkin.js"></script><script src="../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g48[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.gJOBHT .css-13cymwt-control,.gJOBHT .css-t3ipsp-control{background-color:transparent;}/*!sc*/
.gJOBHT .css-13cymwt-control:focus,.gJOBHT .css-t3ipsp-control:focus,.gJOBHT .css-13cymwt-control:hover,.gJOBHT .css-t3ipsp-control:hover{background-color:transparent;border-color:#e5e5e5;}/*!sc*/
.gJOBHT #react-select-3-listbox{background-color:#e5e5e5;font-size:14px !important;}/*!sc*/
.gJOBHT #react-select-2-listbox{font-size:14px !important;}/*!sc*/
data-styled.g114[id="foundry-models__SelectWrapper-sc-1mw2993-0"]{content:"gJOBHT,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../../_next/static/chunks/791-24ca71cf406461ec.js" defer=""></script><script src="../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../../_next/static/chunks/38-4622c00db2d6bb67.js" defer=""></script><script src="../../../_next/static/chunks/pages/product/model/foundry-models-de40efd9e9acae29.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../index.html"><img width="106" height="24" alt="logo" src="../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 mb-6"><div class="grid grid-cols-12 gap-y-4 md:gap-4"><div class="col-span-12 md:col-span-3"><h4 class="font-bold text-2xl md:text-3xl leading-tight font-future text-neutral-900 dark:text-neutral-50" style="font-feature-settings:unset">Models</h4></div><div class="foundry-models__SelectWrapper-sc-1mw2993-0 gJOBHT col-span-9 md:col-span-4 md:pl-4"><style data-emotion="css b62m3t-container">.css-b62m3t-container{position:relative;box-sizing:border-box;}</style><div class="css-b62m3t-container"><style data-emotion="css 7pg0cj-a11yText">.css-7pg0cj-a11yText{z-index:9999;border:0;clip:rect(1px, 1px, 1px, 1px);height:1px;width:1px;position:absolute;overflow:hidden;padding:0;white-space:nowrap;}</style><span id="react-select-3-live-region" class="css-7pg0cj-a11yText"></span><span aria-live="polite" aria-atomic="false" aria-relevant="additions text" role="log" class="css-7pg0cj-a11yText"></span><style data-emotion="css 13cymwt-control">.css-13cymwt-control{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;cursor:default;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;min-height:38px;outline:0!important;position:relative;-webkit-transition:all 100ms;transition:all 100ms;background-color:hsl(0, 0%, 100%);border-color:hsl(0, 0%, 80%);border-radius:4px;border-style:solid;border-width:1px;box-sizing:border-box;}.css-13cymwt-control:hover{border-color:hsl(0, 0%, 70%);}</style><div class="css-13cymwt-control"><style data-emotion="css hlgwow">.css-hlgwow{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:grid;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-overflow-scrolling:touch;position:relative;overflow:hidden;padding:2px 8px;box-sizing:border-box;}</style><div class="css-hlgwow"><style data-emotion="css 1jqq78o-placeholder">.css-1jqq78o-placeholder{grid-area:1/1/2/3;color:hsl(0, 0%, 50%);margin-left:2px;margin-right:2px;box-sizing:border-box;}</style><div class="css-1jqq78o-placeholder" id="react-select-3-placeholder">Search...</div><style data-emotion="css 19bb58m">.css-19bb58m{visibility:visible;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;display:inline-grid;grid-area:1/1/2/3;grid-template-columns:0 min-content;margin:2px;padding-bottom:2px;padding-top:2px;color:hsl(0, 0%, 20%);box-sizing:border-box;}.css-19bb58m:after{content:attr(data-value) " ";visibility:hidden;white-space:pre;grid-area:1/2;font:inherit;min-width:2px;border:0;margin:0;outline:0;padding:0;}</style><div class="css-19bb58m" data-value=""><input class="" style="label:input;color:inherit;background:0;opacity:1;width:100%;grid-area:1 / 2;font:inherit;min-width:2px;border:0;margin:0;outline:0;padding:0" autoCapitalize="none" autoComplete="off" autoCorrect="off" id="react-select-3-input" spellcheck="false" tabindex="0" type="text" aria-autocomplete="list" aria-expanded="false" aria-haspopup="true" role="combobox" aria-activedescendant="" aria-describedby="react-select-3-placeholder" value=""/></div></div><style data-emotion="css 1wy0on6">.css-1wy0on6{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-self:stretch;-ms-flex-item-align:stretch;align-self:stretch;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;box-sizing:border-box;}</style><div class="css-1wy0on6"><style data-emotion="css 1u9des2-indicatorSeparator">.css-1u9des2-indicatorSeparator{-webkit-align-self:stretch;-ms-flex-item-align:stretch;align-self:stretch;width:1px;background-color:hsl(0, 0%, 80%);margin-bottom:8px;margin-top:8px;box-sizing:border-box;}</style><span class="css-1u9des2-indicatorSeparator"></span><style data-emotion="css 1xc3v61-indicatorContainer">.css-1xc3v61-indicatorContainer{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-transition:color 150ms;transition:color 150ms;color:hsl(0, 0%, 80%);padding:8px;box-sizing:border-box;}.css-1xc3v61-indicatorContainer:hover{color:hsl(0, 0%, 60%);}</style><div class="css-1xc3v61-indicatorContainer" aria-hidden="true"><style data-emotion="css 8mmkcg">.css-8mmkcg{display:inline-block;fill:currentColor;line-height:1;stroke:currentColor;stroke-width:0;}</style><svg height="20" width="20" viewBox="0 0 20 20" aria-hidden="true" focusable="false" class="css-8mmkcg"><path d="M4.516 7.548c0.436-0.446 1.043-0.481 1.576 0l3.908 3.747 3.908-3.747c0.533-0.481 1.141-0.446 1.574 0 0.436 0.445 0.408 1.197 0 1.615-0.406 0.418-4.695 4.502-4.695 4.502-0.217 0.223-0.502 0.335-0.787 0.335s-0.57-0.112-0.789-0.335c0 0-4.287-4.084-4.695-4.502s-0.436-1.17 0-1.615z"></path></svg></div></div></div></div></div><div class="col-span-3 md:col-span-5 flex items-center"><div class="flex flex-row w-fit ml-auto border rounded-lg overflow-hidden"><span class=" cursor-pointer p-2 transition-all duration-300 ease-in ease-in duration"><img alt="grid-view" class="h-[20px]" src="../../../static/images/icons/list.svg"/></span><span class="border"></span><span class="bg-white cursor-pointer p-2"><img alt="list-view" class="h-[20px]" src="../../../static/images/icons/grid.svg"/></span></div></div></div></div><div class="col-span-12 hidden md:block md:col-span-3"><div class="lg:pr-12"></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 mb-4 "></div></div></div></div><div class=""><div class="mb-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="Footer__StyledFooter-sc-u68pnv-0 eJChXt"><div class="undefined lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="py-24"><div class=" w-full h-[1px] bg-neutral-200"></div></div><div class="hidden md:block"><img src="../../../static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36"/></div><section class="hidden md:grid footer-grid"></section><section class="social-media"></section><div class="text-center "><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">© Quantumworks Lab, Inc <br/>We enable breakthroughs</p><div class="flex flex-row flex-wrap justify-content-center gap-4 mt-4"><a href="https://docs.labelbox.com/page/terms-of-service" class=" " target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Terms of Service</p></a><div class="mx-1 border"></div><a href="https://docs.labelbox.com/page/privacy-notice" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Privacy Notice</p></a><div class="mx-1 border hidden sm:block"></div><a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Copyright Dispute Policy</p></a></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"name":"Model foundry","metaTags":{"title":"Foundry Models | Quantumworks Lab","description":"Explore Quantumworks Lab Foundry Models for world-class foundations models that enable AI teams to enrich datasets and automate tasks.","url":"https://labelbox.com/product/model/foundry-models/","image":{"title":"social card model foundry","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/2IuU4VBMv46Omo75i9uUzb/dc0ecefe6a995781b2e796fcfd620a49/modelfoundry.png","details":{"size":229892,"image":{"width":2400,"height":1254}},"fileName":"modelfoundry.png","contentType":"image/png"}},"noIndex":false,"canonical":true},"pageUrl":"https://labelbox.com/product/model/foundry-models/","pageContent":[{"name":"Hero section","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":true,"choiceName":"Url"},{"choice":"ctaText","checked":true,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":false,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":true,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"Welcome to the Foundry","flexibleContent":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Build intelligent applications with foundation models","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"url":"https://app.labelbox.com/signin/?attr=modelfoundrypg","ctaText":"Get started","assets":[{"title":"open ai gray","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/6fc9IgitiJ2yLWTIgzEoAl/47428b1dc708a8ef0eea5a8528eb080d/openai.svg","details":{"size":5373,"image":{"width":135,"height":33}},"fileName":"openai.svg","contentType":"image/svg+xml"}},{"title":"google","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/5hLe4oLiq9j6YUUDgQiyYo/9073c0ea024b0cd1399cdc29bc278ba6/google.svg","details":{"size":2884,"image":{"width":104,"height":35}},"fileName":"google.svg","contentType":"image/svg+xml"}},{"title":"anthropic gray","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7yM5tV4zUoYCOjfoS82F85/ebffbe70eb162f4cfb3b1c707e0f1de3/Antrhopic.svg","details":{"size":2285,"image":{"width":145,"height":17}},"fileName":"Antrhopic.svg","contentType":"image/svg+xml"}},{"title":"amazon light","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7KbGCZJZQx9dhOJ3MzMIfi/1b82e78000604df031a008faa4f7da15/amazonlight.svg","details":{"size":5996,"image":{"width":435,"height":132}},"fileName":"amazonlight.svg","contentType":"image/svg+xml"}},{"title":"huggingface light","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7ijth4LuIowk75Lt7vTsT6/3069d700d935ce570f00d6717de88b50/hf.svg","details":{"size":37543,"image":{"width":338,"height":71}},"fileName":"hf.svg","contentType":"image/svg+xml"}},{"title":"Cohere light","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/1azrsFCelR5vR5VZ7YiJ89/9f7dce0a888ec3a092c28a9cb44cc0e5/cohere-light.svg","details":{"size":5667,"image":{"width":182,"height":32}},"fileName":"cohere-light.svg","contentType":"image/svg+xml"}},{"title":"Microsoft light","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/5cdCBIepbrtp6pofwE522Y/f9346da32de2b404945ea2be755975d2/Microsoftlight.svg","details":{"size":6200,"image":{"width":507,"height":108}},"fileName":"Microsoftlight.svg","contentType":"image/svg+xml"}}]},{"name":"Video section","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":true,"choiceName":"Body"},{"choice":"flexibleContent","checked":false,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":true,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":true,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":true,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":true,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"Let foundation models do the work","body":"We are bringing together the world's best AI models to help you perform data labeling and enrichment tasks across all supported data modalities. Achieve breakthroughs in data generation speed and costs. Re-focus human efforts on quality assurance.","flexibleContent":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"See it in action","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"title":"Model Foundry fallback poster","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/qXW5VCoeeZ2Jf6GphwFRC/333acb33bd53599af74c2ac5e4499c6a/MF.png","details":{"size":144323,"image":{"width":1440,"height":1024}},"fileName":"MF.png","contentType":"image/png"}},"video":{"title":"Model Foundry Video","description":"","file":{"url":"//videos.ctfassets.net/j20krz61k3rk/40V5O3XviaKqA4nB3EwCcK/af8e080f67006729475ff516108a3279/Comp_4_1.mp4","details":{"size":30244480},"fileName":"Comp 4_1.mp4","contentType":"video/mp4"}},"videoUrl":"https://demo.arcade.software/HnecUTysIiTR66fMWJ1C?embed","videoUrl2":"https://demo.arcade.software/PqkNFZRaNrGWBPjlotve?embed"},{"nameOfContentSection":"Features","richText":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Kickstart your AI efforts with this AI co-pilot to build intelligent applications faster than ever.","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"linksToEntries":[{"name":"Pre-label data in a few clicks","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":true,"choiceName":"Url"},{"choice":"ctaText","checked":true,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":true,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"Pre-label data in a few clicks","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"AI builders can now enrich datasets and pre-label data in minutes without code using foundation models offered by leading providers or open source alternatives. Model-assisted labeling using Foundry accelerates data labeling tasks on images, text, and documents at a fraction of the typical cost and speed. ","marks":[],"data":{}}],"data":{}}]},"url":"/product/model/foundry-models/","ctaText":"Explore all models","image":{"title":" Use large language models to pre-label data","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/5xlMVFUzH1kt7ad2oGiWjW/d2b190315d04edace38ae68b75fc75e5/Frame_3327.png","details":{"size":293903,"image":{"width":1402,"height":1298}},"fileName":"Frame 3327.png","contentType":"image/png"}}},{"name":"Focus human expertise on where it matters most","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":true,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"Focus human expertise on where it matters most","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"The days of manually labeling from scratch are long gone. Foundation models can ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"outperform crowd-sourced labeling on various tasks","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://arxiv.org/pdf/2304.06588.pdf"}},{"nodeType":"text","value":" with high accuracy – allowing you to focus valuable human efforts on critical review. Combine model-assisted labeling using foundation models with human-in-the-loop review to accelerate your labeling operations and build intelligent AI faster than ever. ","marks":[],"data":{}}],"data":{}}]},"image":{"title":"A co-pilot for your data labeling team","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/57uROtihsCPTpUTQAWr2tg/a08718db99a19d08778962bd4cc4659f/Frame_3307.png","details":{"size":146094,"image":{"width":1344,"height":1024}},"fileName":"Frame 3307.png","contentType":"image/png"}}},{"name":"Automate data tasks with Foundry Apps","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":true,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"Automate data tasks with Foundry Apps","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Create custom Apps with Foundry based on your team’s needs. Deploy a model configuration from Foundry to an App for automated data management or to build custom intelligent applications.","marks":[],"data":{}}],"data":{}}]},"image":{"title":"Find the best model for your data","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/6Fgoi1Hf0yMWhLCNJ8znrH/29e26bfa5740d96d4114afcf44c351e0/Frame_3441__1_.png","details":{"size":809882,"image":{"width":4032,"height":3072}},"fileName":"Frame 3441 (1).png","contentType":"image/png"}}},{"name":"Explore datasets using AI insights","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":true,"choiceName":"Url"},{"choice":"ctaText","checked":true,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":true,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"Explore datasets using AI insights","flexibleContent":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Curate and explore datasets faster than ever. Query your data using predictions generated from the latest and greatest foundation models. Supercharge your data enrichment process and accelerate curation efforts across image and text data modalities. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"url":"https://labelbox.com/product/model/foundry/","ctaText":"Learn more","image":{"title":"Speed up model development with enriched datasets","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/1tQQns7oP959JVZWGHmuSb/bfdbf2ed0c784440cc569a35b2ce2202/master_-__all__1_.png","details":{"size":1141435,"image":{"width":4032,"height":3072}},"fileName":"master -_ all (1).png","contentType":"image/png"}}},{"name":"Accelerate custom model development","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":true,"choiceName":"Url"},{"choice":"ctaText","checked":true,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":true,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"Accelerate custom model development","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Data enriched by Foundry can be used to tailor foundation models to your specific use cases:","marks":[],"data":{}}],"data":{}},{"nodeType":"ordered-list","content":[{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Fine-tune leading foundation models:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Optimize foundation models for your most valuable AI use cases. Enhance performance for specialized tasks in just days instead of months.  ","marks":[],"data":{}}],"data":{}}],"data":{}},{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Distill knowledge into smaller models: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Elaborate foundation models into lightweight models purpose-built for your applications. ","marks":[],"data":{}}],"data":{}}],"data":{}}],"data":{}},{"nodeType":"paragraph","content":[{"nodeType":"text","value":"","marks":[],"data":{}}],"data":{}}]},"url":"/guides/how-to-fine-tune-vertex-ai-models-with-labelbox/","ctaText":"Learn more","image":{"title":"llm providers","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/01bLcU56wCVBGftpt5dCWb/55610f7d86fc7911b265f7e9c289b050/model-01.svg","details":{"size":45824,"image":{"width":672,"height":512}},"fileName":"model-01.svg","contentType":"image/svg+xml"}}}]},{"name":"Blogs / Guides","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":false,"choiceName":"Flexible content"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":false,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":true,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"}]},"heading":"Explore use cases","customData":{"guides":["automatically-label-text-with-96-accuracy-using-foundation-models","automatically-label-images-with-99-accuracy-using-foundation-models","using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks","how-to-accelerate-image-text-pair-generation-with-blip-2"],"blog":["gpt4-vs-palm-assessing-performance-of-llm-models"]}},{"nameOfContentSection":"FAQs section","heading":"FAQ","linksToEntries":[{"name":"Who gets access to the Model Foundry? ","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":false,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"Who gets access to the Model Foundry? ","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"To use Quantumworks Lab Foundry, you will need to be a part of our Starter or Enterprise plans. You will be billed monthly for the pass-through compute costs associated with foundation models run on your data in Labelbox. ","marks":[],"data":{}}],"data":{}},{"nodeType":"hr","content":[],"data":{}},{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Learn more about ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"how to upgrade to a pay-as-you-go Starter plan","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://docs.labelbox.com/docs/billing#easily-upgrade-your-plan-at-any-time"}},{"nodeType":"text","value":" or about our ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"Quantumworks Lab Unit pricing","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://docs.labelbox.com/docs/billing#labelbox-units-lbus"}},{"nodeType":"text","value":". ","marks":[],"data":{}}],"data":{}},{"nodeType":"hr","content":[],"data":{}},{"nodeType":"paragraph","content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"Contact us","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://labelbox.com/sales/"}},{"nodeType":"text","value":" to receive more detailed instructions on how to create a Quantumworks Lab Starter or Enterprise account or upgrade to our Starter tier.","marks":[],"data":{}}],"data":{}}]}},{"name":"How is Foundry priced?","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":false,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"How is Foundry priced?","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Only pay for the models that you want to pre-label or enrich your data with. Foundry pricing will be calculated and billed monthly based on the following:","marks":[],"data":{}}],"data":{}},{"nodeType":"hr","content":[],"data":{}},{"nodeType":"ordered-list","content":[{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Inference cost","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" – Quantumworks Lab will charge customers for inference costs for all models hosted by Labelbox. Inference costs will be bespoke to each model available in Foundry. The inference price is determined based on vendors or our compute costs – these are published publicly on ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"our website","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://labelbox.com/product/model/foundry-pricing/"}},{"nodeType":"text","value":" as well as inside the product. ","marks":[],"data":{}}],"data":{}}],"data":{}},{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Quantumworks Lab's platform cost","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" – each asset with predictions generated by Foundry will accrue LBUs.  ","marks":[],"data":{}}],"data":{}}],"data":{}}],"data":{}},{"nodeType":"hr","content":[],"data":{}},{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Learn more about the pricing of the Foundry add-on for Quantumworks Lab Model on our ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"pricing page","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://labelbox.com/pricing/"}},{"nodeType":"text","value":". ","marks":[],"data":{}}],"data":{}}]}},{"name":"What kind of foundation models are available?","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":false,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"What kind of foundation models are available?","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Quantumworks Lab Foundry currently supports a variety of tasks for computer vision and natural language processing. This includes powerful open-source and third-party models across text generation, object detection, translation, text classification, image segmentation, and more. For a full list of available models, please visit ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"this page","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://labelbox.com/product/model/foundry-models/"}},{"nodeType":"text","value":". ","marks":[],"data":{}}],"data":{}}]}},{"name":"How do I get started with Foundry?","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":false,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"How do I get started with Foundry?","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"If you aren’t currently a Quantumworks Lab user or are on our Free plan, you’ll need to: ","marks":[],"data":{}}],"data":{}},{"nodeType":"hr","content":[],"data":{}},{"nodeType":"ordered-list","content":[{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Create a Quantumworks Lab account ","marks":[],"data":{}}],"data":{}}],"data":{}},{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Upgrade your account to our ","marks":[],"data":{}},{"nodeType":"text","value":"Starter plan","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": ","marks":[],"data":{}}],"data":{}}],"data":{}}],"data":{}},{"nodeType":"paragraph","content":[{"nodeType":"text","value":"In the ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"Billing tab","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://app.labelbox.com/workspace-settings/billing"}},{"nodeType":"text","value":", locate “Starter” in the All Plans list and select “Switch to Plan.” ","marks":[],"data":{}},{"nodeType":"text","value":"The credit card on file will only be charged when you exceed your existing free 10,000 LBU. ","marks":[{"type":"italic"}],"data":{}}],"data":{}},{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Upgrades take effect immediately so you'll have access to Foundry right away on the Starter plan. After upgrading, you’ll see the option to activate Foundry for your organization. ","marks":[],"data":{}}],"data":{}},{"nodeType":"ordered-list","content":[{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Follow the steps below on how to get started generating model predictions with Foundry.","marks":[],"data":{}}],"data":{}}],"data":{}}],"data":{}},{"nodeType":"hr","content":[],"data":{}},{"nodeType":"paragraph","content":[{"nodeType":"text","value":"If you are currently a Quantumworks Lab customer on our Starter or Enterprise plans, you automatically have the ability to opt-in to using Foundry:","marks":[],"data":{}}],"data":{}},{"nodeType":"hr","content":[],"data":{}},{"nodeType":"ordered-list","content":[{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"After selecting data in Catalog and hitting “Predict with Foundry,” you’ll be able to select a foundation model and submit a model run to generate predictions. You can learn more about the Foundry workflow and see it in-action ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"in this guide","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"http://labelbox.com/guides/guide-to-using-model-foundry"}},{"nodeType":"text","value":". ","marks":[],"data":{}}],"data":{}}],"data":{}},{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"When submitting a model run, you’ll see the option to activate Foundry for your organization’s Admins. You’ll need to agree to Quantumworks Lab’s Model Foundry add-on service terms and confirm you understand the associated compute fees.  ","marks":[],"data":{}}],"data":{}}],"data":{}}],"data":{}},{"nodeType":"paragraph","content":[{"nodeType":"text","value":"","marks":[],"data":{}}],"data":{}}]}},{"name":"Does Quantumworks Lab sell or use customer data via Foundry?","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":false,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"Does Quantumworks Lab sell or use customer data via Foundry?","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Quantumworks Lab does not sell or use customer data in a way other than to provide you the services.","marks":[],"data":{}}],"data":{}}]}},{"name":"Do the third-party model providers use customer data to train their models? ","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":false,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"heading":"Do the third-party model providers use customer data to train their models? ","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Quantumworks Lab's third-party model providers will not use your data to train their models, as governed by specific 'opt-out' terms between Quantumworks Lab and our third-party model providers.","marks":[],"data":{}}],"data":{}}]}},{"name":"Can I use the closed source models available through Foundry for pre-labeling?","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":false,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"secondaryCtaText","checked":false,"choiceName":"Secondary Cta Text"},{"choice":"secondaryCtaUrl","checked":false,"choiceName":"Secondary Cta Url"},{"choice":"icon","checked":false,"choiceName":"Icon"},{"choice":"image","checked":false,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"},{"choice":"linkToEntries","checked":false,"choiceName":"linkToEntries"},{"choice":"isAnimatedGraphic","checked":false,"choiceName":"isAnimatedGraphic"}]},"heading":"Can I use the closed source models available through Foundry for pre-labeling?","flexibleContent":{"nodeType":"document","data":{},"content":[{"nodeType":"unordered-list","content":[{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"OpenAI: Quantumworks Lab believes that you have the right to use OpenAI’s closed-source models (e.g, GPT-4, GPT3.5, etc) for pre-labeling through Foundry if you comply with applicable laws and otherwise follow OpenAI’s ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"usage policies","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://openai.com/policies/usage-policies"}},{"nodeType":"text","value":". If you wish to use your own instance of OpenAI, you can do so by setting up a custom model on Foundry. ","marks":[],"data":{}}],"data":{}}],"data":{}},{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Anthropic: Quantumworks Lab believes that you have the right to use Anthropic’s Claude for pre-labeling through Foundry if you comply with applicable laws and otherwise follow Anthropic’s ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"Acceptable Use Policy","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://www.anthropic.com/legal/aup"}},{"nodeType":"text","value":". If you wish to use your own instance of Anthropic, you can do so by setting up a custom model on Foundry. ","marks":[],"data":{}}],"data":{}}],"data":{}},{"nodeType":"list-item","content":[{"nodeType":"paragraph","content":[{"nodeType":"text","value":"Google: Quantumworks Lab believes that you have the right to use Google’s Gemini (or other closed-source models such as Imagen, PalLM, etc.) for pre-labeling through Foundry if you comply with applicable laws and otherwise follow the ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"Gemini API Additional Terms of Service","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://ai.google.dev/terms"}},{"nodeType":"text","value":", including without limitation the ","marks":[],"data":{}},{"nodeType":"hyperlink","content":[{"nodeType":"text","value":"Generative AI Prohibited Use Policy","marks":[{"type":"underline"}],"data":{}}],"data":{"uri":"https://policies.google.com/terms/generative-ai/use-policy"}},{"nodeType":"text","value":". If you wish to use your own instance of Google, you can do so by setting up a custom model on Foundry. ","marks":[],"data":{}}],"data":{}}],"data":{}}],"data":{}},{"nodeType":"paragraph","content":[{"nodeType":"text","value":"","marks":[],"data":{}}],"data":{}}]}}]},{"name":"Texas rangers testimonial","choices":{"showFields":[{"choice":"name","checked":true,"choiceName":"Name"},{"choice":"heading","checked":true,"choiceName":"Heading"},{"choice":"headerStyles","checked":false,"choiceName":"HeaderStyles"},{"choice":"contentType","checked":false,"choiceName":"Content type"},{"choice":"body","checked":true,"choiceName":"Body"},{"choice":"flexibleContent","checked":true,"choiceName":"Flexible content"},{"choice":"flexibleContent2","checked":false,"choiceName":"Flexible Content 2"},{"choice":"searchableContent","checked":false,"choiceName":"Searchable content"},{"choice":"url","checked":false,"choiceName":"Url"},{"choice":"ctaText","checked":false,"choiceName":"Cta text"},{"choice":"icon","checked":true,"choiceName":"Icon"},{"choice":"image","checked":false,"choiceName":"Image"},{"choice":"mobileImage","checked":false,"choiceName":"Mobile Image"},{"choice":"video","checked":false,"choiceName":"Video"},{"choice":"isVideo","checked":false,"choiceName":"isVideo"},{"choice":"isActive","checked":false,"choiceName":"Is active"},{"choice":"assets","checked":false,"choiceName":"Assets"},{"choice":"customData","checked":false,"choiceName":"customData"},{"choice":"videoUrl","checked":false,"choiceName":"videoUrl"},{"choice":"videoUrl2","checked":false,"choiceName":"videoUrl2"},{"choice":"backgroundColor","checked":false,"choiceName":"background color"},{"choice":"image2","checked":false,"choiceName":"Image2"},{"choice":"publishedOn","checked":false,"choiceName":"Published on"},{"choice":"isSmallImage","checked":false,"choiceName":"Is Small Image"}]},"flexibleContent":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"\"We love our initial use of Model Foundry. Instead of going into unstructured text datasets blindly, we can now use pre-existing LLMs to pre-label data or pre-tag parts of it. Model Foundry serves as a co-pilot for training data.\" ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"- Alexander Booth, Assistant Director of the World Series Champions, Texas Rangers\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"icon":{"title":"Texas rangers dark","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/2yINiag19hCwtrhokP7DZj/213ae780dfe8db8d2f0e383004f15dcc/Texas-Rangers.svg","details":{"size":24945,"image":{"width":195,"height":195}},"fileName":"Texas-Rangers.svg","contentType":"image/svg+xml"}}}],"plainObject":{"NLP":["text-classification","token-classification","table-question-answering","text-generation","translation","question-answering","summarization","translation","zero-shot-classification","conversational","text2text-generation","fill-mask","sentense-similarity","named-entity-extraction","custom-ontology"],"Computer vision":["image-classification","object-detection","video-classification","zero-shot-image-classification","image-segmentation","visual-question-answering","image-captioning","video-segmentation"]}},"content":[{"fields":{"title":"OpenAI GPT-4.1","pageurl":"openai-gpt-4-1","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"OpenAI GPT-4.1 is the latest iteration in the GPT series, specifically designed with developers in mind, offering significant advancements in coding, instruction following, and long-context processing. Available exclusively via API, GPT-4.1 aims to enhance the productivity of developers and power more capable AI agents and applications.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Software development (code generation, debugging, testing, code diffs)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Building AI agents and automating complex workflows","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Analyzing and processing large documents and codebases (up to 1M tokens)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Tasks requiring precise instruction following and structured outputs","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multimodal applications involving text and image inputs","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"GPT-4.1 introduces a massive 1 million token input context window, a substantial increase over previous models, enabling it to process and reason over extremely large texts or datasets in a single prompt. This significantly improves performance on long-context tasks, including document analysis and multi-hop reasoning.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The model demonstrates notable improvements in coding performance, scoring 54.6% on the SWE-Bench Verified benchmark and showing higher accuracy in generating code diffs compared to its predecessors. It is also specifically tuned for better instruction following, showing improved adherence to complex, multi-step directions and reduced misinterpretations on benchmarks like MultiChallenge and IFEval.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"GPT-4.1 maintains multimodal capabilities, accepting text and image inputs. It has shown strong performance on multimodal benchmarks, including those involving visual question answering on charts, diagrams, and even extracting information from videos without subtitles. The model is also highlighted for its 100% accuracy on needle-in-a-haystack retrieval across its entire 1M token context window.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Available in a family of models including GPT-4.1, GPT-4.1 mini (optimized for balance of performance and cost), and GPT-4.1 nano (optimized for speed and cost), GPT-4.1 offers flexibility for various developer needs and aims for improved cost-efficiency compared to earlier models like GPT-4o for many use cases.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"API-only availability","nodeType":"text"},{"data":{},"marks":[],"value":": Unlike some previous GPT models, GPT-4.1 is currently available exclusively through the API and is not directly accessible in the ChatGPT consumer interface.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Rate limits","nodeType":"text"},{"data":{},"marks":[],"value":": While offering a large context window, practical usage for extremely high-volume or continuous long-context tasks can be impacted by API rate limits, which vary by usage tier.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Reasoning specialization","nodeType":"text"},{"data":{},"marks":[],"value":": While demonstrating improved reasoning, GPT-4.1 is not primarily positioned as a dedicated reasoning model in the same category as some models specifically optimized for deep, step-by-step logical deduction.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Potential for \"laziness\" in smaller variants","nodeType":"text"},{"data":{},"marks":[],"value":": Some initial user observations have suggested that the smallest variant, GPT-4.1 nano, can occasionally exhibit a tendency for shorter or less detailed responses, potentially requiring more specific prompting.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multimodal output","nodeType":"text"},{"data":{},"marks":[],"value":": While accepting text and image inputs, the primary output modality is text; it does not generate images directly like some other multimodal models.\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Citation","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://openai.com/index/gpt-4-1/"},"content":[{"data":{},"marks":[],"value":"https://openai.com/index/gpt-4-1/","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Question answering","Text generation","Summarization","Conversational","Image classification","Text classification","Custom ontology"],"publishedOn":"2025-05-08"},"sys":{"id":"6VfVIZ34FkIpMSGTf3kEsM","type":"Entry"}},{"fields":{"title":"Google Gemini 2.0 Pro","pageurl":"google-gemini-2-0-pro","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Gemini 2.0 Pro is the strongest model among the family of Gemini models for coding and world knowledge, and it features a 2M long context window. Gemini 2.0 Pro is available as an experimental model in Vertex AI and is an upgrade path for 1.5 Pro users who want better quality, or who are particularly invested in long context and code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multimodal input","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Text output","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Prompt optimizers","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Controlled generation","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Function calling (excluding compositional function calling)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Grounding with Google Search","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Code execution","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Count token","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Google Gemini 2.0 Pro boasts the strongest coding performance and superior ability to handle complex prompts, demonstrating better understanding and reasoning of world knowledge than any previous model. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"It features the largest context window of 2 million tokens, enabling comprehensive analysis and understanding of large amounts of information. Additionally, it can call external tools like Google Search and execute code, enhancing its utility for a wide range of tasks, including coding and knowledge analysis.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"77Snlzn0JhcUbEPGsgVuYg","type":"Asset","createdAt":"2025-03-17T09:36:32.328Z","updatedAt":"2025-03-17T09:36:32.328Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"Google Gemini 2.0 Pro","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/77Snlzn0JhcUbEPGsgVuYg/7c84d7c9b62b0c38e54ce72215445e55/image6.original_JY99INi.png","details":{"size":277523,"image":{"width":1663,"height":1949}},"fileName":"image6.original_JY99INi.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Context: Gemini 2.0 Pro may struggle with maintaining context over extended conversations, leading to inconsistencies in long interactions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Bias: As Gemini 2.0 Pro is trained on a large corpus of internet text, it may inadvertently reflect and perpetuate biases present in the training data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Creativity Boundaries: While capable of creative outputs, Gemini 2.0 Pro may not always meet specific creative standards or expectations for novel and nuanced content.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ethical Concerns: Gemini 2.0 Pro can be used to generate misleading information, offensive content, or be exploited for harmful purposes if not properly moderated.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Comprehension: Gemini 2.0 Pro might not fully understand or accurately interpret highly technical or domain-specific content, especially if it involves recent developments post-training data cutoff.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Dependence on Prompt Quality: The quality and relevance of Gemini 2.0 Pro's output are highly dependent on the clarity and specificity of the input prompts provided by the user.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2#2.0-pro"},"content":[{"data":{},"marks":[],"value":"https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2#2.0-pro","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation","Question answering","Named entity recognition","Video global classification","Zero-shot classification","Summarization","Conversational","Image classification","Text classification","Video per frame classification","Custom ontology","Multimodal"],"publishedOn":"2025-03-17","price":"0.00125 $ per 1000 prompt tokens"},"sys":{"id":"6RNqdLtLTzZHmxJZI0aFP6","type":"Entry"}},{"fields":{"title":"Claude 3.5 Sonnet","pageurl":"claude-3-5-sonnet","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Claude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval). It shows marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone. Claude 3.5 Sonnet operates at twice the speed of Claude 3 Opus. This performance boost, combined with cost-effective pricing, makes Claude 3.5 Sonnet ideal for complex tasks such as context-sensitive customer support and orchestrating multi-step workflows.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-1"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Task automation: plan and execute complex actions across APIs and databases, interactive coding","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"R\u0026D: research review, brainstorming and hypothesis generation, drug discovery","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Strategy: advanced analysis of charts \u0026 graphs, financials and market trends, forecasting","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Advanced Coding ability: In an internal evaluation by Anthropic, ","nodeType":"text"},{"data":{},"marks":[],"value":"Claude 3.5 Sonnet solved 64% of problems, outperforming Claude 3 Opus which solved 38%. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multilingual Capabilities:","nodeType":"text"},{"data":{},"marks":[],"value":" Claude 3.5 Sonnet offers improved fluency in non-English languages such as Spanish and Japanese, enabling use cases like translation services and global content creation.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Vision and Image Processing:","nodeType":"text"},{"data":{},"marks":[],"value":" This model can process and analyze visual input, extracting insights from documents, processing web UI, generating image catalog metadata, and more.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Steerability and Ease of Use:","nodeType":"text"},{"data":{},"marks":[],"value":" Claude 3.5 Sonnet is designed to be easy to steer and better at following directions, giving you more control over model behavior and more predictable, higher-quality outputs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"5dv93rsU2VbBCEOWw6h3qY","type":"Asset","createdAt":"2024-07-15T23:37:40.129Z","updatedAt":"2024-07-15T23:37:40.129Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"claude 3.5 sonnet","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/5dv93rsU2VbBCEOWw6h3qY/cda9d419ef2fbe8b1b31bb929fe18c7f/_claude.webp","details":{"size":135370,"image":{"width":2200,"height":1894}},"fileName":"_claude.webp","contentType":"image/webp"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here are some of the limitations we are aware of:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Medical images: Claude 3.5 is not suitable for interpreting specialized medical images like CT scans and shouldn't be used for medical advice.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Non-English: Claude 3.5 may not perform optimally when handling images with text of non-Latin alphabets, such as Japanese or Korean.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Big text: Users should enlarge text within the image to improve readability for Claude 3.5, but avoid cropping important details.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Rotation: Claude 3.5 may misinterpret rotated / upside-down text or images.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Visual elements: Claude 3.5 may struggle to understand graphs or text where colors or styles like solid, dashed, or dotted lines vary.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Spatial reasoning: Claude 3.5 struggles with tasks requiring precise spatial localization, such as identifying chess positions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Hallucinations: the model can provide factually inaccurate information.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Image shape: Claude 3.5 struggles with panoramic and fisheye images.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Metadata and resizing: Claude 3.5 doesn't process original file names or metadata, and images are resized before analysis, affecting their original dimensions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Counting: Claude 3.5 may give approximate counts for objects in images.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"CAPTCHAS: For safety reasons, Claude 3.5 has a system to block the submission of CAPTCHAs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.anthropic.com/claude/docs/models-overview"},"content":[{"data":{},"marks":[],"value":"https://docs.anthropic.com/claude/docs/models-overview","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Translation","Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Text classification","Multimodal"],"publishedOn":"2024-07-15","price":"0.01500 $ per 1000 response tokens"},"sys":{"id":"4XizlrPJpJdFttXIMzZ9Uy","type":"Entry"}},{"fields":{"title":"Llama 4 Maverick","pageurl":"llama-4-maverick","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Meta Llama 4 Maverick is a cutting-edge, natively multimodal AI model from Meta, part of the new Llama 4 family. Built with a Mixture-of-Experts (MoE) architecture, Maverick is designed for advanced text and image understanding, aiming to provide industry-leading performance for a variety of applications at competitive costs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Developing multimodal assistant applications (image recognition, visual reasoning, captioning, Q\u0026A about images)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Code generation and technical tasks","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"General-purpose text generation and chat","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Enterprise applications requiring multimodal data processing","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Research and development in AI","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Llama 4 Maverick features a Mixture-of-Experts architecture with 17 billion active parameters (from a total of 400 billion), contributing to its efficiency and performance. It is natively multimodal, incorporating early fusion of text and image data during training, which allows for seamless understanding and reasoning across modalities. The model supports a context window of up to 1 million tokens, enabling processing of extensive documents and complex inputs. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmark results highlight Maverick's strong capabilities, particularly in multimodal understanding. It has achieved scores of 59.6 on MMLU Pro, 90.0 on ChartQA, 94.4 on DocVQA, 73.4 on MMMU, and 73.7 on MathVista. In coding, it scored 43.4 on LiveCodeBench (specific timeframe). On the LMSYS Chatbot Arena, an experimental chat version of Maverick reportedly achieved an ELO score of 1417, placing it competitively among top models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Maverick is designed for efficient deployment, capable of running on a single H100 DGX host, with options for distributed inference for larger-scale needs. Its MoE architecture helps balance powerful performance with inference efficiency.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"EU restriction on vision","nodeType":"text"},{"data":{},"marks":[],"value":": Due to regulatory considerations, the use of the vision capabilities for individuals domiciled in, or companies with a principal place of business in, the European Union is not granted under the Llama 4 Community License.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dedicated reasoning focus","nodeType":"text"},{"data":{},"marks":[],"value":": While capable of reasoning, Llama 4 Maverick is not specifically positioned as a dedicated reasoning model in the same vein as some models optimized solely for complex, multi-step logical deduction.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Long context consistency","nodeType":"text"},{"data":{},"marks":[],"value":": Despite a large context window, some initial testing has suggested potential for inconsistent performance or degraded results with exceptionally long prompts.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Image input testing","nodeType":"text"},{"data":{},"marks":[],"value":": The model has been primarily tested for image understanding with up to 5 input images; performance with a larger number of images may vary, and developers are advised to perform additional testing for such use cases.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Benchmark interpretation","nodeType":"text"},{"data":{},"marks":[],"value":": As with many frontier models, there have been discussions regarding the interpretation and representativeness of benchmark scores compared to real-world performance across all possible tasks.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Memory intensity","nodeType":"text"},{"data":{},"marks":[],"value":": The Mixture-of-Experts architecture, while efficient for inference, means the full model still requires significant memory to load.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Citation","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Information gathered from Meta's official Llama website, announcements, technical documentation (including the Llama 4 Community License), and third-party analyses and benchmark reports.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://ai.meta.com/blog/llama-4-multimodal-intelligence/"},"content":[{"data":{},"marks":[],"value":"https://ai.meta.com/blog/llama-4-multimodal-intelligence/","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"3QJ0feD6uwQSQ2AIkd0MGV","type":"Asset","createdAt":"2023-07-26T17:05:15.460Z","updatedAt":"2023-07-26T17:05:15.460Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":6,"revision":1,"locale":"en-US"},"fields":{"title":"[MF] Question answering","file":{"url":"//images.ctfassets.net/j20krz61k3rk/3QJ0feD6uwQSQ2AIkd0MGV/923402a2cece93bb98e41996409aa497/Group_2621__1_.png","details":{"size":1323,"image":{"width":284,"height":152}},"fileName":"Group 2621 (1).png","contentType":"image/png"}}},"category":["Translation","Question answering","Text generation","Summarization","Conversational","Text classification","Custom ontology","Multimodal"],"publishedOn":"2025-05-08"},"sys":{"id":"49Ng8ywxKiqTI2iaiusCEh","type":"Entry"}},{"fields":{"title":"Google Gemini 2.5 Pro","pageurl":"google-gemini-2-5-pro","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Gemini 2.5 is a thinking model, designed to tackle increasingly complex problems. Gemini 2.5 Pro Experimental, leads common benchmarks by meaningful margins and showcases strong reasoning and code capabilities. Gemini 2.5 models are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multimodal input","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Text output","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Prompt optimizers","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Controlled generation","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Function calling (excluding compositional function calling)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Grounding with Google Search","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Code execution","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Count token","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Google’s latest AI model, Gemini 2.5 Pro, represents a major leap in AI performance and reasoning capabilities. Positioned as the most advanced iteration in the Gemini lineup, this experimental release of 2.5 Pro is now the top performer on the LMArena leaderboard, surpassing other models by a notable margin in human preference evaluations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Gemini 2.5 builds on Google’s prior efforts to enhance reasoning in AI, incorporating advanced techniques like reinforcement learning and chain-of-thought prompting. This version introduces a significantly upgraded base model paired with improved post-training, resulting in better contextual understanding and more accurate decision-making. The model is designed as a “thinking model,” capable of deeply analyzing information before responding — a capability now embedded across all future Gemini models.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The reasoning performance of 2.5 Pro stands out across key benchmarks such as GPQA and AIME 2025, even without cost-increasing test-time techniques. It also achieved a state-of-the-art 18.8% score on “Humanity’s Last Exam,” a benchmark crafted by experts to evaluate deep reasoning across disciplines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In terms of coding, Gemini 2.5 Pro significantly outperforms its predecessors. It excels in creating complex, visually rich web apps and agentic applications. On SWE-Bench Verified, a standard for evaluating coding agents, the model scored an impressive 63.8% using a custom setup.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Additional features include a 1 million token context window, with plans to extend to 2 million, enabling the model to manage vast datasets and multimodal inputs — including text, images, audio, video, and code repositories.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"01Atp8tauqzmlnpt2Ocj9J","type":"Asset","createdAt":"2025-04-11T00:04:13.407Z","updatedAt":"2025-04-11T00:04:13.407Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"Gemini 2.5 Pro","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/01Atp8tauqzmlnpt2Ocj9J/62f4439dadc8131c560dad89ad884bf0/unnamed__1_.png","details":{"size":69134,"image":{"width":381,"height":512}},"fileName":"unnamed (1).png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Context: Google Gemini 2.5 Pro may struggle with maintaining context over extended conversations, leading to inconsistencies in long interactions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Bias: As it is trained on a large corpus of internet text, Google Gemini 2.5 Pro may inadvertently reflect and perpetuate biases present in the training data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Creativity Boundaries: While capable of creative outputs, Google Gemini 2.5 Pro may not always meet specific creative standards or expectations for novel and nuanced content.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ethical Concerns: Google Gemini 2.5 Pro can be used to generate misleading information, offensive content, or be exploited for harmful purposes if not properly moderated.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Comprehension: Google Gemini 2.5 Pro might not fully understand or accurately interpret highly technical or domain-specific content, especially if it involves recent developments post-training data cutoff.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Dependence on Prompt Quality: The quality and relevance of the model’s output are highly dependent on the clarity and specificity of the input prompts provided by the user.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2#2.5-pro","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation","Question answering","Named entity recognition","Video global classification","Zero-shot classification","Summarization","Conversational","Image classification","Text classification","Video per frame classification","Custom ontology","Multimodal"],"publishedOn":"2025-04-10"},"sys":{"id":"HoyxXbBe6tXLcSlqeQiLC","type":"Entry"}},{"fields":{"title":"Grok 3","pageurl":"grok-3","description":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Grok 3, developed by xAI, is positioned as a highly advanced AI model engineered for complex problem-solving and real-time information processing. Leveraging a massive computational infrastructure, Grok 3 introduces innovative features like \"Think mode\" for detailed reasoning and \"DeepSearch\" for integrating current web data, aiming to push the boundaries of AI capabilities across various domains.","marks":[],"data":{}}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Intended Use","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Real-time data analysis and research","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Code generation and debugging","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Educational assistance and STEM learning","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Business process automation","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Generating both conversational and detailed responses","marks":[],"data":{}}]}]}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Performance","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Powered by the Colossus supercomputer with over 200,000 NVIDIA H100 GPUs, Grok 3 represents a significant leap in computational power, reportedly 10-15x more powerful than its predecessor. This enables enhanced speed and efficiency in processing complex queries. Grok 3 has demonstrated impressive results across key benchmarks. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It achieved a 93.3% score on the AIME 2025 mathematical assessment and 84.6% on graduate-level expert reasoning tasks (GPQA). In coding challenges, it scored 79.4% on LiveCodeBench. Internal benchmarks from xAI also suggest Grok 3 outperforms several leading models, including Gemini 2.5 Pro, GPT-4o, and Claude 3.5 Sonnet, in specific reasoning, math, and coding tasks. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"A notable feature is \"Think mode,\" which allows Grok 3 to break down problems and show its step-by-step reasoning process, similar to human structured thinking. \"Big Brain mode\" allocates additional computational resources for demanding tasks, delivering higher accuracy and deeper insights. The \"DeepSearch\" capability allows the model to pull and synthesize real-time information from the web, addressing the limitation of relying solely on static training data. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While benchmark performance is strong, real-world performance comparisons show varied results, with Grok 3 excelling in logic-heavy tasks and real-time data integration.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Limitations","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Content coherency","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Grok 3 may struggle with maintaining full coherency in generating very long-form content (e.g., beyond 5-10 pages).","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Real-time data reliability","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": While \"DeepSearch\" provides access to real-time data from sources like X and the web, there is a potential risk of generating unverified or biased information depending on the source quality.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Varied real-world performance","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Despite strong benchmark scores, some early real-world tests indicate that Grok 3 may not consistently outperform all rival models in every specific task.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Creativity nuances","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": The model's creative writing abilities may be perceived as more functional than nuanced compared to models specifically fine-tuned for highly creative tasks.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Resource intensity","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Utilizing advanced reasoning modes like \"Big Brain\" requires significant computational resources, which could impact response times and cost efficiency depending on the application.","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Citation","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Information gathered from various public sources, including xAI announcements, technical reviews, and public benchmark analyses.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://x.ai/news/grok-3"},"content":[{"nodeType":"text","value":"https://x.ai/news/grok-3","marks":[],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"3FmMYWp5E3feSYdHYMYbtr","type":"Asset","createdAt":"2023-07-26T15:47:11.284Z","updatedAt":"2023-09-14T20:23:16.988Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":19,"revision":5,"locale":"en-US"},"fields":{"title":"[MF] Token Classification (Text)","file":{"url":"//images.ctfassets.net/j20krz61k3rk/3FmMYWp5E3feSYdHYMYbtr/fc932e32d651981fe4a96549920932a5/Frame_3409.svg","details":{"size":2351,"image":{"width":284,"height":152}},"fileName":"Frame 3409.svg","contentType":"image/svg+xml"}}},"category":["Text generation","Question answering","Multimodal","Complex reasoning","Image classification","Custom ontology","Text classification"],"publishedOn":"2025-05-08"},"sys":{"id":"3KINcMonP6GmfMON2WYgmH","type":"Entry"}},{"fields":{"title":"OpenAI o4-mini","pageurl":"openai-o4-mini","description":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"OpenAI o4-mini is a recent addition to OpenAI's 'o' series of models, designed to provide fast, cost-efficient, and capable reasoning for a wide range of tasks. It balances performance with efficiency, making advanced AI reasoning more accessible for various applications, including those requiring high throughput.","marks":[],"data":{}}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Intended Use","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Fast technical tasks (data extraction, summarization of technical content)","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Coding assistance (fixing issues, generating code)","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Automating workflows (intelligent email triage, data categorization)","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Processing and reasoning about multimodal inputs (text and images)","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Applications where speed and cost-efficiency are key","marks":[],"data":{}}]}]}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Performance","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"OpenAI o4-mini is optimized for speed and affordability while still demonstrating strong reasoning capabilities. It features a substantial 200,000-token context window, allowing it to handle lengthy inputs and maintain context over extended interactions. The model supports up to 100,000 output tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Despite being a smaller model, o4-mini shows competitive performance on several benchmarks, particularly in STEM and multimodal reasoning tasks. It has achieved scores such as 93.4% on AIME 2024 and 81.4% on GPQA (without tools). In coding, it demonstrates effective performance on benchmarks like SWE-Bench.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"A key capability of o4-mini is its integration with OpenAI's suite of tools, including web Browse, Python code execution, and image analysis. The model is trained to intelligently decide when and how to use these tools to solve complex problems and provide detailed, well-structured responses, even incorporating visual information into its reasoning process. This makes it adept at tasks that require combining information from multiple sources or modalities.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Compared to larger models like OpenAI o3, o4-mini generally offers faster response times and significantly lower costs per token, making it a strong candidate for high-volume or latency-sensitive applications.\n","marks":[],"data":{}}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Limitations","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Usage limits: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Access to o4-mini on ChatGPT is subject to usage limits depending on the subscription tier (Free, Plus, Team, Enterprise/Edu).","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Fine-tuning: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"As of the latest information, fine-tuning capabilities are not yet available for o4-mini.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Reasoning depth: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"While strong for its size, o4-mini may not offer the same depth of reasoning as larger, more powerful models like o3 for the most complex, multi-step problems.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Benchmark vs. real-world: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"As with any model, performance on specific benchmarks may not always perfectly reflect performance on all real-world, highly-nuanced tasks.","marks":[],"data":{}}]}]}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Citation","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Information gathered from OpenAI's official announcements, documentation, and third-party analyses of model capabilities and benchmarks. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://openai.com/index/introducing-o3-and-o4-mini/"},"content":[{"nodeType":"text","value":"https://openai.com/index/introducing-o3-and-o4-mini/","marks":[],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Multimodal","Question answering","Text generation","Summarization","Conversational","Image classification","Text classification","Custom ontology"],"publishedOn":"2025-05-08"},"sys":{"id":"4rpzHmqUhNOowg7XoAOmTb","type":"Entry"}},{"fields":{"title":"Open AI Whisper","pageurl":"open-ai-whisper","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Whisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual data, offering improved robustness to accents, noise, and technical language. It transcribes and translates multiple languages into English.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Whisper is useful as an ASR solution, especially for English speech recognition.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The models are primarily trained and evaluated on ASR and speech translation to English.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"They show strong ASR results in about 10 languages.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"They may exhibit additional capabilities if fine-tuned for tasks like voice activity detection, speaker classification, or speaker diarization.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Speech recognition and translation accuracy is near state-of-the-art.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Performance varies across languages, with lower accuracy on low-resource or low-discoverability languages.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Whisper shows varying performance on different accents and dialects of languages.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Whisper is trained in a weakly supervised manner using large-scale noisy data, leading to potential hallucinations.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Hallucinations occur as the models combine predicting the next word and transcribing audio.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The sequence-to-sequence architecture may generate repetitive text, which can be partially mitigated by beam search and temperature scheduling.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"These issues may be more pronounced in lower-resource and/or lower-discoverability languages.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Higher word error rates may occur across speakers of different genders, races, ages, or other demographics.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://openai.com/index/whisper/","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation"],"publishedOn":"2025-03-17","price":"0.00016 $ per compute second"},"sys":{"id":"3KOkb43AZT7Am9RojOiXxI","type":"Entry"}},{"fields":{"title":"Open AI o3 mini","pageurl":"open-ai-o3-mini","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"OpenAI o3-mini is a powerful and fast model that advances the boundaries of what small models can achieve. It delivers, exceptional STEM capabilities—with particular strength in science, math, and coding—all while maintaining the low cost and reduced latency of OpenAI o1-mini.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"OpenAI o3-mini is designed to be used for tasks that require fast and efficient reasoning, particularly in technical domains like science, math, and coding. It’s optimized for STEM (Science, Technology, Engineering, and Mathematics) problem-solving, offering precise answers with improved speed compared to previous models. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Developers can use it for applications involving function calling, structured outputs, and other technical features. It’s particularly useful in contexts where both speed and accuracy are essential, such as coding, logical problem-solving, and complex technical inquiries.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"OpenAI o3-mini performs exceptionally well in STEM tasks, particularly in science, math, and coding, with improvements in both speed and accuracy compared to its predecessor, o1-mini. It delivers faster responses, with an average response time 24% quicker than o1-mini (7.7 seconds vs. 10.16 seconds).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In terms of accuracy, it produces clearer, more accurate answers, with 39% fewer major errors on complex real-world questions. Expert testers preferred its responses 56% of the time over o1-mini. It also matches o1-mini’s performance in challenging reasoning evaluations, including AIME and GPQA, especially when using medium reasoning effort.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"4syzcexKqif4pK5YvOO6P1","type":"Asset","createdAt":"2025-03-17T09:24:01.148Z","updatedAt":"2025-03-17T09:24:27.677Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"OpenAI o3","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/4syzcexKqif4pK5YvOO6P1/8bef196803c33e19349641371a30d08d/unnamed__5_.png","details":{"size":109273,"image":{"width":512,"height":384}},"fileName":"unnamed (5).png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"No Vision Capabilities","nodeType":"text"},{"data":{},"marks":[],"value":": Unlike some other models, o3-mini does not support visual reasoning tasks, so it's not suitable for image-related tasks.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Complexity in High-Intelligence Tasks","nodeType":"text"},{"data":{},"marks":[],"value":": While o3-mini performs well in most STEM tasks, for extremely complex reasoning, it may still lag behind larger models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Accuracy in Specific Domains","nodeType":"text"},{"data":{},"marks":[],"value":": While o3-mini excels in technical domains, it might not always match the performance of specialized models in certain niche areas, particularly those outside of STEM.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Potential Trade-Off Between Speed and Accuracy","nodeType":"text"},{"data":{},"marks":[],"value":": While users can adjust reasoning effort for a balance, higher reasoning efforts may lead to slightly longer response times.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Limited Fine-Tuning","nodeType":"text"},{"data":{},"marks":[],"value":": Though optimized for general STEM tasks, fine-tuning for specific use cases might be necessary to achieve optimal results in more specialized areas.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://openai.com/index/openai-o3-mini/","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Text classification","Custom ontology"],"publishedOn":"2025-03-17","price":"0.00110 $ per 1000 prompt tokens"},"sys":{"id":"2wU3ZYuhpgRq6I8aUnHsDV","type":"Entry"}},{"fields":{"title":"Claude 3.7 Sonnet","pageurl":"claude-3-7-sonnet","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Claude 3.7 Sonnet, by Anthropic, can produce near-instant responses or extended, step-by-step thinking. Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Claude 3.7 Sonnet is designed to enhance real-world tasks by offering a blend of fast responses and deep reasoning, particularly in coding, web development, problem-solving, and instruction-following.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Optimized for real-world applications rather than competitive math or computer science problems.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Useful in business environments requiring a balance of speed and accuracy.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ideal for tasks like bug fixing, feature development, and large-scale refactoring.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Coding Capabilities","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Strong in handling complex codebases, planning code changes, and full-stack updates.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Introduces ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Claude Code","nodeType":"text"},{"data":{},"marks":[],"value":", an agentic coding tool that can edit files, write and run tests, and manage code repositories like GitHub.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Claude Code significantly reduces development time by automating tasks that would typically take 45+ minutes manually.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Claude Sonnet 3.7 combines the capabilities of a language model (LLM) with advanced reasoning, allowing users to choose between standard mode for quick responses and extended thinking mode for deeper reflection before answering. In extended thinking mode, Claude self-reflects, improving performance in tasks like math, physics, coding, and following instructions. Users can also control the thinking time via the API, adjusting the token budget to balance speed and answer quality.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7C1umvdDLfNLUBI1O6w60p","type":"Asset","createdAt":"2025-03-17T09:08:20.030Z","updatedAt":"2025-03-17T09:08:20.030Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"Claude Sonnet 3.7","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7C1umvdDLfNLUBI1O6w60p/c5a436d9b15e22902c4d8e72244dc6de/unnamed__4_.png","details":{"size":639829,"image":{"width":1600,"height":1452}},"fileName":"unnamed (4).png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Early testing demonstrated Claude’s superiority in coding, with significant improvements in handling complex codebases, advanced tool usage, and planning code changes. It also excels at full-stack updates and producing production-ready code with high precision, as seen in use cases with platforms like Vercel, Replit, and Canva. Claude's performance is particularly strong in developing sophisticated web apps, dashboards, and reducing errors. This makes it a top choice for developers working on real-world coding tasks.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Context: Claude 3.7 Sonnet may struggle with maintaining context over extended conversations, leading to inconsistencies in long interactions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Bias: As Claude 3.7 Sonnet trained on a large corpus of internet text, it may inadvertently reflect and perpetuate biases present in the training data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Creativity Boundaries: While capable of creative outputs, Claude 3.7 Sonnet may not always meet specific creative standards or expectations for novel and nuanced content.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ethical Concerns: Claude 3.7 Sonnet can be used to generate misleading information, offensive content, or be exploited for harmful purposes if not properly moderated.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Comprehension: Claude 3.7 Sonnet might not fully understand or accurately interpret highly technical or domain-specific content, especially if it involves recent developments post-training data cutoff.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Dependence on Prompt Quality: The quality and relevance of the output of Claude 3.7 Sonnet are highly dependent on the clarity and specificity of the input prompts provided by the user.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anthropic.com/news/claude-3-7-sonnet"},"content":[{"data":{},"marks":[],"value":"https://www.anthropic.com/news/claude-3-7-sonnet","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Image classification","Text classification","Custom ontology","Translation","Named entity recognition"],"publishedOn":"2025-03-17","price":"0.00300 $ per 1000 prompt tokens"},"sys":{"id":"5jkFFkEQMjfpZbrqxaBBAl","type":"Entry"}},{"fields":{"title":"Amazon Nova Pro","pageurl":"amazon-nova-pro","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Amazon Nova Pro is a highly capable multimodal model that combines accuracy, speed, and cost for a wide range of tasks. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The capabilities of Amazon Nova Pro, coupled with its focus on high speeds and cost efficiency, makes it a compelling model for almost any task, including video summarization, Q\u0026A, mathematical reasoning, software development, and AI agents that can execute multistep workflows. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In addition to state-of-the-art accuracy on text and visual intelligence benchmarks, Amazon Nova Pro excels at instruction following and agentic workflows as measured by Comprehensive RAG Benchmark (CRAG), the Berkeley Function Calling Leaderboard, and Mind2Web.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multimodal Processing","nodeType":"text"},{"data":{},"marks":[],"value":": It can process and understand text, images, documents, and video, making it well suited for applications like video captioning, visual question answering, and other multimedia tasks.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Complex Language Tasks","nodeType":"text"},{"data":{},"marks":[],"value":": Nova Pro is designed to handle complex language tasks with high accuracy, such as deep reasoning, multi-step problem solving, and mathematical problem-solving.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Agentic Workflows","nodeType":"text"},{"data":{},"marks":[],"value":": It powers AI agents capable of performing multi-step tasks, integrated with retrieval-augmented generation (RAG) for improved accuracy and data grounding.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Customizable Applications","nodeType":"text"},{"data":{},"marks":[],"value":": Developers can fine-tune it with multimodal data for specific use cases, such as enhancing accuracy, reducing latency, or optimizing cost.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Fast Inference","nodeType":"text"},{"data":{},"marks":[],"value":": It’s optimized for fast response times, making it suitable for real-time applications in industries like customer service, automation, and content creation.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Amazon Nova Pro provides high performance, particularly in complex reasoning, multimodal tasks, and real-time applications, with speed and flexibility for developers.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"1CBiPnCnngIBgjlVb3sQ0X","type":"Asset","createdAt":"2025-03-17T09:00:32.999Z","updatedAt":"2025-03-17T09:00:32.999Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"Amazon Nova Pro","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/1CBiPnCnngIBgjlVb3sQ0X/373cbdbc2313210ea9cf4475e31b5a08/Screenshot_2025-03-13_at_4.36.06_AM.png","details":{"size":274635,"image":{"width":1412,"height":1344}},"fileName":"Screenshot 2025-03-13 at 4.36.06 AM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Domain Specialization","nodeType":"text"},{"data":{},"marks":[],"value":": While it performs well across a variety of tasks, it may not always be as specialized in certain niche areas or highly specific domains compared to models fine-tuned for those purposes.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Resource-Intensive","nodeType":"text"},{"data":{},"marks":[],"value":": As a powerful multimodal model, Nova Pro can require significant computational resources for optimal performance, which might be a consideration for developers working with large datasets or complex tasks.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Training Data","nodeType":"text"},{"data":{},"marks":[],"value":": Nova Pro's performance is highly dependent on the quality and diversity of the multimodal data it's trained on. Its performance in tasks involving complex or obscure multimedia content might be less reliable.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Fine-Tuning Requirements","nodeType":"text"},{"data":{},"marks":[],"value":": While customizability is a key feature, fine-tuning the model for very specific tasks or datasets might still require considerable effort and expertise from developers.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card"},"content":[{"data":{},"marks":[],"value":"https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Image classification","Text classification","Custom ontology"],"publishedOn":"2025-03-17","price":"0.00320 $ per 1000 response tokens"},"sys":{"id":"1x0N4JxIdEJ9nUDiIIeMNc","type":"Entry"}},{"fields":{"title":"Grok","pageurl":"grok","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Grok is a general purpose model that can be used for a variety of tasks, including generating and understanding text, code, and function calling.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Text and code: Generate code, extract data, prepare summaries and more.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Vision: Identify objects, analyze visuals, extract text from documents and more.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Function calling: Connect Grok to external tools and services for enriched interactions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"1vCOp9AG80ag9Mte3GwTl0","type":"Asset","createdAt":"2024-12-03T00:50:40.047Z","updatedAt":"2024-12-03T00:50:40.047Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"grok","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/1vCOp9AG80ag9Mte3GwTl0/79d6c4727c3255e70648d924bfd03f35/Screenshot_2024-11-26_at_5.59.34_PM.png","details":{"size":213926,"image":{"width":2676,"height":1069}},"fileName":"Screenshot 2024-11-26 at 5.59.34 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Context","nodeType":"text"},{"data":{},"marks":[],"value":": Grok may struggle with maintaining context over extended conversations, leading to inconsistencies in long interactions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Bias:","nodeType":"text"},{"data":{},"marks":[],"value":" As Grok trained on a large corpus of internet text, it may inadvertently reflect and perpetuate biases present in the training data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Creativity Boundaries:","nodeType":"text"},{"data":{},"marks":[],"value":" While capable of creative outputs, Grok may not always meet specific creative standards or expectations for novel and nuanced content.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ethical Concerns:","nodeType":"text"},{"data":{},"marks":[],"value":" Grok can be used to generate misleading information, offensive content, or be exploited for harmful purposes if not properly moderated.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Comprehension:","nodeType":"text"},{"data":{},"marks":[],"value":" Grok might not fully understand or accurately interpret highly technical or domain-specific content, especially if it involves recent developments post-training data cutoff.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dependence on Prompt Quality:","nodeType":"text"},{"data":{},"marks":[],"value":" The quality and relevance of the output of Grok are highly dependent on the clarity and specificity of the input prompts provided by the user.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://x.ai/blog/grok-2","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"3FmMYWp5E3feSYdHYMYbtr","type":"Asset","createdAt":"2023-07-26T15:47:11.284Z","updatedAt":"2023-09-14T20:23:16.988Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":19,"revision":5,"locale":"en-US"},"fields":{"title":"[MF] Token Classification (Text)","file":{"url":"//images.ctfassets.net/j20krz61k3rk/3FmMYWp5E3feSYdHYMYbtr/fc932e32d651981fe4a96549920932a5/Frame_3409.svg","details":{"size":2351,"image":{"width":284,"height":152}},"fileName":"Frame 3409.svg","contentType":"image/svg+xml"}}},"category":["Translation","Question answering","Named entity recognition","Text generation","Zero-shot classification","Summarization","Conversational","Text classification","Custom ontology"],"publishedOn":"2024-11-20","price":"0.00950 $ per 1000 prompt tokens"},"sys":{"id":"46fshF2cQLIytKu0ic3xDp","type":"Entry"}},{"fields":{"title":"Llama 3.2","pageurl":"llama-3-2","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Llama 3.2's 11B and 90B models support image reasoning, enabling tasks like understanding charts/graphs, captioning images, and pinpointing objects based on language descriptions. For example, the models can answer questions about sales trends from a graph or trail details from a map. They bridge vision and language by extracting image details, understanding the scene, and generating descriptive captions to tell the story, making them powerful for both visual and textual reasoning tasks.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Llama 3.2's 1B and 3B models support multilingual text generation and on-device applications with strong privacy. Developers can create personalized, agentic apps where data stays local, enabling tasks like summarizing messages, extracting action items, and sending calendar invites for follow-up meetings.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Llama 3.2 vision models outperform competitors like Gemma 2.6B and Phi 3.5-mini in tasks like image recognition, instruction-following, and summarization.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"5pBnlY7DJEfFAHmZeUbDcQ","type":"Asset","createdAt":"2024-11-20T23:56:29.041Z","updatedAt":"2024-11-21T00:00:29.913Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"llama 3.2","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/5pBnlY7DJEfFAHmZeUbDcQ/209e91b793627e183326d8b0bec86ae6/461288018_1255239495501495_271827633811450582_n__1_.png","details":{"size":280028,"image":{"width":3840,"height":3050}},"fileName":"461288018_1255239495501495_271827633811450582_n (1).png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Context:","nodeType":"text"},{"data":{},"marks":[],"value":" May struggle with maintaining context over extended conversations, leading to inconsistencies in long interactions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Medical images:","nodeType":"text"},{"data":{},"marks":[],"value":" Gemini Pro is not suitable for interpreting specialized medical images like CT scans and shouldn't be used for medical advice.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Bias:","nodeType":"text"},{"data":{},"marks":[],"value":" As it is trained on a large corpus of internet text, it may inadvertently reflect and perpetuate biases present in the training data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Creativity Boundaries:","nodeType":"text"},{"data":{},"marks":[],"value":" While capable of creative outputs, it may not always meet specific creative standards or expectations for novel and nuanced content.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ethical Concerns:","nodeType":"text"},{"data":{},"marks":[],"value":" Can be used to generate misleading information, offensive content, or be exploited for harmful purposes if not properly moderated.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Comprehension:","nodeType":"text"},{"data":{},"marks":[],"value":" Might not fully understand or accurately interpret highly technical or domain-specific content, especially if it involves recent developments post-training data cutoff.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dependence on Prompt Quality:","nodeType":"text"},{"data":{},"marks":[],"value":" The quality and relevance of the output are highly dependent on the clarity and specificity of the input prompts provided by the user.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"3QJ0feD6uwQSQ2AIkd0MGV","type":"Asset","createdAt":"2023-07-26T17:05:15.460Z","updatedAt":"2023-07-26T17:05:15.460Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":6,"revision":1,"locale":"en-US"},"fields":{"title":"[MF] Question answering","file":{"url":"//images.ctfassets.net/j20krz61k3rk/3QJ0feD6uwQSQ2AIkd0MGV/923402a2cece93bb98e41996409aa497/Group_2621__1_.png","details":{"size":1323,"image":{"width":284,"height":152}},"fileName":"Group 2621 (1).png","contentType":"image/png"}}},"category":["Translation","Question answering","Named entity recognition","Text generation","Zero-shot classification","Summarization","Conversational","Text classification","Custom ontology"],"publishedOn":"2024-11-20","price":"0.00950 $ per 1000 prompt tokens"},"sys":{"id":"MvfYqtBKdlPrgplv687rK","type":"Entry"}},{"fields":{"title":"Claude 3.5 Haiku","pageurl":"claude-3-5-haiku","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Claude 3.5 Haiku, the next generation of Anthropic's fastest and most cost-effective model, is optimal for use cases where speed and affordability matter. It improves on its predecessor across every skill set.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Claude 3.5 Haiku offers fast speeds, improved instruction-following, and accurate tool use, making it ideal for user-facing products and personalized experiences. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Key use cases include code completion, streamlining development workflows with quick, accurate code suggestions. It powers interactive chatbots for customer service, e-commerce, and education, handling high user interaction volumes. It excels at data extraction and labeling, processing large datasets in sectors like finance and healthcare. Additionally, it provides real-time content moderation for safe online environments.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"75pbhfj3wBI5MWufwkPEbQ","type":"Asset","createdAt":"2024-11-21T17:03:28.283Z","updatedAt":"2024-11-21T17:03:28.283Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"claude haiku 3.5","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/75pbhfj3wBI5MWufwkPEbQ/6bd03c7f0532cd46317be0054b15d6c8/_claude2.png","details":{"size":380079,"image":{"width":1600,"height":1168}},"fileName":"_claude2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Context:","nodeType":"text"},{"data":{},"marks":[],"value":" May struggle with maintaining context over extended conversations, leading to inconsistencies in long interactions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Bias:","nodeType":"text"},{"data":{},"marks":[],"value":" As it is trained on a large corpus of internet text, it may inadvertently reflect and perpetuate biases present in the training data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Creativity Boundaries: ","nodeType":"text"},{"data":{},"marks":[],"value":"While capable of creative outputs, it may not always meet specific creative standards or expectations for novel and nuanced content.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ethical Concerns:","nodeType":"text"},{"data":{},"marks":[],"value":" Can be used to generate misleading information, offensive content, or be exploited for harmful purposes if not properly moderated.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Comprehension:","nodeType":"text"},{"data":{},"marks":[],"value":" Might not fully understand or accurately interpret highly technical or domain-specific content, especially if it involves recent developments post-training data cutoff.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dependence on Prompt Quality:","nodeType":"text"},{"data":{},"marks":[],"value":" The quality and relevance of the output are highly dependent on the clarity and specificity of the input prompts provided by the user.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.anthropic.com/claude/docs/models-overview"},"content":[{"data":{},"marks":[],"value":"https://docs.anthropic.com/claude/docs/models-overview","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"3FmMYWp5E3feSYdHYMYbtr","type":"Asset","createdAt":"2023-07-26T15:47:11.284Z","updatedAt":"2023-09-14T20:23:16.988Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":19,"revision":5,"locale":"en-US"},"fields":{"title":"[MF] Token Classification (Text)","file":{"url":"//images.ctfassets.net/j20krz61k3rk/3FmMYWp5E3feSYdHYMYbtr/fc932e32d651981fe4a96549920932a5/Frame_3409.svg","details":{"size":2351,"image":{"width":284,"height":152}},"fileName":"Frame 3409.svg","contentType":"image/svg+xml"}}},"category":["Translation","Question answering","Named entity recognition","Text generation","Zero-shot classification","Summarization","Conversational","Text classification","Custom ontology"],"publishedOn":"2024-11-20","price":"0.00100 $ per 1000 prompt tokens"},"sys":{"id":"7cP2QkiDJH7qvKPMlOVK6h","type":"Entry"}},{"fields":{"title":"Amazon Rekognition ","pageurl":"amazon-rekognition","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Common objects detection and image classification model by AWS Rekognition. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Amazon Rekognition's object detection model is primarily used for detecting objects, scenes, activities, landmarks, faces, dominant colors, and image quality in images and videos. Some common use cases include: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Detect and label common objects in images ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Identify activities and scenes in visual content","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Enable content moderation and filtering","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Enhance image search capabilities","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html"},"content":[{"data":{},"marks":[],"value":"Learn more","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Amazon Rekognition's object detection model has been reported to have high accuracy ind detecting objects and scenes in images and videos. Its capabilities include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Can detect thousands of object categories","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Provides bounding boxes for object locations","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Assigns confidence scores to detections","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/rekognition/faqs/"},"content":[{"data":{},"marks":[],"value":"Learn more","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"the performance of the model may be limited by factors such as the quality and quantity of training data, the complexity of the image content, or the accuracy of the annotations. Additionally, Amazon Rekognition may have detection issues with black and white images and elderly people. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Other limitations include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"May struggle with small or partially obscured objects","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Performance can vary based on image quality and lighting","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Limited ability to understand context or relationships between objects","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Cannot identify specific individuals (separate face recognition API for that)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"May have biases in detection rates across different demographics","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/rekognition/"},"content":[{"data":{},"marks":[],"value":"Amazon Rekognition documentation ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"4beW2AgTGJdAqhZ6V6JaYB","type":"Asset","createdAt":"2023-09-14T18:50:53.476Z","updatedAt":"2023-09-14T18:50:53.476Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"[Model Foundry] object detection ","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/4beW2AgTGJdAqhZ6V6JaYB/29014839913c716a0956f82bca94e230/Frame_3406.svg","details":{"size":4193,"image":{"width":363,"height":232}},"fileName":"Frame 3406.svg","contentType":"image/svg+xml"}}},"category":["Object detection","Image classification"],"publishedOn":"2024-08-05","price":"0.001 $ per api call"},"sys":{"id":"3UQFKWJ0Se50fSKbtHjapp","type":"Entry"}},{"fields":{"title":"OpenAI o1-mini","pageurl":"o1-mini","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The o1 series of large language models are designed to perform advanced reasoning through reinforcement learning. These models engage in deep internal thought processes before delivering responses, enabling them to handle complex queries. o1-mini is a small specialized model optimized for STEM-related reasoning during its pretraining. Despite its reduced size, the o1-mini undergoes the same high-compute reinforcement learning pipeline as the larger o1 models, achieving comparable performance on many reasoning tasks while being significantly more cost-efficient.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"72dOqUaOS1mSQwX3RgHm8F","type":"Asset","createdAt":"2024-10-24T21:43:50.433Z","updatedAt":"2024-10-24T21:43:50.433Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"o1 mini","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/72dOqUaOS1mSQwX3RgHm8F/4d28ff438329b20d938c4c019c982b24/_1.png","details":{"size":45462,"image":{"width":1322,"height":806}},"fileName":"_1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Human raters compared o1-mini to GPT-4o on challenging, open-ended prompts across various domains to assess performance and accuracy in different types of tasks. As seen from the graph above, o-1 mini is optimized for STEM related tasks. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Optimization for STEM Knowledge:","nodeType":"text"},{"data":{},"marks":[],"value":" o1-mini is not optimized for tasks requiring non-STEM factual knowledge, which may result in less accurate responses when handling queries outside of technical or scientific domains.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Domain Preference:","nodeType":"text"},{"data":{},"marks":[],"value":" o1-mini is preferred to GPT-4o in reasoning-heavy domains, but is not preferred to GPT-4o in language-focused domains, where linguistic nuance and fluency are more critical.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Context:","nodeType":"text"},{"data":{},"marks":[],"value":" May struggle with maintaining context over extended conversations, leading to inconsistencies in long interactions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Bias:","nodeType":"text"},{"data":{},"marks":[],"value":" As it is trained on a large corpus of internet text, it may inadvertently reflect and perpetuate biases present in the training data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Creativity Boundaries:","nodeType":"text"},{"data":{},"marks":[],"value":" While capable of creative outputs, it may not always meet specific creative standards or expectations for novel and nuanced content.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ethical Concerns: ","nodeType":"text"},{"data":{},"marks":[],"value":"Can be used to generate misleading information, offensive content, or be exploited for harmful purposes if not properly moderated.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Comprehension:","nodeType":"text"},{"data":{},"marks":[],"value":" Might not fully understand or accurately interpret highly technical or domain-specific content, especially if it involves recent developments post-training data cutoff.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dependence on Prompt Quality:","nodeType":"text"},{"data":{},"marks":[],"value":" The quality and relevance of the output are highly dependent on the clarity and specificity of the input prompts provided by the user.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Image classification","Text classification","Custom ontology"],"publishedOn":"2024-10-24","price":"0.00300 $ per 1000 prompt tokens"},"sys":{"id":"10TC3Hf5wRYcqB5Dca92z4","type":"Entry"}},{"fields":{"title":"OpenAI O1 Preview","pageurl":"openai-o1-preview","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The o1 series of large language models are designed to perform advanced reasoning through reinforcement learning. These models engage in deep internal thought processes before delivering responses, enabling them to handle complex queries. o1 preview is designed to reason about hard problems using broad general knowledge about the world.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"25dfV7xgzd5vv0tF1P2M16","type":"Asset","createdAt":"2024-10-24T22:14:55.914Z","updatedAt":"2024-10-24T22:14:55.914Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"o1 preview ","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/25dfV7xgzd5vv0tF1P2M16/7fccad0dc08d270ee21d0659e56496d0/_2.png","details":{"size":41870,"image":{"width":512,"height":296}},"fileName":"_2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Human raters compared o1-preview to GPT-4o on challenging, open-ended prompts across various domains to assess performance and accuracy in different types of tasks. As seen from the graph above, o-1 preview is optimized for STEM related tasks. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Optimization for STEM Knowledge:","nodeType":"text"},{"data":{},"marks":[],"value":" o1-preview is not optimized for tasks requiring non-STEM factual knowledge, which may result in less accurate responses when handling queries outside of technical or scientific domains.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Domain Preference","nodeType":"text"},{"data":{},"marks":[],"value":": o1-preview is preferred to GPT-4o in reasoning-heavy domains, but is not preferred to GPT-4o in language-focused domains, where linguistic nuance and fluency are more critical.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Context:","nodeType":"text"},{"data":{},"marks":[],"value":" May struggle with maintaining context over extended conversations, leading to inconsistencies in long interactions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Bias:","nodeType":"text"},{"data":{},"marks":[],"value":" As it is trained on a large corpus of internet text, it may inadvertently reflect and perpetuate biases present in the training data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Creativity Boundaries:","nodeType":"text"},{"data":{},"marks":[],"value":" While capable of creative outputs, it may not always meet specific creative standards or expectations for novel and nuanced content.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ethical Concerns:","nodeType":"text"},{"data":{},"marks":[],"value":" Can be used to generate misleading information, offensive content, or be exploited for harmful purposes if not properly moderated.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Comprehension:","nodeType":"text"},{"data":{},"marks":[],"value":" Might not fully understand or accurately interpret highly technical or domain-specific content, especially if it involves recent developments post-training data cutoff.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dependence on Prompt Quality: ","nodeType":"text"},{"data":{},"marks":[],"value":"The quality and relevance of the output are highly dependent on the clarity and specificity of the input prompts provided by the user.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://openai.com/index/introducing-openai-o1-preview/","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Image classification","Text classification","Custom ontology"],"publishedOn":"2024-10-24","price":"0.01500 $ per 1000 prompt tokens"},"sys":{"id":"6sQbpsNXwahGJbCD3cag3C","type":"Entry"}},{"fields":{"title":"OpenAI GPT-4o","pageurl":"openai-gpt-4o","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"GPT-4o (“o” for “omni”) is the most advanced OpenAI model. It is multimodal (accepting text or image inputs and outputting text), and it has the same high intelligence as GPT-4 Turbo but is much more efficient—it generates text 2x faster and is 50% cheaper. Additionally, GPT-4o has the best vision and performance across non-English languages of any OpenAI model. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"1q9snDGPbqSouUgsWGXZGm","type":"Asset","createdAt":"2024-08-06T00:01:43.223Z","updatedAt":"2024-08-06T00:01:43.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":5,"revision":1,"locale":"en-US"},"fields":{"title":"gpt-4vo","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/1q9snDGPbqSouUgsWGXZGm/9200c12bdb29fa7f53aebb692740b2c3/Screenshot_2024-08-05_at_5.00.34_PM.png","details":{"size":113338,"image":{"width":1240,"height":695}},"fileName":"Screenshot 2024-08-05 at 5.00.34 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"As measured on traditional benchmarks, GPT-4o achieves GPT-4 Turbo-level performance on text, reasoning, and coding intelligence, while setting new high watermarks on multilingual, audio, and vision capabilities.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Accuracy:","nodeType":"text"},{"data":{},"marks":[],"value":" While GPT-4o can provide detailed and accurate responses, it may occasionally generate incorrect or nonsensical answers, particularly for highly specialized or obscure queries.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Context:","nodeType":"text"},{"data":{},"marks":[],"value":" May struggle with maintaining context over extended conversations, leading to inconsistencies in long interactions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Bias:","nodeType":"text"},{"data":{},"marks":[],"value":" As it is trained on a large corpus of internet text, it may inadvertently reflect and perpetuate biases present in the training data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Creativity Boundaries:","nodeType":"text"},{"data":{},"marks":[],"value":" While capable of creative outputs, it may not always meet specific creative standards or expectations for novel and nuanced content.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ethical Concerns:","nodeType":"text"},{"data":{},"marks":[],"value":" Can be used to generate misleading information, offensive content, or be exploited for harmful purposes if not properly moderated.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Comprehension:","nodeType":"text"},{"data":{},"marks":[],"value":" Might not fully understand or accurately interpret highly technical or domain-specific content, especially if it involves recent developments post-training data cutoff.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dependence on Prompt Quality: ","nodeType":"text"},{"data":{},"marks":[],"value":"The quality and relevance of the output are highly dependent on the clarity and specificity of the input prompts provided by the user.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://platform.openai.com/docs/models/gpt-4o","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Image classification","Text classification","Custom ontology"],"publishedOn":"2024-08-05","price":"0.00500 $ per 1000 prompt tokens"},"sys":{"id":"5RZQrn2rTBmEJ1wmzmRGf5","type":"Entry"}},{"fields":{"title":"Google Gemini 1.5 Flash","pageurl":"google-gemini-1-5-flash","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Google introduces Gemini 1.5 Flash that is faster and cheaper than Gemini 1.5 Pro. Gemini 1.5 Flash also has a context window of up to 1 million tokens, it can process vast amounts of information, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"1 hour of video ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"11 hours of audio ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"over 700,000 words","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This substantial capacity makes Gemini 1.5 Flash particularly well-suited for real-time and context-intensive applications, enabling seamless processing of large volumes of information.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-1"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Rapid text generation and completion","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Real-time conversation and chatbot applications","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Quick information retrieval and summarization","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multimodal understanding (text, images, audio, video)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Code generation and analysis","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Task planning and execution","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"May sacrifice some accuracy for speed compared to larger models","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Performance on highly specialized or technical tasks may vary","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Could exhibit biases present in its training data","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Limited by the knowledge cutoff of its training data","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Cannot access real-time information or browse the internet","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"May struggle with tasks requiring deep logical reasoning or complex mathematical computations","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://deepmind.google/technologies/gemini/flash/","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Question answering","Named entity recognition","Text generation","Zero-shot classification","Summarization","Conversational","Image classification","Text classification","Video per frame classification","Custom ontology"],"publishedOn":"2024-08-06","price":"0.00135 $ per 1000 response tokens"},"sys":{"id":"6H5nIpZUbNg5neQ7UYhnbI","type":"Entry"}},{"fields":{"title":"BLIP2 (blip2-flan-t5-xxl)","pageurl":"blip2-flan-t5-xxl","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"BLIP2 is a visual language model (VLM) that can perform multi-modal tasks such as image captioning and visual question answering. This model is the BLIP-2, Flan-T5-XXL variant.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"BLIP2 is a visual language model (VLM) that can perform multi-modal tasks such as image captioning and visual question answering. It can also be used for ","nodeType":"text"},{"data":{},"marks":[],"value":"chat-like conversations by feeding the image and the previous conversation as prompt to the model.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Best performance within the BLIP2 family of models.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"BLIP2 is fine-tuned on image-text datasets (e.g. ","nodeType":"text"},{"data":{"uri":"https://laion.ai/blog/laion-400-open-dataset/"},"content":[{"data":{},"marks":[],"value":"LAION","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") collected from the internet. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data. Other limitations include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"May struggle with highly abstract or culturally specific visual concepts","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Performance can vary based on image quality and complexity","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Limited by the training data of its component models (vision encoder and language model)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Cannot generate or edit images (only processes and describes them)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Requires careful prompt engineering for optimal performance in some tasks","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Li, J., Li, D., Savarese, S., \u0026 Hoi, S. (2023). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation"],"publishedOn":"2024-08-05","price":"$0.004 per compute second"},"sys":{"id":"2EK6RsS4k4URN7iKo5t6G3","type":"Entry"}},{"fields":{"title":"Amazon Textract","pageurl":"amazon-textract","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use this optical character recognition (OCR) model to extract text from images. The model will take images as input and generate text annotations, classifying them within bounding boxes. The bounding boxes will be grouped by words.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-1"},{"data":{},"content":[{"data":{},"marks":[],"value":"Amazon Textract extracts text, handwriting, and structured data from scanned documents, including forms and tables, surpassing basic OCR capabilities. It provides extracted data with bounding box coordinates and confidence scores to help users accurately assess and utilize the information.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Custom Queries","nodeType":"text"},{"data":{},"marks":[],"value":": Amazon Textract allows customization of its pretrained Queries feature to enhance accuracy for specific document types while retaining data control. Users can upload and annotate a minimum of ten sample documents through the AWS Console to tailor the Queries feature within hours.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Layout","nodeType":"text"},{"data":{},"marks":[],"value":": Amazon Textract extracts various layout elements from documents, including paragraphs, titles, and headers, via the Analyze Document API. This feature can be used independently or in conjunction with other document analysis features.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Optical Character Recognition (OCR)","nodeType":"text"},{"data":{},"marks":[],"value":": Textract’s OCR detects both printed and handwritten text from documents and images, handling various fonts, styles, and text distortions through machine learning. It is capable of recognizing text in noisy or distorted conditions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Form Extraction","nodeType":"text"},{"data":{},"marks":[],"value":": Textract identifies and retains key-value pairs from documents automatically, preserving their context for easier database integration. Unlike traditional OCR, it maintains the relationship between keys and values without needing custom rules.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Table Extraction","nodeType":"text"},{"data":{},"marks":[],"value":": The service extracts and maintains the structure of tabular data in documents, such as financial reports or medical records, allowing for easy import into databases. Data in rows and columns, like inventory reports, is preserved for accurate application.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Signature Detection","nodeType":"text"},{"data":{},"marks":[],"value":": Textract detects signatures on various documents and images, including checks and loan forms, and provides the location and confidence scores of these signatures in the API response.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Query-Based Extraction","nodeType":"text"},{"data":{},"marks":[],"value":": Textract enables data extraction using natural language queries, eliminating the need to understand document structure or format variations. It’s pre-trained on a diverse set of documents, reducing post-processing and manual review needs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Analyze Lending","nodeType":"text"},{"data":{},"marks":[],"value":": The Analyze Lending API automates the extraction and classification of information from mortgage loan documents. It uses preconfigured machine learning models to organize and process loan packages upon upload.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Invoices and Receipts","nodeType":"text"},{"data":{},"marks":[],"value":": Textract leverages machine learning to extract key data from invoices and receipts, such as vendor names, item prices, and payment terms, despite varied layouts. This reduces the complexity of manual data extraction.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Identity Documents","nodeType":"text"},{"data":{},"marks":[],"value":": Textract uses ML to extract and understand details from identity documents like passports and driver’s licenses, including implied information. This facilitates automated processes in ID verification, account creation, and more without template reliance.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"May struggle with highly stylized fonts or severe document degradation","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Handwriting recognition accuracy can vary based on writing style","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Performance may decrease with complex, multi-column layouts","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Limited ability to understand document context or interpret extracted data","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"May have difficulty with non-Latin scripts or specialized notation","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://docs.aws.amazon.com/textract/\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation"],"publishedOn":"2024-08-05","price":"0.0015 $ per api call"},"sys":{"id":"2NBTCqO1tvvqMV4t55VIPs","type":"Entry"}},{"fields":{"title":"Claude 3 Haiku","pageurl":"claude-3-haiku","description":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Claude 3 Haiku stands out as the fastest and most cost-effective model in its class. Boasting cutting-edge vision capabilities and exceptional performance on industry benchmarks, Haiku offers a versatile solution for a broad spectrum of enterprise applications.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-1","data":{},"content":[{"nodeType":"text","value":"Intended Use","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Performance","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Near-instant results:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" The Claude 3 models excel in powering real-time tasks such as live customer chats, auto-completions, and data extraction. Haiku, the fastest and most cost-effective model, and Sonnet, which is twice as fast as Claude 2 and 2.1, both offer superior intelligence and performance for a variety of demanding applications.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Vision and Image Processing:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" This model can process and analyze visual input, extracting insights from documents, processing web UI, generating image catalog metadata, and more.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Long context and near-perfect recall:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Haiku offers a 200K context window and can process inputs exceeding 1 million tokens, with Claude 3 Opus achieving near-perfect recall surpassing 99% accuracy in the \"Needle In A Haystack\".","marks":[],"data":{}}]}]}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"6XotdPpwjKt79z3cBFDlPA","type":"Asset","createdAt":"2024-07-31T00:14:36.914Z","updatedAt":"2024-07-31T00:14:36.914Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"Claude 3 Haiku","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/6XotdPpwjKt79z3cBFDlPA/687971c339719fe1b4a455ab7d5a9639/Screenshot_2024-07-30_at_4.54.01_PM.png","details":{"size":397472,"image":{"width":1236,"height":1129}},"fileName":"Screenshot 2024-07-30 at 4.54.01 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Limitations","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Medical images:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Claude 3 Haiku is not suitable for interpreting specialized medical images like CT scans and shouldn't be used for medical advice.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Non-English:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Claude 3 Haiku may not perform optimally when handling images with text of non-Latin alphabets, such as Japanese or Korean.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Big text:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Users should enlarge text within the image to improve readability for Claude 3.5, but avoid cropping important details.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Rotation:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Claude 3 Haiku may misinterpret rotated / upside-down text or images.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Visual elements:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Claude 3 Haiku may struggle to understand graphs or text where colors or styles like solid, dashed, or dotted lines vary.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Spatial reasoning:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Claude 3 Haiku struggles with tasks requiring precise spatial localization, such as identifying chess positions.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Hallucinations:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" The model can provide factually inaccurate information.","marks":[],"data":{}}]}]}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Citation","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"https://docs.anthropic.com/claude/docs/models-overview","marks":[],"data":{}}]}]},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text classification","Conversational","Summarization","Zero-shot classification","Text generation","Question answering","Translation"],"publishedOn":"2024-07-30","price":"0.00125 $ per 1000 response tokens"},"sys":{"id":"6K4I6WF247UY8sgitPOsYM","type":"Entry"}},{"fields":{"title":"Llama 3.1 405B","pageurl":"llama-3-1-405b","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Llama 3.1 builds upon the success of its predecessors, offering enhanced performance, improved safety measures, and greater flexibility for researchers and developers. It demonstrates exceptional proficiency in language understanding, generation, and reasoning tasks, making it a powerful tool for a wide range of applications. It is one of the most powerful open source AI models, which you can fine-tune, distill and deploy anywhere. The latest instruction-tuned model is available in 8B, 70B and 405B versions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-1"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Research and Development:","nodeType":"text"},{"data":{},"marks":[],"value":" Ideal for exploring cutting-edge AI research, developing new model architectures, and fine-tuning for specific tasks.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Open-Source Community:","nodeType":"text"},{"data":{},"marks":[],"value":" Designed to foster collaboration and accelerate innovation in the open-source AI community.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Education and Experimentation:","nodeType":"text"},{"data":{},"marks":[],"value":" A valuable resource for students and researchers to learn about and experiment with state-of-the-art LLM technology.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Enhanced Performance:","nodeType":"text"},{"data":{},"marks":[],"value":" Llama 3.1 boasts improvements in various benchmarks, including language modeling, question answering, and text summarization.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Improved Safety:","nodeType":"text"},{"data":{},"marks":[],"value":" The model has undergone rigorous safety training to reduce the risk of generating harmful or biased outputs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Increased Flexibility:","nodeType":"text"},{"data":{},"marks":[],"value":" Llama 3.1 is available in multiple sizes, allowing users to choose the model that best suits their compute resources and specific needs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7n4d9L8odXl0QojTwrTjPG","type":"Asset","createdAt":"2024-07-23T21:15:42.090Z","updatedAt":"2024-07-23T21:15:42.090Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"llama 3.1 performance","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7n4d9L8odXl0QojTwrTjPG/1320ccdd40034fe68e6fd462458fe5b6/Screenshot_2024-07-23_at_2.14.30_PM.png","details":{"size":332533,"image":{"width":2088,"height":1284}},"fileName":"Screenshot 2024-07-23 at 2.14.30 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Data Freshness: The pretraining data has a cutoff of December 2023.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Citations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md"},"content":[{"data":{},"marks":[],"value":"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Translation","Question answering","Text generation","Summarization","Conversational","Text classification","Zero-shot classification"],"publishedOn":"2024-07-23","price":"0.00950 $ per 1000 response tokens"},"sys":{"id":"38v6zu0PFhjmUHRva01q2e","type":"Entry"}},{"fields":{"title":"Claude 3 Opus","pageurl":"claude-3-opus","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Claude 3 Opus represents the most powerful and advanced model in the Claude 3 family. This state-of-the-art AI delivers unparalleled performance on highly complex tasks, demonstrating fluency and human-like understanding that sets it apart from other AI models.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Task automation: plan and execute complex actions across APIs and databases, interactive coding","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"R\u0026D: research review, brainstorming and hypothesis generation, drug discovery","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Strategy: advanced analysis of charts \u0026 graphs, financials and market trends, forecasting","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Use cases","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multilingual Capabilities:","nodeType":"text"},{"data":{},"marks":[],"value":" Claude 3 Opus offers improved fluency in non-English languages such as Spanish and Japanese, enabling use cases like translation services and global content creation.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Vision and Image Processing:","nodeType":"text"},{"data":{},"marks":[],"value":" This model can process and analyze visual input, extracting insights from documents, processing web UI, generating image catalog metadata, and more.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Steerability and Ease of Use:","nodeType":"text"},{"data":{},"marks":[],"value":" Claude 3 Opus is designed to be easy to steer and better at following directions, giving you more control over model behavior and more predictable, higher-quality outputs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmark","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"GPT-4","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Evaluated few-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Few-shot SOTA","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"SOTA","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Best external model (includes benchmark-specific training)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"VQAv2","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"VQA score (test-dev)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"77.2%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"0-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"67.6%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Flamingo 32-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"84.3%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"PaLI-17B","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"TextVQA","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"VQA score (val)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"78.0%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"0-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"37.9%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Flamingo 32-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"71.8%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"PaLI-17B","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"ChartQA","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Relaxed accuracy (test)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"78.5%","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"-","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"58.6%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pix2Struct Large","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"AI2 Diagram (AI2D)","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Accuracy (test)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"78.2%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"0-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"-","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"42.1%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pix2Struct Large","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"DocVQA","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ANLS score (test)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"88.4%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"0-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"-","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"88.4%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ERNIE-Layout 2.0","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Infographic VQA","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ANLS score (test)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"75.1%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"0-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"-","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"61.2%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Applica.ai TILT","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"TVQA","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Accuracy (val)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"87.3%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"0-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"-","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"86.5%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"MERLOT Reserve Large","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"LSMDC","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Fill-in-the-blank accuracy (test)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"45.7%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"0-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"31.0%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"MERLOT Reserve Large","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"52.9%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"MERLOT","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"}],"nodeType":"table"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here are some of the limitations we are aware of:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Medical images: Claude 3 is not suitable for interpreting specialized medical images like CT scans and shouldn't be used for medical advice.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Spatial reasoning: Claude 3 struggles with tasks requiring precise spatial localization, such as identifying chess positions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Hallucinations: the model can provide factually inaccurate information.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Image shape: Claude 3 struggles with panoramic and fisheye images.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Metadata and resizing: Claude 3 doesn't process original file names or metadata, and images are resized before analysis, affecting their original dimensions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"CAPTCHAS: For safety reasons, Claude 3 has a system to block the submission of CAPTCHAs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.anthropic.com/claude/docs/models-overview"},"content":[{"data":{},"marks":[],"value":"https://docs.anthropic.com/claude/docs/models-overview","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Translation","Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Text classification"],"publishedOn":"2024-03-19","price":"0.000075 $ per response token"},"sys":{"id":"5Oswse0QZyrYOcEnyMR8vd","type":"Entry"}},{"fields":{"title":"Google Gemini 1.5 Pro","pageurl":"gemini-1-5-pro","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Google Gemini 1.5 Pro is a large scale language model trained jointly across image, audio, video, and text data for the purpose of building a model with both strong generalist capabilities across modalities alongside cutting-edge understanding and reasoning performance. This model is using Gemini Pro on Vertex AI, with enhanced performance, scalability, deployability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Gemini 1.5, the next-generation model from Google, offers significant performance enhancements. Built on a Mixture-of-Experts (MoE) architecture, it delivers improved efficiency in training and serving. Gemini 1.5 Pro, the mid-size multimodal model, exhibits comparable quality to the larger 1.0 Ultra, featuring a breakthrough experimental long-context understanding feature. With a context window of up to 1 million tokens, it can process vast amounts of information, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"1 hour of video ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"11 hours of audio ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"over 700,000 words","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Use cases","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Gemini is good at a wide variety of multimodal use cases, including but not limited to:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Info Seeking: Fusing world knowledge with information extracted from the images and videos.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Object Recognition: Answering questions related to fine-grained identification of the objects in images and videos.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Digital Content Understanding: Answering questions and extracting information from various contents like infographics, charts, figures, tables, and web pages.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Structured Content Generation: Generating responses in formats like HTML and JSON, based on provided prompt instructions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Captioning / Description: Generating descriptions of images and videos with varying levels of detail. For example, for images, the prompt can be: “Can you write a description about the image?”. For videos, the prompt can be:  “Can you write a description about what's happening in this video?","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Extrapolations: Suggesting what else to see based on location, what might happen next/before/between images or videos, and enabling creative uses like writing stories based on visual inputs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here are some of the limitations we are aware of:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Medical images: Gemini Pro is not suitable for interpreting specialized medical images like CT scans and shouldn't be used for medical advice.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Hallucinations: the model can provide factually inaccurate information.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Counting: Gemini Pro may give approximate counts for objects in images.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"CAPTCHAS: For safety reasons, Gemini Pro has a system to block the submission of CAPTCHAs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-turn (multimodal) chat: Not trained for chatbot functionality or answering questions in a chatty tone, and can perform less effectively in multi-turn conversations.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Following complex instructions: Can struggle with tasks requiring multiple reasoning steps. Consider breaking down instructions or providing few-shot examples for better guidance.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Counting: Can only provide rough approximations of object counts, especially for obscured objects.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Spatial reasoning: Can struggle with precise object/text localization in images. It may be less accurate in understanding rotated images.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#sundar-note"},"content":[{"data":{},"marks":[],"value":"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#sundar-note","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation","Question answering","Zero-shot classification","Summarization","Conversational","Text classification"],"publishedOn":"2024-03-19","price":"$0 compute costs for a limited time"},"sys":{"id":"2GE20hQdH7RcJ4LHquMAws","type":"Entry"}},{"fields":{"title":"OpenAI GPT4","pageurl":"gpt4","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"ChatGPT is an advanced conversational artificial intelligence language model developed by OpenAI. This It is based on the GPT-4 architecture and has been trained on a diverse range of internet text to generate human-like responses in natural language conversations. This model is latest version.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nIntended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"GPT stands for Generative Pre-trained Transformer (GPT), a type of language model that uses deep learning to generate human-like, conversational text. As a multimodal model, GPT-4 is able to accept both text and image outputs. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, OpenAI has not yet made the GPT-4 model's visual input capabilities available through any platform. Currently the only way to access the text-input capability through OpenAI is with a subscription to ChatGPT Plus.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nThe GPT-4 model is optimized for conversational interfaces and can be used to generate text summaries, reports, and responses. Currently, only text modality is supported. \n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nPerformance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nGPT-4 is a highly advanced model that can accept both image and text inputs, making it more versatile than its predecessor, GPT-3. However, it is important to use the appropriate techniques to get the best results, as the model behaves differently than older GPT models.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"OpenAI published results for the GPT-4 model comparing it to other state-of-the-art models (SOTA) including its previous GPT-3.5 model.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmark","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-header-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"GPT-4","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Evaluated few-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-header-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"GPT-3.5","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Evaluated few-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-header-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"LM SOTA","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Best external LM evaluated few-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-header-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"SOTA","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Best external model (includes benchmark-specific training)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-header-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"MMLU","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Multiple-choice questions in 57 subjects (professional \u0026 academic)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"86.4%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"5-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"70.0%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"5-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"70.7%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"5-shot U-PaLM","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"75.2%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"5-shot Flan-PaLM","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"HellaSwag","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Commonsense reasoning around everyday events","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"95.3%\n10-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"85.5%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"10-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"84.2%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"LLAMA (validation set)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"85.6%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ALUM","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"AI2 ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reasoning Challenge (ARC)","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Grade-school multiple choice science questions. Challenge-set.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"96.3%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"25-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"85.2%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"25-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"84.2%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"8-shot PaLM","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"85.6%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ST-MOE","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"WinoGrande","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Commonsense reasoning around pronoun resolution","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"87.5%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"5-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"81.6%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"5-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"84.2%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"5-shot PALM","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"85.6%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"5-shot PALM","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"HumanEval","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Python coding tasks","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"67.0%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"0-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"48.1%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"0-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"26.2%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"0-shot PaLM","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"65.8%","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"CodeT + GPT-3.5","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"DROP ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"(f1 score)","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reading comprehension \u0026 arithmetic.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"80.9","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"3-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"64.1","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"3-shot","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"70.8","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"1-shot PaLM","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"88.4","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"QDGAT","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"table-cell"}],"nodeType":"table-row"}],"nodeType":"table"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nLimitations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nThe underlying format of the GPT-4 model is more likely to change over time, and it may provide less useful responses if interacted with in the same way as older models. The GPT-4 model has similar limitations to previous GPT models, such as being prone to ","nodeType":"text"},{"data":{"uri":"https://labelbox.com/blog/what-does-it-mean-when-an-llm-hallucinates/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LLM hallucination and reasoning errors","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". OpenAI claims that GPT-4 hallucinates less often than other models, regardless.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nLimitations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://openai.com/research/gpt-4","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"211lFrASr0m1jYMznPaIbD","type":"Asset","createdAt":"2023-05-23T13:08:42.930Z","updatedAt":"2023-05-23T13:08:42.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":3,"revision":1,"locale":"en-US"},"fields":{"title":"LM","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/211lFrASr0m1jYMznPaIbD/2d3223f3b2270a1821eaf10201d1f6e7/lm.svg","details":{"size":1075,"image":{"width":284,"height":152}},"fileName":"lm.svg","contentType":"image/svg+xml"}}},"category":["Translation","Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Text classification","Named entity recognition"],"publishedOn":"2023-06-01","price":"$0.00006 per token"},"sys":{"id":"5BhDGwTkp9vwxta0Rlal4D","type":"Entry"}},{"fields":{"title":"Grounding Dino + SAM","pageurl":"grounding-dino-sam","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Grounding Dino + SAM, or Grounding SAM, uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models by enabling users to create segmentation masks quickly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Create segmentation masks using SAM and classify the masks using Grounding Dino. The masks are intended to be used as pre-labels.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Inaccurate classification might occur, especially for aerial images for classification like roof and solar panels.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The accuracy of masks is suboptimal in areas with complex shapes, low contrast zones, and small objects.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others. (2023). Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Chen, Jiaqi and Yang, Zeyu and Zhang, Li. (2023). Semantic Segment Anything.","nodeType":"text"},{"data":{"uri":"https://github.com/fudan-zvg/Semantic-Segment-Anything"},"content":[{"data":{},"marks":[],"value":" https://github.com/fudan-zvg/Semantic-Segment-Anything","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"60Uo0leJIJrMiABPqR1Ak2","type":"Asset","createdAt":"2023-05-23T13:04:35.114Z","updatedAt":"2023-05-23T13:04:35.114Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-US"},"fields":{"title":"Segment","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/60Uo0leJIJrMiABPqR1Ak2/fed42bf5e852e22b30155a263fcb8c53/line.svg","details":{"size":2925,"image":{"width":284,"height":152}},"fileName":"line.svg","contentType":"image/svg+xml"}}},"category":["Custom ontology","Image segmentation","Video segmentation"],"publishedOn":"2024-04-16","price":"0.00145 $ per compute second"},"sys":{"id":"1dxL5IT9sOlFPoyr4xj5O7","type":"Entry"}},{"fields":{"title":"CLIP ViT LAION (Classification)","pageurl":"clip-vit-laion-classification","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Zero shot image classification. The model will produce exactly one prediction per classification task unless the max classification score is less than the confidence threshold.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Zero shot image classification. The model will produce exactly one prediction per classification task unless the max classification score is less than the confidence threshold.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B (","nodeType":"text"},{"data":{"uri":"https://laion.ai/blog/laion-5b/"},"content":[{"data":{},"marks":[],"value":"https://laion.ai/blog/laion-5b/","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") using OpenCLIP (","nodeType":"text"},{"data":{"uri":"https://github.com/mlfoundations/open_clip"},"content":[{"data":{},"marks":[],"value":"https://github.com/mlfoundations/open_clip","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k"},"content":[{"data":{},"marks":[],"value":"Learn more at Hugging Face","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"3jPq8XMrnDB1JUuW3ukrD0","type":"Asset","createdAt":"2023-05-23T13:12:20.571Z","updatedAt":"2023-05-23T13:12:20.571Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":3,"revision":1,"locale":"en-US"},"fields":{"title":"clip","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/3jPq8XMrnDB1JUuW3ukrD0/b8e70de819d1cc8fe839344318fc9074/clip.svg","details":{"size":1966,"image":{"width":284,"height":152}},"fileName":"clip.svg","contentType":"image/svg+xml"}}},"category":["Image classification"],"publishedOn":"2023-05-09","price":"$ 0.0018 per compute second"},"sys":{"id":"52l5L5tggJDJr96rYKhbpl","type":"Entry"}},{"fields":{"title":"OWL-ViT","pageurl":"owl-vit","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in Simple Open-Vocabulary Object Detection with Vision Transformers by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nIntended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\nPerformance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"OWL-ViT achieves zero-shot detection results competitive with much more complex approaches on the challenging LVIS benchmark and outperforms pre-existing methods on image-conditioned detection by a large margin.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"3ugnDSgmvpqRovvrkX40Db","type":"Asset","createdAt":"2023-05-23T13:03:12.547Z","updatedAt":"2023-05-23T13:13:33.249Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"box","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/3ugnDSgmvpqRovvrkX40Db/89e60374258a43c86c7f3fe03daccd56/yolo.svg","details":{"size":4101,"image":{"width":283,"height":152}},"fileName":"yolo.svg","contentType":"image/svg+xml"}}},"category":["Object detection"],"publishedOn":"2023-05-10","price":"$0.0003 per compute second"},"sys":{"id":"3b26xDwyTdnjs8wKwF14Mi","type":"Entry"}},{"fields":{"title":"Grounding DINO","pageurl":"groundingdino","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Open-set object detector that by combines a Transformer-based detector DINO with grounded pre-training. It can detect arbitrary objects with human inputs such as category names or referring expressions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Useful for zero shot object detection tasks.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n Performance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a 52.5 AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean 26.1 AP","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/IDEA-Research/GroundingDINO"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/IDEA-Research/GroundingDINO","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"underline"}],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/2303.05499"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://arxiv.org/abs/2303.05499","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"3ugnDSgmvpqRovvrkX40Db","type":"Asset","createdAt":"2023-05-23T13:03:12.547Z","updatedAt":"2023-05-23T13:13:33.249Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"box","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/3ugnDSgmvpqRovvrkX40Db/89e60374258a43c86c7f3fe03daccd56/yolo.svg","details":{"size":4101,"image":{"width":283,"height":152}},"fileName":"yolo.svg","contentType":"image/svg+xml"}}},"category":["Object detection"],"publishedOn":"2023-07-26","price":"$0.0003 per compute second"},"sys":{"id":"4hLmDP5MJ0MqpGEAiyuMZB","type":"Entry"}},{"fields":{"title":"Google Gemini Pro","pageurl":"google-gemini","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Google Gemini is a large scale language model trained jointly across image, audio, video, and text data for the purpose of building a model with both strong generalist capabilities across modalities alongside cutting-edge understanding and reasoning performance. This model is using Gemini Pro on Vertex AI, with enhanced performance, scalability, deployability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Gemini is designed to process and reason across different inputs like text, images, video, and code. On Quantumworks Lab platform, Gemini supports wide range of image and language tasks such as text generation, question answering, classification, visual understanding, answering questions about math, etc.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nPerformance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Gemini is Google’s largest and most capable model to date. It is the first AI model to surpass human experts on the Massive Multitask Language Understanding (MMLU) benchmark, and supposed SOTA performances on multi-modal tasks.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nLimitations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"There is a continued need for research and development on reducing “hallucinations” generated by LLMs. LLMs also struggle with tasks requiring high level reasoning abilities such as casual understanding, logical deduction, and counterfactual reasoning.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nCitation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"},"content":[{"data":{},"marks":[],"value":"Technical report on Gemini: a Family of Highly Capable Multimodal Models ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Text classification","Named entity recognition","Visual question answering"],"publishedOn":"2023-12-11","price":"$0 compute costs for a limited time"},"sys":{"id":"1dScS8RtwBNKnfS66rONQp","type":"Entry"}},{"fields":{"title":"Tesseract OCR","pageurl":"tesseract-ocr","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Tesseract was originally developed at Hewlett-Packard Laboratories Bristol UK and at Hewlett-Packard Co, Greeley Colorado USA between 1985 and 1994, with some more changes made in 1996 to port to Windows, and some C++izing in 1998. In 2005 Tesseract was open sourced by HP. From 2006 until November 2018 it was developed by Google (","nodeType":"text"},{"data":{"uri":"https://github.com/tesseract-ocr/tesseract#brief-history"},"content":[{"data":{},"marks":[],"value":"Source","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":")","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[],"value":"This model uses Tesseract (","nodeType":"text"},{"data":{"uri":"https://github.com/tesseract-ocr/tesseract"},"content":[{"data":{},"marks":[],"value":"https://github.com/tesseract-ocr/tesseract","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") for OCR, and writes the output as a text annotation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nPerformance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tesseract works best with straight, well scanned text. For text in the wild, handwriting and other use cases, other models should be used.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nCitation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://github.com/tesseract-ocr/tesseract","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation"],"publishedOn":"2023-12-06","price":"$0.0003 per compute second"},"sys":{"id":"7fyDTWzlNlNBXF28yE3MC3","type":"Entry"}},{"fields":{"title":"BLIP2 (OPT 6.7B)","pageurl":"blip2-opt-6-7b","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"BLIP2 is a visual language model (VLM) that can perform multi-modal tasks such as image captioning and visual question answering. This model is the BLIP-2, OPT 6.7B variant","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"BLIP2 is a visual language model (VLM) that can perform multi-modal tasks such as image captioning and visual question answering.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nPerformance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"BLIP2 is fine-tuned on image-text datasets (e.g. LAION ) collected from the internet. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nCitation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Li, J., Li, D., Savarese, S., \u0026 Hoi, S. (2023). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation"],"publishedOn":"2023-08-16","price":"$0.002 per compute second"},"sys":{"id":"7tRQ55OtB2kKduwD6Gnrch","type":"Entry"}},{"fields":{"title":"OpenAI GPT-3.5 Turbo","pageurl":"gpt-3-5","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"ChatGPT is an advanced conversational artificial intelligence language model developed by OpenAI. It is based on the GPT-3.5 architecture and has been trained on a diverse range of internet text to generate human-like responses in natural language conversations. This model is latest version.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The primary intended use of ChatGPT-3.5 is to provide users with a conversational AI system that can assist with a wide range of language-based tasks. It is designed to engage in interactive conversations, answer questions, provide explanations, offer suggestions, and facilitate information retrieval. It can also engage in more specialized conversations such as prose writing, programming, script and dialogues, and explaining scientific concepts to varying degrees of complexity.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nChatGPT-3.5 can also be employed in customer service applications, virtual assistants, educational tools, and other systems that require natural language understanding and generation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"ChatGPT-3.5 has demonstrated strong performance across various language tasks, including understanding and generating text in a conversational context. It is capable of producing coherent and contextually relevant responses to user input as well as storing information short-term to offer meaningful information and engage in meaningful dialogue with a user. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The model has been trained on a vast amount of internet text, enabling it to leverage a wide range of knowledge and information. OpenAI has ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nHowever, it is important to note that ChatGPT-3.5 may occasionally produce incorrect or nonsensical answers, especially when presented with ambiguous queries or lacking relevant context.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nLimitations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nWhile ChatGPT-3.5 exhibits impressive capabilities, it also has certain limitations that users should be aware of:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Lack of Real-Time Information:","nodeType":"text"},{"data":{},"marks":[],"value":" ChatGPT-3.5’s training data is current until September\n2021. Therefore, it may not be aware of recent events or have access to real-time\ninformation. Consequently, it may provide outdated or inaccurate responses to queries\nrelated to current affairs or time-sensitive topics.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Sensitivity to Input Phrasing:","nodeType":"text"},{"data":{},"marks":[],"value":" ChatGPT-3.5 is sensitive to slight rephrasing of questions\nor prompts. While it strives to generate consistent responses, minor changes in\nphrasing can sometimes lead to different answers or interpretations. Users should\nbe mindful of this when interacting with ChatGPT.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Propensity for Biases:","nodeType":"text"},{"data":{},"marks":[],"value":" ChatGPT-3.5 is trained on a broad range of internet text, which\nmay include biased or objectionable content. While efforts have been made to mitigate\nbiases during training, the model may still exhibit some biases or respond to sensitive\ntopics inappropriately. It is important to use ChatGPT's responses critically and\nbe aware of potential biases.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Inability to Verify Information: ","nodeType":"text"},{"data":{},"marks":[],"value":"ChatGPT-3.5 does not have the capability to verify\nthe accuracy or truthfulness of the information it generates. It relies solely on\npatterns in the training data and may occasionally provide incorrect or misleading\ninformation. Users are encouraged to independently verify any critical or factual\ninformation obtained from the model.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Lack of Context Awareness: ","nodeType":"text"},{"data":{},"marks":[],"value":"Although ChatGPT-3.5 can maintain short-term context within a conversation, it lacks long-term memory. Consequently, it may sometimes provide inconsistent or contradictory answers within the same conversation. Users should ensure they provide sufficient context to minimize potential misunderstandings.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"LLM Hallucation:","nodeType":"text"},{"data":{},"marks":[],"value":" ChatGPT-3.5, much like many other large language models, is prone to a phenomenon called ","nodeType":"text"},{"data":{"uri":"https://labelbox.com/blog/what-does-it-mean-when-an-llm-hallucinates/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"“LLM Hallucation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"”. At its core, GPT3.5 and other LLMs are neural networks trained on a large amount of text data. It is a statistical machine, and essentially \"learns\" to predict the next word in a sentence based on the context provided by preceding words. As a result, LLM hallucination occurs when the model's primary objective is to generate text that is coherent and contextually appropriate, rather than factually accurate.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In addition, OpenAI has since released GPT-4, which makes significant improvements on the GPT-3.5 architecture. From parameters, to memory, and now accepting both text and image data compared to GPT-3.5, GPT-4.0 is a significant improvement.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nCitation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"https://platform.openai.com/docs/models","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"211lFrASr0m1jYMznPaIbD","type":"Asset","createdAt":"2023-05-23T13:08:42.930Z","updatedAt":"2023-05-23T13:08:42.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":3,"revision":1,"locale":"en-US"},"fields":{"title":"LM","description":"","file":{"url":"//images.ctfassets.net/j20krz61k3rk/211lFrASr0m1jYMznPaIbD/2d3223f3b2270a1821eaf10201d1f6e7/lm.svg","details":{"size":1075,"image":{"width":284,"height":152}},"fileName":"lm.svg","contentType":"image/svg+xml"}}},"category":["Translation","Question answering","Text generation","Zero-shot classification","Summarization","Conversational","Text classification","Named entity recognition"],"publishedOn":"2023-05-04","price":"$0.000002 per token"},"sys":{"id":"ikjTcZPUQdLSOvRbw1x6G","type":"Entry"}},{"fields":{"title":"Llava13B","pageurl":"llava13b","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"LLaVA v1.5 is a Large Language and Vision Assistant.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Multimodal Chat Capabilities: LLaVA has been designed to exhibit impressive chat capabilities, particularly when fine-tuned alongside models like GPT-4, for general-purpose visual and language understanding.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Visual Instruction Following: LLaVA is a pioneering effort in the field of Visual Instruction Tuning, a technique that fine-tunes large language models to understand and execute instructions based on visual cues. This is particularly beneficial in scenarios where a model needs to describe an image, perform an action in a virtual environment, or answer questions about a scene in a photograph.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nPerformance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.","nodeType":"text"},{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Limitations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hallucination: Similar to GPT-3.5, LLaVA has limitations like hallucination, where the model\nmight generate inaccurate or false information.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Mathematical Problem-Solving: LLaVA faces limitations in mathematical problem-solving, an area where other models or systems might have superior capabilities.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Translation Tasks: LLaVA 1.5 struggles with translation tasks, indicating a potential limitation in language translation capabilities.","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"","nodeType":"text"},{"data":{"uri":"https://llava-vl.github.io/"},"content":[{"data":{},"marks":[],"value":"https://llava-vl.github.io/","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"License","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/haotian-liu/LLaVA/blob/main/LICENSE"},"content":[{"data":{},"marks":[],"value":"https://github.com/haotian-liu/LLaVA/blob/main/LICENSE","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation"],"publishedOn":"2023-08-16","price":"$0.000925 per compute second"},"sys":{"id":"7xwl0cCzz5Lddhm2VqxfFa","type":"Entry"}},{"fields":{"title":"Google Imagen","pageurl":"google-imagen","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Google's Imagen generates a relevant description or caption for a given image. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The model generates image captioning, allowing users to generate a relevant description for an image. You can use this information for a variety of use cases: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Creators can generate captions for uploaded images","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Generate captions to describe products","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Integrate Imagen captioning with an app using the API to create new experiences","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Imagen currently supports five languages: English, German, French, Spanish and Italian.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Imagen model has reported to achieve high accuracy, however may have limitations in generating captions for complex or abstract images. The model may also generate captions that reflect biases present in the training data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nCitations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning"},"content":[{"data":{},"marks":[],"value":"Google Image captioning documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" \n","nodeType":"text"},{"data":{"uri":"https://cloud.google.com/vertex-ai/docs/generative-ai/image/visual-question-answering"},"content":[{"data":{},"marks":[],"value":"Google visual question answering documentation ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation"],"publishedOn":"2023-09-15","price":"$0.0015 per api call"},"sys":{"id":"3dg6zGZkZcxOO2bQWLa00F","type":"Entry"}},{"fields":{"title":"BLIP2 (Flan-T5 XL COCO)","pageurl":"blip-2-flan-t5-xl-coco","description":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"BLIP2 is a visual language model (VLM) that can perform multi-modal tasks such as image captioning and visual question answering.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nIntended Use","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nBLIP2 is a visual language model (VLM) that can perform multi-modal tasks such as image captioning and visual question answering.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nPerformance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"BLIP-2 ViT-g OPT2.7B has a score of 52.3 on VQAv2 dataset","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Citation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Li, J., Li, D., Savarese, S., \u0026 Hoi, S. (2023). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"image":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"j20krz61k3rk"}},"id":"7BwHZa3jT5iSvHxamLXxMD","type":"Asset","createdAt":"2023-07-26T16:53:43.975Z","updatedAt":"2023-09-14T16:17:55.869Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":8,"revision":2,"locale":"en-US"},"fields":{"title":"[MF] Text generation ","file":{"url":"//images.ctfassets.net/j20krz61k3rk/7BwHZa3jT5iSvHxamLXxMD/8603c2f68ca3813f9fe472029189f7a8/Frame_2127__1_.svg","details":{"size":1138,"image":{"width":363,"height":232}},"fileName":"Frame 2127 (1).svg","contentType":"image/svg+xml"}}},"category":["Text generation"],"publishedOn":"2023-06-02","price":"$ 0.003 per compute second"},"sys":{"id":"47KHbox6c1IxkB3aEpN29P","type":"Entry"}}]},"__N_SSG":true},"page":"/product/model/foundry-models","query":{},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/product/model/foundry-models/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 10:45:12 GMT -->
</html>