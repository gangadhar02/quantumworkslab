<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/working-with-videos-using-gemini-1-5-and-multimodal-models/?ref=labelbox.ghost.io by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 13:29:43 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Working with videos using Gemini 1.5 and multimodal models</title><meta name="description" data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Working with videos using Gemini 1.5 and multimodal models" data-next-head=""/><meta property="og:description" data-next-head=""/><meta property="og:url" content="https://labelbox-guides.ghost.io/working-with-videos-using-gemini-1-5-and-multimodal-models/" data-next-head=""/><meta property="og:image" content="https://labelbox-guides.ghost.io/content/images/2024/05/mmbazel_a_photorealistic_depiction_of_analyzing_video_frames_of_26a0629d-30a4-4c38-8bd2-30877fefb937-1.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Working with videos using Gemini 1.5 and multimodal models" data-next-head=""/><meta name="twitter:description" data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox-guides.ghost.io/working-with-videos-using-gemini-1-5-and-multimodal-models/" data-next-head=""/><meta property="twitter:image" content="https://labelbox-guides.ghost.io/content/images/2024/05/mmbazel_a_photorealistic_depiction_of_analyzing_video_frames_of_26a0629d-30a4-4c38-8bd2-30877fefb937-1.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g34[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.giShFC .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.giShFC .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.giShFC .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:2px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h2{padding-top:10px;}}/*!sc*/
.giShFC .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h3{padding-top:10px;}}/*!sc*/
.giShFC .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.giShFC .content a:hover{color:#1e40af;}/*!sc*/
.giShFC .content li{margin-bottom:20px;}/*!sc*/
.giShFC .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.giShFC .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.giShFC .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.giShFC .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}/*!sc*/
.giShFC .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.giShFC .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100% !important;height:100% !important;margin:0 auto;}/*!sc*/
.giShFC .content .kg-button-card{margin-bottom:20px;height:auto;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn{-webkit-transition-property:all;transition-property:all;-webkit-transition-timing-function:cubic-bezier(.4,0,.2,1);transition-timing-function:cubic-bezier(.4,0,.2,1);-webkit-transition-duration:.15s;transition-duration:.15s;background-color:#2563eb;padding:0.75rem;font-size:1rem;line-height:1.5rem;font-weight:500;border-radius:0.5rem;color:#fafafa;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn:hover{background-color:#1d4ed8;}/*!sc*/
.giShFC .content .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 0 0 0.75em;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;}/*!sc*/
data-styled.g35[id="id__PostContentWrapper-sc-1ct5gml-0"]{content:"giShFC,"}/*!sc*/
.kUXEED #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.kUXEED .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.kUXEED .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.kUXEED #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.kUXEED #image-viewer .close:hover,.kUXEED #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.kUXEED .modal-content{width:100%;}}/*!sc*/
data-styled.g36[id="id__ImageModal-sc-1ct5gml-1"]{content:"kUXEED,"}/*!sc*/
@media (max-width:1026px){.cwKcJJ.toc-container{display:none;}}/*!sc*/
.cwKcJJ.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g37[id="id__TocContainer-sc-1ct5gml-2"]{content:"cwKcJJ,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/guides/%5bid%5d-78cf43cbe169ea75.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="id__ImageModal-sc-1ct5gml-1 kUXEED"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All guides</a><main class="id__TocContainer-sc-1ct5gml-2 cwKcJJ toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">Working with videos using Gemini 1.5 and multimodal models</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox-guides.ghost.io/content/images/2024/05/mmbazel_a_photorealistic_depiction_of_analyzing_video_frames_of_26a0629d-30a4-4c38-8bd2-30877fefb937-1.png"/></div><main class="id__PostContentWrapper-sc-1ct5gml-0 giShFC md:px-24"><div class="content js-toc-content"><h2 id="introduction">Introduction&nbsp;</h2><p>Given the pace of innovation in AI, teams are continually looking to integrate various data types like text, images, and video as a way to unlock new functionality for delivering next-gen applications and experiences. The development of multimodal models, which can process and understand diverse data inputs, is one of the most promising advancements.&nbsp;</p><p>Notably, combining video processing with the capabilities of large language models (LLMs) is a breakthrough feature for teams who want to highlight specific objects, scenes, and actions from high-volumes of video content.</p><p>However, many multimodal models, such as Gemini 1.5, require teams to convert videos to 1 frame per second (FPY) for analysis. Converting FPS, while tedious, aligns the video data with the model’s optimal processing capabilities, ensuring that no critical information is lost while maintaining compatibility with the model’s precision.</p><p>In this blog post, we’ll show how easy it is to convert videos to 1FPS and upload them to Quantumworks Lab Catalog for generating predictions in Model Foundry.&nbsp;</p><hr><p></p><h1 id="preparing-videos-for-inference-with-multimodal-models">Preparing videos for inference with multimodal models</h1><p>The two main approaches for ensuring videos meet the exact 1FPS requirements of multimodal models like Gemini 1.5 include:</p><ol><li>Video Upload: Converting a video to 1 FPS and uploading this converted video into Quantumworks Lab Catalog.</li><li>Frame Extraction Upload: Converting a video to 1 FPS, extracting each of the video frames, and uploading the extracted video frame images to Catalog.&nbsp;</li></ol><p>By doing so, users will be able to use various multimodal models like GPT- 4v, Claude 3 Opus and Amazon Rekognition (as well as additional models natively supported by Quantumworks Lab). It is important to note that 1FPS is not necessary for Model Foundry’s use on video datarows, but this approach may be helpful when using certain multimodal models.&nbsp;</p><h2 id="approach-1upload-a-1-fps-video-to-catalog">Approach #1 - Upload a 1 FPS Video to Catalog</h2><p>The first approach is converting a video to 1 frame per second (FPS) and uploading the converted video to Catalog.&nbsp;&nbsp;</p><p>You can follow along in this <a href="https://colab.research.google.com/drive/1mJv6L5PbSZf4TmskWG1XMAqj4A4QS1Ye?usp=sharing"><u>Google Colab Notebook</u></a>.</p><h3 id="steps"><strong>Steps</strong></h3><p><strong>Step 1: Download Video From Google Cloud Storage (GCS)</strong></p><figure class="kg-card kg-image-card"><img src="../../../labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png" class="kg-image" alt="" loading="lazy" width="1114" height="202" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png 1114w" sizes="(min-width: 720px) 720px"></figure><ul><li>The function ‘convert_video_to_1fps_and_download’ is defined to handle the video conversion and upload process.&nbsp;</li><li>It starts by downloading the specified video file from a Google Cloud Storage bucket to the local Colab environment.&nbsp;</li></ul><p><strong>Step 2: Convert the Video to 1 FPS</strong></p><figure class="kg-card kg-image-card"><img src="../../../labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png" class="kg-image" alt="" loading="lazy" width="1044" height="82" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png 1044w" sizes="(min-width: 720px) 720px"></figure><ul><li>Using the ‘ffmpeg’ tool, the video is converted to 1 frame per second (FPS). This step is crucial and can be easily modified to change the FPS by altering the ‘-vf fps=1’ parameter in the ‘subprocess.run’ command. For example, to convert the video to 2 FPS, you would change ‘fps=1’ to ‘fps =2.&nbsp;</li></ul><p><strong>Step 3: Upload the Converted Video Back to GCS</strong></p><figure class="kg-card kg-image-card"><img src="../../../labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.14.20-AM.png" class="kg-image" alt="" loading="lazy" width="843" height="74" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.14.20-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.14.20-AM.png 843w" sizes="(min-width: 720px) 720px"></figure><ul><li>After conversion, the video is saved locally and then uploaded back to the GCS bucket.</li></ul><p><strong>Step 4:&nbsp; Upload to Catalog</strong></p><figure class="kg-card kg-image-card"><img src="../../../labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png" class="kg-image" alt="" loading="lazy" width="1055" height="238" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png 1055w" sizes="(min-width: 720px) 720px"></figure><ul><li>The converted video is then integrated with Quantumworks Lab by creating a dataset and adding the video as a data row.</li><li>The code uses the Quantumworks Lab SDK to create a dataset and upload the converted video, making it ready for further processing and labeling in Labelbox.</li></ul><h2 id="approach-2upload-extracted-video-frames-to-catalog">Approach #2 - Upload Extracted Video Frames to Catalog</h2><p>The second approach is converting a video to 1 frame per second (FPS), extracting video frames, and uploading the extracted video frame images to Catalog.</p><p>You can follow along in this <a href="https://colab.research.google.com/drive/1ZYye--AOyHUzXaF24mU1nHuGS0myViJl?usp=sharing"><u>Google Colab Notebook</u></a><u>.</u></p><h3 id="steps-1"><strong>Steps</strong></h3><p><strong>Step 1: Download Video From Google Cloud Storage (GCS)&nbsp;</strong></p><figure class="kg-card kg-image-card"><img src="../../../labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png" class="kg-image" alt="" loading="lazy" width="1175" height="232" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png 1175w" sizes="(min-width: 720px) 720px"></figure><ul><li>The function ‘process_video_from_gcs’ is designed to handle the entire workflow of downloading the video, extracting frames, and uploading them back to GCS.</li><li>It downloads the specified video file from a GCS bucket to the local Colab environment.</li></ul><p><strong>Step 2: Extract Video Frames at 1 FPS</strong></p><figure class="kg-card kg-image-card"><img src="../../../labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png" class="kg-image" alt="" loading="lazy" width="1148" height="39" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png 1148w" sizes="(min-width: 720px) 720px"></figure><ul><li>The frames are saved locally in a specified directory.</li><li>Using the ‘ffmpeg’ tool, the video is processed to extract frames at 1 frame per second. Again, this can be easily adjusted by changing the ‘-vf fps=1’ parameter in the ‘subprocess.run’ command to any desired frame rate.</li></ul><p><strong>Step 3: Rename and Organize Frames</strong></p><figure class="kg-card kg-image-card"><img src="../../../labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.17.28-AM.png" class="kg-image" alt="" loading="lazy" width="896" height="447" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.17.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.17.28-AM.png 896w" sizes="(min-width: 720px) 720px"></figure><ul><li>Each extracted frame is renamed sequentially for better organization, using a consistent naming pattern (e.g. ‘image_frame_0001.jpg’).&nbsp;</li></ul><p><strong>Step 4: Upload Frames to GCS</strong></p><ul><li>The renamed frames are uploaded back to the GCS bucket into a directory specific to the frames of the video.&nbsp;</li><li>Public URLs and global keys for each uploaded frame are generated and stored for later use.</li></ul><figure class="kg-card kg-image-card"><img src="../../../labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.18.07-AM.png" class="kg-image" alt="" loading="lazy" width="672" height="235" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.18.07-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.18.07-AM.png 672w"></figure><p><strong>Step 5: Upload to Catalog</strong></p><ul><li>The extracted frames are uploaded to Catalog by creating a dataset and adding each frame as an individual data row, using the Quantumworks Lab SDK.</li></ul><hr><p></p><h1 id="additional-considerations">Additional Considerations&nbsp;</h1><p>When deciding between uploading 1FPS videos or extracted video frames to Catalog, there are some important features to consider:</p><p><strong>For 1FPS Videos</strong></p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/docsz/AD_4nXcGfqLUwq4Vt_KSDpedsvAvF3Z9rP3bdesbBACTSMAf6NDeDlRxKSCsTw_QSV_GulMPPGda9XMK-kKw1a6osW8UFL1jjEe_8QW0JxMvyHqRI-jkbBN4OUdiCYk22o5kb4AbVR_ZoYaoac-YASMQGwcROso.3b4b990.d?key=kFXSU0zZGAC6bV-HbWQZiQ" class="kg-image" alt="" loading="lazy" width="437" height="175"></figure><p></p><ul><li><strong>Temporal Context Preservation</strong>:<ul><li>Provides a continuous video stream and maintains the temporal relationships and sequences between frames</li></ul></li><li><strong>Simplified Workflow</strong>:&nbsp;<ul><li>Managing a single video file is typically simpler than handling multiple image files, reducing the number of files to manage</li></ul></li><li><strong>Limited Flexibility in Frame Manipulation</strong>:<ul><li>Videos offer less flexibility for individual frame manipulation and augmentation compared to separate image files<br></li></ul></li></ul><p><strong>For Extracted Video Frames</strong></p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/docsz/AD_4nXeSVX1V08D-2UrE4e5c5VflZgVPJU7EguUFQR1Faa7-JI4iGLa9ZwwzpywKgKQtgz7-jSZ--82W9x2JrfWPgKYl93of_NbdlWphywYp6Sz77LsQKr9kG34CBEcMqht_fOoHL0ZrmVBOjZ47Zhwpg5A-MFs.3b5b990.d?key=kFXSU0zZGAC6bV-HbWQZiQ" class="kg-image" alt="" loading="lazy" width="1081" height="418"></figure><ul><li><strong>Increased Optionality</strong>:&nbsp;<ul><li>Process only frames that are most relevant, allowing for greater control over the dataset</li><li>Extracted frames can be pre-processed or filtered according to specific criteria, potentially enhancing the quality of input data</li><li>Focus the analysis on the most important moments in the video</li></ul></li><li><strong>Loss of Temporal Context</strong>:<ul><li>Individual frames lack the temporal continuity present in videos, which might be crucial based on the specific use case.</li></ul></li><li><strong>Increased File Management</strong>:&nbsp;<ul><li>Handling a large number of individual image files within Quantumworks Lab Catalog</li></ul></li></ul><h2 id="next-steps">Next steps&nbsp;</h2><p>Once a video dataset has been converted to 1FPS via one of the two approaches highlighted above, Gemini 1.5 and other multimodal models can be used to <a href="../harnessing-ai-for-efficient-video-labeling/index.html#introduction"><u>harness AI for efficient video labeling</u></a>, enabling precise and accurate frame classification to enhance data insights and model training.</p><hr><p></p><h1 id="conclusion">Conclusion&nbsp;</h1><p>In this blog post, we explored the importance of preparing video data for multimodal models like Gemini 1.5, which analyze video data at 1 frame per second (FPS). This ensures maximum compatibility with the model's processing capabilities for accurate and efficient analysis.</p><p>Choosing between uploading 1 FPS videos and extracted video frames depends on your project's specific needs. As a rule of thumb, uploading videos preserves temporal context and simplifies file management, while extracting frames allows for detailed analysis and greater control, but with more file handling.</p><p>By understanding these considerations, you can effectively leverage multimodal models like Gemini 1.5, optimizing your workflow for enhanced performance and accuracy in video classification tasks.&nbsp;</p><p>If you are not already using Quantumworks Lab, you can <a href="https://app.labelbox.com/signup?utm_keyword=Quantumworks Lab%2520annotation&amp;utm_source=house&amp;utm_medium=email&amp;utm_campaign=524&amp;gclid=Cj0KCQjw2PSvBhDjARIsAKc2cgM16FFzTqltH9d4iQrrMQBwayH1ftA6F6A8dBgcdgz7MIR6dpv773oaAmgzEALw_wcB&amp;landingPageAnonymousId=%22b04806c5-ff2b-4a09-9379-a9837b1801a9%22&amp;referrer_url=https://www.google.com/">get started for free</a> or <a href="../../sales/index.html">contact us</a> to learn more about using multimodal models for better video classification.&nbsp;</p></div></main></div></div></div><div class="mt-5 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="my-20 w-full h-[1px] bg-neutral-200"></div><div class=""><div class=""><h2 class="mb-12 text-center text-3xl md:text-4xl font-medium">Continue reading</h2></div><div class="flex flex-wrap justify-content-center"><div class="max-w-md mx-2 "><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../end-to-end-workflow-for-knowledge-distillation-with-nlp/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/indexd0a1.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../end-to-end-workflow-for-knowledge-distillation-with-nlp/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Distilling a faster and smaller custom LLM using Google Gemini</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. </p></a></div></div></div></div></div><div class="max-w-md mx-2 "><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../end-to-end-workflow-with-model-distillation-for-computer-vision/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index98f9.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../end-to-end-workflow-with-model-distillation-for-computer-vision/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">End-to-end workflow with model distillation for computer vision</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. </p></a></div></div></div></div></div></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"working-with-videos-using-gemini-1-5-and-multimodal-models","id":"6657a8439c01af00013c4eb1","uuid":"31996de5-5a27-4754-878d-e496d9de1c00","title":"Working with videos using Gemini 1.5 and multimodal models","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u0026nbsp;\u003c/h2\u003e\u003cp\u003eGiven the pace of innovation in AI, teams are continually looking to integrate various data types like text, images, and video as a way to unlock new functionality for delivering next-gen applications and experiences. The development of multimodal models, which can process and understand diverse data inputs, is one of the most promising advancements.\u0026nbsp;\u003c/p\u003e\u003cp\u003eNotably, combining video processing with the capabilities of large language models (LLMs) is a breakthrough feature for teams who want to highlight specific objects, scenes, and actions from high-volumes of video content.\u003c/p\u003e\u003cp\u003eHowever, many multimodal models, such as Gemini 1.5, require teams to convert videos to 1 frame per second (FPY) for analysis. Converting FPS, while tedious, aligns the video data with the model’s optimal processing capabilities, ensuring that no critical information is lost while maintaining compatibility with the model’s precision.\u003c/p\u003e\u003cp\u003eIn this blog post, we’ll show how easy it is to convert videos to 1FPS and upload them to Quantumworks Lab Catalog for generating predictions in Model Foundry.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"preparing-videos-for-inference-with-multimodal-models\"\u003ePreparing videos for inference with multimodal models\u003c/h1\u003e\u003cp\u003eThe two main approaches for ensuring videos meet the exact 1FPS requirements of multimodal models like Gemini 1.5 include:\u003c/p\u003e\u003col\u003e\u003cli\u003eVideo Upload: Converting a video to 1 FPS and uploading this converted video into Quantumworks Lab Catalog.\u003c/li\u003e\u003cli\u003eFrame Extraction Upload: Converting a video to 1 FPS, extracting each of the video frames, and uploading the extracted video frame images to Catalog.\u0026nbsp;\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eBy doing so, users will be able to use various multimodal models like GPT- 4v, Claude 3 Opus and Amazon Rekognition (as well as additional models natively supported by Quantumworks Lab). It is important to note that 1FPS is not necessary for Model Foundry’s use on video datarows, but this approach may be helpful when using certain multimodal models.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"approach-1upload-a-1-fps-video-to-catalog\"\u003eApproach #1 - Upload a 1 FPS Video to Catalog\u003c/h2\u003e\u003cp\u003eThe first approach is converting a video to 1 frame per second (FPS) and uploading the converted video to Catalog.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can follow along in this \u003ca href=\"https://colab.research.google.com/drive/1mJv6L5PbSZf4TmskWG1XMAqj4A4QS1Ye?usp=sharing\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"steps\"\u003e\u003cstrong\u003eSteps\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eStep 1: Download Video From Google Cloud Storage (GCS)\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1114\" height=\"202\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png 1114w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe function ‘convert_video_to_1fps_and_download’ is defined to handle the video conversion and upload process.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIt starts by downloading the specified video file from a Google Cloud Storage bucket to the local Colab environment.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 2: Convert the Video to 1 FPS\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1044\" height=\"82\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png 1044w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eUsing the ‘ffmpeg’ tool, the video is converted to 1 frame per second (FPS). This step is crucial and can be easily modified to change the FPS by altering the ‘-vf fps=1’ parameter in the ‘subprocess.run’ command. For example, to convert the video to 2 FPS, you would change ‘fps=1’ to ‘fps =2.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 3: Upload the Converted Video Back to GCS\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.14.20-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"843\" height=\"74\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.14.20-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.14.20-AM.png 843w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAfter conversion, the video is saved locally and then uploaded back to the GCS bucket.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 4:\u0026nbsp; Upload to Catalog\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1055\" height=\"238\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png 1055w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe converted video is then integrated with Quantumworks Lab by creating a dataset and adding the video as a data row.\u003c/li\u003e\u003cli\u003eThe code uses the Quantumworks Lab SDK to create a dataset and upload the converted video, making it ready for further processing and labeling in Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"approach-2upload-extracted-video-frames-to-catalog\"\u003eApproach #2 - Upload Extracted Video Frames to Catalog\u003c/h2\u003e\u003cp\u003eThe second approach is converting a video to 1 frame per second (FPS), extracting video frames, and uploading the extracted video frame images to Catalog.\u003c/p\u003e\u003cp\u003eYou can follow along in this \u003ca href=\"https://colab.research.google.com/drive/1ZYye--AOyHUzXaF24mU1nHuGS0myViJl?usp=sharing\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003ch3 id=\"steps-1\"\u003e\u003cstrong\u003eSteps\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eStep 1: Download Video From Google Cloud Storage (GCS)\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1175\" height=\"232\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png 1175w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe function ‘process_video_from_gcs’ is designed to handle the entire workflow of downloading the video, extracting frames, and uploading them back to GCS.\u003c/li\u003e\u003cli\u003eIt downloads the specified video file from a GCS bucket to the local Colab environment.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 2: Extract Video Frames at 1 FPS\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1148\" height=\"39\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png 1148w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe frames are saved locally in a specified directory.\u003c/li\u003e\u003cli\u003eUsing the ‘ffmpeg’ tool, the video is processed to extract frames at 1 frame per second. Again, this can be easily adjusted by changing the ‘-vf fps=1’ parameter in the ‘subprocess.run’ command to any desired frame rate.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 3: Rename and Organize Frames\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.17.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"896\" height=\"447\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.17.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.17.28-AM.png 896w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eEach extracted frame is renamed sequentially for better organization, using a consistent naming pattern (e.g. ‘image_frame_0001.jpg’).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 4: Upload Frames to GCS\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe renamed frames are uploaded back to the GCS bucket into a directory specific to the frames of the video.\u0026nbsp;\u003c/li\u003e\u003cli\u003ePublic URLs and global keys for each uploaded frame are generated and stored for later use.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.18.07-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"672\" height=\"235\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.18.07-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.18.07-AM.png 672w\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eStep 5: Upload to Catalog\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe extracted frames are uploaded to Catalog by creating a dataset and adding each frame as an individual data row, using the Quantumworks Lab SDK.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"additional-considerations\"\u003eAdditional Considerations\u0026nbsp;\u003c/h1\u003e\u003cp\u003eWhen deciding between uploading 1FPS videos or extracted video frames to Catalog, there are some important features to consider:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFor 1FPS Videos\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcGfqLUwq4Vt_KSDpedsvAvF3Z9rP3bdesbBACTSMAf6NDeDlRxKSCsTw_QSV_GulMPPGda9XMK-kKw1a6osW8UFL1jjEe_8QW0JxMvyHqRI-jkbBN4OUdiCYk22o5kb4AbVR_ZoYaoac-YASMQGwcROso?key=kFXSU0zZGAC6bV-HbWQZiQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"437\" height=\"175\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTemporal Context Preservation\u003c/strong\u003e:\u003cul\u003e\u003cli\u003eProvides a continuous video stream and maintains the temporal relationships and sequences between frames\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSimplified Workflow\u003c/strong\u003e:\u0026nbsp;\u003cul\u003e\u003cli\u003eManaging a single video file is typically simpler than handling multiple image files, reducing the number of files to manage\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLimited Flexibility in Frame Manipulation\u003c/strong\u003e:\u003cul\u003e\u003cli\u003eVideos offer less flexibility for individual frame manipulation and augmentation compared to separate image files\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eFor Extracted Video Frames\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXeSVX1V08D-2UrE4e5c5VflZgVPJU7EguUFQR1Faa7-JI4iGLa9ZwwzpywKgKQtgz7-jSZ--82W9x2JrfWPgKYl93of_NbdlWphywYp6Sz77LsQKr9kG34CBEcMqht_fOoHL0ZrmVBOjZ47Zhwpg5A-MFs?key=kFXSU0zZGAC6bV-HbWQZiQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1081\" height=\"418\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIncreased Optionality\u003c/strong\u003e:\u0026nbsp;\u003cul\u003e\u003cli\u003eProcess only frames that are most relevant, allowing for greater control over the dataset\u003c/li\u003e\u003cli\u003eExtracted frames can be pre-processed or filtered according to specific criteria, potentially enhancing the quality of input data\u003c/li\u003e\u003cli\u003eFocus the analysis on the most important moments in the video\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLoss of Temporal Context\u003c/strong\u003e:\u003cul\u003e\u003cli\u003eIndividual frames lack the temporal continuity present in videos, which might be crucial based on the specific use case.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIncreased File Management\u003c/strong\u003e:\u0026nbsp;\u003cul\u003e\u003cli\u003eHandling a large number of individual image files within Quantumworks Lab Catalog\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"next-steps\"\u003eNext steps\u0026nbsp;\u003c/h2\u003e\u003cp\u003eOnce a video dataset has been converted to 1FPS via one of the two approaches highlighted above, Gemini 1.5 and other multimodal models can be used to \u003ca href=\"https://labelbox.com/guides/harnessing-ai-for-efficient-video-labeling/#introduction\"\u003e\u003cu\u003eharness AI for efficient video labeling\u003c/u\u003e\u003c/a\u003e, enabling precise and accurate frame classification to enhance data insights and model training.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u0026nbsp;\u003c/h1\u003e\u003cp\u003eIn this blog post, we explored the importance of preparing video data for multimodal models like Gemini 1.5, which analyze video data at 1 frame per second (FPS). This ensures maximum compatibility with the model's processing capabilities for accurate and efficient analysis.\u003c/p\u003e\u003cp\u003eChoosing between uploading 1 FPS videos and extracted video frames depends on your project's specific needs. As a rule of thumb, uploading videos preserves temporal context and simplifies file management, while extracting frames allows for detailed analysis and greater control, but with more file handling.\u003c/p\u003e\u003cp\u003eBy understanding these considerations, you can effectively leverage multimodal models like Gemini 1.5, optimizing your workflow for enhanced performance and accuracy in video classification tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you are not already using Quantumworks Lab, you can \u003ca href=\"https://app.labelbox.com/signup?utm_keyword=Quantumworks Lab%2520annotation\u0026amp;utm_source=house\u0026amp;utm_medium=email\u0026amp;utm_campaign=524\u0026amp;gclid=Cj0KCQjw2PSvBhDjARIsAKc2cgM16FFzTqltH9d4iQrrMQBwayH1ftA6F6A8dBgcdgz7MIR6dpv773oaAmgzEALw_wcB\u0026amp;landingPageAnonymousId=%22b04806c5-ff2b-4a09-9379-a9837b1801a9%22\u0026amp;referrer_url=https://www.google.com/\"\u003eget started for free\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/\"\u003econtact us\u003c/a\u003e to learn more about using multimodal models for better video classification.\u0026nbsp;\u003c/p\u003e","comment_id":"6657a8439c01af00013c4eb1","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/05/mmbazel_a_photorealistic_depiction_of_analyzing_video_frames_of_26a0629d-30a4-4c38-8bd2-30877fefb937-1.png","featured":false,"status":"published","visibility":"public","created_at":"2024-05-29T22:12:19.000Z","updated_at":"2024-05-30T17:22:24.000Z","published_at":"2024-05-30T17:03:49.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"681516a56e02120001d5ab84","name":"#mikiko","slug":"hash-mikiko","description":null,"feature_image":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2025-05-02T19:01:57.000Z","updated_at":"2025-05-02T19:01:57.000Z","url":"https://labelbox-guides.ghost.io/404/"}],"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","email":"manu+labelboxguides@labelbox.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2024-09-23T20:01:04.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-10T14:26:43.000Z","updated_at":"2024-09-23T20:01:04.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","email":"manu+labelboxguides@labelbox.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2024-09-23T20:01:04.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-10T14:26:43.000Z","updated_at":"2024-09-23T20:01:04.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},"primary_tag":null,"email_segment":"all","url":"https://labelbox-guides.ghost.io/working-with-videos-using-gemini-1-5-and-multimodal-models/","excerpt":"Introduction \n\nGiven the pace of innovation in AI, teams are continually looking to integrate various data types like text, images, and video as a way to unlock new functionality for delivering next-gen applications and experiences. The development of multimodal models, which can process and understand diverse data inputs, is one of the most promising advancements. \n\nNotably, combining video processing with the capabilities of large language models (LLMs) is a breakthrough feature for teams who ","reading_time":5,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":false},"recommended":[{"id":"65c97ca417c84d0001395ccb","uuid":"ed465aa9-c5cf-4c1e-bf18-fa38cc037a18","title":"Distilling a faster and smaller custom LLM using Google Gemini","slug":"end-to-end-workflow-for-knowledge-distillation-with-nlp","html":"\u003cp\u003eThe race to both mimic and create competitor models to OpenAI’s GPT3.5 energized the interest in model compression and quantization techniques.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKnowledge distillation, \u003c/strong\u003ealso known as \u003cstrong\u003emodel distillation,\u003c/strong\u003e is one of many techniques that have grown in popularity and importance in enabling small teams to leverage foundation models in developing small (but mighty) custom models, used in intelligent applications.\u003c/p\u003e\u003cp\u003eIn “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, we illustrated some of the conceptual foundations for how model distillation works as well as why we even need smaller models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe also provided an in-depth guide with a worked example in the second part of our series,\u0026nbsp;“\u003ca href=\"https://labelbox.com/guides/end-to-end-workflow-with-model-distillation-for-computer-vision/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnd-to-end workflow with model distillation for computer vision\u003c/u\u003e\u003c/a\u003e”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eNow we turn our attention to demonstrating the flexibility and power of model distillation in another domain and use case, where increased efficiency through supervised training of a smaller model by a foundation model is necessary.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll demonstrate an end-to-end workflow for natural language processing, using model distillation to fine-tune a BERT model with labels created in Model Foundry using \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGoogle Gemini\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll show how easy it is to go from raw data to cutting-edge models, customized to your use case, using a \u003ca href=\"https://huggingface.co/datasets/SetFit/emotion?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esentiment dataset\u003c/u\u003e\u003c/a\u003e (additional public datasets can be found \u003ca href=\"https://labelbox.com/datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e or on sites like Kaggle or HuggingFace).\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn less than 30 min you’ll learn how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIngest\u003c/strong\u003e, \u003cstrong\u003eexplore\u003c/strong\u003e and \u003cstrong\u003eprepare\u003c/strong\u003e the sentiment dataset;\u003c/li\u003e\u003cli\u003ePick and configure \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany text-based foundation model\u003c/u\u003e\u003c/a\u003e to \u003cstrong\u003eautomatically label\u003c/strong\u003e data with just a few clicks;\u003c/li\u003e\u003cli\u003eExport the labeled predictions to a project as a potential set-up for \u003cstrong\u003emanual evaluation\u003c/strong\u003e;\u003c/li\u003e\u003cli\u003eUse the labeled predictions dataset to \u003cstrong\u003efine-tune a student model\u003c/strong\u003e using a cloud-based notebook provider;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEvaluate performance\u003c/strong\u003e of the fine-tuned student model.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAt the end of the tutorial we’ll also discuss advanced considerations in scaling your models up and out, such as automating data ingestion and labeling, and resources for incorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e\u0026nbsp; into your workflows.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-use-model-distillation-to-fine-tune-a-small-task-specific-language-model\"\u003eSee it in action: How to use model distillation to fine-tune a small, task-specific Language Model\u003c/h3\u003e\u003cp\u003e\u003cem\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab\u0026gclid=Cj0KCQiA7OqrBhD9ARIsAK3UXh1OL7LeJadsNvbP_43MnwLSpLlRPD0IkilIwyvf3VwKOzxQ5ioGon8aApXREALw_wcB\u0026referrer_url=https://www.google.com/\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial. You’ll also need to \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003ecreate API keys for accessing the SDK\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eNotebook: \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=1FMXvdze28AZ\"\u003e\u003cu\u003eText Bert Model Distillation\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003ch3 id=\"the-model-distillation-workflow-for-nlp\"\u003eThe Model Distillation Workflow for NLP\u003c/h3\u003e\u003cp\u003eIn our prior post on model distillation concepts, we discussed the different model distillation patterns, based on the following criteria:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe type of \u003cstrong\u003eTeacher\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eStudent\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eknowledge\u003c/strong\u003e being transferred from teacher to student model(s) (\u003cstrong\u003eresponse-based\u003c/strong\u003e,\u003cstrong\u003e feature-based\u003c/strong\u003e, \u003cstrong\u003erelation-based\u003c/strong\u003e knowledge);\u003c/li\u003e\u003cli\u003eThe cadence and scheme for how the student mode is trained (\u003cstrong\u003eoffline\u003c/strong\u003e, \u003cstrong\u003eonline\u003c/strong\u003e, \u003cstrong\u003eself\u003c/strong\u003e).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e“Benefits of Using Model Distillation”\u003c/strong\u003e, Source: “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, Fig \u003cstrong\u003e3.1\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll be demonstrating the most popular and easiest pattern to get started with: \u003cstrong\u003eoffline, response-based model\u003c/strong\u003e (or \u003cstrong\u003eknowledge\u003c/strong\u003e) \u003cstrong\u003edistillation\u003c/strong\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe teacher model we’ll be using to produce the responses is \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGoogle \u003c/u\u003eGemini\u003c/a\u003e and the student model is \u003ca href=\"https://huggingface.co/docs/transformers/en/model_doc/bert?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBERT\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://huggingface.co/distilbert/distilbert-base-uncased?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edistilbert-base-uncased\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs you’ll see, we could have chosen \u003ca href=\"https://labelbox.com/guides/how-to-fine-tune-large-language-models-with-labelbox/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany combination of teacher or student models\u003c/u\u003e\u003c/a\u003e, because the \u003cstrong\u003eoffline, response-based\u003c/strong\u003e pattern of model distillation is incredibly flexible.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen implementing this process for your own use case, it’s important to understand the relative strengths and weaknesses of each model and match them according to your requirements and use cases (whether it’s \u003ca href=\"https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edetecting and removing PII to be GDPR compliant\u003c/u\u003e\u003c/a\u003e or \u003ca href=\"https://labelbox.com/guides/how-to-build-a-content-moderation-model-to-detect-disinformation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edetecting unsavory content\u003c/u\u003e\u003c/a\u003e).\u003c/p\u003e\u003ch3 id=\"using-the-labelbox-platform-to-automate-model-distillation\"\u003eUsing The Quantumworks Lab Platform To Automate Model Distillation\u0026nbsp;\u003c/h3\u003e\u003cp\u003eLabelbox is the data factory for genAI, providing an end-to-end solution for curating, transforming, annotating, evaluating and orchestrating unstructured data for data science and machine learning.\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab platform supports the development of intelligent applications using the model distillation and fine-tuning workflow enabling AI developers to easily:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImport, curate, filter and eventually select the text that will be labeled for use in student model training and fine-tuning with \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e. Additionally you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/model-foundry-automating-data-tasks-with-foundation-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomate the labeling and annotation of the original text dataset using foundation models\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eor any pre-trained model, including custom models\u003c/u\u003e\u003c/a\u003e) in \u003ca href=\"https://labelbox.com/guides/guide-to-using-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e. The ability to leverage a variety of open source or third-party models to accelerate pre-labeling can cut labeling costs by up to 90% for teams with existing labeling initiatives (and kickstart new and budget constrained AI developers operating without\u0026nbsp; manual labeling capabilities).\u003c/li\u003e\u003cli\u003eLeverage \u003ca href=\"https://labelbox.com/customers/intuitive-surgical-customer-story/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman-in-the-loop\u003c/u\u003e\u003c/a\u003e evaluation (and \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e) through the Foundry to Annotate integration, so labels can be reviewed before being used for training or fine-tuning.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIntegrate with some of the most common cloud providers like \u003ca href=\"https://learn.labelbox.com/accelerate-retail-and-ecommerce-ai-roi/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Cloud Platform\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://labelbox.com/company/partnerships/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eDatabricks\u003c/u\u003e\u003c/a\u003e for additional MLOps services.\u003c/li\u003e\u003cli\u003eOrchestrate and schedule future automated labeling and model runs as \u003ca href=\"https://labelbox.com/blog/10x-faster-uploads-with-labelboxs-new-ingestion-upgrades/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enew data flows into the Quantumworks Lab platform\u003c/u\u003e\u003c/a\u003e through Model Apps (including \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emultimodal data labeling\u003c/u\u003e\u003c/a\u003e). \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"introduction-to-data-preparation-for-natural-language-processing-with-catalog\"\u003eIntroduction To Data Preparation for Natural Language Processing With Catalog\u003c/h2\u003e\u003cp\u003eBefore beginning the tutorial:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCreate a free HuggingFace account (in order to access the \u003ca href=\"https://huggingface.co/datasets/SetFit/emotion?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003e\"Setfit/emotion\" dataset\u003c/u\u003e\u003c/a\u003e)\u003cul\u003e\u003cli\u003eDownload the dataset locally\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab\u0026gclid=Cj0KCQiA7OqrBhD9ARIsAK3UXh1OL7LeJadsNvbP_43MnwLSpLlRPD0IkilIwyvf3VwKOzxQ5ioGon8aApXREALw_wcB\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eCreate a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e (in order to \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecreate a free API key\u003c/u\u003e\u003c/a\u003e)\u003cul\u003e\u003cli\u003eNavigate to “\u003cstrong\u003eCatalog\u003c/strong\u003e” in the sidebar\u003c/li\u003e\u003cli\u003eSelect “\u003cstrong\u003e+New\u003c/strong\u003e”\u003c/li\u003e\u003cli\u003eUpload the dataset from Kaggle\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNote\u003c/strong\u003e: The easiest method is to use \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#create-a-dataset-via-the-app-ui\"\u003e\u003cu\u003ethe Quantumworks Lab Web UI\u003c/u\u003e\u003c/a\u003e\u0026nbsp; to manually upload your dataset.\u0026nbsp;\u003cul\u003e\u003cli\u003eIf your goal is to scale the data ingestion process for future labeling or data refreshes, check out our SDK.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’re able to see your dataset in Quantumworks Lab Catalog, you’ll be able to do the following:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e for more granular data curation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003eFor additional details on how to use Catalog to enable data selection for downstream data-centric workflows (such as data labeling, model training, model evaluation, error analysis, and active learning), check out \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour documentation\u003c/u\u003e\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"using-a-large-nlp-model-or-llm-to-generate-and-distill-predictions-for-fine-tuning\"\u003eUsing A Large NLP Model Or LLM To Generate And Distill Predictions For Fine-Tuning\u003c/h2\u003e\u003cp\u003eThe first step of model distillation is to \u003cstrong\u003eidentify an appropriate teacher model\u003c/strong\u003e, which will be used to produce responses that, when combined with the original text, will serve as the \u003cstrong\u003efine-tuning dataset for the student model\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResponse-based\u003c/strong\u003e model distillation is powerful because it can be used even when access to the original model weights is limited (or the model is so big that downloading a copy of the model would take a really long time). Response-based distillation also doesn’t require the user to have trained the model themselves; just that the model was pre-trained.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox allows you to pick any of the \u003ca href=\"https://labelbox.com/blog/6-key-llms-to-power-your-text-based-ai-applications/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecurrently hosted, state-of-the-art models to use\u003c/u\u003e\u003c/a\u003e (as well as upload your own custom models) to use as the teacher model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor now, let’s get started with preparing the text we’ll be labeling, or generating predictions with, using Google Gemini. The combination of text and label pairs will be used for BERT.\u003c/p\u003e\u003ch3 id=\"step-1-select-text-assets-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select text assets and choose a foundation model of interest\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1002\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePreview of text in Quantumworks Lab Catalog\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to your uploaded Emotions dataset in Catalog.\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the text on which the predictions should be made.e.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “\u003cstrong\u003ePredict with Model Foundry\u003c/strong\u003e”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run (in this case \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGemini\u003c/u\u003e\u003c/a\u003e).\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task -\u0026nbsp; such as \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003etext classification, summarization, and text generation\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eTo locate a \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003especific model\u003c/u\u003e\u003c/a\u003e, you can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003eStep 2: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eWhen developing ML based applications, developers need to quickly and iteratively prepare and version training data, launch model experiments, and use the performance metrics to further refine the input data sources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe performance of a model can vary wildly depending on the data used, the quality of the annotations, and even the model architecture itself. A necessary requirement for replicability is being able to see the exact version of all the artifacts used or generated as a result of an experiment.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox will snapshot the experiment, the data artifacts as well as the trained model, as a saved process known as a \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis includes the types of items the model is supposed to identify and label, known as an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eontology\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of settings, which you can find in the Advanced model setting.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce you’ve located Gemini, you can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIn this case, we will enter the following prompt:\u0026nbsp;\u003cul\u003e\u003cli\u003e\u003cem\u003eFor the given text, answer the following. Classify emotions, pick one of the options: [sadness, joy, love, anger, fear, surprise]. Return the result as a JSON object. {\"emotions\" : \"\"}.\u0026nbsp;\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis prompt is designed to facilitate responses from the model with one of the following: \u003cem\u003esadness, joy, love, anger, fear, surprise\u003c/em\u003e.\u003c/p\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-review-predictions-in-the-model-tab\"\u003eStep 3: Review predictions in the Model tab \u003c/h3\u003e\u003cp\u003eBecause each \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run is submitted with a unique name\u003c/u\u003e\u003c/a\u003e, it’s easy to distinguish between each subsequent model run.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/foundry-view-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eView prediction results\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results \u003ca href=\"https://docs.labelbox.com/docs/comparing-model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eacross a variety of model runs\u003c/u\u003e\u003c/a\u003e different models\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-2.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-2.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eView predictions in Model tab for the model run\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-4.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-4.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-4.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eIn this case, Gemini Pro predicted this text to be \"joy\"\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThese generated labels are now ready to be used for fine-tuning the BERT student model.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-enriching-and-evaluating-predictions-using-human-in-the-loop-and-annotate\"\u003eStep 4: Enriching and evaluating predictions using human-in-the-loop and Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAlthough fine-tuning a foundation model requires less data than pre-training a large foundation model from scratch, the data (specifically the labels) need to be high-quality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEven big, powerful foundation models make mistakes or miss edge cases.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou might also find that there are additional categories that the parent model didn’t identify correctly because the ontology was incomplete.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce a parent model like Gemini has been used for the initial model-assisted labeling run, those predictions \u003ca href=\"https://docs.labelbox.com/docs/foundry-annotate-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then be sent\u003c/u\u003e\u003c/a\u003e to a \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eproject\u003c/u\u003e\u003c/a\u003e, a container \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhere all your labeling processes happen\u003c/u\u003e\u003c/a\u003e. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bpsrmes4da\" title=\"Distilling a faster and smaller LLM using BERT and Gemini_SendToAnnotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eIn this case, we feel fairly confident\u003cstrong\u003e \u003c/strong\u003ein how well Gemini performed so we’ll send the inferences to the corresponding Quantumworks Lab project and treat them as the ground truth that the student model will be fine-tuned on. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"fine-tuning-the-student-model-bert\"\u003eFine-Tuning The Student Model (BERT)\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWe’ve shown the first half of the model distillation to fine-tuning workflow.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe identified the categories we wanted the parent model (Gemini) to detect and label in the form of an ontology.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe used Gemini to automatically label the texts as \u003cem\u003e\"sadness\"\u003c/em\u003e or \u003cem\u003e\"fear\"\u003c/em\u003e (for example).\u003c/li\u003e\u003cli\u003eWe exported the generated labels to a Quantumworks Lab project, at which point we could review the labels manually and enrich them further using the Quantumworks Lab editor.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe next step is to use the generated labels, along with the original texts, to fine-tune a student model in Colab.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: You’ll now need the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=cclwx_KmJr3Z\u0026line=3\u0026uniqifier=1\"\u003e\u003cem\u003e\u003cu\u003eAPI keys from earlier\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to follow along with the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=1FMXvdze28AZ\"\u003e\u003cem\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003ch3 id=\"step-5-fetch-the-ground-truth-labels-from-the-project-via-labelbox-sdk\"\u003eStep 5: Fetch the ground truth labels from the project via Quantumworks Lab SDK\u003c/h3\u003e\u003cp\u003eFor brevity, we’ve omitted the surrounding code samples but you can copy or run the corresponding blocks in the provided notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our documentation to find out all the ways you can automate the model lifecycle (\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eincluding labeling\u003c/u\u003e\u003c/a\u003e) using our SDK. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore fine-tuning the BERT student model, we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=Jq-1tQs2QBrj\"\u003e\u003cu\u003eneed to fetch the generated labels\u003c/u\u003e\u003c/a\u003e from the Quantumworks Lab project using the Quantumworks Lab SDK as well as the \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=jKMysiHvUI1D\"\u003e\u003cu\u003eground truth\u003c/u\u003e\u003c/a\u003e.\u0026nbsp; \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-6-additionally-text-data-processing\"\u003eStep 6: Additionally Text Data Processing \u003c/h3\u003e\u003cp\u003eThere's additional processing that needs to happen, which we walk through below.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNext we’ll ensure the labels are \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=HFiQMbcSQSks\"\u003e\u003cu\u003eexported into a .csv file\u003c/u\u003e\u003c/a\u003e that contains two columns, the original ‘text’ and the generated ‘label’.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=B2_znE_XHbvG\u0026line=2\u0026uniqifier=1\"\u003e\u003cu\u003eread the csv file into a pandas dataframe\u003c/u\u003e\u003c/a\u003e, perform a series of aggregation operations to help us splits the text into train and test sets based on the category count.\u003c/li\u003e\u003cli\u003eWe’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=k9eNSPuLRAJx\"\u003e\u003cu\u003einitialize a tokenizer\u003c/u\u003e\u003c/a\u003e and encode the train and test texts.\u003c/li\u003e\u003cli\u003eFinally we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=PnHQrghbQ7tD\" rel=\"noreferrer\"\u003efinish creating the training \u0026amp; validation dataset\u003c/a\u003e. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNote\u003c/strong\u003e: Expand the “Export labels into .CSV file” block in the Colab notebook for the full code sample.\u0026nbsp; \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-7-fine-tune-student-bert-model-using-labels-generated-by-google-gemini\"\u003eStep 7: Fine tune student BERT model using labels generated by Google Gemini\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn Colab we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=gjMPaBgAIAGd\u0026line=2\u0026uniqifier=1\" rel=\"noreferrer\"\u003einstantiate a BERT model\u003c/a\u003e and \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=cmbMl5wXIB3I\u0026line=6\u0026uniqifier=1\" rel=\"noreferrer\"\u003etrain it using the data\u003c/a\u003e, which includes both text and labels. Specifically we'll fine-tune a text classifier model called \u003cem\u003e“distilbert-base-uncased”\u003c/em\u003e to classify text as one of the following categories in the ontology: \u003cem\u003esadness, joy, love, anger, fear, surprise\u003c/em\u003e. \u003c/li\u003e\u003cli\u003eWe’ll also \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=VzlNe83KRh98\"\u003esave the model\u003c/a\u003e and \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=RbPplpJdRuXL\"\u003etest the prediction\u003c/a\u003e.\u003cul\u003e\u003cli\u003eBy saving the model (or every model we create) we have the option of A/B testing models and using the models for downstream use cases (as well as share the models with other key stakeholders through a model registry, like MLFlow). \u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1339\" height=\"716\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 1339w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1262\" height=\"344\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 1262w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-8-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 8: Create a model run with predictions and ground truth\u003c/h3\u003e\u003cp\u003eOftentimes the initial training or fine-tuning step isn’t the final stop on the journey of developing a model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOne of the biggest differences between the traditional method of training models in the classroom versus the real-world is how much control you have over the quality of your data, and consequently the quality of the model produced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we mentioned earlier, developers can upload predictions and use the Model product to \u003ca href=\"https://labelbox.com/blog/how-to-confidently-compare-test-and-evaluate-models-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ediagnose performance issues with models and compare them across multiple experiments\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDoing so automatically populates model metrics that make it easy to evaluate the model’s performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e \u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, you’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=mjDaoyV3L3l1\"\u003egrab the model’s ID\u003c/a\u003e to \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=HsDr_g3qqUFo\"\u003ecreate a new model run\u003c/a\u003e (if needed).\u003c/li\u003e\u003cli\u003eThen you’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=K8D7lE9nrOLG\"\u003eget the ground truth from your project\u003c/a\u003e via the export as well as the \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=-cv2GJ6zrU5A\"\u003elabel IDs from ground truth\u003c/a\u003e. \u003c/li\u003e\u003cli\u003eNext you’ll create the predictions by \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=qxwq7kKZrfzL\"\u003erunning the fine-tuned BERT model on the original text assets\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eYou \u003ca href=\"https://docs.labelbox.com/docs/import-ground-truth?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then upload the ground truth labels\u003c/u\u003e\u003c/a\u003e (the labels generated by Gemini and used as the fine-tuning dataset) and the \u003ca href=\"https://docs.labelbox.com/docs/upload-model-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003epredictions\u003c/u\u003e\u003c/a\u003e from the fine-tuned BERT model \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=i7Xq-fldsiRY\"\u003eto the corresponding Quantumworks Lab model\u003c/a\u003e. \u003c/li\u003e\u003cli\u003eYou can see an example of how model metrics are automatically populated by Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"997\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-10.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-10.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-10.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-10.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAuto generated metrics\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"evaluating-model-performance\"\u003eEvaluating Model Performance\u003c/h2\u003e\u003cp\u003eThere’s no single metric to rule them all \u003ca href=\"https://labelbox.com/blog/gpt4-vs-palm-assessing-performance-of-llm-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhen evaluating how your fine-tuned LLM performs\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBoth qualitative and quantitative measures must be considered, combined with sampling and manual review.\u003c/p\u003e\u003cp\u003eWith that being said, Model offers a number of the most common out-of-the-box. With the \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io#metrics-view\"\u003e\u003cu\u003e‘Metrics view’\u003c/u\u003e\u003c/a\u003e users can drill into crucial model metrics, such as \u003ca href=\"https://docs.labelbox.com/docs/upload-model-metrics-1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econfusion matrix, precision, recall, F1 score, false positive, and more\u003c/u\u003e\u003c/a\u003e, to surface model errors.\u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive, which means you can click on any chart or metric to immediately open up the gallery view of the model run and see corresponding examples, as well as visually compare model predictions between multiple model runs. \u003c/p\u003e\u003ch3 id=\"step-9-evaluate-predictions-from-different-bert-model-runs-in-labelbox-model\"\u003eStep 9: Evaluate predictions from different BERT model runs in Quantumworks Lab Model \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBERT fine tuned on labels created by Gemini Pro vs ground truth labels \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to “\u003cstrong\u003eModel\u003c/strong\u003e”\u003c/li\u003e\u003cli\u003eIn this case, we fine-tuned two models, one using 1000 ground truth labels and the other with 1000 labels generated by the Gemini model. We see very similar results and leveraging an off the shelf model is almost as good as using ground truth labels.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"examples-of-predictions-from-fine-tuned-bert-model\"\u003e\u003cstrong\u003eExamples of predictions from fine tuned BERT model\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eHow does our fine-tuned model perform? \u003c/p\u003e\u003cp\u003eLet's manually inspect a few examples of predictions from the fine-tuned BERT model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-12.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-12.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-12.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-12.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “anger”.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-11.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-11.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-11.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-11.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “joy”.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “fear”.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"advanced-considerations\"\u003eAdvanced Considerations\u003c/h2\u003e\u003cp\u003eIn this step-by-step walkthrough, we’ve shown how anyone with any text-based dataset can leverage an LLM to label, fine-tune and analyze a smaller but mighty custom model.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"610\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e“Leveraging FMOps To Develop intelligent Applications”, Source: “\u003c/span\u003e\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eA pragmatic introduction to model distillation for AI developers\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e”, Fig 5.2.4\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAdditional considerations users should address for scaling similar projects include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCollecting feedback \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efrom users \u0026amp; human SME’s to improve\u003c/u\u003e\u003c/a\u003e the fine-tuning dataset quality on a continuous basis, including error analysis and \u003ca href=\"https://docs.labelbox.com/docs/llm-human-preference?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman preference modeling\u003c/u\u003e\u003c/a\u003e;\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003eStrategic planning for incorporating multiple data modalities besides text, including image, audio, and video;\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomating future data ingestion, curation, enrichment, and labeling when the fine-tuned model needs to be retrained due to drift via a \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erobust SDK\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eEasy-to-use, user interface that can be customized for various modalities of data when multiple users are involved (as well as \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM data generation\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eIn this tutorial we demonstrated an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language processing\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a BERT model with labels created in Model Foundry using \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Gemini\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHopefully you were able to see how easy it is to go from raw data to cutting-edge custom models in less than 30 min.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou learned how the Quantumworks Lab platform enables model distillation by allowing developers to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIngest, explore and prepare text-based datasets using \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eUse any \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM\u003c/u\u003e\u003c/a\u003e to automatically label data using \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e as well as how to incorporate human-in-the-loop evaluation using \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eExport these labeled predictions to a cloud-based training environment for fine-tuning;\u003c/li\u003e\u003cli\u003eAutomate the various workflows using the \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox SDK\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eEvaluate model performance and analyze model errors using \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox Model\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you’re interested in learning more about model distillation, check out the previous posts in this series: “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, “\u003ca href=\"https://labelbox.com/guides/end-to-end-workflow-with-model-distillation-for-computer-vision/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnd-to-end workflow with model distillation for computer vision\u003c/u\u003e\u003c/a\u003e”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLooking to implement a production-ready model distillation and fine-tuning in your organization but not sure how to get started leveraging your unstructured data?\u0026nbsp;\u003c/p\u003e\u003cp\u003eAsk \u003ca href=\"https://community.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour community\u003c/u\u003e\u003c/a\u003e or reach out to \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour solutions engineers\u003c/u\u003e\u003c/a\u003e!\u003c/p\u003e","comment_id":"65c97ca417c84d0001395ccb","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/distilling.png","featured":false,"visibility":"public","created_at":"2024-02-12T02:04:20.000+00:00","updated_at":"2024-11-20T23:09:07.000+00:00","published_at":"2024-02-15T19:25:02.000+00:00","custom_excerpt":"Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"681516a56e02120001d5ab84","name":"#mikiko","slug":"hash-mikiko","description":null,"feature_image":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/404/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"65bd4ff882e5680001e078f9","name":"Model Distillation","slug":"model-distillation","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/tag/model-distillation/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/end-to-end-workflow-for-knowledge-distillation-with-nlp/","excerpt":"Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. ","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65bd22d782e5680001e07877","uuid":"3e5a16b6-f3f5-494e-bb8b-4162abb3e830","title":"End-to-end workflow with model distillation for computer vision","slug":"end-to-end-workflow-with-model-distillation-for-computer-vision","html":"\u003cp\u003eModel distillation, also known as \u003cstrong\u003eknowledge distillation\u003c/strong\u003e, is a technique that focuses on creating efficient models by transferring knowledge from large, complex models to smaller, deployable ones.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel distillation is one of many techniques that have grown in popularity and importance in enabling small teams to leverage foundation models in developing small (but mighty) custom models, used in intelligent applications.\u003c/p\u003e\u003cp\u003eIn \u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e“A Pragmatic Introduction to Model Distillation for AI Developers\u003c/u\u003e\u003c/a\u003e”, we illustrated some of the conceptual foundations for how model distillation works.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe described how distillation can be leveraged in any domain or data modality requiring efficiency and model optimization, whether the use case is \u003cu\u003ecomputer vision or NLP related\u003c/u\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll demonstrate an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003ecomputer vision\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a YOLO model with labels created in Model Foundry using Amazon Rekognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll show how easy it is to go from raw data to cutting-edge models, customized to your use case, using a \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efashion products dataset\u003c/u\u003e\u003c/a\u003e (additional public datasets can be found \u003ca href=\"https://labelbox.com/datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e or on sites like Kaggle).\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn less than 30 min you’ll learn how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIngest\u003c/strong\u003e, \u003cstrong\u003eexplore\u003c/strong\u003e and \u003cstrong\u003eprepare\u003c/strong\u003e the products fashion dataset;\u003c/li\u003e\u003cli\u003ePick and configure\u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e any image based foundation model\u003c/u\u003e\u003c/a\u003e to \u003cstrong\u003eautomatically label\u003c/strong\u003e data with just a few clicks;\u003c/li\u003e\u003cli\u003eExport the labeled predictions to a project as a potential set-up for \u003cstrong\u003emanual evaluation\u003c/strong\u003e;\u003c/li\u003e\u003cli\u003eUse the labeled predictions dataset to \u003cstrong\u003efine-tune a student model\u003c/strong\u003e using a cloud-based notebook provider;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEvaluate performance\u003c/strong\u003e of the fine-tuned student model.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAt the end of the tutorial we’ll also discuss advanced considerations in scaling your models up and out, such as automating data ingestion and labeling, and resources for incorporating \u003ca href=\"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e\u0026nbsp; into your workflows.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-use-model-distillation-to-fine-tune-a-small-task-specific-computer-vision-model\"\u003eSee it in action: How to use model distillation to fine-tune a small, task-specific Computer Vision Model\u003c/h3\u003e\u003cp\u003e\u003cem\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial. You’ll also need to \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003ecreate API keys for accessing the SDK\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNotebook\u003c/strong\u003e: \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cu\u003eCV YOLO Model Distillation\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003ch3 id=\"the-model-distillation-workflow-for-computer-vision\"\u003eThe Model Distillation Workflow for Computer Vision\u003c/h3\u003e\u003cp\u003eIn our prior post on model distillation concepts, we discussed the different model distillation patterns, based on the following criteria:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe type of \u003cstrong\u003eTeacher\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eStudent\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eknowledge\u003c/strong\u003e being transferred from teacher to student model(s) (\u003cstrong\u003eresponse-based\u003c/strong\u003e,\u003cstrong\u003e feature-based\u003c/strong\u003e, \u003cstrong\u003erelation-based\u003c/strong\u003e knowledge);\u003c/li\u003e\u003cli\u003eThe cadence and scheme for how the student mode is trained (\u003cstrong\u003eoffline\u003c/strong\u003e, \u003cstrong\u003eonline\u003c/strong\u003e, \u003cstrong\u003eself\u003c/strong\u003e).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this tutorial we’ll be demonstrating the most popular and easiest pattern to get started with: \u003cstrong\u003eoffline, response-based model\u003c/strong\u003e (or \u003cstrong\u003eknowledge\u003c/strong\u003e) \u003cstrong\u003edistillation\u003c/strong\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe teacher model we’ll be using to produce the responses is Amazon Rekognition and the student model is \u003ca href=\"https://labelbox.com/product/model/foundry-models/yolov8-object-detection/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eYOLOv8 Object Detection\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs you’ll see, we could have chosen \u003ca href=\"https://labelbox.com/blog/6-cutting-edge-foundation-models-for-computer-vision-and-how-to-use-them/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany combination of teacher or student models\u003c/u\u003e\u003c/a\u003e, because the \u003cstrong\u003eoffline, response-based\u003c/strong\u003e pattern of model distillation is incredibly flexible.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen implementing this process for your own use case, it’s important to understand the relative strengths and weaknesses of each model and match them according to your requirements.\u003c/p\u003e\u003ch3 id=\"using-the-labelbox-platform-to-automate-model-distillation\"\u003eUsing The Quantumworks Lab Platform To Automate Model Distillation\u0026nbsp;\u003c/h3\u003e\u003cp\u003eLabelbox is the leading data-centric AI platform, providing an end-to-end platform for curating, transforming, annotating, evaluating and orchestrating unstructured data for data science, machine learning, and generative AI.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab platform supports the development of intelligent applications using the model distillation and fine-tuning workflow. \u003c/p\u003e\u003cp\u003eAI developers are enabled to easily:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImport, curate, filter and eventually select the images that will be labeled for use in student model training and fine-tuning with \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e. Additionally you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/model-foundry-automating-data-tasks-with-foundation-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomate the labeling and annotation of the original image dataset using foundation models\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eor any pre-trained model, including custom models\u003c/u\u003e\u003c/a\u003e) in \u003ca href=\"https://labelbox.com/guides/guide-to-using-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e. The ability to leverage a variety of open source or third-party models to accelerate pre-labeling can cut labeling costs by up to 90% for teams with existing labeling initiatives (and kickstart new and budget constrained AI developers operating without\u0026nbsp; manual labeling capabilities).\u003c/li\u003e\u003cli\u003eLeverage \u003ca href=\"https://labelbox.com/customers/intuitive-surgical-customer-story/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman-in-the-loop\u003c/u\u003e\u003c/a\u003e evaluation (and \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e) through the native Foundry to Annotate integration, so labels can be reviewed before being used for training or fine-tuning.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIntegrate with some of the most common cloud providers like \u003ca href=\"https://learn.labelbox.com/accelerate-retail-and-ecommerce-ai-roi/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Cloud Platform\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://labelbox.com/company/partnerships/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eDatabricks\u003c/u\u003e\u003c/a\u003e for additional MLOps services.\u003c/li\u003e\u003cli\u003eOrchestrate and schedule future automated labeling and model runs as \u003ca href=\"https://labelbox.com/blog/10x-faster-uploads-with-labelboxs-new-ingestion-upgrades/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enew data flows into the Quantumworks Lab platform\u003c/u\u003e\u003c/a\u003e through Model Apps (including \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emultimodal data labeling\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"introduction-to-data-preparation-for-computer-vision-with-catalog\"\u003eIntroduction To Data Preparation for Computer Vision With Catalog\u003c/h2\u003e\u003cp\u003eBefore beginning the tutorial:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eCreate a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e (in order to \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecreate a free API key\u003c/u\u003e\u003c/a\u003e)\u003c/li\u003e\u003cli\u003eCheck that you can access an \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eexisting fashion products dataset\u003c/u\u003e\u003c/a\u003e in Catalog\u003cul\u003e\u003cli\u003eIf not, check out sites like Kaggle for similar datasets.\u003c/li\u003e\u003cli\u003eDownload the images and their metadata.\u003c/li\u003e\u003cli\u003eChoose whether to upload data via \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#create-a-dataset-via-the-app-ui\"\u003e\u003cu\u003ethe Quantumworks Lab Web UI\u003c/u\u003e\u003c/a\u003e (preferred method) or the SDK.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’re able to see your dataset in Quantumworks Lab Catalog, you’ll be able to do the following:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e for more granular data curation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor additional details on how to use Catalog to enable data selection for downstream data-centric workflows (such as data labeling, model training, model evaluation, error analysis, and active learning), check out \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour documentation\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"using-a-large-computer-vision-model-to-generate-and-distill-predictions\"\u003eUsing A Large Computer Vision Model To Generate And Distill Predictions\u003c/h2\u003e\u003cp\u003eThe first step of model distillation is to \u003cstrong\u003eidentify an appropriate teacher model\u003c/strong\u003e, which will be used to produce responses that, when combined with the original images, will serve as the \u003cstrong\u003efine-tuning dataset for the student model\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResponse-based\u003c/strong\u003e model distillation is powerful because it can be used even when access to the original model weights is limited (or the model is so big that downloading a copy of the model would take a really long time). Response-based distillation also doesn’t require the user to have trained the model themselves; just that the model was pre-trained.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox allows you to pick any of the \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecurrently hosted, state-of-the-art models to use\u003c/u\u003e\u003c/a\u003e (as well as upload your own custom models) to use as the teacher model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor now, let’s get started with preparing the images we’ll be labeling, or generating predictions with, using Amazon Rekognition. The combination of image and label pairs will be used for YOLO.\u003c/p\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select images and choose a foundation model of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to the \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efashion products dataset\u003c/u\u003e\u003c/a\u003e in Catalog.\u003c/li\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “\u003cstrong\u003ePredict with Model Foundry\u003c/strong\u003e”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run (in this case Rekognition).\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task - such as \u003ca href=\"https://labelbox.com/usecases/computer-vision/image-classification/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eimage classification\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/usecases/computer-vision/object-detection/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eobject detection\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/guides/how-to-build-generative-captioning-using-foundation-models-for-product-listings/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage captioning\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003eStep 2: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eWhen developing ML based applications, developers need to quickly and iteratively prepare and version training data, launch model experiments, and use the performance metrics to further refine the input data sources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe performance of a model can vary wildly depending on the data used, the quality of the annotations, and even the model architecture itself. A necessary requirement for replicability is being able to see the exact version of all the artifacts used or generated as a result of an experiment.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox will snapshot the experiment, the data artifacts as well as the trained model, as a saved process known as a \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis includes the types of items the model is supposed to identify and label, known as an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eontology\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of settings, which you can find in the Advanced model setting.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce you’ve located Rekognition, you can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIn this case, we set the ontology to detect “Jacket” and we can see a preview of running the model on this ontology above.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo get an idea of how your current model settings affect the final predictions, you can generate preview predictions on up to five data rows. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-review-predictions-in-the-model-tab\"\u003eStep 3: Review predictions in the Model tab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eBecause each \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run is submitted with a unique name\u003c/u\u003e\u003c/a\u003e, it’s easy to distinguish between each subsequent model run.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/foundry-view-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eView prediction results\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results \u003ca href=\"https://docs.labelbox.com/docs/comparing-model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eacross a variety of model runs\u003c/u\u003e\u003c/a\u003e different models\u003c/li\u003e\u003cli\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe can see that for the most part, “jackets” were correctly identified and labeled.\u003c/li\u003e\u003cli\u003eThese generated labels are now ready to be used for fine-tuning the student model YOLO.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-enriching-and-evaluating-predictions-using-human-in-the-loop-and-annotate\"\u003eStep 4: Enriching and evaluating predictions using human-in-the-loop and Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAlthough fine-tuning a foundation model requires less data than pre-training a large foundation model from scratch, the data (specifically the labels) need to be high-quality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEven big, powerful foundation models make mistakes or miss edge cases.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou might also find that there are additional objects that the parent model didn’t identify because the items in the image weren’t initially identified as being important in the ontology (for example, “boots” or “ski hats”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce a parent model like Rekognition has been used for the initial model-assisted labeling run, those predictions \u003ca href=\"https://docs.labelbox.com/docs/foundry-annotate-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then be sent\u003c/u\u003e\u003c/a\u003e to a Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eproject\u003c/u\u003e\u003c/a\u003e, a container \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhere all your labeling processes happen\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/la022y1i78\" title=\"End-to-End Workflow for Model Distillation with Computer Vision - Send to annotate Jacket YOLO Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"484\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn this case, we feel fairly confident\u003cstrong\u003e \u003c/strong\u003ein how well Rekognition performed so we’ll send the inferences to the corresponding Quantumworks Lab project and treat them as the ground truth that the student model will be fine-tuned on. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"fine-tuning-the-student-model-yolo\"\u003eFine-Tuning The Student Model (YOLO)\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWe’ve shown the first half of the model distillation to fine-tuning workflow.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe identified the items we wanted the parent model (Rekognition) to detect and label in the form of an ontology.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe used Rekognition to automatically label items like “jackets”.\u003c/li\u003e\u003cli\u003eWe exported the generated labels to a Quantumworks Lab project, at which point we could review the labels manually and enrich them further using the Quantumworks Lab editor.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe next step is to use the generated labels, along with the original image, to fine-tune a student model in Colab.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: You’ll now need the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=PgdwI9SR5HHd\"\u003e\u003cem\u003e\u003cu\u003eAPI keys from earlier\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to follow along with the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cem\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003ch3 id=\"step-5-fetch-the-ground-truth-labels-from-the-project-via-labelbox-sdk-and-convert-the-images-into-the-relevant-format\"\u003eStep 5: Fetch the ground truth labels from the project via Quantumworks Lab SDK and convert the images into the relevant format.\u003c/h3\u003e\u003cp\u003eFor brevity, we’ve omitted the relevant code samples but you can copy or run the corresponding blocks in the provided notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our documentation to find out all the ways you can automate the model lifecycle (\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eincluding labeling\u003c/u\u003e\u003c/a\u003e) using our SDK.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"977\" height=\"542\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png 977w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore fine-tuning the YOLO student model, we’ll \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=7hs-oTEXOQKx\"\u003e\u003cu\u003eneed to fetch the generated labels\u003c/u\u003e\u003c/a\u003e from the Quantumworks Lab project using the Quantumworks Lab SDK as well as the \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=jKMysiHvUI1D\"\u003e\u003cu\u003eground truth\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe’ll also need to \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cu\u003econvert the images and ensure they’re in the right format\u003c/u\u003e\u003c/a\u003e for fine-tuning, specifically the COCO format.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-6-fine-tune-student-yolo-model-using-labels-generated-by-amazon-rekognition\"\u003eStep 6: Fine-tune student YOLO model using labels generated by Amazon Rekognition\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"608\" height=\"109\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png 608w\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn Colab we’ll \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=Becr0BZQO_Ze\u0026line=1\u0026uniqifier=1\" rel=\"noreferrer\"\u003einstantiate a YOLO model and train it using the data\u003c/a\u003e, which includes both images and labels.\u003c/li\u003e\u003cli\u003eWe’ll then \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=_quHpcfihY7s\" rel=\"noreferrer\"\u003erun the fine-tuned student YOLO model\u003c/a\u003e on the images to generate the predictions for analysis.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSee the example notebook for omitted code.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"923\" height=\"307\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png 923w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-7-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 7: Create a model run with predictions and ground truth\u0026nbsp;\u003c/h3\u003e\u003cp\u003eOftentimes the initial training or fine-tuning step isn’t the final stop on the journey of developing a model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOne of the biggest differences between the traditional method of training models in the classroom versus the real-world is how much control you have over the quality of your data, and consequently the quality of the model produced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we mentioned earlier, developers can upload predictions and use the Model product to \u003ca href=\"https://labelbox.com/blog/how-to-confidently-compare-test-and-evaluate-models-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ediagnose performance issues with models and compare them across multiple experiments\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDoing so automatically populates model metrics that make it easy to evaluate the model’s performance.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"857\" height=\"266\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png 857w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eHere we show how you \u003ca href=\"https://docs.labelbox.com/docs/import-ground-truth?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan upload the ground truth labels\u003c/u\u003e\u003c/a\u003e (the labels generated by Rekognition and used as the fine-tuning dataset) and the \u003ca href=\"https://docs.labelbox.com/docs/upload-model-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003epredictions\u003c/u\u003e\u003c/a\u003e from the fine-tuned YOLO model to the corresponding Quantumworks Lab model. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"evaluating-model-performance\"\u003eEvaluating Model Performance\u003c/h2\u003e\u003cp\u003eThere’s no single metric to rule them all when evaluating your \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecomputer vision model performance\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith that being said, Model offers a number of the most common out-of-the-box. With the \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io#metrics-view\"\u003e\u003cu\u003e‘Metrics view’\u003c/u\u003e\u003c/a\u003e users can drill into crucial model metrics, such as \u003ca href=\"https://docs.labelbox.com/docs/upload-model-metrics-1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econfusion matrix, precision, recall, F1 score, false positive, and more\u003c/u\u003e\u003c/a\u003e, to surface model errors.\u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive, which means you can click on any chart or metric to immediately open up the gallery view of the model run and see corresponding examples, as well as visually compare model predictions between multiple model runs.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-7-evaluate-predictions-from-different-yolo-model-runs-in-labelbox-model\"\u003eStep 7: Evaluate predictions from different YOLO model runs in Quantumworks Lab Model\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to “Model”\u003c/li\u003e\u003cli\u003eBy fine-tuning the Yolo v8 model with approximately 1000 images annotated using Amazon Rekognition, we can achieve performance similar to the Rekognition model within roughly one hour.\u003c/li\u003e\u003cli\u003eWe can now manually inspect examples of predictions from the fine-tuned YOLO model\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"advanced-considerations\"\u003eAdvanced Considerations\u003c/h2\u003e\u003cp\u003eIn this step-by-step walkthrough, we’ve shown how anyone with any image-based dataset can leverage an image-based foundation model to label, fine-tune and analyze a smaller but mighty custom model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAdditional considerations users should address for scaling similar projects include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAutomating future data ingestion, curation, enrichment, and labeling when the fine-tuned model needs to be retrained due to drift.\u003c/li\u003e\u003cli\u003eIncorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman evaluators and human-in-the-loop\u003c/u\u003e\u003c/a\u003e, as well as error analysis for identifying and addressing edge cases.\u0026nbsp;\u003c/li\u003e\u003cli\u003eEasy-to-use, user interface that can be customized for various modalities of data when multiple users are involved.\u0026nbsp;\u003c/li\u003e\u003cli\u003eA \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erobust SDK\u003c/u\u003e\u003c/a\u003e for integrating with any MLOps solutions provider, especially when incorporating model monitoring and complex deployment patterns.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEven with fine-tuned models, there’s no such thing as “setting and forgetting”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAll models eventually need to be retrained, with the data refreshed to account for changes.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eIn this tutorial, we demonstrated an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003ecomputer vision\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a YOLO model with labels created in Model Foundry using Amazon Rekognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHopefully you were able to see how easy it is to go from raw data to cutting-edge custom models in less than 30 min.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou learned how the Quantumworks Lab platform enables model distillation by allowing developers to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIngest, explore and prepare image-based datasets using Catalog;\u003c/li\u003e\u003cli\u003eUse any \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage based foundation model\u003c/u\u003e\u003c/a\u003e to automatically label data using Model Foundry as well as how to incorporate human-in-the-loop evaluation using Annotate;\u003c/li\u003e\u003cli\u003eExport these labeled predictions to a cloud-based training environment for fine-tuning;\u003c/li\u003e\u003cli\u003eAutomate the various workflows using the Quantumworks Lab SDK;\u003c/li\u003e\u003cli\u003eEvaluate model performance and analyze model errors using Quantumworks Lab Model.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn the next part of this series we replicate a very similar workflow for NLP, using model distillation to fine-tune a BERT model with labels created in Model Foundry using PaLM2. \u003c/p\u003e","comment_id":"65bd22d782e5680001e07877","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Social-Cards-Example--1-.jpg","featured":false,"visibility":"public","created_at":"2024-02-02T17:13:59.000+00:00","updated_at":"2024-10-02T00:02:14.000+00:00","published_at":"2024-02-01T20:23:00.000+00:00","custom_excerpt":"Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"681516a56e02120001d5ab84","name":"#mikiko","slug":"hash-mikiko","description":null,"feature_image":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/404/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"65bd4ff882e5680001e078f9","name":"Model Distillation","slug":"model-distillation","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/tag/model-distillation/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/end-to-end-workflow-with-model-distillation-for-computer-vision/","excerpt":"Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. ","reading_time":11,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}]},"__N_SSG":true},"page":"/guides/[id]","query":{"id":"working-with-videos-using-gemini-1-5-and-multimodal-models"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script>
    

    

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>

                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="/static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer>
</body>
<!-- Mirrored from labelbox.com/guides/working-with-videos-using-gemini-1-5-and-multimodal-models/?ref=labelbox.ghost.io by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 13:29:43 GMT -->
</html>