<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/how-to-build-generative-captioning-using-foundation-models-for-product-listings/?ref=labelbox.ghost.io by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 13:29:43 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">How to build generative captioning and enrich product listings faster with foundation models</title><meta name="description" data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="How to build generative captioning and enrich product listings faster with foundation models" data-next-head=""/><meta property="og:description" data-next-head=""/><meta property="og:url" content="https://labelbox-guides.ghost.io/how-to-build-generative-captioning-using-foundation-models-for-product-listings/" data-next-head=""/><meta property="og:image" content="https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.16.33-AM.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="How to build generative captioning and enrich product listings faster with foundation models" data-next-head=""/><meta name="twitter:description" data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox-guides.ghost.io/how-to-build-generative-captioning-using-foundation-models-for-product-listings/" data-next-head=""/><meta property="twitter:image" content="https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.16.33-AM.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g34[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.giShFC .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.giShFC .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.giShFC .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:2px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h2{padding-top:10px;}}/*!sc*/
.giShFC .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h3{padding-top:10px;}}/*!sc*/
.giShFC .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.giShFC .content a:hover{color:#1e40af;}/*!sc*/
.giShFC .content li{margin-bottom:20px;}/*!sc*/
.giShFC .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.giShFC .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.giShFC .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.giShFC .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}/*!sc*/
.giShFC .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.giShFC .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100% !important;height:100% !important;margin:0 auto;}/*!sc*/
.giShFC .content .kg-button-card{margin-bottom:20px;height:auto;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn{-webkit-transition-property:all;transition-property:all;-webkit-transition-timing-function:cubic-bezier(.4,0,.2,1);transition-timing-function:cubic-bezier(.4,0,.2,1);-webkit-transition-duration:.15s;transition-duration:.15s;background-color:#2563eb;padding:0.75rem;font-size:1rem;line-height:1.5rem;font-weight:500;border-radius:0.5rem;color:#fafafa;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn:hover{background-color:#1d4ed8;}/*!sc*/
.giShFC .content .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 0 0 0.75em;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;}/*!sc*/
data-styled.g35[id="id__PostContentWrapper-sc-1ct5gml-0"]{content:"giShFC,"}/*!sc*/
.kUXEED #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.kUXEED .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.kUXEED .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.kUXEED #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.kUXEED #image-viewer .close:hover,.kUXEED #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.kUXEED .modal-content{width:100%;}}/*!sc*/
data-styled.g36[id="id__ImageModal-sc-1ct5gml-1"]{content:"kUXEED,"}/*!sc*/
@media (max-width:1026px){.cwKcJJ.toc-container{display:none;}}/*!sc*/
.cwKcJJ.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g37[id="id__TocContainer-sc-1ct5gml-2"]{content:"cwKcJJ,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/guides/%5bid%5d-78cf43cbe169ea75.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="id__ImageModal-sc-1ct5gml-1 kUXEED"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All guides</a><main class="id__TocContainer-sc-1ct5gml-2 cwKcJJ toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">How to build generative captioning and enrich product listings faster with foundation models</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.16.33-AM.png"/></div><main class="id__PostContentWrapper-sc-1ct5gml-0 giShFC md:px-24"><div class="content js-toc-content"><p>The rise of foundation models has enabled companies to seamlessly enrich all of their products and services with rich captions and descriptions in minimal time and with little human effort required. Organizations can now use AI taught to automatically generate descriptions for product listings based on a wide range of images or product specifications. By incorporating a powerful generative captioning system, companies that span retail and internet &amp; media can now readily enhance customer assets and foster stronger connections to boost customer loyalty and increase key metrics such as conversion rate, engagement, and average order value.</p><p>However, building a robust and effective AI-powered captioning system can be challenging for many teams. Some key challenges include: </p><ul><li><strong>Data quality and quantity: </strong>Building a strong captioning system that makes accurate predictions requires a vast amount of high-quality data. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights. Furthermore,  there are situations where the volume and speed of text generation tasks required means it is not efficiently achieved through human input alone. </li><li><strong>Scalability: </strong>As a business grows and their catalog expands, the system should be able to handle new and incoming data. Additionally, it can be a costly process when allocating significant portions of an individual’s or team’s time to repetitively generating text outputs. Ensuring scalability and maintaining model performance with new data can be particularly challenging for teams relying on in-house solutions or disparate ML tools.</li><li><strong>Privacy and Security: </strong>When it comes to customer data and specific product information, ensuring user privacy and safeguarding against potential security violations is critical to maintain trust with customers and maintaining a relevant website/app. </li></ul><p>Quantumworks Lab is a data-centric AI platform that can help automate generative captioning systems. Rather than spending valuable time building in-house or relying on disparate systems and applications, teams can leverage Quantumworks Lab’s platform to seamlessly build an end-to-end workflow that integrates with your existing tech stack and helps teams build AI systems faster.</p><figure class="kg-card kg-image-card"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png" class="kg-image" alt="" loading="lazy" width="1730" height="970" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 1730w" sizes="(min-width: 720px) 720px"></figure><p>In this guide, we’ll walk through how you can leverage Quantumworks Lab’s platform to build a powerful generative captioning system, ensuring your customers get deeper personalization from LLMs and for your internal teams to derive insights faster from your website product listings.</p><hr><h2 id="see-it-in-action-how-to-build-a-powerful-generative-captioning-system-in-labelbox">See it in action: How to build a powerful generative captioning system in Quantumworks Lab</h2><p><em>The  walkthrough below covers Quantumworks Lab’s platform across </em><a href="../../product/catalog/index.html"><em>Catalog</em></a><em>, </em><a href="../../product/annotate/index.html"><em>Annotate</em></a><em>, and </em><a href="../../product/model/index.html"><em>Model</em></a><em>. We recommend that you </em><a href="https://app.labelbox.com/signup"><em>create a Quantumworks Lab account</em></a><em> to best follow along with this tutorial.</em></p><h2 id="part-1-explore-and-prepare-your-data">Part 1: Explore and prepare your data</h2><h3 id="painlessly-consolidate-all-your-product-listing-data">Painlessly consolidate all your product listing data</h3><p>Building a generative captioning system requires consolidating data of different types from various sources. Such data can include product, business, and customer information that might be siloed or stored in different databases. To holistically browse and visualize your entire product catalog, leverage Quantumworks Lab Catalog to bring and view all of your data in a single place.</p><figure class="kg-card kg-embed-card"><iframe src="https://fast.wistia.net/embed/iframe/u35epr8pyd" title="Solution Accelerator - Description Generation - Image Intro Video" allow="autoplay; fullscreen" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" msallowfullscreen="" width="960" height="492"></iframe>
<script src="../../../fast.wistia.net/assets/external/E-v1.js" async=""></script></figure><h3 id="accelerate-product-discovery-across-your-entire-catalog">Accelerate product discovery across your entire catalog</h3><p>An effective captioning system for product listings relies on training a model with a thorough understanding of your product data, encompassing product tags, categories, and more. However, organizations often have an ever-growing product catalog with hundreds or thousands of products. Dealing with this volume of data at scale and effectively searching, organizing, and managing data for machine learning tasks can be a challenge.</p><p>You can leverage Quantumworks Lab Catalog to visualize, browse, and curate your product listings.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.30.38.png" class="kg-image" alt="" loading="lazy" width="1450" height="661" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2023-12-14-at-17.30.38.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2023-12-14-at-17.30.38.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.30.38.png 1450w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Explore &amp; curate product listings by creating slices</span></figcaption></figure><p><strong>Search and curate data</strong></p><p>With Catalog, you will be able to see the sample your product listings dataset and query specific items such as shoes, tops, trousers, and more. Try searching across key product-specific metadata such as category, the year the item was released, season, type, and more. With Catalog, you can contextualize your data with <a href="https://docs.labelbox.com/docs/datarow-metadata">custom metadata</a> and <a href="https://docs.labelbox.com/docs/catalog-overview#visualize-attachments">attachments</a> to each asset for greater context. </p><p>Leverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:</p><ul><li><a href="https://docs.labelbox.com/docs/search">Search across datasets</a> to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)</li><li>Automatically <a href="../how-to-find-similar-data-in-one-click/index.html">find similar data</a> in seconds with off-the-shelf embeddings </li><li>Filter data based on <a href="https://docs.labelbox.com/docs/natural-language-search">natural language</a> and flexibly <a href="https://docs.labelbox.com/docs/search#how-filters-work">layer structured and unstructured filters</a> for more granular data curation</li></ul><h3 id="categorize-and-curate-product-listings-faster">Categorize and curate product listings faster</h3><figure class="kg-card kg-embed-card"><iframe src="https://fast.wistia.net/embed/iframe/nm4oggk5cv" title="Solution Accelerator - Description Generation - Image Main Video" allow="autoplay; fullscreen" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" msallowfullscreen="" width="960" height="488"></iframe>
<script src="../../../fast.wistia.net/assets/external/E-v1.js" async=""></script></figure><p>Once we have appropriately explored and curated our data, we're now in a position to begin generating product descriptions for the images that we have available to us. In some cases, it may be beneficial to create a product description for all images at once. However, in a real-world setting, it may be the case that specific teams are responsible for creating product listings for various departments such as shoe wear tops or dresses. </p><p>To replicate this scenario, we can navigate to the slices that we created earlier. For example, one team may be responsible for generating product descriptions for the tops department. In this instance, you can select a subset of the data or you can select all data rows available within the slice. Having done so, you can then predict with Model Foundry which allows API connectivity to state-of-the-art foundational models  as well as the option to integrate custom models that you may have trained within your own organization.</p><h2 id="part-2-streamline-captioning-product-listings-and-labeling-automation-with-foundry"><strong>Part 2: Streamline captioning product listings and labeling automation with Foundry</strong></h2><p>In this example, we're interested in generating a text description for the images provided and will therefore be working with a multi-modal model (i.e., OpenAI's GPT-4 Vision). </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.32.29.png" class="kg-image" alt="" loading="lazy" width="1872" height="817" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2023-12-14-at-17.32.29.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2023-12-14-at-17.32.29.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2023-12-14-at-17.32.29.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.32.29.png 1872w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Choose from a variety of foundation models to enrich your product listing data.</span></figcaption></figure><p>As a next step, you'll have the option to consider model run parameters and provide a prompt. The prompt helps you describe to the model what it is you want it to execute during inference.</p><p>You can generate a prompt in a conversational manner specifying various characteristics such as the type of tone that you want your generated response to have, the aspects of the picture that you're interested in or the desired length of the response.</p><p>Here, we asked the model to create a description for the main item of clothing shown in the picture and asked it to exclude words such as "the item", "these are" or "this shows", and also to speak in certainties by ignoring such phrases as "it appears". We also ask for a specific prompt response length. </p><p>Having done so, we can configure any of the additional parameters as required, and then we have the option to generate preview. Generating preview allows you to run inference on just a sample of the data. In this case, the maximum is five however, if of interest, we can decrease this as required.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png" class="kg-image" alt="" loading="lazy" width="2000" height="910" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">GPT-4 Vision outputting the preview for a specific product listing</span></figcaption></figure><p>After completing your first model run, we can navigate back to your model to explore the outputs. We now have the option to explore each data row and observe the outputted description that GPT4-Vision has generated. Here we can see a remarkable level of detail and accuracy based on the image provided. </p><p>At this point, the descriptions may be appropriate to pass straight to our website for the product listing. However, in some instances, we may wish to have a human in the loop to either tweak or validate the product descriptions that our model outputs.</p><p><strong><em>Send a subset of data to the labeling project for human-in-the-loop validation</em></strong></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png" class="kg-image" alt="" loading="lazy" width="2000" height="892" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Set up your ontology and create a human-in-the-loop review process.</span></figcaption></figure><p>1) When it comes to setting up an Annotate project that enables human in the loop validation for our model outputs, the first thing to do is to set up our ontology. When doing so, it may also be of interest to have a binary classification, such as whether or not we needed to tweak the overall description, such as validation and we can have an option edited or accepted.</p><p>2) Once we've established this ontology, you can create it, and we're now in a position to set up our annotate project. You can navigate to Annotate, create a new project and name it something appropriate such as "product description validation".</p><p>3) Next, attach the ontology that we've just created and we're now in a position where our project has set up and the last thing we need to do is to add our data. For this, let's navigate back to the model and select the appropriate model run as before and select all our data rows. </p><p>4) We now want to "Send to Annotate" and we can include our model predictions. An option will appear on whether we want to to map the text output that the model is created to our description free text classification. Select "Map", and select description, and we can then select text, and then "Save" to ensure that we have correctly mapped the text output from the model to the description option within our ontology.</p><p>5) Next, let's set our batch settings. In this case we can set it to "1" given this is the highest priority within our project, and dictate what step of the workflow we want to put it in. In our case, we'll select "Initial labeling task". Having done so, we can navigate back to our Annotate project where we'll see that the data rules have been successfully added and we're now in a position for the our subject matter experts to validate all of the outputs coming from the model, and to start our labeling process (as shown below).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png" class="kg-image" alt="" loading="lazy" width="2000" height="923" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Example product description that can be acceped</span></figcaption></figure><p>We can see above that the description provided, "a pale blue cable knit sweater with a high, ribbed turtleneck collar and long sleeves" appears to be appropriate so we may not need to edit it and can simply hit "Accept".</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png" class="kg-image" alt="" loading="lazy" width="2000" height="840" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Example product description that needs to be edited.</span></figcaption></figure><p>In&nbsp;an&nbsp;instance&nbsp;such&nbsp;as&nbsp;this&nbsp;one above,&nbsp;we&nbsp;may&nbsp;wish&nbsp;to&nbsp;edit&nbsp;the&nbsp;description&nbsp;slightly. In&nbsp;this&nbsp;case,&nbsp;the&nbsp;sleeves&nbsp;are&nbsp;"long",&nbsp;but&nbsp;not&nbsp;"full&nbsp;length", so let's describe them as "quarter length" sleeves and keep everything else as consistent with what the model outputted, but mark it as "edited".</p><p>We can continue with this workflow until all of our data rows have been processed with a human-in-the-loop review. The advantage of this approach is that it allows you to validate whether or not the model is performing is expected, offering a certain level of security by having domain experts assess each of the product descriptions before they go live on your website or app. </p><h2 id="part-3-using-text-inputs-to-generate-and-compare-detailed-product-descriptions-with-foundry"><strong>Part 3: Using text inputs to generate and compare detailed product descriptions with Foundry</strong></h2><figure class="kg-card kg-embed-card"><iframe src="https://fast.wistia.net/embed/iframe/ztmzjm7ps1" title="Solution Accelerator - Description Generation - Text Video" allow="autoplay; fullscreen" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" msallowfullscreen="" width="960" height="542"></iframe>
<script src="../../../fast.wistia.net/assets/external/E-v1.js" async=""></script></figure><p>In the walkthrough above, we looked at how we could create product listing descriptions based on an image input. Next, let's look at how we can generate  detailed product descriptions based on text input containing rough product specifications, such as those found on online marketplaces.</p><p>1) The first step is to set up your ontology again by navigating to the schema tab, creating a new scheme and selecting the "text" media type as we're dealing with text inputs this time. We recommend naming it something descriptive such as "multi description", and then add the overall text classifications for our descriptions, with free text responses. In this example, we'll be generating 3 product descriptions. We will then need some indicator to say which of the preferred descriptions has been identified as the best by the human labeler (e.g., description 1, description 2, description 3, preferred description) as shown below .</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png" class="kg-image" alt="" loading="lazy" width="2000" height="1058" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Set up a multi-description comparison workflow.</span></figcaption></figure><p>2) We're now in a position to execute our Model Foundry run, which we can do by selecting all the data rows within this data set and by predicting with Foundry. Since we're interested in generating an output text based on the input provided, let's use OpenAI's GPT-4 again in this instance. </p><p>3) As a next step, let's select an ontology. We're going to want to edit this ontology because we're not interested in asking the model to provide the preferred description but simply to output each of the three descriptions. To do so, we can click "edit", then we can ignore the preferred description option. Clicking "save" will update the automatically generated prompt before and we can add some additional information via prompt to direct the model in our desired manner.</p><p>4) As shown above, we'll be asking the model to provide three different output descriptions based on the input specifications provided. We can then edit any of the parameters available to us as required and once we're ready, we can generate the preview as shown below. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png" class="kg-image" alt="" loading="lazy" width="2000" height="1305" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Generate description previews.</span></figcaption></figure><p>5) After we've generated our preview, we can begin to explore each of the outputted results for the sample of five data rows. To see the full response, we can navigate to to "view log" and see the outputted response from our model which has provided three different descriptions. As it appears the model is performing as expected and we can now execute the full model run (as shown below).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png" class="kg-image" alt="" loading="lazy" width="2000" height="1173" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">View log to see model output.</span></figcaption></figure><p>6) Once our model run has been complete, we're in a position to navigate to the Model Run of interest. We can explore the three different descriptions that the model has generated for each of our data rows. Let's pass these to an Annotate project for a human labeler to select which of the three descriptions are preferable. To do so, we'll set up an Annotate project again, making sure to select "text", add our ontology that we used earlier, and then pass our model run outputs, and then "Send to Annotate".</p><p>7) Finally, we can view the 3 descriptions that the model has outputted. We can now have a human labeler review each of these descriptions, and select which one of the descriptions is most appropriate to include in the website listing as shown below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png" class="kg-image" alt="" loading="lazy" width="2000" height="986" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Human labelers can review each description for the preferred description.</span></figcaption></figure><p>We can continue with this workflow until labelers have reviewed and assessed all data row descriptions. This allows us to build a dataset ready to extract and identify preferred descriptions and pass these description downstream for whatever use case is required such as inclusion on the product listing of our website. </p><p>We hope that this walkthrough gives you an idea of how you can leverage Quantumworks Lab's Foundry capabilities to create automatically generated product descriptions for a wide range of use cases. This should dramatically reduce the time taken and costs associated with generating outputted text (from product descriptions, to alt-text, to articles), while leveraging foundation models to automate the first pass. </p><hr><p></p><p>As consumer businesses in media, retail, and internet strive to distinguish themselves in a competitive market, the power of AI-driven captioning systems for automating product listings serves as a powerful lever for speeding up manual tagging processes. Companies can tap into their vast data stores and harness the capabilities of advanced algorithms to enrich the customer experience. </p><p>Quantumworks Lab is a data-centric AI platform that empowers teams to more quickly build intelligent applications. To get started, <a href="https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..">sign up for a free Quantumworks Lab account</a> or <a href="../../sales/index.html">request a demo</a>. </p></div></main></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="Footer__StyledFooter-sc-u68pnv-0 eJChXt"><div class="undefined lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="py-24"><div class=" w-full h-[1px] bg-neutral-200"></div></div><div class="hidden md:block"><img src="../../static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36"/></div><section class="hidden md:grid footer-grid"></section><section class="social-media"></section><div class="text-center "><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">© Quantumworks Lab, Inc <br/>We enable breakthroughs</p><div class="flex flex-row flex-wrap justify-content-center gap-4 mt-4"><a href="https://docs.labelbox.com/page/terms-of-service" class=" " target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Terms of Service</p></a><div class="mx-1 border"></div><a href="https://docs.labelbox.com/page/privacy-notice" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Privacy Notice</p></a><div class="mx-1 border hidden sm:block"></div><a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Copyright Dispute Policy</p></a></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"how-to-build-generative-captioning-using-foundation-models-for-product-listings","id":"65b14b577aa1d10001dfcda3","uuid":"2189684b-4093-4d42-87ea-9edaae64f147","title":"How to build generative captioning and enrich product listings faster with foundation models","html":"\u003cp\u003eThe rise of foundation models has enabled companies to seamlessly enrich all of their products and services with rich captions and descriptions in minimal time and with little human effort required. Organizations can now use AI taught to automatically generate descriptions for product listings based on a wide range of images or product specifications. By incorporating a powerful generative captioning system, companies that span retail and internet \u0026amp; media can now readily enhance customer assets and foster stronger connections to boost customer loyalty and increase key metrics such as conversion rate, engagement, and average order value.\u003c/p\u003e\u003cp\u003eHowever, building a robust and effective AI-powered captioning system can be challenging for many teams. Some key challenges include: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eBuilding a strong captioning system that makes accurate predictions requires a vast amount of high-quality data. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights. Furthermore,  there are situations where the volume and speed of text generation tasks required means it is not efficiently achieved through human input alone. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability: \u003c/strong\u003eAs a business grows and their catalog expands, the system should be able to handle new and incoming data. Additionally, it can be a costly process when allocating significant portions of an individual’s or team’s time to repetitively generating text outputs. Ensuring scalability and maintaining model performance with new data can be particularly challenging for teams relying on in-house solutions or disparate ML tools.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrivacy and Security: \u003c/strong\u003eWhen it comes to customer data and specific product information, ensuring user privacy and safeguarding against potential security violations is critical to maintain trust with customers and maintaining a relevant website/app. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that can help automate generative captioning systems. Rather than spending valuable time building in-house or relying on disparate systems and applications, teams can leverage Quantumworks Lab’s platform to seamlessly build an end-to-end workflow that integrates with your existing tech stack and helps teams build AI systems faster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1730\" height=\"970\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 1730w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how you can leverage Quantumworks Lab’s platform to build a powerful generative captioning system, ensuring your customers get deeper personalization from LLMs and for your internal teams to derive insights faster from your website product listings.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-build-a-powerful-generative-captioning-system-in-labelbox\"\u003eSee it in action: How to build a powerful generative captioning system in Quantumworks Lab\u003c/h2\u003e\u003cp\u003e\u003cem\u003eThe  walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/\"\u003e\u003cem\u003eCatalog\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/\"\u003e\u003cem\u003eAnnotate\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/\"\u003e\u003cem\u003eModel\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup\"\u003e\u003cem\u003ecreate a Quantumworks Lab account\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial.\u003c/em\u003e\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003ch3 id=\"painlessly-consolidate-all-your-product-listing-data\"\u003ePainlessly consolidate all your product listing data\u003c/h3\u003e\u003cp\u003eBuilding a generative captioning system requires consolidating data of different types from various sources. Such data can include product, business, and customer information that might be siloed or stored in different databases. To holistically browse and visualize your entire product catalog, leverage Quantumworks Lab Catalog to bring and view all of your data in a single place.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/u35epr8pyd\" title=\"Solution Accelerator - Description Generation - Image Intro Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"492\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"accelerate-product-discovery-across-your-entire-catalog\"\u003eAccelerate product discovery across your entire catalog\u003c/h3\u003e\u003cp\u003eAn effective captioning system for product listings relies on training a model with a thorough understanding of your product data, encompassing product tags, categories, and more. However, organizations often have an ever-growing product catalog with hundreds or thousands of products. Dealing with this volume of data at scale and effectively searching, organizing, and managing data for machine learning tasks can be a challenge.\u003c/p\u003e\u003cp\u003eYou can leverage Quantumworks Lab Catalog to visualize, browse, and curate your product listings.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.30.38.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1450\" height=\"661\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2023-12-14-at-17.30.38.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2023-12-14-at-17.30.38.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.30.38.png 1450w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExplore \u0026amp; curate product listings by creating slices\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSearch and curate data\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith Catalog, you will be able to see the sample your product listings dataset and query specific items such as shoes, tops, trousers, and more. Try searching across key product-specific metadata such as category, the year the item was released, season, type, and more. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context. \u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search\"\u003eSearch across datasets\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/\"\u003efind similar data\u003c/a\u003e in seconds with off-the-shelf embeddings \u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search\"\u003enatural language\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search#how-filters-work\"\u003elayer structured and unstructured filters\u003c/a\u003e for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"categorize-and-curate-product-listings-faster\"\u003eCategorize and curate product listings faster\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/nm4oggk5cv\" title=\"Solution Accelerator - Description Generation - Image Main Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"488\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce we have appropriately explored and curated our data, we're now in a position to begin generating product descriptions for the images that we have available to us. In some cases, it may be beneficial to create a product description for all images at once. However, in a real-world setting, it may be the case that specific teams are responsible for creating product listings for various departments such as shoe wear tops or dresses. \u003c/p\u003e\u003cp\u003eTo replicate this scenario, we can navigate to the slices that we created earlier. For example, one team may be responsible for generating product descriptions for the tops department. In this instance, you can select a subset of the data or you can select all data rows available within the slice. Having done so, you can then predict with Model Foundry which allows API connectivity to state-of-the-art foundational models  as well as the option to integrate custom models that you may have trained within your own organization.\u003c/p\u003e\u003ch2 id=\"part-2-streamline-captioning-product-listings-and-labeling-automation-with-foundry\"\u003e\u003cstrong\u003ePart 2: Streamline captioning product listings and labeling automation with Foundry\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIn this example, we're interested in generating a text description for the images provided and will therefore be working with a multi-modal model (i.e., OpenAI's GPT-4 Vision). \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.32.29.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1872\" height=\"817\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2023-12-14-at-17.32.29.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2023-12-14-at-17.32.29.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2023-12-14-at-17.32.29.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.32.29.png 1872w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eChoose from a variety of foundation models to enrich your product listing data.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAs a next step, you'll have the option to consider model run parameters and provide a prompt. The prompt helps you describe to the model what it is you want it to execute during inference.\u003c/p\u003e\u003cp\u003eYou can generate a prompt in a conversational manner specifying various characteristics such as the type of tone that you want your generated response to have, the aspects of the picture that you're interested in or the desired length of the response.\u003c/p\u003e\u003cp\u003eHere, we asked the model to create a description for the main item of clothing shown in the picture and asked it to exclude words such as \"the item\", \"these are\" or \"this shows\", and also to speak in certainties by ignoring such phrases as \"it appears\". We also ask for a specific prompt response length. \u003c/p\u003e\u003cp\u003eHaving done so, we can configure any of the additional parameters as required, and then we have the option to generate preview. Generating preview allows you to run inference on just a sample of the data. In this case, the maximum is five however, if of interest, we can decrease this as required.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"910\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eGPT-4 Vision outputting the preview for a specific product listing\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAfter completing your first model run, we can navigate back to your model to explore the outputs. We now have the option to explore each data row and observe the outputted description that GPT4-Vision has generated. Here we can see a remarkable level of detail and accuracy based on the image provided. \u003c/p\u003e\u003cp\u003eAt this point, the descriptions may be appropriate to pass straight to our website for the product listing. However, in some instances, we may wish to have a human in the loop to either tweak or validate the product descriptions that our model outputs.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSend a subset of data to the labeling project for human-in-the-loop validation\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"892\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSet up your ontology and create a human-in-the-loop review process.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e1) When it comes to setting up an Annotate project that enables human in the loop validation for our model outputs, the first thing to do is to set up our ontology. When doing so, it may also be of interest to have a binary classification, such as whether or not we needed to tweak the overall description, such as validation and we can have an option edited or accepted.\u003c/p\u003e\u003cp\u003e2) Once we've established this ontology, you can create it, and we're now in a position to set up our annotate project. You can navigate to Annotate, create a new project and name it something appropriate such as \"product description validation\".\u003c/p\u003e\u003cp\u003e3) Next, attach the ontology that we've just created and we're now in a position where our project has set up and the last thing we need to do is to add our data. For this, let's navigate back to the model and select the appropriate model run as before and select all our data rows. \u003c/p\u003e\u003cp\u003e4) We now want to \"Send to Annotate\" and we can include our model predictions. An option will appear on whether we want to to map the text output that the model is created to our description free text classification. Select \"Map\", and select description, and we can then select text, and then \"Save\" to ensure that we have correctly mapped the text output from the model to the description option within our ontology.\u003c/p\u003e\u003cp\u003e5) Next, let's set our batch settings. In this case we can set it to \"1\" given this is the highest priority within our project, and dictate what step of the workflow we want to put it in. In our case, we'll select \"Initial labeling task\". Having done so, we can navigate back to our Annotate project where we'll see that the data rules have been successfully added and we're now in a position for the our subject matter experts to validate all of the outputs coming from the model, and to start our labeling process (as shown below).\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"923\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample product description that can be acceped\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe can see above that the description provided, \"a pale blue cable knit sweater with a high, ribbed turtleneck collar and long sleeves\" appears to be appropriate so we may not need to edit it and can simply hit \"Accept\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"840\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample product description that needs to be edited.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn\u0026nbsp;an\u0026nbsp;instance\u0026nbsp;such\u0026nbsp;as\u0026nbsp;this\u0026nbsp;one above,\u0026nbsp;we\u0026nbsp;may\u0026nbsp;wish\u0026nbsp;to\u0026nbsp;edit\u0026nbsp;the\u0026nbsp;description\u0026nbsp;slightly. In\u0026nbsp;this\u0026nbsp;case,\u0026nbsp;the\u0026nbsp;sleeves\u0026nbsp;are\u0026nbsp;\"long\",\u0026nbsp;but\u0026nbsp;not\u0026nbsp;\"full\u0026nbsp;length\", so let's describe them as \"quarter length\" sleeves and keep everything else as consistent with what the model outputted, but mark it as \"edited\".\u003c/p\u003e\u003cp\u003eWe can continue with this workflow until all of our data rows have been processed with a human-in-the-loop review. The advantage of this approach is that it allows you to validate whether or not the model is performing is expected, offering a certain level of security by having domain experts assess each of the product descriptions before they go live on your website or app. \u003c/p\u003e\u003ch2 id=\"part-3-using-text-inputs-to-generate-and-compare-detailed-product-descriptions-with-foundry\"\u003e\u003cstrong\u003ePart 3: Using text inputs to generate and compare detailed product descriptions with Foundry\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ztmzjm7ps1\" title=\"Solution Accelerator - Description Generation - Text Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"542\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn the walkthrough above, we looked at how we could create product listing descriptions based on an image input. Next, let's look at how we can generate  detailed product descriptions based on text input containing rough product specifications, such as those found on online marketplaces.\u003c/p\u003e\u003cp\u003e1) The first step is to set up your ontology again by navigating to the schema tab, creating a new scheme and selecting the \"text\" media type as we're dealing with text inputs this time. We recommend naming it something descriptive such as \"multi description\", and then add the overall text classifications for our descriptions, with free text responses. In this example, we'll be generating 3 product descriptions. We will then need some indicator to say which of the preferred descriptions has been identified as the best by the human labeler (e.g., description 1, description 2, description 3, preferred description) as shown below .\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1058\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSet up a multi-description comparison workflow.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e2) We're now in a position to execute our Model Foundry run, which we can do by selecting all the data rows within this data set and by predicting with Foundry. Since we're interested in generating an output text based on the input provided, let's use OpenAI's GPT-4 again in this instance. \u003c/p\u003e\u003cp\u003e3) As a next step, let's select an ontology. We're going to want to edit this ontology because we're not interested in asking the model to provide the preferred description but simply to output each of the three descriptions. To do so, we can click \"edit\", then we can ignore the preferred description option. Clicking \"save\" will update the automatically generated prompt before and we can add some additional information via prompt to direct the model in our desired manner.\u003c/p\u003e\u003cp\u003e4) As shown above, we'll be asking the model to provide three different output descriptions based on the input specifications provided. We can then edit any of the parameters available to us as required and once we're ready, we can generate the preview as shown below. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1305\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eGenerate description previews.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e5) After we've generated our preview, we can begin to explore each of the outputted results for the sample of five data rows. To see the full response, we can navigate to to \"view log\" and see the outputted response from our model which has provided three different descriptions. As it appears the model is performing as expected and we can now execute the full model run (as shown below).\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1173\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eView log to see model output.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e6) Once our model run has been complete, we're in a position to navigate to the Model Run of interest. We can explore the three different descriptions that the model has generated for each of our data rows. Let's pass these to an Annotate project for a human labeler to select which of the three descriptions are preferable. To do so, we'll set up an Annotate project again, making sure to select \"text\", add our ontology that we used earlier, and then pass our model run outputs, and then \"Send to Annotate\".\u003c/p\u003e\u003cp\u003e7) Finally, we can view the 3 descriptions that the model has outputted. We can now have a human labeler review each of these descriptions, and select which one of the descriptions is most appropriate to include in the website listing as shown below.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"986\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHuman labelers can review each description for the preferred description.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe can continue with this workflow until labelers have reviewed and assessed all data row descriptions. This allows us to build a dataset ready to extract and identify preferred descriptions and pass these description downstream for whatever use case is required such as inclusion on the product listing of our website. \u003c/p\u003e\u003cp\u003eWe hope that this walkthrough gives you an idea of how you can leverage Quantumworks Lab's Foundry capabilities to create automatically generated product descriptions for a wide range of use cases. This should dramatically reduce the time taken and costs associated with generating outputted text (from product descriptions, to alt-text, to articles), while leveraging foundation models to automate the first pass. \u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAs consumer businesses in media, retail, and internet strive to distinguish themselves in a competitive market, the power of AI-driven captioning systems for automating product listings serves as a powerful lever for speeding up manual tagging processes. Companies can tap into their vast data stores and harness the capabilities of advanced algorithms to enrich the customer experience. \u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to more quickly build intelligent applications. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/\"\u003erequest a demo\u003c/a\u003e. \u003c/p\u003e","comment_id":"65b14b577aa1d10001dfcda3","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.16.33-AM.png","featured":false,"status":"published","visibility":"public","created_at":"2024-01-24T17:39:35.000Z","updated_at":"2024-01-25T21:17:14.000Z","published_at":"2024-01-25T17:06:09.000Z","custom_excerpt":"Learn how to leverage Quantumworks Lab’s platform and Foundry to build a powerful generative captioning system, ensuring your customers get deeper personalization from LLMs and for your internal teams to derive insights faster from your website product listings.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","email":"product@labelbox.co","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2022-09-19T20:07:21.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-19T20:07:20.000Z","updated_at":"2022-09-19T20:07:21.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","email":"product@labelbox.co","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2022-09-19T20:07:21.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-19T20:07:20.000Z","updated_at":"2022-09-19T20:07:21.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":null,"email_segment":"all","url":"https://labelbox-guides.ghost.io/how-to-build-generative-captioning-using-foundation-models-for-product-listings/","excerpt":"Learn how to leverage Quantumworks Lab’s platform and Foundry to build a powerful generative captioning system, ensuring your customers get deeper personalization from LLMs and for your internal teams to derive insights faster from your website product listings.","reading_time":11,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":false},"recommended":[]},"__N_SSG":true},"page":"/guides/[id]","query":{"id":"how-to-build-generative-captioning-using-foundation-models-for-product-listings"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/how-to-build-generative-captioning-using-foundation-models-for-product-listings/?ref=labelbox.ghost.io by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 13:29:43 GMT -->
</html>