<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/tag/build-ai/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:31:49 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../static/scripts/munchkin.js"></script><script src="../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../_next/static/chunks/pages/guides/tag/%5bid%5d-cc2bd40a983392d8.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../index.html"><img width="106" height="24" alt="logo" src="../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Build AI</a><a href="../use-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Use AI</a><a href="../explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="../label-data-for-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Label data for AI</a><a href="../train-fine-tune-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="mb-10"><h2 class="text-3xl md:text-4xl font-medium mb-4">Build AI</h2><p class="text-base max-w-2xl font-medium text-neutral-500"></p></div><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexa17d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Quantumworks Lab Leaderboards: Redefining AI evaluations with human-centric assessments</p><p class="text-base max-w-2xl undefined line-clamp-3">Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../programmatically-launch-human-data-jobs-for-rlhf-and-evaluation/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexbb2a.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../programmatically-launch-human-data-jobs-for-rlhf-and-evaluation/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Programmatically launch human data jobs for RLHF and evaluation</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to harness the SDK to manage human data labeling jobs for RLHF and model evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../evaluating-leading-text-to-speech-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index6ff8.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../evaluating-leading-text-to-speech-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Evaluating leading text-to-speech models</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../metrics-based-rag-development-with-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexd10c.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../metrics-based-rag-development-with-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Metrics-based RAG Development with Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index96b1.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Unlocking precision: The &quot;Needle-in-a-Haystack&quot; test for LLM evaluation</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our &quot;Needle-in-a-Haystack&quot; experiment. Learn how to enhance accuracy and efficiency in complex data annotations.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../a-comprehensive-approach-to-evaluating-text-to-video-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index9db2.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../a-comprehensive-approach-to-evaluating-text-to-video-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">A comprehensive approach to evaluating text-to-video models</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../a-comprehensive-approach-to-evaluating-text-to-image-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexea06.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../a-comprehensive-approach-to-evaluating-text-to-image-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">A comprehensive approach to evaluating text-to-image models</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../using-multimodal-chat-to-enhance-a-customers-online-support-experience/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexdc84.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../using-multimodal-chat-to-enhance-a-customers-online-support-experience/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using multimodal chat to enhance a customer’s online support experience</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../ai-foundations-understanding-embeddings/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexfbc8.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../ai-foundations-understanding-embeddings/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">AI foundations: Understanding embeddings</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../harnessing-ai-for-efficient-video-labeling/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index23ca.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../harnessing-ai-for-efficient-video-labeling/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to harness AI for efficient video labeling</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. </p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8">Page 1 of 8<a class="ml-9 text-neutral-700 mb-1" href="page/2/index.html">&gt;</a></div></div></div></div></div></div><footer class="Footer__StyledFooter-sc-u68pnv-0 eJChXt"><div class="undefined lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="py-24"><div class=" w-full h-[1px] bg-neutral-200"></div></div><div class="hidden md:block"><img src="../../../static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36"/></div><section class="hidden md:grid footer-grid"></section><section class="social-media"></section><div class="text-center "><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">© Quantumworks Lab, Inc <br/>We enable breakthroughs</p><div class="flex flex-row flex-wrap justify-content-center gap-4 mt-4"><a href="https://docs.labelbox.com/page/terms-of-service" class=" " target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Terms of Service</p></a><div class="mx-1 border"></div><a href="https://docs.labelbox.com/page/privacy-notice" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Privacy Notice</p></a><div class="mx-1 border hidden sm:block"></div><a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Copyright Dispute Policy</p></a></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"684755a1e82e4e00013fe307","uuid":"fab31394-c337-43cc-bc44-725a7b69cc61","title":"Quantumworks Lab Leaderboards: Redefining AI evaluations with human-centric assessments","slug":"labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments","html":"\u003cp\u003eIn the rapidly evolving landscape of artificial intelligence, traditional benchmarks are no longer sufficient to capture the true capabilities of AI models. At Quantumworks Lab, we're excited to introduce our groundbreaking \u003ca href=\"https://labelbox.com/leaderboards/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e—an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks. \u003c/p\u003e\u003ch2 id=\"the-limitations-of-current-benchmarks-and-leaderboards\"\u003e\u003cstrong\u003eThe limitations of current benchmarks and leaderboards\u003c/strong\u003e\u003c/h2\u003e\u003ch3 id=\"benchmark-contamination\"\u003e\u003cstrong\u003eBenchmark contamination\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOne of the most pressing issues in AI evaluation today is benchmark contamination. As large language models are trained on vast amounts of internet data, they often inadvertently include the very datasets used to evaluate them. This leads to inflated performance metrics that don't accurately reflect real-world capabilities. For example:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe LAMBADA dataset, designed to test language understanding, has been found in the training data of several popular language models, with an \u003ca href=\"https://hitz-zentroa.github.io/lm-contamination/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLM Contamination Index\u003c/u\u003e\u003c/a\u003e of 29.3%.\u003c/li\u003e\u003cli\u003ePortions of the SQuAD question-answering dataset have been discovered in the pretraining corpora of multiple large language models.\u003c/li\u003e\u003cli\u003eEven coding benchmarks like HumanEval have seen their \u003ca href=\"https://arxiv.org/pdf/2407.07565?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esolutions leaked online\u003c/u\u003e\u003c/a\u003e, potentially contaminating future model training.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis contamination makes it increasingly difficult to trust traditional benchmark results, as models may be “cheating” by memorizing test data rather than demonstrating true understanding or capability.\u003c/p\u003e\u003ch3 id=\"existing-leaderboards-a-step-forward-but-not-enough\"\u003e\u003cstrong\u003eExisting leaderboards: A step forward, but not enough\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWhile several leaderboards have emerged to address the limitations of traditional benchmarks, they each come with their own set of challenges.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eLMSYS chatbot arena\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLMSYS Chatbot Arena, despite its broad accessibility, faces notable challenges in providing objective AI evaluations. Its reliance on non-expert assessments and emphasis on chat-based evaluations may introduce personal biases, potentially favoring engaging responses over true intelligence. Researchers worry that this approach could lead companies to prioritize optimizing for superficial metrics rather than genuine real-world performance. Furthermore, LMSYS's commercial ties raise concerns about impartiality and the potential for an uneven evaluation playing field, as usage data may be selectively shared with certain partners.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScale AI's SEAL\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eScale’s Safety, Evaluations, and Alignment Lab (SEAL), released few months ago, offers detailed insights/evaluations for topics such as reasoning, coding, and agentic tool use. However, the infrequent updates and primary focus on language models, while useful, may not capture the full spectrum of rapidly advancing multimodal AI capabilities.\u003c/p\u003e\u003ch2 id=\"challenges-in-ai-evaluation\"\u003e\u003cstrong\u003eChallenges in AI evaluation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThese and other existing leaderboards all run into core challenges with AI evaluations:\u003c/p\u003e\u003cp\u003e1) Data contamination and overfitting to public benchmarks\u003c/p\u003e\u003cp\u003e2) Scalability issues as models improve and more are added\u003c/p\u003e\u003cp\u003e3) Lack of standards for evaluation instructions and criteria\u003c/p\u003e\u003cp\u003e4) Difficulty in linking evaluation results to real-world outcomes\u003c/p\u003e\u003cp\u003e5) Potential bias in human evaluations\u003c/p\u003e\u003ch2 id=\"introducing-the-labelbox-leaderboards-a-comprehensive-approach-to-ai-evaluation\"\u003e\u003cstrong\u003eIntroducing the Quantumworks Lab Leaderboards: A comprehensive approach to AI evaluation\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Quantumworks Lab Leaderboards are the first to tackle these challenges by conducting structured evaluations on subjective AI model outputs using human experts and a scientific process that provides detailed feature-level metrics and multiple ratings. Leaderboards are available for \u003ca href=\"https://labelbox.com/leaderboards/complex-reasoning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eComplex Reasoning\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/multimodal-reasoning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eMultimodal Reasoning\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eImage Generation\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/speech-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSpeech Generation\u003c/u\u003e\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/leaderboards/video-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eVideo Generation\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eOur goal is to go beyond traditional leaderboards and benchmarks by incorporating the following elements:\u003c/p\u003e\u003ch3 id=\"1-multimodal-and-niche-focus\"\u003e\u003cstrong\u003e1. Multimodal and niche focus\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eUnlike leaderboards that primarily focus on text-based large language models, we evaluate a diverse range of AI modalities and specialized applications, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImage generation and analysis\u003c/li\u003e\u003cli\u003eAudio processing and synthesis\u003c/li\u003e\u003cli\u003eVideo creation and manipulation\u003c/li\u003e\u003cli\u003eComplex and multimodal reasoning\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"2-expert-human-evaluation\"\u003e\u003cstrong\u003e2. Expert human evaluation\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFor every evaluation, public or private, it’s critical for the raters to reflect your target audience. We place expert human judgment, using our \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAlignerr\u003c/u\u003e\u003c/a\u003e workforce, at the core of the evaluation process to ensure:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSubjective quality assessment:\u003c/strong\u003e Humans assess aspects like aesthetic appeal, realism, and expressiveness.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContextual understanding:\u003c/strong\u003e Evaluators consider the broader context and intended use.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAlignment with human preferences:\u003c/strong\u003e Raters ensure evaluations reflect criteria that matter to end-users.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResistance to contamination:\u003c/strong\u003e Human evaluations on novel tasks are less prone to data contamination.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"3-reliable-and-transparent-methodology\"\u003e\u003cstrong\u003e3. Reliable and transparent methodology\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWe are committed to performing trustworthy evaluations using a variety of sophisticated metrics. Quantumworks Lab balances privacy with openness by providing detailed feature-level metrics (e.g. prompt alignment, visual appeal, and numerical count for text-image models) and multiple ratings.\u003c/p\u003e\u003cp\u003eIn addition to critical human experts performing the evaluations, our methodology utilizes the \u003ca href=\"https://labelbox.com/product/platform/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLablebox Platform\u003c/a\u003e to generate advanced metrics on both the rater and model performance. We provide the following metrics across our three leaderboards:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eElo rating system:\u003c/strong\u003e Adapted from competitive chess, our Elo system provides a dynamic rating that adjusts based on head-to-head comparisons between models. This allows us to capture relative performance in a way that's responsive to improvements over time.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTrueSkill rating:\u003c/strong\u003e Originally developed for Xbox Live, TrueSkill offers a more nuanced rating that accounts for both a model's performance and the uncertainty in that performance. This is particularly useful for newer models or those with fewer evaluations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank percentages:\u003c/strong\u003e We track how often each model achieves each rank (1st through 5th) in direct comparisons. This provides insight into not just average performance, but consistency of top-tier results.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAverage rating:\u003c/strong\u003e A straightforward metric that gives an overall sense of model performance across all evaluations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to these key metrics, our methodology incorporates the following characteristics to ensure a balanced and fair evaluation:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eExpert evaluators:\u003c/strong\u003e Utilizing skilled professionals from our Alignerr platform to provide nuanced, context-aware assessments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComprehensive and novel datasets:\u003c/strong\u003e Curated to reflect real-world scenarios while minimizing contamination.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTransparent reporting:\u003c/strong\u003e Detailed insights into our methodologies and results without compromising proprietary information.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"4-continuously-updated-evaluations\"\u003e\u003cstrong\u003e4. Continuously updated evaluations\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOur leaderboard isn't static; we plan to regularly update our evaluations to include the latest models and evaluation metrics, ensuring stakeholders have access to current and relevant information.\u003c/p\u003e\u003ch2 id=\"leaderboard-insights-a-glimpse-into-the-image-generation-leaderboard\"\u003e\u003cstrong\u003eLeaderboard insights: A glimpse into the image generation leaderboard\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo illustrate the power of our comprehensive evaluation approach, let's explore the \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eimage generation leaderboard\u003c/a\u003e. For each evaluation of the latest image-generating models, we capture and publish four key pieces of data to help understand capabilities and areas of opportunity for each model.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Elo ratings:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXf_TDIoHT0uGUl1DnLLMI3qkaV4g9M9yrK4GVWRc1Ze495hZYjwXE5Yq0wZefgLQecqsXA8cS7bSmTNz923B8CYgza7d2PkPn25crjiCrd0I3W2MG53hWiTgo-N8BTL8y11b3vuog?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT Image 1 leads with 1069.17, followed by GPT 4.1 at 1039.62 and Recraft v3 at 1039.37\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2) TrueSkill ratings:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdIblItim2rjJ5kSmyMtNqbhzIQZM24C6TwogiUeuNFCwBNyUPydDpZ3vOlYPSimzksITCDgQ_ej4Czavte2eI8aT0OEjCB0FwDDagBBP-yQLgt2T_FV8dTy0DdFqWUfk4cbMSYcg?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT Image 1 again leads with 982.86, with GPT 4.1 following at 979.89\u003c/li\u003e\u003cli\u003eThis indicates high expected performance for GPT Image 1with relatively low uncertainty\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Rank percentages:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe8jjQCJegixK47fNhDT67NDHe8fhN6U0Kwm-mbkrIa9HWWFcwKbIubLDO7uy0_KtVVu1gHOcIk0Zh9VOHGgV6zrBBkGxbaCHW9a5Uu48lSo8L-J_48nAq6Q-bFqD9k-aYwPvY3?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT 4.1 achieves the top rank 60.14% of the time, followed by GPT Image 1 at 59.31%\u003c/li\u003e\u003cli\u003eThis shows GPT 4.1 consistency in achieving top results, but also highlights GPT Image 1 and DALL-E 3’s strong performance\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4) Average rank (lower better):\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemzdbe3m9BuWRyjcZTnpDJMSAn8a4UuMJTaepEJHsQwJ_JocoixeC4UQcftoenMWuZ3y9tToZU7UtSwxRMNCVdLwytjw72Tq_8bMC5MAJOxB0VYMpE4P4-EcVxs-uqZsEPlM6K?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eNote: Lower score is better here so GPT 4.1 leads in average rank.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT 4.1 slightly edges out GPT Image 1 with an average rating of 1.4 vs 1.41 (lower is better)\u003c/li\u003e\u003cli\u003eThis suggests GPT 4.1 performs well in direct comparisons despite lower Elo and TrueSkill ratings\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese metrics provide a multi-faceted view of model performance, allowing users to understand not just which model is \"best\" overall, but which might be most suitable for their specific use case. For instance, while GPT Image 1 and GPT 4.1 leads in most metrics, DALL-E 3’s and Imagen 3’s strong average rating suggests it is a reliable choice for consistent performance across a range of tasks.\u003c/p\u003e\u003ch2 id=\"join-the-revolution-beyond-the-benchmark\"\u003e\u003cstrong\u003eJoin the revolution: Beyond the benchmark\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Quantumworks Lab Leaderboards represent a significant advance in AI evaluation, pushing past traditional leaderboards by incorporating expert human evaluations for subjective generative AI models using comprehensive metrics. We are uniquely able to achieve this thanks to our modern AI data factory that combines human experts and our scalable platform with years of operational excellence evaluating AI models.\u003c/p\u003e\u003cp\u003eWe invite you to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCheck out the \u003ca href=\"http://labelbox.com/leaderboards?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e to explore our latest evaluations across various AI modalities and niche applications.\u003c/li\u003e\u003cli\u003e\u003cu\u003eLet us know\u003c/u\u003e if you have suggestions or want a specific model included in future assessments.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eContact us\u003c/u\u003e\u003c/a\u003e to learn more about how we can help you evaluate and improve your AI models across all modalities.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eReady to go beyond the benchmark? Let's redefine AI evaluation — together—and drive the field toward more meaningful, human-aligned progress that truly captures the capabilities of next-generation AI models.\u003c/p\u003e","comment_id":"684755a1e82e4e00013fe307","feature_image":"https://labelbox-guides.ghost.io/content/images/2025/06/guide_LeaderboxHero2.png","featured":false,"visibility":"public","created_at":"2025-06-09T21:44:01.000+00:00","updated_at":"2025-06-11T17:30:22.000+00:00","published_at":"2025-06-10T23:15:53.000+00:00","custom_excerpt":"Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"664e43b39132db0001f20548","name":"Michael Haag","slug":"michael","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/michael/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"664e43b39132db0001f20548","name":"Michael Haag","slug":"michael","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/michael/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/","excerpt":"Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Quantumworks Lab Leaderboards: Redefining AI Evaluation with Human Experts","meta_description":"Introducing our groundbreaking Quantumworks Lab leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66cceba90693fd000117e1ca","uuid":"baef90d7-868a-4181-b171-0c1bfec81d5c","title":"Programmatically launch human data jobs for RLHF and evaluation","slug":"programmatically-launch-human-data-jobs-for-rlhf-and-evaluation","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eLabelbox’s Python SDK provides AI teams with a powerful approach to orchestrate human data labeling projects. In this guide, we’ll walk through how to harness the Python SDK to manage human data labeling jobs for RLHF and evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.\u003c/p\u003e\u003ch2 id=\"getting-started-set-up-the-labelbox-python-sdk\"\u003eGetting started: Set up the Quantumworks Lab Python SDK\u003c/h2\u003e\u003cp\u003eLet's begin by first setting up the Quantumworks Lab Python SDK in four simple steps:\u003c/p\u003e\u003cp\u003e1) \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCreate an API key\u003c/u\u003e\u003c/a\u003e to start using Quantumworks Lab Python SDK\u003c/p\u003e\u003cp\u003e2) pip install \"Quantumworks Lab[data]\" in terminal or !pip install \"Quantumworks Lab[data]\" in your notebook\u003c/p\u003e\u003cp\u003e3) Authentication can be done by saving your key to an environment variable:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003euser@machine:~$ export LABELBOX_API_KEY=\"\u0026lt;your_api_key\u0026gt;\"\nuser@machine:~$ python3\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e4) Then, import and initialize the API Client. \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab as lb \nclient = lb.Client()\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"importing-your-data-into-labelbox-methods-and-supported-formats\"\u003eImporting your data into Quantumworks Lab: Methods and supported formats\u003c/h2\u003e\u003cp\u003eNow that the SDK has been set up,\u0026nbsp; let's look at an example of uploading LLM response evaluation data for RLHF:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003e# Create a dataset\ndataset = client.create_dataset(\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"RLHF asset upload example\"+str(uuid.uuid4()),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;iam_integration=None\n)\n# Upload assets\ntask = dataset.create_data_rows([\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_1.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;},\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_2.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;},\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_3.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;}\n\u0026nbsp;\u0026nbsp;])\ntask.wait_till_done()\nprint(\"Errors:\",task.errors)\nprint(\"Failed data rows:\", task.failed_data_rows)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLearn more about all supported data types and editors \u003ca href=\"https://docs.labelbox.com/docs/label-data?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003ch2 id=\"creating-an-ontology-using-the-sdk\"\u003eCreating an ontology using the SDK\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWith the data imported, the next step is to create your ontology for the project. The ontology defines the structure and relationships within the data for your labeling process. Below is an example of how to create an ontology using the Quantumworks Lab Python SDK:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003eimport Quantumworks Lab as lb\nontology_builder = lb.OntologyBuilder(\n\u0026nbsp;\u0026nbsp;classifications=[\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.TEXT,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Free form text example\"),\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.CHECKLIST,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Checklist example\",\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"first_checklist_answer\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"second_checklist_answer\")\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;),\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.RADIO,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Radio example\",\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"first_radio_answer\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"second_radio_answer\")\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.RADIO,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Rank #1\", # More ranks can be created like this\u0026nbsp; with N number of options\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;required = True,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 1\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 2\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 3\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;,)\n\u0026nbsp;\u0026nbsp;]\n)\n\n\nontology = client.create_ontology(\"RLHF classification example\", ontology_builder.asdict(), media_type=lb.MediaType.Conversational)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eFor more information about ontology creation, please refer to the \u003ca href=\"https://docs.labelbox.com/reference/ontology-examples?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edocumentation\u003c/u\u003e\u003c/a\u003e for more examples.\u003c/p\u003e\u003ch2 id=\"best-practices-for-ontology-design\"\u003eBest practices for ontology design\u003c/h2\u003e\u003ch3 id=\"leverage-existing-ontologies-wisely\"\u003eLeverage existing ontologies wisely\u003c/h3\u003e\u003cp\u003eLabelbox allows users to reuse ontologies from previous projects, saving time and ensuring consistency across related tasks. However, be cautious when modifying shared ontologies:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCopy existing ontologies: To prevent unintended changes to previous projects, create a copy of an existing ontology. This creates a new schema node while retaining all your classes.\u003c/li\u003e\u003cli\u003eUsers can customize the ontology for their current project. After copying, they can freely modify the ontology to suit the new project's needs without affecting earlier work.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"optimize-object-ordering-for-logical-workflows\"\u003eOptimize object ordering for logical workflows\u003c/h3\u003e\u003cp\u003eThe order of objects in the ontology can significantly impact the labeling process:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePrioritize common objects: Create the most frequently used objects first. They'll appear at the top of the list, making them easily accessible to labelers.\u003c/li\u003e\u003cli\u003eDesign a logical flow: For complex tasks like model response comparisons, structure the ontology to guide labelers through a step-by-step analysis:\u003c/li\u003e\u003c/ul\u003e\u003col\u003e\u003cli\u003eStart with individual model evaluation criteria.\u003c/li\u003e\u003cli\u003ePlace comparative questions (e.g., \"Which model response is best?\") at the end.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis approach ensures labelers have thoroughly analyzed each option before making final comparisons.\u003c/p\u003e\u003ch3 id=\"enhance-visual-clarity-with-color-coding\"\u003eEnhance visual clarity with color coding\u003c/h3\u003e\u003cp\u003eImprove the visual experience for labelers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConsistent color schemes: Assign and edit colors for each object in the ontology.\u003c/li\u003e\u003cli\u003eMaintain color consistency: Use the same colors throughout the project to reduce cognitive load and improve labeling speed and accuracy.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"provide-easy-access-to-labeling-instructions\"\u003eProvide easy access to labeling instructions\u003c/h3\u003e\u003cp\u003eMake sure labelers have all the information they need at their fingertips:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAttach PDF instructions: Upload labeling guidelines as a PDF document.\u003c/li\u003e\u003cli\u003eSide-by-side viewing: Labelers can reference the instructions within Quantumworks Lab, displayed alongside the project for convenient access.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"use-advanced-classification-features\"\u003eUse advanced classification features\u003c/h3\u003e\u003cp\u003eTake advantage of Quantumworks Lab's classification capabilities to create more nuanced and accurate labels:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImplement nested classifications: This allows for more detailed object identification. For example, after drawing a segmentation mask over a tree, labelers can further classify it as healthy or unhealthy.\u003c/li\u003e\u003cli\u003eSet required questions: Ensure critical information is always captured by making certain questions mandatory for each asset.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy following these best practices, users will create more efficient labeling jobs, leading to higher quality data and improved model performance.\u003c/p\u003e\u003ch2 id=\"labelbox-labeling-services\"\u003eLabelbox labeling services\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFor Enterprise plan users, Quantumworks Lab offers data labeling services, connecting them with professional labelers to process large amounts of data quickly and efficiently. Key features include:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Rapid Data Processing\u003c/strong\u003e: Quickly handle large volumes of data without the overhead of hiring additional staff.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Specialized Expertise\u003c/strong\u003e: Access labelers with specialized knowledge, including:\u003c/p\u003e\u003col\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eMedical experts\u003c/li\u003e\u003cli\u003eVarious language specialists\u003c/li\u003e\u003cli\u003eOther certified specialties\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003c/ol\u003e\u003cp\u003e\u003cstrong\u003e3) Flexibility\u003c/strong\u003e: Scale your labeling service up or down based on project needs without long-term commitments.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4) Quality Assurance\u003c/strong\u003e: Professional labelers are trained to maintain high standards of accuracy and consistency.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5) Time and Resource Savings\u003c/strong\u003e: Eliminate the need for recruitment, training, and management of an in-house labeling team.\u003c/p\u003e\u003cp\u003eBy leveraging labeling services, enterprise users can significantly accelerate their data labeling projects, especially when dealing with complex datasets or when requiring domain-specific expertise. This service complements Quantumworks Lab's robust data import and management capabilities, providing a comprehensive solution for large-scale AI and machine learning projects.\u003c/p\u003e\u003cp\u003eTo leverage labeling services, Quantumworks Lab provides programmatic methods to request labeling services as, shown here:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Getting labeling service information:\u003c/strong\u003e Users can retrieve information about the labeling service for a specific project:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003elabeling_service = project.get_labeling_service()\nprint(labeling_service)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e This will return details such as the service ID, project ID, creation date, status, and more.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Requesting labeling services for faster results:\u003c/strong\u003e Once data and an ontology with instructions has been added, users can initiate a boost request:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003elabeling_service.request()\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e This call initiates the labeling services service for your project.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Monitoring your labeling service’s status\u003c/strong\u003e:The Quantumworks Lab labeling service requested can be easily monitored via the UI as shown below:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXekVuCZ2W9OORuo0lpeibkcuoC79eXyUJ1KIU4s1OYsnE_VgNaLcMBLaJ5OHsc57ae_3BfSIM3hv4GacVmkhbOvbPNb-PetLKLK4vUN7fDitccn5y-PIhV_nZj_OU7TeOln9Y-tRbyClHvSN3vXPHVczmk?key=Eb16GHK2ItC9fBn058nPbA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"799\"\u003e\u003c/figure\u003e\u003ch2 id=\"simple-export-via-the-labelbox-sdk\"\u003eSimple export via the Quantumworks Lab SDK\u003c/h2\u003e\u003cp\u003eOnce the labeling project is complete, users can easily export the labels using the SDK, as shown below.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eexport_task = project.export(params=export_params, filters=filters)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis simple command allows users to retrieve labeled data that is ready for use in machine learning pipelines. Please refer to \u003ca href=\"https://docs.labelbox.com/reference/export-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edocumentation\u003c/u\u003e\u003c/a\u003e for flexible ways of exporting a project with filters.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab Python SDK offers teams with a convenient and powerful way to programmatically manage human data labeling projects. By providing control over every aspect of the labeling process - from data import and ontology design to project monitoring and data export - the SDK enables AI teams with the ability to incorporate high-quality labeled data into their workflows seamlessly.\u003c/p\u003e\u003cp\u003eWe hope you found this guide helpful for gaining a deeper understanding of how to capitalize on an SDK-driven approach to simplify complex tasks and enhance productivity. Whether you’re working on small-scale projects or large, distributed labeling efforts, the Quantumworks Lab SDK offers the full-suite of tooling needed to efficiently manage your\u0026nbsp; data labeling needs and accelerate their AI development process.\u003c/p\u003e\u003cp\u003eIf you're interested in implementing an SDK approach to jumpstart your human data jobs for RLHF and model evaluation,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out, or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more.\u003c/p\u003e","comment_id":"66cceba90693fd000117e1ca","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-26-at-2.03.08-PM.png","featured":false,"visibility":"public","created_at":"2024-08-26T20:55:05.000+00:00","updated_at":"2024-08-28T15:34:08.000+00:00","published_at":"2024-08-26T21:03:56.000+00:00","custom_excerpt":"Learn how to harness the SDK to manage human data labeling jobs for RLHF and model evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/programmatically-launch-human-data-jobs-for-rlhf-and-evaluation/","excerpt":"Learn how to harness the SDK to manage human data labeling jobs for RLHF and model evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bfded301c06500016e8b1c","uuid":"a2c40e88-6376-4824-b57f-e0e3673d8b1b","title":"Evaluating leading text-to-speech models","slug":"evaluating-leading-text-to-speech-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in speech generation evaluation, visit \u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe evolution of Text-to-Speech (TTS) models has been groundbreaking, with various companies offering high-quality solutions for a range of applications. Whether it's enhancing accessibility, automating customer service, or generating voiceovers, TTS models play a vital role in today's digital landscape. \u003c/p\u003e\u003cp\u003eWe recently evaluated the performance of six leading TTS models—Google TTS, Cartesia, AWS Polly, OpenAI TTS, Deepgram, and Eleven Labs—using key evaluation metrics such as Word Error Rate (WER) and human preference ratings.\u003c/p\u003e\u003cp\u003eTo provide an in-depth analysis, we evaluated each model on 500 prompts. Each prompt was reviewed by three expert labelers to ensure accuracy and provide diverse perspectives.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"evaluation-process\"\u003e\u003cstrong\u003eEvaluation Process\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate speech outputs across several key criteria:\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer\"\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWER measures the accuracy of the generated speech by calculating the number of insertions, deletions, and substitutions compared to a reference transcription. The lower the WER, the better the model performed in terms of generating accurate and coherent speech.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1120\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"word-errors\"\u003eWord errors\u003c/h3\u003e\u003cp\u003eRaters evaluated the presence of word errors in the generated TTS audio by counting the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Insertion Errors\u003c/strong\u003e: Additional words not present in the original text.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Deletion Errors\u003c/strong\u003e: Words from the original text that were omitted.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Substitution Errors\u003c/strong\u003e: Incorrect replacements for original words.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: No word errors or only minor errors (0-1 error).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some errors (2-3 errors).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Significant errors (4 or more errors).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"speech-naturalness\"\u003eSpeech naturalness\u003c/h3\u003e\u003cp\u003eRaters assessed how human-like the generated speech sounded.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Very human-like, natural flow with appropriate pauses and inflections.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some human qualities but with occasional robotic or awkward elements.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Clearly robotic or artificial, with choppy or monotone speech.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"pronunciation-accuracy\"\u003ePronunciation accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated how clearly and correctly words were pronounced in the TTS output.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: All words are pronounced clearly and correctly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: 1-2 words are mispronounced or unclear.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: 3 or more words are mispronounced or difficult to understand.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"noise\"\u003eNoise\u003c/h3\u003e\u003cp\u003eRaters evaluated the level of background noise or artifacts in the audio.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNone\u003c/strong\u003e: No detectable noise or artifacts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Minor noise, such as a faint hiss, but not distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Noticeable noise or artifacts that may be somewhat distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Significant noise or artifacts that interfere with comprehension.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"context-awareness\"\u003eContext awareness\u003c/h3\u003e\u003cp\u003eRaters assessed the TTS system's ability to adjust to context, including tone, emphasis, and punctuation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Excellent adaptation to context, with clear tonal shifts and pauses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Adequate adaptation but misses some nuances.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Little to no adaptation; reads text monotonously without context cues.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"prosody-accuracy\"\u003eProsody accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated the rhythm, stress, and intonation (prosody) of the generated speech.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Natural-sounding rhythm and intonation that closely mimics human speech.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Mostly natural but with occasional awkwardness in stress or rhythm.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Monotonous or robotic intonation with unnatural pauses and emphasis\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"prompts-curation-process\"\u003ePrompts curation process\u003c/h2\u003e\u003cp\u003eThe diverse set of prompts used in our TTS evaluation provides a comprehensive test of model capabilities across multiple dimensions. \u003c/p\u003e\u003cp\u003eConsider these examples:\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Wow! Did you see the fireworks last night? They were spectacular!\"\u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003e This short, exclamatory sentence tests the model's ability to convey excitement and emphasis. It challenges the TTS system to appropriately modulate tone and pitch to express enthusiasm.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"I wish you were on the right path...I-I-I truly do, but you're not,' he said sadly.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis prompt evaluates the model's handling of emotional nuance, hesitation (indicated by the stuttering \"I-I-I\"), and the use of ellipsis for pauses. It also tests the system's ability to convey sadness as specified in the dialogue tag.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Alright, listen up, team! The mission briefing is as follows: Operation Nighthawk commences at 0300 hours. Rendezvous at coordinates 51°30'26.5\"N 0°07'39.0\"W. Your login credentials for the secure server are: Username: NightOwl_Alpha, Password: 3ch3l0n_X9#. Remember, this is a stealth operation. Radio silence must be maintained except for emergency protocol Sierra-Tango-Seven. If compromised, use the failsafe code: Zulu-Yankee-4-7-2-Bravo. Any questions? No? Then synchronize your watches... now. Good luck, and godspeed.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis complex prompt tests multiple aspects:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical pronunciation of alphanumeric strings and military jargon\u003c/li\u003e\u003cli\u003eConveying urgency and authority\u003c/li\u003e\u003cli\u003eHandling of coordinates and precise numerical information\u003c/li\u003e\u003cli\u003eProper pacing for a lengthy, detailed instruction set\u003c/li\u003e\u003cli\u003eAppropriate pausing, indicated by punctuation and paragraph breaks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese prompts demonstrate how we cover a range of linguistic features, emotional tones, and formatting challenges. They span from simple exclamations to complex, technical instructions, ensuring a thorough assessment of prosody, emphasis, and versatility.\u003c/p\u003e\u003cp\u003eThis diversity in prompts matters because it enables a comprehensive evaluation that reflects real-world applicability. By covering such a broad spectrum of scenarios, we can identify specific strengths and weaknesses in each TTS model, allowing for fair comparisons and pinpointing areas for improvement. Whether it's expressing joy over fireworks, conveying emotional conflict, or delivering a detailed mission briefing, these prompts help us ensure that TTS models can handle the variety and complexity of human communication.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overall-ranking-best-to-worst-model-samples\"\u003eOverall Ranking best to worst model samples\u003c/h2\u003e\u003cp\u003eRaters ranked each TTS sample from best to worst based on performance across all categories above.\u003c/p\u003e\u003ch3 id=\"model-configurations-used-at-test-time\"\u003e\u003cstrong\u003eModel configurations used at test time\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: tts-1-hd\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: alloy\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle Text-to-Speech (TTS)\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel:\u003c/strong\u003e Default\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice: \u003c/strong\u003eDefault with neutral gender voice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eElevenLabs\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: eleven_monolingual_v1 (As of 30th September, eleven_monolingual_v1 is an outdated version and Quantumworks Lab will run a new experiment with\u0026nbsp;eleven_multilingual_v2 in the future)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: pMsXgVXv3BLzUgSXRplE\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: aura-asteria-en\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Default\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAmazon Polly\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Generative\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Ruth\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: sonic-english\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Teacher Lady\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe selected these models for our evaluation as they represent the cutting-edge offerings from leading companies in the TTS space, showcasing the latest advancements in voice synthesis technology.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"results\"\u003eResults\u003c/h1\u003e\u003cp\u003eOur evaluation of 500 diverse and complex prompts provided valuable insights into the capabilities of Google, Cartesia, AWS Polly, OpenAI, Deepgram, and Eleven Labs TTS models. Each prompt was meticulously assessed by three different raters to ensure accurate and diverse perspectives. This rigorous testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of these leading TTS models:\u003c/p\u003e\u003ch3 id=\"rank-distribution-percentages\"\u003eRank distribution percentages\u003cstrong\u003e\u0026nbsp; \u0026nbsp; \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"overall-ranking\"\u003eOverall ranking\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1:\u003c/strong\u003e\u0026nbsp; (Open AI TTS) - occurred 607 times, 42.93%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2:\u003c/strong\u003e\u0026nbsp; (Cartesia) - occurred 460 times, 32.53%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 410 times, 29.00%\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 4:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 338 times, 23.90%. Followed by Eleven labs as a close second with occurrence 335, 23.69%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 5:\u003c/strong\u003e\u0026nbsp; (Deepgram) - occurred 438 times, 30.98%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 6:\u003c/strong\u003e\u0026nbsp; (Google TTS) - occurred 868 times, 61.39%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote: Rank is determined by the most frequently selected model for each position, which is why some AWS Polly appears twice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1122\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003eFig: % of times each model was ranked first.\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer-results\"\u003eWord Error Rate (WER) Results\u003c/h3\u003e\u003cp\u003eThe WER scores offer an objective look at how accurately each model converts text to speech.\u003c/p\u003e\u003cul\u003e\u003cli\u003eDeepgram: 5.67%\u003c/li\u003e\u003cli\u003eOpenAI TTS: 4.19%\u003c/li\u003e\u003cli\u003eGoogle TTS: 3.36%\u003c/li\u003e\u003cli\u003eAWS Polly: 3.18%\u003c/li\u003e\u003cli\u003eCartesia: 3.87%\u003c/li\u003e\u003cli\u003eEleven Labs: 2.83%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs achieved the lowest WER at 2.83%, making it the most accurate model. Deepgram, however, had the highest WER at 5.67%, indicating room for improvement in transcription accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" title=\"Chart\" width=\"2000\" height=\"1126\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1123\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-specific-performance-results\"\u003eModel-Specific Performance Results\u003c/h2\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each TTS model based on key evaluation criteria.\u003c/p\u003e\u003ch3 id=\"openai-tts\"\u003eOpenAI TTS\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 4.1898%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 89.60% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 87.13% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 92.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 63.37% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 64.57% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOpenAI TTS excels at producing natural-sounding speech, making it a top choice for applications requiring lifelike vocal output. It demonstrates exceptional accuracy in pronunciation and performs excellently in producing clean audio with minimal background noise. The model shows strong capabilities in understanding and conveying contextual nuances in speech and is proficient in delivering appropriate intonation and rhythm. Despite ranking high in human preference, it shows moderate accuracy in word reproduction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cm24ur6wwf\" title=\"OpenAI: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gicqvkqc5e\" title=\"OpenAI: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kg9wqqftu4\" title=\"OpenAI: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"cartesia\"\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 58.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 53.11% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 93.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 80.98% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 77.02% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.8707%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCartesia demonstrates good accuracy in word reproduction. While not as consistent as OpenAI TTS in naturalness, it still performs very well in producing natural-sounding speech. It shows excellent accuracy in pronunciation and excels at producing clean audio with minimal background noise. The model displays good capabilities in understanding contextual nuances and performs well in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/3gukhi7z2j\" title=\"Cartesia: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y3lbes0sin\" title=\"Cartesia: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f0lmkto1xk\" title=\"Cartesia: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"aws-polly\"\u003e\u003cstrong\u003eAWS Polly\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 56.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 55.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.39% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 84.72% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Medium in 58.20% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.1770%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAWS Polly shows very good accuracy in word reproduction, ranking second among the models. It struggles more with producing natural-sounding speech compared to top performers but maintains excellent pronunciation accuracy. The model performs well in producing clean audio and shows good capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qawx1dwc9g\" title=\"Polly: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/klmnfcsno5\" title=\"AWS Polly: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ubzrqtjnc9\" title=\"AWS Polly: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"eleven-labs\"\u003e\u003cstrong\u003eEleven Labs\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 2.8302%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: Medium in 44.98% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 81.97% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 80.27% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 44.70% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 46.25% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs demonstrates the best accuracy in word reproduction among all models. It shows mixed results in producing natural-sounding speech but maintains excellent pronunciation accuracy. The model performs reasonably well in producing clean audio but has more noise issues than some competitors. It shows moderate capabilities in understanding contextual nuances and has room for improvement in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/x3qzhkt0ab\" title=\"Eleven Labs: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kl0lc7oqmz\" title=\"Eleven Labs: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bc19r3iqip\" title=\"Eleven Labs: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"deepgram\"\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 5.6743%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: High in 57.78% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 64.43% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 88.83% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 53.18% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 55.52% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDeepgram shows the highest error rate in word reproduction among all models. It performs moderately well in producing natural-sounding speech and demonstrates good accuracy in pronunciation, although lagging behind some competitors. The model performs well in producing clean audio with minimal background noise but shows moderate capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mngnvgtzx3\" title=\"Deepgram: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6h3bgmcg6d\" title=\"Deepgram: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uaj3x9bln8\" title=\"Deepgram: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"google-tts\"\u003e\u003cstrong\u003eGoogle TTS\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.3574%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Low in 78.01% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 77.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.46% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: Medium in 39.25% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: Low in 45.83% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z27ett9q8m\" title=\"Google TTS: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/06pqi5qc13\" title=\"Google TTS: Wow Did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kwotkto0u6\" title=\"Google TTS: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation reveals that while Eleven Labs leads in transcription accuracy with the lowest WER, OpenAI TTS emerges as the top choice when it comes to human preference, particularly for speech naturalness, pronunciation accuracy, and prosody.\u003c/p\u003e\u003cp\u003eConversely, Google TTS, despite a relatively low WER, ranks last in human preference due to poor performance in categories like speech naturalness and context awareness. This demonstrates the importance of a balanced evaluation that considers both quantitative metrics like WER and qualitative factors like user satisfaction.\u003c/p\u003e\u003cp\u003eAs the TTS field continues to evolve, the key to further advancements will lie in combining high transcription accuracy with natural, expressive, and context-aware speech. Models like OpenAI TTS and Eleven Labs are setting the standard, but there remains room for improvement across all models to fully meet the growing demands of diverse TTS applications.\u003c/p\u003e\u003cp\u003eIn summary, while WER is crucial, the broader context of user experience—focusing on factors like speech naturalness, context awareness, and prosody accuracy—is essential in evaluating the overall effectiveness of TTS models.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-speech models presented here represents a significant advancement in assessing AI-generated speech. By leveraging this methodology alongside Quantumworks Lab's cutting-edge platform, AI teams can dramatically accelerate the development and refinement of sophisticated, domain-specific speech generation models.\u003c/p\u003e\u003cp\u003eOur solution offers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eEfficient dataset curation\u003c/li\u003e\u003cli\u003eAutomated evaluation techniques\u003c/li\u003e\u003cli\u003eHuman-in-the-loop quality assurance\u003c/li\u003e\u003cli\u003eCustomizable workflows for TTS model assessment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-speech model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdohXa2KeUawTMTWS7MfKSUYC6fVPCHjtmcFCRt5ycmB7S4k4-JElTwnmRb6ufW2kR7F3Bqu1PwzbvL83LKcqgz8jbkoY02tefklFLp98wvqm2liOKBglEGaRITIk6Wx1tY3fi-43TaCJnpwEwRVCz7q101?key=Qy_gZn4bA_q73M6n7nwEIA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"1024\"\u003e\u003c/figure\u003e","comment_id":"66bfded301c06500016e8b1c","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-12.12.46-PM.png","featured":false,"visibility":"public","created_at":"2024-08-16T23:20:51.000+00:00","updated_at":"2024-11-26T00:11:38.000+00:00","published_at":"2024-08-17T19:13:25.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/evaluating-leading-text-to-speech-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bff4f801c06500016e8bba","uuid":"e5bf7da8-d960-4a06-9050-b06f63a8a4c0","title":"Metrics-based RAG Development with Quantumworks Lab","slug":"metrics-based-rag-development-with-labelbox","html":"\u003ch1 id=\"overview\"\u003e\u003cstrong\u003eOverview\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOne of the biggest challenges with RAG Application development is evaluating the quality of responses. As it currently stands, there are a series of different benchmark metrics, tools, and frameworks to help with RAG application evaluation.\u003c/p\u003e\u003cp\u003eLLM evaluation tools (such as RAGAS and DeepEval) provide a plethora of quality scores designed to evaluate the efficiency of the RAG model, ranging from retrieval to generation. In this guide, we’ll take a metrics based approach to enhance RAG development by focusing on 2 target metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext recall\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eThis measures (on a scale from 0-1) how well the retrieved content aligns with the Ground Truth Answer\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext precision\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eIdeally, the most relevant context chunks should be retrieved first. This metric measures whether the RAG application can return the most relevant chunks (with respect to the Ground Truth) at the top.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"generating-ground-truth-data-and-%E2%80%98synthetic%E2%80%99-data\"\u003e\u003cstrong\u003eGenerating ground truth data and ‘\u003cem\u003esynthetic’\u003c/em\u003e data\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo calculate performance metrics, we have to compare the RAG response with ground truth. Ground truths are factually verified responses to a given user query and must be linked back to a source document.\u003c/p\u003e\u003cul\u003e\u003cli\u003eOn the other hand, ground truth data can be automatically generated by pre-trained models. LLM Frameworks, such as LangChain, have modules that generate Question / Answer pairs based on a source document. While creating Synthetic ground truth is less labor intensive, drawbacks include quality issues, lacking real-world nuances, and inheriting model bias.\u003c/li\u003e\u003cli\u003eGround truth data is typically manually created by experts through the collection and verification of internal source documents. While labor intensive and slower to collect, manual data labeling optimizes for accuracy and quality.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo start off, we’ll use the Quantumworks Lab Platform to orchestrate a ground truth creation workflow with 2 approaches.\u003c/p\u003e\u003col\u003e\u003cli\u003eManual ground truth generation\u003c/li\u003e\u003cli\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"manual-ground-truth-generation\"\u003e\u003cstrong\u003eManual ground truth generation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eBy uploading common user questions (prompts) to the Quantumworks Lab Platform, we can set up an annotation project. Now subject matter experts can craft a thoughtful answer and provide the appropriate source.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"938\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"synthetic-ground-truth-generation-with-human-in-the-loop\"\u003e\u003cstrong\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1154\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeveraging the latest Foundation Models on Quantumworks Lab Foundry, we can combine an automated ground truth generation process with a Human in the Loop component for quality assurance.\u003c/p\u003e\u003cp\u003eTo start off, we will include the source document for each user query (PDF in this case). We can then prompt a foundation model (Google Gemini 1.5 Pro in this case) for each user question and attach the PDF context needed to answer the question.\u003c/p\u003e\u003cp\u003eThe provided prompt in this case is: *For the given text and PDF attachment, answer the following. Given the attached PDF, generate an answer using the information from the attachment. Respond with I don't know if the PDF attachment doesn't contain the answer to the question.*\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"724\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eNext, we can include the response generated from Gemini \u003cstrong\u003eas pre-labels\u003c/strong\u003e to an annotation project.\u0026nbsp;\u003c/p\u003e\u003cp\u003eInstead of starting from scratch while answering the question, human experts can modify and verify the Gemini response along with the corresponding document and user query, optimizing for quality and efficiency.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1480\" height=\"570\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1480w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"improving-rag\"\u003eImproving RAG\u003c/h1\u003e\u003cp\u003eA RAG based application consists of many components. Summarized in a recent research survey by Gao et al. (2023), a few factors that impact RAG system performance include (but not limited to):\u003c/p\u003e\u003cul\u003e\u003cli\u003eImproving the retriever (e.g. Reranking)\u003c/li\u003e\u003cli\u003eFine-tuning the embedding model for source documents\u003c/li\u003e\u003cli\u003eAdding document metadata tagging (for rerouting or semantic search)\u003c/li\u003e\u003cli\u003eQuery rewriting\u003c/li\u003e\u003cli\u003eOptimizing the chunking strategy\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’ll focus on the following strategies for improving metrics based RAG evaluation:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eOptimizing the chunking strategy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-tuning the embedding model\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"chunking-strategy\"\u003e\u003cstrong\u003eChunking strategy\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhile keeping LLM generation model (GPT Turbo 3.5) and Embeddings Model (sentence-transformers/distiluse-base-multilingual-cased-v2) consistent, we will compare the retrieval metrics derived from RAGAs.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will export the question and response pairs generated from Quantumworks Lab to a Python Environment.\u003c/li\u003e\u003cli\u003eNext we will generate the RAG response based on the following chunking strategies:\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRecursive character splitter\u003c/strong\u003e (500 token size with 150 chunk overlap)\u003cul\u003e\u003cli\u003eThis is a heuristic approach and involves dynamically splitting text based on a set of rules (e.g. new lines / tables / page breaks) while maintaining coherence.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpacy text splitter\u003c/strong\u003e\u003cul\u003e\u003cli\u003espaCy uses a rule-based approach combined with machine learning models to perform tokenization.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnce the RAG response and context have been generated, we create a table with the following fields required for RAGAs metrics evaluation: (Question, Ground Truth, RAG Answer, Context)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"432\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-5.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-5.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eBy calculating RAGAs metrics evaluation using the necessary Query, Response, Ground Truth, and Source information, we can derive performance results for the entire dataset and aggregate the findings. The Context Precision and Context Recall metrics for Recursive and Spacy are shown below:\u003c/p\u003e\u003ch3 id=\"recursive-text-chunking\"\u003e\u003cstrong\u003eRecursive text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1008\" height=\"624\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-4.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png 1008w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"spacy-text-chunking\"\u003e\u003cstrong\u003eSpacy text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1054\" height=\"652\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png 1054w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eFrom the results, we observe that Spacy yielded slightly better context precision. Given the volume of our dataset (~100 QA pairs), the difference is likely statistically insignificant. However, we can conclude that choosing the Chunking strategy and parameters will certainly affect the output of the chatbot.\u003c/p\u003e\u003cp\u003eGiven the distribution of our Recall and Precision metrics, both techniques seem to struggle with the same data rows. This indicates that the key semantics are not captured by our default embeddings (\u003cstrong\u003esentence-transformers/distiluse-base-multilingual-cased-v2\u003c/strong\u003e). We’ll test out another embedding model to see if we can improve RAG performance metrics.\u003c/p\u003e\u003ch2 id=\"embeddings-model\"\u003e\u003cstrong\u003eEmbeddings model\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAn Embedding Model is responsible for generating embeddings, which is a vector representation to any piece of information, such as a chunk of text in a RAG application. A solid embeddings model is able to better capture semantic meaning of a user question and the corpus of source documents, therefore returning the most relevant chunks as the context for answer generation.\u003c/p\u003e\u003cp\u003eWhile holding the chunking strategy constant (Recursive), we will change the underlying embeddings model in the RAG workflow before evaluating with RAGAs. The Huggingface\u003ca href=\"https://huggingface.co/spaces/mteb/leaderboard?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003eLeaderboard\u003c/u\u003e\u003c/a\u003e includes performance benchmarks for various embeddings models. Beyond retrieval performance, model size, latency, and context length are also important when choosing an embeddings model.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will generate responses using the \u003cstrong\u003ebge-base-en-v1.5\u003c/strong\u003e model and evaluate with RAGAs\u003cul\u003e\u003cli\u003eWhile ranking 35 on the leaderboards, this model is extremely lightweight (109 million parameters)\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1096\" height=\"632\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png 1096w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eFrom the results, changing the embeddings model (from distiluse-base-multilingual-cased-v2 to bge-base-en-v1.5) significantly improved the retrieval metrics, around ~10 points for both context recall and precision.\u003cul\u003e\u003cli\u003eThis indicates that the retrieved context chunks are more relevant to the ground truth and ranked higher on average as well.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"fine-tuning-an-embeddings-model\"\u003e\u003cstrong\u003eFine-tuning an embeddings model\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eMost embedding models are trained on general knowledge. Specific domains, such as internal organization information, may limit the effectiveness of these models. Therefore, we can choose to fine-tune an embeddings model with the goal of better understanding and generating domain specific information.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo fine tune embeddings, we need labeled examples of positive and negative context chunks.\u003cul\u003e\u003cli\u003eUsing a training dataset, we’ll upload the user query and the top X chunks to Quantumworks Lab\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing Quantumworks Lab Annotate, subject matter experts can determine if each chunk is relevant in answering the given question.\u003c/li\u003e\u003cli\u003e\u003c/li\u003e\u003cli\u003eUsing the Quantumworks Lab SDK we can now export our labels and convert to the following format for fine tuning: {\"query\": str, \"pos\": List[str], \"neg\":List[str]}\u003cul\u003e\u003cli\u003eThe fine tuning process for the BGE family of models is also described\u003ca href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/examples/finetune/README.md?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing the fine tuned embeddings model, we can generate updated responses and evaluate.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom the example in the screenshot, bge tends to incorrectly extract the citations section, which the expert labelers can correct and mark as irrelevant.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1005\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the resulting metrics, we can see that by fine tuning on 50 data rows is able to improve context recall and context precision by 2-3 points each. We expect this to improve as more labels are provided to continuously fine tune the embeddings model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1098\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image.png 1098w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eEvaluating the quality of responses in developing RAG Applications is essential for creating efficient and reliable systems. By incorporating the Quantumworks Lab platform in metrics based RAG development, from ground truth generation to feedback for embedding models, developers can enhance the overall performance and reliability of RAG applications. \u003c/p\u003e","comment_id":"66bff4f801c06500016e8bba","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.55.12-PM.png","featured":false,"visibility":"public","created_at":"2024-08-17T00:55:20.000+00:00","updated_at":"2024-09-03T20:00:44.000+00:00","published_at":"2024-08-17T01:07:55.000+00:00","custom_excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/metrics-based-rag-development-with-labelbox/","excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66b28fe6c2887d000107cdde","uuid":"40792083-45ad-4606-8583-7781bc74c305","title":"Unlocking precision: The \"Needle-in-a-Haystack\" test for LLM evaluation","slug":"unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation","html":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\u003cp\u003eSelecting the optimal large language model (LLM) for specific tasks is crucial for maximizing efficiency and accuracy. One of the key challenges faced by teams is selecting the best models for pre-labeling tasks, especially when dealing with large datasets and complex annotations.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox’s Model Foundry provides a robust platform for evaluation and determining the most suitable model for various applications. To illustrate this, the Quantumworks Lab Labs team conducted an experiment simulating the \"Needle-in-a-Haystack\" test. This test involves identifying specific elements within vast amounts of data, ensuring the model’s precision and reliability.\u003c/p\u003e\u003cp\u003eBy utilizing Quantumworks Lab Model Foundry’s advanced experiments and evaluation tools, teams can compare multiple LLMs to identify the one that delivers the highest accuracy and efficiency for pre-labeling on complex tasks, thus saving time and enhancing the quality of predictions.\u003c/p\u003e\u003cp\u003eIn this blog post, we’ll dive into the intricacies of the \"Needle-in-a-Haystack\", exploring how to leverage Foundry to find the best model for your pre-labeling or data enrichment needs.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"the-needle-in-a-haystack-test\"\u003eThe \"Needle-in-a-Haystack\" test\u003c/h1\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test is a specialized evaluation method designed to gauge the performance of large language models (LLMs) in identifying specific, often infrequent, elements in large datasets.\u0026nbsp;\u003c/p\u003e\u003cp\u003eImagine you have a massive dataset filled with a mix of common and rare pieces of information, similar to a haystack with a few needles hidden inside. The challenge is to determine how effectively a model can find those needles (rare information) without getting distracted by the surrounding hay (common information ).\u0026nbsp; This rare information could be anything from specific keywords in a text document to unique objects in a video.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"669\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/08/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePrevious analysis of Claude-2.1 \u003c/span\u003e\u003ca href=\"https://twitter.com/GregKamradt/status/1722386725635580292?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eperformed by Greg Kamradt\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e, member of the ARC Prize team.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"628\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/08/image-2.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePrevious analysis of GPT-4 \u003c/span\u003e\u003ca href=\"https://twitter.com/GregKamradt/status/1722386725635580292?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eperformed by Greg Kamradt\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e, member of the ARC Prize team.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"why-is-this-a-good-test-for-model-foundry\"\u003e\u003cstrong\u003eWhy is this a good test for Model Foundry?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test is a great fit for Model Foundry for several reasons:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReal-world relevance:\u003c/strong\u003e This test simulates real-world conditions, where critical information is buried in a large dataset. This ensures that models are being simulated in environments that match actual applications.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComprehensive evaluation:\u003c/strong\u003e Quantumworks Lab Model Foundry offers advanced tools that make setting up experiments, running evaluations, and comparing results efficient and easy.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eBetter decision making:\u003c/strong\u003e The insights gained from the Needle in a Haystack test can facilitate stronger decision-making when we are choosing the most suitable LLM for a task. This ensures investment in models that offer the best performance for application.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch1 id=\"creating-the-needle-in-a-haystack-internally\"\u003eCreating the \"Needle-in-a-Haystack\" internally\u003c/h1\u003e\u003cp\u003eThe first step in our experiment was to create a detailed labeling instructions set that we could eventually send to LLMs for pre-labeling. It is important to note that we decided to use Text data for our study. Various other asset types such as Video and Image can also emulate a similar test.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1254\" height=\"698\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 1254w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"instructions-overview\"\u003e\u003cstrong\u003eInstructions overview\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe wanted to build a dataset that consisted of conversations between users and a customer support chatbot, focusing on banking and financial transactions. Each conversation would be categorized into specific issues related to accounts, banking services, and transactions.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs a result, our instruction set would include detailed descriptions of each category, example conversations to guide the labeling process, and clear decision-making guidelines to help annotators distinguish between closely-related issues.\u003c/p\u003e\u003ch2 id=\"ontology\"\u003e\u003cstrong\u003eOntology\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe ontology included categories such as:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eACCOUNT_TERMINATION\u003c/li\u003e\u003cli\u003eACCOUNT_RECOVERY\u003c/li\u003e\u003cli\u003eACCOUNT_SECURITY_BREACH\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_UPDATE_DETAILS\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eBANKING_LOAN_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_FEE_DISPUTE\u003c/li\u003e\u003cli\u003eBANKING_OVERDRAFT_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_WIRE_TRANSFER_HELP\u003c/li\u003e\u003cli\u003eBANKING_SAVINGS_PLANS\u003c/li\u003e\u003cli\u003eBANKING_INVESTMENT_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eBANKING_INSURANCE_PRODUCTS\u003c/li\u003e\u003cli\u003eBANKING_MOBILE_APP_SUPPORT\u003c/li\u003e\u003cli\u003eBANKING_DEBIT_CARD_ACTIVATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eTRANSACTION_DISPUTE\u003c/li\u003e\u003cli\u003eTRANSCATION_REFUND\u003c/li\u003e\u003cli\u003eTRANSACTION_VERIFICATION\u003c/li\u003e\u003cli\u003eTRANSACTION_FRAUD_REPORT\u003c/li\u003e\u003cli\u003eTRANSACTION_LIMIT_INCREASE\u003c/li\u003e\u003cli\u003eTRANSACTION_HISTORY_QUERY\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAs seen, we chose closely-correlated categories and provided precise instructions so that while there were many similarities between subcategories, there were slight differences and nuances that our chosen LLM would have to notice and use to drive the decision-making process.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"building-the-dataset\"\u003e\u003cstrong\u003eBuilding the dataset\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCreating the dataset involved curating and structuring the data row to reflect real-world scenarios that modeled the above ontology. This ensured the dataset was comprehensive and challenging for the models.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"dataset-composition\"\u003e\u003cstrong\u003eDataset composition\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData Row Content: \u003c/strong\u003eEach data row represented a conversation between a user and customer support chatbot.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"sample-data-rows\"\u003e\u003cstrong\u003eSample data rows\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eACCOUNT_TERMIATION: User conversations requesting closure of their account\u003c/li\u003e\u003cli\u003eBANKING_LOAN_SERVICES: Inquiries about applying for or managing loans\u003c/li\u003e\u003cli\u003eTRANSACTION_FRAUD_REPORT: Reports of suspected fraudulent activities\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo view our labeling instructions, click the link \u003ca href=\"https://storage.googleapis.com/labelbox-datasets/lb_rahul/pdfs/Customer%20Support%20Ticket%20LLM%20Instructions.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"model-evaluation-using-foundry-llms\"\u003eModel evaluation using Foundry LLMs\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1246\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 1246w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"model-selection\"\u003e\u003cstrong\u003eModel selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe decided to evaluate our dataset on four leading LLMs currently on the market:\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"gemini-15-pro\"\u003e\u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eReleased by Google as part of the Gemini series;\u003c/li\u003e\u003cli\u003eKnown for its strong multimodal capabilities;\u003c/li\u003e\u003cli\u003eDesigned for complex reasoning and task completion.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gpt-4o\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eDeveloped by OpenAI;\u003c/li\u003e\u003cli\u003eAn advanced iteration of the GPT (Generative Pre-trained Transformer) series;\u003c/li\u003e\u003cli\u003eKnown for its strong natural language understanding and generation;\u003c/li\u003e\u003cli\u003eOptimized for faster response times and efficient computational resource usage.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"claude-35-sonnet\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eCreated by Anthropic;\u003c/li\u003e\u003cli\u003ePart of the Claude 3 model family;\u003c/li\u003e\u003cli\u003eKnown for its strong performance in writing and complex tasks;\u003c/li\u003e\u003cli\u003eCapable of engaging in nuanced conversations and providing detailed explanations.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gemini-15-flash\"\u003e\u003cstrong\u003eGemini 1.5 Flash\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eAnother model in the Google Gemini series;\u003c/li\u003e\u003cli\u003eOptimized for speed and efficiency;\u003c/li\u003e\u003cli\u003eDesigned for tasks requiring quick responses;\u003c/li\u003e\u003cli\u003eSuitable for applications where real-time responses are crucial.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"analysis-and-insights\"\u003e\u003cstrong\u003eAnalysis and insights\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1314\" height=\"712\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 1314w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce we had created Model Foundry Predictions on our dataset for all four LLMs, we placed them into a Model Experiment for model evaluation. Creating an experiment allowed us to dive deeply into the intricacies of each model to determine their overall performance on a needle in a haystack application.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFrom the exhibits above, we can see which models performed best from an precision perspective:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eGemini 1.5 Pro (81.55%)\u003c/li\u003e\u003cli\u003eClaude 3.5 Sonnet (80.98%)\u003c/li\u003e\u003cli\u003eGPT-4o (79.02%)\u003c/li\u003e\u003cli\u003eGemini 1.5 Flash (76.96%)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1220\" height=\"772\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 1220w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eConfusion matrices and Precision graphs are also available in Model Experiment, giving us a better understanding of the above precision scores.\u003c/p\u003e\u003cp\u003eFrom the graphs and further analysis, we can see the categories in the ontology that each model struggled with. Note that a struggle indicates a precision score of less than 0.75.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1306\" height=\"734\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 1306w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"gemini-15-pro-1\"\u003e\u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"claude-35-sonnet-1\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gpt-4o-1\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eBANKING_MISC_QUERY\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gemini-15-flash-1\"\u003e\u003cstrong\u003eGemini 1.5 Flash\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_FEE_DISPUTE\u003c/li\u003e\u003cli\u003eBANKING_INSURANCE_PRODUCTS\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eBANKING_MISC_QUERY\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBased on the performance breakdown, we can draw several insights:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTop performers\u003c/strong\u003e: Gemini 1.5 Pro and Claude 3.5 Sonnet emerge as the leading models for this particular needle in a haystack task, with very similar performance profiles.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCommon challenges\u003c/strong\u003e: All models struggled with certain categories, particularly ACCOUNT_ID_CONFIRMATION and BANKING_CREDIT_CARD_ISSUES. This suggests these categories may be inherently more difficult to classify or may require more specific training data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrecision vs. Speed\u003c/strong\u003e: While Gemini 1.5 Pro achieved the highest accuracy, teams should consider their specific needs. If real-time responses are crucial, Gemini 1.5 Flash might be a better choice despite its lower accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRoom for improvement\u003c/strong\u003e: Even the top-performing models have areas where they struggle. This information can be valuable for fine-tuning models or adjusting the labeling instructions for future iterations.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch1 id=\"leveraging-model-foundry-for-decision-making-and-pre-labeling\"\u003eLeveraging Model Foundry for decision making and pre-labeling\u003c/h1\u003e\u003cp\u003eThe experiment demonstrates the power of Quantumworks Lab Model Foundry in facilitating data-driven decision-making for model selection and optimizing the pre-labeling process. By providing comprehensive evaluation tools and visualizations, Model Foundry enables teams to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompare \u003cstrong\u003emultiple\u003c/strong\u003e models \u003cstrong\u003esimultaneously\u003c/strong\u003e\u003c/li\u003e\u003cli\u003eIdentify \u003cstrong\u003especific\u003c/strong\u003e strengths and weaknesses of \u003cstrong\u003eeach\u003c/strong\u003e \u003cstrong\u003emodel\u003c/strong\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eMake informed decisions based on \u003cstrong\u003eprecision\u003c/strong\u003e, \u003cstrong\u003erecall\u003c/strong\u003e, and \u003cstrong\u003eoverall accuracy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePinpoint\u003c/strong\u003e areas for potential model \u003cstrong\u003eimprovement\u003c/strong\u003e or \u003cstrong\u003efine-tuning\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to model evaluation, Model Foundry significantly enhances the pre-labeling workflow:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEfficient pre-labeling\u003c/strong\u003e: Once the best-performing model is identified, it can be seamlessly integrated into the pre-labeling pipeline, significantly reducing manual labeling efforts\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eQuality assurance\u003c/strong\u003e: By understanding model strengths and weaknesses, teams can strategically allocate human resources to review and correct pre-labels in categories where models struggle\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIterative improvement\u003c/strong\u003e: As more data is labeled and models are retained, teams can continuously evaluate and update their pre-labeling model, ensuring ongoing optimization of the labeling process.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost reduction\u003c/strong\u003e: By selecting the most accurate model for pre-labeling, teams can minimize the need for manual corrections, leading to substantial time and cost savings in large-scale labeling projects.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy leveraging Model Foundry for both decision-making and pre-labeling processes, teams can significantly enhance the efficiency and accuracy of their entire data labeling pipeline.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"next-steps\"\u003eNext Steps\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/2024/08/image-1.png 1000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo further improve model performance and decision-making, consider the following steps:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eFine-tune models on challenging categories.\u003c/li\u003e\u003cli\u003eConduct additional experiments with different data types or industry-specific datasets.\u003c/li\u003e\u003cli\u003eImplement regular evaluations and feedback loops to identify areas for improvement and adapt to changing requirements.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy continually refining your approach and leveraging the insights gained from Model Foundry, you can ensure that your team is always using the most effective LLM for your specific needs, driving efficiency and accuracy in your AI-powered workflows.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test, as implemented through Quantumworks Lab Model Foundry, proves to be an effective method for evaluating LLM performance on complex, nuanced tasks. By simulating real-world scenarios and leveraging Model’s advanced evaluation tools, teams can select the most suitable model for their specific pre-labeling needs.\u003c/p\u003e\u003cp\u003eIn our experiment, Gemini 1.5 Pro and Claude 3.5 Sonnet demonstrated superior performance, but the choice between them (or other models) would depend on the specific requirements of the project, including factors like speed, resource efficiency, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs the field of AI continues to evolve rapidly, tools like Quantumworks Lab Model Foundry become increasingly valuable, enabling teams to stay at the forefront of the space by consistently evaluating and selecting the best models for their unique challengers.\u003c/p\u003e","comment_id":"66b28fe6c2887d000107cdde","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-2.55.05-PM.png","featured":false,"visibility":"public","created_at":"2024-08-06T21:04:38.000+00:00","updated_at":"2024-09-03T20:04:14.000+00:00","published_at":"2024-08-06T22:08:47.000+00:00","custom_excerpt":"Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our \"Needle-in-a-Haystack\" experiment. Learn how to enhance accuracy and efficiency in complex data annotations.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"66b29da3c2887d000107ce2e","name":"Raj Jain","slug":"raj","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/raj/"},{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"66b29da3c2887d000107ce2e","name":"Raj Jain","slug":"raj","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/raj/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation/","excerpt":"Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our \"Needle-in-a-Haystack\" experiment. Learn how to enhance accuracy and efficiency in complex data annotations.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66b1410331089400019364ee","uuid":"2207c60e-9f86-4ef2-9613-168f8707aee5","title":"A comprehensive approach to evaluating text-to-video models","slug":"a-comprehensive-approach-to-evaluating-text-to-video-models","html":"\u003cp\u003eThe emergence of text-to-video AI models has marked a significant milestone in artificial intelligence, with models from Runway ML (Gen-3), Luma Labs, and Pika transforming written descriptions into dynamic and lifelike videos. This technology is reshaping industries from video production to digital marketing, democratizing visual storytelling.\u003c/p\u003e\u003cp\u003eHowever, despite their impressive capabilities, these models often fall short of human expectations, producing results that lack prompt adherence, realism, or fidelity to the input text. To accelerate the development of text-to-video models, it is crucial to establish comprehensive evaluation methodologies to pinpoint areas for improvement.\u003c/p\u003e\u003cp\u003eThis article presents a rigorous approach to assessing the strengths and limitations of Runway ML (Gen-3), Luma Labs, and Pika using human preference ratings. Let’s dive into how we systematically analyzed these leading models.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"human-preference-evaluation\"\u003e\u003cstrong\u003eHuman preference evaluation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate video outputs across several key criteria:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1245\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 2246w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eChecklist for evaluating text-to-video models using human preferences.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"prompt-adherence\"\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRaters assessed how well each generated video matched the given text prompt on a scale of high, medium, or low. For example, in the given prompt: “A peaceful Zen garden with carefully raked sand, bonsai trees, and a small koi pond.” Raters looked to see if there was prompt adherence by looking at the presence of key concepts for the prompt.\u003c/p\u003e\u003cul\u003e\u003cli\u003eIs there a garden?\u003c/li\u003e\u003cli\u003eDoes it look peaceful?\u003c/li\u003e\u003cli\u003eIs the sand present, and is it raked?\u003c/li\u003e\u003cli\u003eAre there bonsai trees?\u003c/li\u003e\u003cli\u003eIs the a small koi pond?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: If all or most of the key concepts are present.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e:\u0026nbsp; If half the key concepts are present.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: If less than half of key concepts are present .\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdENKR1QEaokNDikPODa-kTi209qknyV3PBGvEm5xa4QWqugwI5sx0zmlAZOIH2XrpQRjB9xuAAb4gshoWZfSl7mjgNmNDIWcL_bXxK4pqk0TnzP0-Xn7EG0LTznK4sBhXk1ZxuiG4p_WwoCXqx2d3dj5V_?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"509\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 1: \"A romantic Parisian street scene with couples walking and street musicians playing\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"video-realism\"\u003e\u003cstrong\u003eVideo realism\u003c/strong\u003e \u003c/h4\u003e\u003cp\u003eRaters assessed how closely the video resembled reality.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Realistic lighting, textures, and proportions.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Somewhat realistic but with slight issues in shadows or textures.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Animated or artificial appearance.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd4QhDzWX5l35W_ecjUSerUNd0sTSA_DtOLU2DjwPLenslTtGrRyf4tPjhEcwQdIxFw50JqfnPwk86brTWRtowMUFRHbMdG1hr-dqGZ69-nYFebJ9KVMslxPNvHQdjrBWPA6soRH5LZ3uUDZBggfO_tJSFl?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"504\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 2: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"video-resolution\"\u003e\u003cstrong\u003eVideo resolution\u003c/strong\u003e \u003c/h4\u003e\u003cp\u003eThis criterion evaluates the level of detail and overall clarity in the video.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Fine details visible (e.g., individual leaves, fabric textures).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Major elements are clear, but finer details are somewhat lacking.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Overall blurry or lacking significant detail.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdbrYYgTHolz9LOsLQzJHJVpzFx7TWbyIXrutMv52jbQl6nu_qHjrEV5kFXbCH01iYVLRTgoJV3BDjOIdZKU8TOod8uA_Ev-5QemuPmYTYiymsr2XN7nMs6F2AbWoVph_NnkZ54BRHe2Ldq8-IJK2J1ggNj?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"502\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 3: \"An ancient temple in the jungle with hidden traps and treasures waiting to be discovered\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"artifacts\"\u003e\u003cstrong\u003eArtifacts \u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRaters identified any visible artifacts, distortions, or errors, such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnnatural distortions in objects or backgrounds\u003c/li\u003e\u003cli\u003eMisplaced or floating elements\u003c/li\u003e\u003cli\u003eInconsistent lighting or shadows\u003c/li\u003e\u003cli\u003eUnnatural repeating patterns\u003c/li\u003e\u003cli\u003eUnnatural movements\u003c/li\u003e\u003cli\u003eBlurred or pixelated areas\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: If all or 5 of the errors are present.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: If 2 or 3 errors are present.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: \u0026nbsp;If 1 or 0 errors are present.\u0026nbsp;\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcOz4iP5QSqnQJpBB5F4neKuaS-jElkj40E21sIahJZmgm2qEE2oBxWOhDBTriFa26xOPwHbpQUGPjzUMFIfIjzWxl8I8vI8Z904UjiO-Jut14igYMNzk55VBU0U82is1tavV3FJA7uSU1kXtjCEuAmhB6I?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"502\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 4: \"Cat following a mouse\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"results\"\u003e\u003cstrong\u003eResults\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOur evaluation of 25 diverse set of complex prompts, generated by GPT-4, was stress-tested and provided valuable insights into the capabilities of Runway ML, Luma Labs and Pika. Each prompt was assessed by three different raters to ensure more accurate and diverse perspectives. This rigorous stress testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of the top models: Runway ML (Gen-3), Luma Labs, and Pika.\u003c/p\u003e\u003ch3 id=\"overall-ranking-for-human-preference-evaluations\"\u003e\u003cstrong\u003eOverall ranking for human-preference evaluations\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1\u003c/strong\u003e: Runway ML (Gen-3) ranked 1st in 65.22% of cases, Luma Labs in 18.84% and Pika in 15.94%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2\u003c/strong\u003e: Luma Labs ranked 2nd in 59.42% of cases, Runway ML (Gen-3) in 21.74% and Pika in 18.84%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3\u003c/strong\u003e: Pika ranked 3rd in 65.22% of cases, Luma Labs in 21.74% and\u0026nbsp; Runway ML in 13.04%\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1248\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRank 1 standings: Runway ML (Gen-3), Luma Labs, Pika\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"model-specific-performance-results\"\u003e\u003cstrong\u003eModel-Specific Performance results\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1248\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch4 id=\"runway-ml-gen-3\"\u003e\u003cstrong\u003eRunway ML (Gen-3)\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 59.42% of cases. This model excels at accurately reflecting the input prompts, making it a reliable choice for generating intended content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 17.39% of cases. Although it performs well relative to other models, it still has room for improvement in minimizing artifacts and errors.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 46.38% of cases. Runway ML generates realistic videos nearly half the time, indicating strong capabilities in producing lifelike content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 56.52% of cases. This model is proficient in delivering high-resolution videos, enhancing the viewing experience.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uyngtw0382\" title=\"RUNWAYML_____Gen-3 Alpha 2891026367, A grand fantasy cast Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"576\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRunway ML (Gen-3): \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"luma-labs\"\u003e\u003cstrong\u003eLuma Labs\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 37.68% of cases. While not as consistent as Runway ML, it still performs reasonably well in adhering to prompts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 17.39% of cases. Similar to Runway ML, it needs improvements to reduce visual defects.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 20.29% of cases. This model struggles more with realism, making it less suitable for applications requiring lifelike video content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 30.43% of cases. Luma Labs offers moderate video resolution quality but lags behind Runway ML.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/pyz5tqupa7\" title=\"LUMA_______A_grand_fantasy_castle_surrounded_by_lush_landscapes_and_mythical_creatures__85043b Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"530\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLuma Labs: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"pika\"\u003e\u003cstrong\u003ePika\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 36.23% of cases. Comparable to Luma Labs, Pika maintains a fair level of consistency with input prompts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 43.48% of cases. Pika has the highest occurrence of artifacts and errors, indicating significant areas for enhancement.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 20.29% of cases. Like Luma Labs, Pika also faces challenges in producing realistic videos.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 23.19% of cases. Pika offers the least in terms of video resolution among the three models, suggesting a need for improvement in this aspect.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/k1kvephgc9\" title=\"PIKA____A_grand_fantasy_castle_surrounded_by_lush_landscapes_and_mythical_creatures._seed3125151661828847 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePika: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIt's worth noting that the scope of this study was constrained by two key factors: \u003c/p\u003e\u003cul\u003e\u003cli\u003eThe absence of a public API for large-scale video generation from prompts; \u003c/li\u003e\u003cli\u003eOur deliberate use of a diverse prompt dataset. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis dataset encompassed a wide range of complexity, from simple to intricate descriptions. Additionally, we attempted to use automatic evaluations, such as assessing video quality based on all video frames and evaluations by large language models (LLMs) that support video. However, due to conflicting results, these methods were omitted from the blog post.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation of state-of-the-art text-to-video models reveals a clear preference hierarchy among Runway ML (Gen-3), Luma Labs, and Pika. Runway ML (Gen-3) emerges as the top performer, securing the first rank in 65.22% of cases, thanks to its high prompt adherence and superior video resolution. However, it still exhibits a notable occurrence of artifacts and errors, suggesting room for enhancement.\u003c/p\u003e\u003cp\u003eLuma Labs, while trailing behind Runway ML, demonstrates moderate performance, particularly in maintaining prompt consistency and video resolution. Its primary weakness lies in generating realistic videos, which is crucial for lifelike content applications. On the other hand, Pika, ranking third, shows the highest need for improvement, especially in minimizing artifacts and enhancing video resolution.\u003c/p\u003e\u003cp\u003eWhile each model has its strengths and weaknesses, Runway ML (Gen-3) stands out for its robust performance across most evaluation criteria, making it the preferred choice for generating high-quality, realistic videos. As the field of text-to-video generation continues to evolve, addressing the identified shortcomings will be key to advancing the capabilities of these models.\u003c/p\u003e\u003cp\u003eBy targeting these key areas, we can drive the next wave of innovations in text-to-video technology, creating more sophisticated and versatile text-to-video systems that cater to a broader range of applications and user needs.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-video models presented here represents a significant advance in assessing AI-generated videos. When combined with Quantumworks Lab's platform, AI teams can accelerate the development and refinement of sophisticated, domain-specific video generation models with greater efficiency and quality through our dataset curation, automated evaluation techniques, and human-in-the-loop QA.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-video model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more. \u003c/p\u003e\u003cp\u003eWe'd love to hear from you and discuss how we can support your AI evaluation needs.\u003c/p\u003e","comment_id":"66b1410331089400019364ee","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.13.57-AM.png","featured":false,"visibility":"public","created_at":"2024-08-05T21:15:47.000+00:00","updated_at":"2024-09-03T20:08:05.000+00:00","published_at":"2024-08-05T22:14:00.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/a-comprehensive-approach-to-evaluating-text-to-video-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66a978d931089400019364a5","uuid":"bbe7d436-1e06-4439-bce3-780f72151bdf","title":"A comprehensive approach to evaluating text-to-image models","slug":"a-comprehensive-approach-to-evaluating-text-to-image-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in image generation evaluation, visit\u0026nbsp;\u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAs text-to-image AI models continue to evolve, it's become increasingly important to develop robust evaluation methods that can assess their performance across multiple dimensions. In this post, we'll explore a comprehensive approach to evaluating 3 leading text-to-image models - \u003cem\u003eDALL-E 3, Stable Diffusion, and Imagen 2 \u003c/em\u003e- using both human preference ratings and automated evaluation techniques.\u003c/p\u003e\u003ch2 id=\"the-rise-of-text-to-image-models\"\u003eThe rise of text-to-image models\u003c/h2\u003e\u003cp\u003eText-to-image generation has seen remarkable progress in recent years. Models like \u003cem\u003eDALL-E 3, Stable Diffusion, and Imagen 2\u003c/em\u003e can now produce strikingly realistic and creative images from natural language descriptions. This technology has many useful applications, from graphic design and content creation to scientific visualization and beyond.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe5C5TuOA7TCKRT_7O_PQ5hnqXPPAlM4jOO4KvRk2DR0y0k9OWNYJzNhGIL5rqanTMlYK9reVzCb_pwx__rvW6rTmRkRljBX6aIjnA3DuEH1L_-ahgR0MnJU9JD-vWdQnDi8pZeoRIvpXD0kskzRf3WdSxH?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"723\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemOVe_M8fg0ddRqLYwN1MaKDcuJtPGMXgkP1hlaXtbjU7ZKsbunWQZMVM34ttGlsa8ulDjWoCxW-KVagWUiNG4xdUc3yCYdWMcEKsC1Pl8da6kyk0UgjjL66-qxaua9sYL4IUNTOSNsyggkztxBhXbIur6?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"733\"\u003e\u003c/figure\u003e\u003cp\u003eAs these models become more advanced, having reliable ways to compare their performance and identify areas for improvement are paramount. Let’s next dive into how we developed a two-fold evaluation approach for getting more granularity into their performance.\u003c/p\u003e\u003ch2 id=\"1-human-preference-evaluation\"\u003e[1] Human preference evaluation\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfTKedTbSWhlq5swtSUlaHwVE--84lNrAPDAGpko3p_AnEz0jooz4-lin5Mpg-SwHG3CJktHiRqSHTCJcuavjLCJOau4-GxczB2Odq4mQOXSaH7ijz1wCWZic_o0B_npX0x5qgxAqiHULmNouzCOv2l8nMy?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"902\"\u003e\u003c/figure\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human labelers per data row, allowing us to tap into a network of expert raters to evaluate image outputs across several key criteria:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAlignment with prompt: \u003c/strong\u003eRaters assessed how well each generated image matched the given text prompt on a scale of high, medium, or low. For example, for the prompt \"A red apple on a wooden table\":\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Image shows a clear, realistic red apple on a wooden table\u003c/li\u003e\u003cli\u003eMedium: Image shows an apple, but it's green or the table isn't clearly wooden\u003c/li\u003e\u003cli\u003eLow: Image shows an unrelated scene\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePhotorealism: \u003c/strong\u003eThis criterion evaluates how closely the image resembled a real photograph:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Realistic lighting, textures, and proportions\u003c/li\u003e\u003cli\u003eMedium: Somewhat realistic but with slight issues in shadows or textures\u003c/li\u003e\u003cli\u003eLow: Cartoonish or artificial appearance\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eDetail: \u003c/strong\u003eRaters then determined the level of detail and overall clarity:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Fine details visible (e.g., individual leaves, fabric textures)\u003c/li\u003e\u003cli\u003eMedium: Major elements clear, but finer details somewhat lacking\u003c/li\u003e\u003cli\u003eLow: Overall blurry or lacking significant detail\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eArtifacts: \u003c/strong\u003eFinally, raters identified any visible artifacts, distortions, or errors, such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnnatural distortions in objects or backgrounds\u003c/li\u003e\u003cli\u003eMisplaced or floating elements\u003c/li\u003e\u003cli\u003eInconsistent lighting or shadows \u003c/li\u003e\u003cli\u003eUnnatural repeating patterns\u003c/li\u003e\u003cli\u003eBlurred or pixelated areas\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"2-automated-evaluations\"\u003e[2] Automated evaluations\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXduxNvNrJswCQvT7DGZfSyEoIGUOLdlCVN1c3UTP_Bbz-s-sL-P246muageVr1aPVyH7DlaXtqkmGS6Lf9fm_MuztOdJAddLr2muQLG7MzHCC4BqHfBLlN9YAO7teoaN1Qma8wTq8kRv53xfh-V1iGRVMqO?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"907\"\u003e\u003c/figure\u003e\u003cp\u003eTo complement human ratings, we implemented several automated evaluation techniques. Here’s an example for one \u003ca href=\"https://storage.googleapis.com/text_image_eval/gen_images/014ac7aa527c953fd0a7aeb08e238dea/results.json?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eImage Quality Score: \u003c/strong\u003eWe calculated an objective image quality score based on several key metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSharpness: Using Laplacian variance to assess image clarity.\u003c/li\u003e\u003cli\u003eContrast: Evaluating the range between minimum and maximum pixel values.\u003c/li\u003e\u003cli\u003eNoise Estimation: Employing a filter-based approach to quantify image noise.\u003c/li\u003e\u003cli\u003eStructural Similarity Index (SSIM): Comparing the image to a slightly blurred version to assess structural integrity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese metrics were combined into a comprehensive quality score in order to provide an objective measure of the image's technical attributes.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePrompt adherence: \u003c/strong\u003eWe utilized the CLIP (Contrastive Language-Image Pre-training) model to measure similarity between the text prompt and the generated image in a shared embedding space. This approach provides an automated assessment of how well the image aligns with the given prompt, offering insights into the model's ability to accurately interpret and visualize textual descriptions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDetailed scoring: \u003c/strong\u003eWe employed Claude, an advanced AI model, to provide detailed scoring and analysis of the generated images. This multifaceted evaluation includes several key components.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cu\u003eElement accuracy scores\u003c/u\u003e: For each key element in the prompt, Claude assesses its presence, provides a description, and assigns an accuracy score out of 10 reflecting how well these elements matched the prompt.\u003c/li\u003e\u003cli\u003e\u003cu\u003eCategory scores\u003c/u\u003e: Claude evaluates images across various categories such as objects, colors, spatial relations, activities, and materials. Each category receives a score out of 10, providing a comprehensive view of the image's content accuracy.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eClaude prompt adherence\u003c/u\u003e: Claude assigns an overall similarity score, expressed as a percentage, indicating how closely the entire image matches the given prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eUnexpected elements and inconsistencies\u003c/u\u003e: Claude identifies any unexpected elements or inconsistencies in the image that do not align with the intended prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eOverall impression:\u003c/u\u003e Claude provides an overall impression of the image, summarizing how well it captures the essence of the prompt. This includes a qualitative assessment of the image's strengths and areas for improvement.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"results\"\u003eResults\u0026nbsp;\u003c/h2\u003e\u003cp\u003eOur evaluation of 100 images across a diverse set of complex prompts, generated by GPT-4, stress-tested and provided valuable insights into the capabilities of Stable Diffusion, DALL-E, and Imagen 2.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHere’s a summary of the human-preference evaluations:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eModel rankings and initial findings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eStable Diffusion ranked first in 50.7% of cases, DALL-E in 38%, and Imagen 2 in 11.3%.\u003c/li\u003e\u003cli\u003eStable Diffusion ranked second (37%), followed by DALL-E (33%) and Imagen 2 (30%)\u003c/li\u003e\u003cli\u003eImagen 2 was ranked third (59%), while DALL-E and Stable Diffusion were ranked last less often (29% and 12% respectively\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ePerformance metrics (as percentages of maximum possible scores):\u003c/p\u003e\u003cul\u003e\u003cli\u003eStable Diffusion: 84.3% prompt alignment, 85.3% photorealism, 91.7% detail/clarity.\u003c/li\u003e\u003cli\u003eDALL-E: 84.3% prompt alignment, 58.3% photorealism, 83.7% detail/clarity.\u003c/li\u003e\u003cli\u003eImagen 2: 61.3% prompt alignment, 74.7% photorealism, 71.3% detail/clarity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eHere’s a summary of the detailed auto evaluation metrics:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eStable Diffusion emerged as a consistent performer across various metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003eClaude’s prompt adherence score: 82.27%. More specifically, attributed to accurate/realistic colors (89.40%) and depicting objects (89.30%)\u003c/li\u003e\u003cli\u003eHowever, image quality (34.98%) was relatively low\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDALL-E excelled in prompt interpretation and visualization:\u003c/p\u003e\u003cul\u003e\u003cli\u003eClaude’s prompt adherence score: 87.04%. More specifically, attributed to depicting objects well (91.60%) and displaying moving activities accurately (81.10%)\u003c/li\u003e\u003cli\u003eStrongest in translating textual descriptions into visual elements\u003c/li\u003e\u003cli\u003eHowever, image quality (30.11%) was lowest\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eImagen 2 performed the worst, but had a higher technical quality for images:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLowest Claude prompt adherence score (76.86%) and aspect accuracy (70.92%)\u003c/li\u003e\u003cli\u003eMuch weaker in moving activities (72.20%) and detailed attributes (77.70%)\u003c/li\u003e\u003cli\u003eHigher image quality (55.55%) than the other two models\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eAnalysis and insights:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"831\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eComparing the auto evaluation metrics to the human preference evaluations reveals some additional interesting findings:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStable Diffusion's balanced performance:\u003c/strong\u003e Stable Diffusion emerged as the top performer overall in human evaluations, ranking first in 50.7% of cases. It showed consistent high scores across human-evaluated metrics, particularly excelling in detail/clarity (91.7%) and photorealism (85.3%). However, the auto evaluation revealed a relatively low image quality score (34.98%), suggesting that technical image quality doesn't always correlate with human perception of quality.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDALL-E's strengths and weaknesses:\u003c/strong\u003e While DALL-E ranked first in 38% of human evaluations, it showed a significant weakness in human-perceived photorealism (58.3%). Interestingly, it had the highest Claude prompt adherence score (87.04%) in the auto evaluation, which aligns with its strong performance in human-evaluated prompt alignment (84.3%). This suggests DALL-E excels at interpreting and executing prompts, but may struggle with realistic rendering.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImagen 2's technical quality vs. human preference:\u003c/strong\u003e Imagen 2 consistently ranked lower in human preferences, struggling particularly with prompt alignment (61.3%). However, it had the highest technical image quality score (55.55%) in the auto evaluation. This discrepancy highlights that technical image quality doesn't necessarily translate to human preference or perceived prompt adherence.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrompt alignment discrepancies\u003c/strong\u003e: While human evaluations showed Stable Diffusion and DALL-E tied in prompt alignment (84.3% each), the auto evaluation gave DALL-E a higher score (87.04%) compared to Stable Diffusion (82.27%). This suggests that human and AI perceptions of prompt adherence may differ slightly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePhotorealism and image quality\u003c/strong\u003e: The human-evaluated photorealism scores don't align with the auto-evaluated image quality scores. Stable Diffusion led in human-perceived photorealism (85.3%) but had low technical image quality (34.98%). Conversely, Imagen 2 had the highest technical image quality (55.55%) but ranked second in human-perceived photorealism (74.7%).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDetail and clarity vs. technical metrics\u003c/strong\u003e: Stable Diffusion stood out in human-evaluated detail and clarity (91.7%), which aligns with its high auto-evaluated scores in depicting objects (89.30%) and accurate colors (89.40%). This suggests a correlation between these technical aspects and human perception of detail and clarity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIt's also important to note that this study didn't include Midjourney due to its Discord-only integration, which made it challenging to implement into our evaluation study. While Midjourney is recognized for its high-quality output, its unconventional access method can be a barrier for users seeking traditional API or web-based interactions. Additionally, Google’s Imagen 2 implements strict safety and content filters across a wide range of topics, which did limit versatility and required additional pre-processing. Such factors, alongside the technical and human-based perceptual metrics evaluated in our study, also influence the overall usability and adoption of AI image generation models in real-world scenarios.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eOur comprehensive evaluation of leading text-to-image models, combining human preference ratings with automated metrics, reveals intriguing contrasts between quantitative performance and human perception. Stable Diffusion emerged as the overall top performer in human evaluations, excelling in detail/clarity and photorealism despite a lower technical image quality score. This underscores the complex relationship between technical metrics and human perception of quality. DALL-E demonstrated strength in prompt interpretation and adherence across both human and automated evaluations, although it showed weakness in human-perceived photorealism. Imagen 2, while scoring highest in technical image quality, consistently ranked lower in human preferences, particularly struggling with prompt alignment.\u003c/p\u003e\u003cp\u003eAs these technologies continue to evolve, our results indicate that each model has distinct strengths and areas for improvement. Stable Diffusion offers balanced performance across various criteria, making it suitable for a wide range of applications. DALL-E excels in prompt interpretation and execution, making it ideal for tasks requiring precise visualization of detailed descriptions. Imagen 2's high technical quality suggests it could be particularly useful in applications where image fidelity is prioritized, although improvements in prompt adherence would enhance its overall performance. Future research should focus on bridging the gap between technical metrics and human perception, as well as addressing specific weaknesses identified in each model, such as DALL-E’s photorealism or Imagen 2’s prompt alignment. By refining these aspects, we can push the boundaries of AI-generated imagery and develop more versatile and capable text-to-image systems that better meet the needs of various applications and user preferences.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003eGet started today\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-image models presented here represents a significant advance in assessing AI-generated images. When combined with Quantumworks Lab's platform, AI teams can accelerate the development and refinement of sophisticated, domain-specific image generation models with greater efficiency and quality through our dataset curation, automated evaluation techniques, and human-in-the-loop QA.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-image model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more. We'd love to hear from you and discuss how we can support your AI evaluation needs.\u003c/p\u003e","comment_id":"66a978d931089400019364a5","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-4.35.13-PM.png","featured":false,"visibility":"public","created_at":"2024-07-30T23:35:53.000+00:00","updated_at":"2024-11-26T00:11:57.000+00:00","published_at":"2024-07-31T00:02:09.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"66a9822f31089400019364c9","name":"Krish Maniar","slug":"krish","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/krish/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"66a9822f31089400019364c9","name":"Krish Maniar","slug":"krish","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/krish/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/a-comprehensive-approach-to-evaluating-text-to-image-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"668c2f59878a56000112a4e0","uuid":"93014cdf-3555-4d2b-b480-2b9db686e2db","title":"Using multimodal chat to enhance a customer’s online support experience","slug":"using-multimodal-chat-to-enhance-a-customers-online-support-experience","html":"\u003cp\u003eCustomer service systems are prime targets for multimodal chat as this new technology leverages an intuitive interface for answering user questions\u0026nbsp; while enabling companies to unlock previously hidden insights about customer journeys and sentiment.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can streamline the process of labeling and managing data for various use cases, including intent classification, dialogue retrieval, state tracking, and model evaluation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. We’ll illustrate the various labeling tasks that Quantumworks Lab can accomplish to ensure the chatbot is intelligent and effective, providing customers with relevant and timely answers.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://files.readme.io/ce6e056-Labelbox_2024-06-04_10-37-061.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"678\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eLabelbox offers a \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/docs/live-multimodal-chat-evaluation?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003enumber of editor types\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, including the live multimodal chat editor.\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"labeling-tasks\"\u003e\u003cstrong\u003eLabeling Tasks\u003c/strong\u003e\u003c/h1\u003e\u003ch2 id=\"multimodal-intent-classification-dialogue-retrieval-and-state-tracking\"\u003e\u003cstrong\u003eMultimodal Intent Classification, Dialogue Retrieval and State Tracking\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xs53lmro56\" title=\"Customer Query Resolution Training - Part 1 - Training Data Collection + Using Classifications Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 1: Generate prompt-response pairs for supervised fine-tuning of agentic workflows.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eLabel the customer’s intent and sentiments in a given turn of the chat, so you can identify the appropriate next steps accordingly.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1427\" height=\"1333\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/first-one.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/first-one.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png 1427w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe customer appears frustrated, so the sentiment is labeled as “frustrated.”\u003c/p\u003e\u003cp\u003eFurthermore, the customer appears to be reporting their order was damaged and is requesting a new one, so both of those requests are labeled using checklist classifications.\u003c/p\u003e\u003cp\u003eWith these labels, you know the customer’s intent (get a new piece) and can also record the current state of the conversation (damage reported but no action taken yet). As the conversation proceeds, you can update the state based on what actions are taken and keep checking the intent and tone of the customer to ensure they are satisfied.\u003c/p\u003e\u003cp\u003eLabelbox allows you to label every aspect of every turn-by-turn conversation, enabling you to develop fine-grained data sets of annotated real-world conversations.\u003c/p\u003e\u003cp\u003eYou can use these labels to train your agent to retrieve similar past interactions and follow similar response patterns or simply be inspired by them.\u003c/p\u003e\u003ch2 id=\"model-evaluation-ranking-and-selection\"\u003e\u003cstrong\u003eModel evaluation, ranking and selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eYou trained a few models offline with the previously generated data using supervised fine-tuning or RLHF. How do you know which of the models is the best performing, and how often do they make mistakes?\u003c/p\u003e\u003cp\u003eLabelbox allows you to connect these models hosted behind your custom endpoints. Then, with the Live Multimodal Chat editor, labelers can interact with all of them simultaneously by feeding prompts, comparing and contrasting responses, and even rewriting the best ones to be even better.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1vpskixih4\" title=\"Customer Query Resolution Training - Part 2 - LLM Development \u0026amp; Evaluation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 2: Evaluate a single model, generate multiple responses and write your own gold standard response.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eWhen evaluating a single model, Quantumworks Lab allows you to generate multiple responses and also add your own response. This is very useful for use cases where all of the model’s responses are lacking in quality and a human-written response provides the gold standard for further downstream training. Furthermore, in the course of the labeling conversation, you can choose one of these responses to be passed in as prior context into later turns, allowing you to choose the path of the conversation in real-time for better training data generation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uzesocscxk\" title=\"Customer Query Resolution Training - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 3: Evaluate multiple models at once in “chat arena” style.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/secone-one-1-1-1-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"450\" height=\"640\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox also supports evaluating multiple models in a “chat arena” style setup.\u003c/p\u003e\u003cp\u003eSay you have trained candidate models and now want to compare them against each other and also foundation models like GPT 4o. Connect your own models with Quantumworks Lab as \u003cem\u003ecustom\u003c/em\u003e models, and easily choose any latest and greatest foundation model from Foundry within Labelbox. The chat arena excludes information about which response was generated from which model so as not to bias the labeling process.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWith Quantumworks Lab's multimodal chat functionalities, you can generate high-quality datasets to train, evaluate, and test your custom LLMs or pit off-the-shelf LLMs like GPT4o and Gemini 1.5 against each other in a Chatbot Arena-style competition to see which one suits your needs best.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/b3f423vxob\" title=\"Customer Query Resolution Training - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 4: Survey of all LLM capabilities in Quantumworks Lab\u003c/em\u003e\u003c/p\u003e\u003cp\u003eAs the landscape of LLMs evolves rapidly, Quantumworks Lab will continue to build functionality to ensure our customers can align models to their use cases quickly and reliably. With differentiated data, streamlined operations, and transparent orchestration, you can build and refine foundation models that meet your specific needs.\u003c/p\u003e\u003ch2 id=\"evaluate-multimodal-chat-data-today-for-free\"\u003eEvaluate multimodal chat data today for free\u003c/h2\u003e\u003cp\u003eReady to deliver on the next-generation of generative AI?\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eSign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more and we’d love to hear from you.\u003c/p\u003e","comment_id":"668c2f59878a56000112a4e0","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/mmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp","featured":false,"visibility":"public","created_at":"2024-07-08T18:26:33.000+00:00","updated_at":"2024-07-10T16:09:14.000+00:00","published_at":"2024-07-09T16:06:15.000+00:00","custom_excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-multimodal-chat-to-enhance-a-customers-online-support-experience/","excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6656135f9132db0001f20551","uuid":"f173905b-1c7a-4f87-8064-ac8af0ccdaa7","title":"AI foundations: Understanding embeddings","slug":"ai-foundations-understanding-embeddings","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eEmbeddings transform complex data into meaningful vector representations, enabling powerful applications across various domains.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis post covers the theoretical foundations, practical examples, and demonstrates how embeddings enhance key use cases at Labelbox.\u003c/p\u003e\u003cp\u003eWhether you’re new to the concept or looking to deepen your understanding, this guide will provide valuable insights into the world of embeddings and their practical uses.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"what-is-an-embedding\"\u003eWhat is an embedding?\u003c/h1\u003e\u003cp\u003eAt its core, an embedding is a vector representation of information. This information can be of any modality—video, audio, text, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe process of generating embeddings involves a deep learning model trained on specific data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach index in a vector represents a numerical value, often a floating-point value between 0 and 1.\u003c/p\u003e\u003ch2 id=\"the-relationship-between-dimensions-and-detail\"\u003eThe relationship between dimensions and detail\u003c/h2\u003e\u003cp\u003eThe dimensions of a vector describe the level of detail it can capture. Higher dimensionality means higher detail and accuracy in the similarity score.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, a vector with only two dimensions is unlikely to store all relevant information, leading to simpler but less accurate representations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1362\" height=\"626\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1362w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"vector-representations-of-images\"\u003eVector representations of images\u003c/h2\u003e\u003cp\u003eFor instance, consider an image. The underlying compression algorithm converts this image into a vector representation, preserving all relevant information. These embeddings can then be used to determine the similarity between different pieces of data. For example, images of a cat and a dog would have different embeddings, resulting in a low similarity score.\u003c/p\u003e\u003cp\u003eAnother example: Let's consider a model trained on images of helicopters, planes, and humans. If we input an image of a helicopter, the model generates a vector representation reflecting this. If the image contains both a helicopter and humans, the representation changes accordingly. This process helps in understanding how embeddings are generated and used.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1240\" height=\"618\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1240w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Helicopter or not helicopter?\" captured via embeddings.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"importance-and-applications-of-embeddings\"\u003eImportance and applications of embeddings\u003c/h1\u003e\u003ch2 id=\"why-embeddings-matter\"\u003eWhy embeddings matter\u0026nbsp;\u003c/h2\u003e\u003cp\u003eAs we’ve described in the prior sections, understanding what embeddings are and how they work is crucial for several reasons:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCapturing Semantic Relationships\u003c/strong\u003e: Embeddings help capture complex relationships within data, enabling more accurate predictions and recommendations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Search Capabilities\u003c/strong\u003e: Embeddings facilitate efficient and accurate similarity searches, essential in applications like search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"applications-of-embeddings\"\u003eApplications of embeddings\u003c/h2\u003e\u003cp\u003eEmbeddings are extremely useful in search problems, especially within recommender systems.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHere are a few examples:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImage Similarity\u003c/strong\u003e: Finding images similar to an input image, like searching for all images of dogs based on a sample image.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eText-to-Image Search\u003c/strong\u003e: Using a textual query, such as \"image of a dog,\" to find relevant images.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContent Recommendations\u003c/strong\u003e: Identifying articles or movies similar to ones you've enjoyed, enhancing search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1296\" height=\"874\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1296w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEmbeddings are the essential ingredient to a smooth and enjoyable family movie night via recommendations systems, which can suggest movies similar to ones you've enjoyed in the past.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"how-embeddings-are-generated\"\u003eHow embeddings are generated\u003c/h1\u003e\u003cp\u003eDeep learning models, trained on either generalized or specific datasets, learn patterns from data to generate embeddings. Models trained on specific datasets, like cat images, are good at identifying different breeds of cats but not dogs. Generalized models, trained on diverse data, can identify various entities.\u003c/p\u003e\u003cp\u003eModern models produce embeddings of over 1024 dimensions, increasing accuracy and detail. However, this also raises challenges related to the cost of embedding generation, storage, and computational resources.\u003c/p\u003e\u003cp\u003eSome of the earliest work with creating embedding models included word2vec for NLP and convolutional neural networks for computer vision.\u003c/p\u003e\u003ch2 id=\"the-significance-of-word2vec\"\u003eThe significance of word2vec\u003c/h2\u003e\u003cp\u003eThe introduction of word2vec by Mikolov et al. in 2013 marked a significant milestone in creating word embeddings. Word2vec enabled the capture of semantic relationships between words, such as the famous king-queen and man-woman analogy, demonstrating how vectors can represent complex relationships.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis breakthrough revolutionized natural language processing (NLP), enhancing tasks like machine translation, sentiment analysis, and information retrieval.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWord2vec laid the foundation for subsequent embedding techniques and deep learning models in NLP.\u003c/p\u003e\u003ch2 id=\"equivalent-of-word2vec-for-images\"\u003eEquivalent of word2vec for Images\u003c/h2\u003e\u003cp\u003eFor images, convolutional neural networks (CNNs) serve as the equivalent of word2vec. Models like AlexNet, VGG, and ResNet revolutionized image processing by creating effective image embeddings.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese models convert images into high-dimensional vectors, preserving spatial hierarchies and semantic information.\u0026nbsp;\u003c/p\u003e\u003cp\u003eJust as word2vec transformed text data into meaningful vectors, CNNs transform images into embeddings that capture essential features, enabling tasks like object detection, image classification, and visual similarity search.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k_icon.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"250\" height=\"250\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eImageNet is a large-scale visual database designed for use in visual object recognition research, containing millions of labeled images across thousands of categories. Models like AlexNet, VGG, and ResNet demonstrated the power of CNNs using datasets like ImageNet.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"uploading-custom-embeddings-to-labelbox\"\u003eUploading custom embeddings to Quantumworks Lab\u0026nbsp;\u0026nbsp;\u003c/h2\u003e\u003cp\u003eLabelbox now supports importing and exporting custom embeddings seamlessly, allowing for better integration into workflows. This new feature provides flexibility in how embeddings are utilized, making it easier to leverage embeddings for various applications.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eimport Quantumworks Lab as lb\nimport transformers\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nimport requests\nimport numpy as np\n\n# Add your API key\nAPI_KEY = \"your_api_key_here\"\nclient = lb.Client(API_KEY)\n\n# Get images from a Quantumworks Lab dataset\nDATASET_ID = \"your_dataset_id_here\"\ndataset = client.get_dataset(DATASET_ID)\nexport_task = dataset.export_v2()\nexport_task.wait_till_done()\n\ndata_row_urls = [dr_url['data_row']['row_data'] for dr_url in export_task.result]\n\n# Get ResNet-50 from HuggingFace\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = transformers.ResNetModel.from_pretrained(\"microsoft/resnet-50\")\n\nimg_emb = []\n\nfor url in data_row_urls:\n    response = requests.get(url, stream=True)\n    image = Image.open(response.raw).convert('RGB').resize((224, 224))\n    img_hf = image_processor(image, return_tensors=\"pt\")\n    with torch.no_grad():\n        last_layer = model(**img_hf, output_hidden_states=True).last_hidden_state\n        resnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\n        resnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\n        img_emb.append(resnet_embeddings.cpu().numpy())\n\ndata_rows = []\n\nfor url, embedding in zip(data_row_urls, img_emb):\n    data_rows.append({\n        \"row_data\": url,\n        \"embeddings\": [{\"embedding_id\": new_custom_embedding_id, \"vector\": embedding[0].tolist()}]\n    })\n\ndataset = client.create_dataset(name='image_custom_embedding_resnet', iam_integration=None)\ntask = dataset.create_data_rows(data_rows)\nprint(task.errors)\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eTo compute your ow embeddings and send them to Quantumworks Lab, you can use models from Hugging Face. Here’s a brief example using ResNet-50:\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003cscript src=\"https://gist.github.com/mmbazel-labelbox/edb8efbab1904106009e1ed630199e29.js\"\u003e\u003c/script\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eThis code snippet demonstrates how to create and upload custom embeddings, which can then be used for similarity searches within Labelbox.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1694\" height=\"1176\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1694w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"why-you-should-care-about-custom-embeddings\"\u003eWhy you should care about custom embeddings\u003c/h3\u003e\u003cp\u003eCustom embeddings provide several advantages:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTailored Representations\u003c/strong\u003e: Custom embeddings can be tailored to your specific dataset, improving accuracy in domain-specific tasks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Performance\u003c/strong\u003e: They allow for more precise similarity searches and recommendations by capturing nuances specific to your data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFlexibility\u003c/strong\u003e: Custom embeddings offer flexibility in handling various types of data and use cases, making them a versatile tool in machine learning workflows.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"using-embeddings-for-better-search\"\u003eUsing embeddings for better search\u003c/h1\u003e\u003cp\u003eEarlier we mentioned that search (a core activity of search engines, recommendation systems, data curation, etc) is a common and important application of embeddings. How? By comparing how similar two (or many more) items are to each other.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"measuring-similarity\"\u003eMeasuring similarity\u003c/h2\u003e\u003cp\u003eThe most popular algorithm for measuring similarity between two vectors is cosine similarity. It calculates the angle between two vectors: the smaller the angle, the more similar they are. Cosine similarity scores range from 0 to 1, or -1 to 1 for dissimilarity.\u003c/p\u003e\u003cp\u003eFor example, if vectors W and V are close, their cosine similarity might be 0.79. If they are opposite, the similarity is lower, indicating dissimilarity. This method helps in visually and computationally understanding the similarity between different assets.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J0ha8suIAyD2qrFVEWI9TqDRLb2c8ForgqTmyvKQ8ZKsXLeHf5H96V0tQSMV_bCbqBsLWcBb3ljxSP8vKYDKElnlTv1zHq36uQG7mwzyP9HELFlHgGf7FTZO8AWH_ndg5OAXm3yLmbbEpVPg1Cvp108\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"718\" height=\"521\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOriginal source: \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Similarity-between-vectors-can-be-estimated-by-calculating-the-cosine-of-the-angle-th_fig1_333734049?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Similarity between vectors can be estimated by calculating the cosine of the angle θ between them.\"\u003c/span\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBecause embeddings are essentially vectors, we can apply many of the same operations used to analyze vectors to embeddings.\u003c/p\u003e\u003ch3 id=\"strategies-for-similarity-computation\"\u003eStrategies for similarity computation\u003c/h3\u003e\u003cp\u003eAdditional strategies for performing similarity computation include:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eBrute Force Search\u003c/strong\u003e: Comparing one asset against every other asset. This provides the highest accuracy but requires significant computational resources and time. For instance, running brute force searches on a dataset with millions of entries can lead to significant delays and high computational costs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eApproximate Nearest Neighbors (ANN)\u003c/strong\u003e: Comparing one asset against a subset of assets. This method reduces computational resources and latency while maintaining high accuracy. ANN algorithms like locality-sensitive hashing (LSH) and KD-trees provide faster search times, making them suitable for real-time applications, although they may sacrifice some accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHashing Methods\u003c/strong\u003e: Techniques like MinHash and SimHash provide efficient similarity searches by hashing data into compact representations, allowing quick comparison of hash values to estimate similarity. These methods are particularly useful for text and document similarity.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTree-based methods (KD-trees, R-trees, and VP-trees) and graph-based methods (like Hierarchical Navigable Small World, also known as HNSW) are also options.\u003c/p\u003e\u003ch2 id=\"leveraging-embeddings-for-data-curation-management-and-analysis-in-labelbox\"\u003eLeveraging embeddings for data curation, management, and analysis in Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThrough search, embeddings improve the efficiency and accuracy of data curation, management, and analysis in several ways:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eAutomated Data Organization\u003c/strong\u003e: Embeddings help group similar data points together, making it easier to organize and manage large datasets.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Search and Retrieval\u003c/strong\u003e: By transforming data into embeddings, search and retrieval operations become faster and more accurate, enabling quick access to relevant information.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Data Analysis\u003c/strong\u003e: Embeddings capture underlying patterns and relationships within data, facilitating more effective analysis and insights extraction.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAnd these are exactly the ways we use embeddings in our products at Quantumworks Lab to \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimprove search for data\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, in \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e, users can find images similar to an input image, such as searching for basketball players on a court. Our natural language search allows users to input a textual query, like \"dog,\" and find relevant images.\u003c/p\u003e\u003cp\u003eLabelbox offers several features to streamline embedding workflows, including:\u003c/p\u003e\u003ch3 id=\"data-curation-and-management\"\u003eData curation and management\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/custom-embeddings?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCustom Embeddings\u003c/u\u003e\u003c/a\u003e in Catalog: Surfaces high-impact data, with automatic computation for various data types.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/manage-selections?ref=labelbox-guides.ghost.io#smart-select\"\u003e\u003cu\u003eSmart Select\u003c/u\u003e\u003c/a\u003e in Catalog: Curates data using random, ordered, and cluster-based methods.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/cluster-view?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCluster View\u003c/u\u003e\u003c/a\u003e: Discovers similar data rows and identifies labeling or model mistakes.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1620\" height=\"838\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1620w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Cluster view to explore relationships between data rows, identify edge cases, and select rows for pre-labeling or human review.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eNatural Language\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSimilarity Search\u003c/u\u003e\u003c/a\u003e: Supports video and audio assets.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-metrics?ref=labelbox-guides.ghost.io#automatic-metrics\"\u003e\u003cu\u003ePrediction-level Custom Metrics\u003c/u\u003e\u003c/a\u003e: Filters and sorts by custom metrics, enhancing model error analysis.\u003c/li\u003e\u003cli\u003ePre-label Generation from \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFoundry\u003c/u\u003e\u003c/a\u003e Models: Streamlines project workflows.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/video-annotations?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnhanced Video Player\u003c/u\u003e\u003c/a\u003e: Aids in curating, evaluating, and visualizing video data.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1630\" height=\"904\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1630w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWould a rose smell as sweet if it was merely one of a thousand easily discovered in a flower dataset using a natural language search? Trick question: you can't smell photos. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"embeddings-schema\"\u003eEmbeddings schema\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/working-with-features?ref=labelbox-guides.ghost.io#create-features-from-the-schema-tab\"\u003e\u003cu\u003eSchema Tab\u003c/u\u003e\u003c/a\u003e: Includes an Embeddings subtab for viewing and creating custom embeddings.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"api-and-inferencing\"\u003eAPI and inferencing\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/run-foundry-apps?ref=labelbox-guides.ghost.io#run-app-using-rest-api\"\u003e\u003cu\u003eInferencing Endpoints\u003c/u\u003e\u003c/a\u003e: Generate predictions via REST API without creating data rows or datasets.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"additional-workflow-enhancements\"\u003eAdditional workflow enhancements\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBoost Express: Request additional labelers for data projects.\u003c/li\u003e\u003cli\u003eExport v2 Workflows and Improved SDK: Streamlines data management.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese features ensure that embeddings are utilized effectively, making your workflows more efficient and your models more accurate.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eEmbeddings are a powerful tool in machine learning, enabling efficient and accurate similarity searches and recommendations. We hope this post has clarified what embeddings are and how they can be utilized.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor more on uploading custom embeddings, check out our \u003ca href=\"https://docs.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/page/tutorials?ref=labelbox-guides.ghost.io\"\u003edeveloper guides\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAdditional resources on embeddings can be found here:\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jbwzrxh814\" title=\"Embeddings Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWatch the video: \u003ca href=\"https://labelbox.wistia.com/medias/jbwzrxh814?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eUnderstanding embeddings in machine learning\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eLearn more in our community about:\u003cu\u003e \u003c/u\u003e\u003ca href=\"https://community.labelbox.com/t/how-to-upload-custom-embedding-s-and-why-you-should-care/1169?ref=labelbox-guides.ghost.io#why-would-i-need-embedding-2\"\u003e\u003cu\u003eHow to upload custom embeddings and why you should care\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThanks to our contributors Tomislav Peharda, Paul Tancre, and Mikiko Bazeley! \u003c/p\u003e","comment_id":"6656135f9132db0001f20551","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/05/YT-Thumbnails--23-.jpg","featured":false,"visibility":"public","created_at":"2024-05-28T17:24:47.000+00:00","updated_at":"2024-09-04T18:02:27.000+00:00","published_at":"2024-05-28T20:32:29.000+00:00","custom_excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},"url":"https://labelbox-guides.ghost.io/ai-foundations-understanding-embeddings/","excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"662804d1d733f0000145b1c4","uuid":"3db0aafe-d6b7-470a-8ff0-37835d066fe4","title":"How to harness AI for efficient video labeling","slug":"harnessing-ai-for-efficient-video-labeling","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eWorking in collaboration with numerous leading companies in artificial intelligence, we're observing a surge in enthusiasm for using advanced models to initially label data, integrating human expertise later to refine and tailor previously labor-intensive and time-consuming tasks.\u003c/p\u003e\u003cp\u003eThese AI models are transforming one of the most daunting tasks in machine learning—the creation of high-quality video datasets. Utilizing such models allows machine learning teams to leverage automated tools to pre-label or enrich data, facilitating a range of applications from monitoring driver behavior to detecting objects in manufacturing environments.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis blog post will explore how models like Gemini 1.5 Pro, Grounding DINO, and SAM are redefining the video labeling landscape, while boosting efficiency and speed. \u003c/p\u003e\u003cp\u003eBy automating the labor-intensive labeling tasks, these models not only accelerate the workflow, but also liberate time for users and decrease labeling costs.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"steps\"\u003eSteps\u003c/h2\u003e\u003ch3 id=\"step-1-select-video\"\u003eStep 1: Select video\u0026nbsp;\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eTo select a video: \u003c/p\u003e\u003cul\u003e\u003cli\u003eNarrow in on a subset of data. Users can use Quantumworks Lab Catalog filters, including media attributes, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eUsers can click “Predict with Foundry” once the data of interest is selected.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-choose-a-model-of-interest\"\u003eStep 2: Choose a model of interest\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eTo select a model: \u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, users will be prompted to choose a model of interest for a model run.\u003c/li\u003e\u003cli\u003eThen, select a model from the ‘model gallery’ based on the type of task - such as video classification, video object detection, and video segmentation.\u003c/li\u003e\u003cli\u003eTo locate a specific model, users can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine-learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-configure-model-settings-and-submit-a-model-run\"\u003eStep 3: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eOnce the model of interest is selected, users can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of settings, which can be found in the Advanced model setting.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can generate preview predictions on up to five data rows to understand how the current model settings affect the final predictions.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows users to confirm the configuration settings confidently:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf users are unhappy with the generated preview predictions, they can edit the model settings and continue to generate preview predictions until they're satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eAfter users are satisfied with the preview predictions, a model run can be submitted.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-send-the-images-to-annotate\"\u003eStep 4: Send the images to Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eUsers can transfer the results to a labeling project using the UI via the \"Send to Annotate\" feature. Labelers can then quickly review labels for accuracy.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"pre-labeling-use-cases\"\u003ePre-labeling Use Cases\u003c/h2\u003e\u003ch3 id=\"example-1-segmentation-mask-using-grounding-dino-sam\"\u003eExample 1: Segmentation mask using Grounding DINO + SAM\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eSegmentation masks\u003c/a\u003e are used for autonomous vehicles, medical imagery, retail applications, face recognition and analysis, video surveillance, satellite image analysis, etc. Masks are some of the most time-consuming annotations to make for video. Below, we see an example of how this can be automated with Grounding DINO + SAM so the reviewers can make small edits if needed instead of starting from scratch.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7k184d0h65\" title=\"Pre-labeling Use Case: Segmentation mask using Grounding DINO + SAM Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"example-2-bounding-box-using-grounding-dino\"\u003eExample 2: Bounding box using Grounding DINO\u003c/h3\u003e\u003cp\u003eBounding boxes are utilized in similar scenarios as segmentation masks, but these scenarios demand less precision than those requiring pixel-level (masks) detail. Bounding boxes can be automated using Grounding DINO, as illustrated below with detection of a person in video.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ie9s2r3zff\" title=\"Pre-labeling Use Case: Bounding Box using Grounding DINO Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"example-3-global-classification-using-gemini-15-pro\"\u003eExample 3: Global classification using Gemini 1.5 Pro\u003c/h3\u003e\u003cp\u003eGlobal classification for video is used when the overall classification for video is required like when a driver safety system needs to detect if a driver is distracted. Gemini 1.5 Pro can analyze an hour long video and provide answers about events that took place in the video. This automation reduces the need for human intervention, allowing personnel to focus on reviewing videos only when they are flagged with specific classifications. \u003c/p\u003e\u003ch3 id=\"example-4-frame-based-classification-using-gemini-15-pro\"\u003eExample 4: Frame based classification using Gemini 1.5 Pro\u003c/h3\u003e\u003cp\u003eFrame-based classification is utilized in scenarios similar to segmentation masks. Gemini 1.5 Pro can analyze an hour-long video and identify the specific timestamps for a particular event. \u003c/p\u003e\u003ch2 id=\"additional-considerations\"\u003e\u003cstrong\u003eAdditional considerations\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAdditional considerations as users incorporate Foundry labels into their projects and workflows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo produce frame based binary classifications in Gemini 1.5 Pro, users are recommended to experiment with the prompt and provide as much context as possible to get the best results. \u003cul\u003e\u003cli\u003eFor example, the following yields better results:\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003c!--kg-card-begin: html--\u003e\n“For the given video, what timestamps have a banana and be as thorough as possible about checking each second for a banana. Make sure there is no overlap in timestamps. Return the result as a dictionary without any new lines. {“banana”: “\u003ctimestamps-with-banana\u003e“, “no_banana”:  “\u003ctimestamps-without-banana\u003e“}” than using “For the given video, what timestamps have a banana. Return the result as a dictionary without any new lines. {“banana”: “\u003ctimestamps-with-banana\u003e” }”\n\u003c!--kg-card-end: html--\u003e\n\u003cul\u003e\u003cli\u003eUsers can also \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA/B test different models \u003c/u\u003e\u003c/a\u003efrom Foundry to find the model that best fits the use case using Quantumworks Lab Model.\u003c/li\u003e\u003cli\u003eIf we do not support your use case or any questions arise, feel free to contact our \u003ca href=\"https://docs.labelbox.com/docs/contacting-customer-support?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esupport team\u003c/u\u003e\u003c/a\u003e, as we would love to hear your feedback to improve Foundry.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAnnotating video data has traditionally been a tedious and time-consuming task. The integration of advanced AI models from Quantumworks Lab Foundry into the video labeling process marks a significant transformation in how video data is annotated. By leveraging Foundry's capabilities, users can drastically speed up their video labeling projects. This acceleration not only diminishes the time required to bring products to market but also substantially reduces the costs involved in model development.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our additional resources on how to utilize state-of-the-art AI models in Foundry, including using \u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003emodel distillation\u003c/a\u003e and fine-tuning to leverage the power of foundation models:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-accelerate-labeling-projects/?ref=labelbox-guides.ghost.io#conclusion\"\u003e\u003cu\u003eHow to accelerate labeling projects using GPT–4V in Foundry\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io#what-are-the-benefits-of-using-image-segmentation-for-my-machine-learning-model\"\u003e\u003cu\u003eHow to create high-quality image segmentation masks quickly and easily\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://event.on24.com/wcc/r/4460171/5D5357846BEAB1FD1E12A5ED146DE6E1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomated Labeling With Any Foundation Model\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFine tune LLMs\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"662804d1d733f0000145b1c4","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/image.png","featured":false,"visibility":"public","created_at":"2024-04-23T18:58:25.000+00:00","updated_at":"2024-11-22T23:53:12.000+00:00","published_at":"2024-04-23T23:40:19.000+00:00","custom_excerpt":"Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/harnessing-ai-for-efficient-video-labeling/","excerpt":"Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":74,"allPosts":[{"id":"684755a1e82e4e00013fe307","uuid":"fab31394-c337-43cc-bc44-725a7b69cc61","title":"Quantumworks Lab Leaderboards: Redefining AI evaluations with human-centric assessments","slug":"labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments","html":"\u003cp\u003eIn the rapidly evolving landscape of artificial intelligence, traditional benchmarks are no longer sufficient to capture the true capabilities of AI models. At Quantumworks Lab, we're excited to introduce our groundbreaking \u003ca href=\"https://labelbox.com/leaderboards/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e—an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks. \u003c/p\u003e\u003ch2 id=\"the-limitations-of-current-benchmarks-and-leaderboards\"\u003e\u003cstrong\u003eThe limitations of current benchmarks and leaderboards\u003c/strong\u003e\u003c/h2\u003e\u003ch3 id=\"benchmark-contamination\"\u003e\u003cstrong\u003eBenchmark contamination\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOne of the most pressing issues in AI evaluation today is benchmark contamination. As large language models are trained on vast amounts of internet data, they often inadvertently include the very datasets used to evaluate them. This leads to inflated performance metrics that don't accurately reflect real-world capabilities. For example:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe LAMBADA dataset, designed to test language understanding, has been found in the training data of several popular language models, with an \u003ca href=\"https://hitz-zentroa.github.io/lm-contamination/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLM Contamination Index\u003c/u\u003e\u003c/a\u003e of 29.3%.\u003c/li\u003e\u003cli\u003ePortions of the SQuAD question-answering dataset have been discovered in the pretraining corpora of multiple large language models.\u003c/li\u003e\u003cli\u003eEven coding benchmarks like HumanEval have seen their \u003ca href=\"https://arxiv.org/pdf/2407.07565?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esolutions leaked online\u003c/u\u003e\u003c/a\u003e, potentially contaminating future model training.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis contamination makes it increasingly difficult to trust traditional benchmark results, as models may be “cheating” by memorizing test data rather than demonstrating true understanding or capability.\u003c/p\u003e\u003ch3 id=\"existing-leaderboards-a-step-forward-but-not-enough\"\u003e\u003cstrong\u003eExisting leaderboards: A step forward, but not enough\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWhile several leaderboards have emerged to address the limitations of traditional benchmarks, they each come with their own set of challenges.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eLMSYS chatbot arena\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLMSYS Chatbot Arena, despite its broad accessibility, faces notable challenges in providing objective AI evaluations. Its reliance on non-expert assessments and emphasis on chat-based evaluations may introduce personal biases, potentially favoring engaging responses over true intelligence. Researchers worry that this approach could lead companies to prioritize optimizing for superficial metrics rather than genuine real-world performance. Furthermore, LMSYS's commercial ties raise concerns about impartiality and the potential for an uneven evaluation playing field, as usage data may be selectively shared with certain partners.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScale AI's SEAL\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eScale’s Safety, Evaluations, and Alignment Lab (SEAL), released few months ago, offers detailed insights/evaluations for topics such as reasoning, coding, and agentic tool use. However, the infrequent updates and primary focus on language models, while useful, may not capture the full spectrum of rapidly advancing multimodal AI capabilities.\u003c/p\u003e\u003ch2 id=\"challenges-in-ai-evaluation\"\u003e\u003cstrong\u003eChallenges in AI evaluation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThese and other existing leaderboards all run into core challenges with AI evaluations:\u003c/p\u003e\u003cp\u003e1) Data contamination and overfitting to public benchmarks\u003c/p\u003e\u003cp\u003e2) Scalability issues as models improve and more are added\u003c/p\u003e\u003cp\u003e3) Lack of standards for evaluation instructions and criteria\u003c/p\u003e\u003cp\u003e4) Difficulty in linking evaluation results to real-world outcomes\u003c/p\u003e\u003cp\u003e5) Potential bias in human evaluations\u003c/p\u003e\u003ch2 id=\"introducing-the-labelbox-leaderboards-a-comprehensive-approach-to-ai-evaluation\"\u003e\u003cstrong\u003eIntroducing the Quantumworks Lab Leaderboards: A comprehensive approach to AI evaluation\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Quantumworks Lab Leaderboards are the first to tackle these challenges by conducting structured evaluations on subjective AI model outputs using human experts and a scientific process that provides detailed feature-level metrics and multiple ratings. Leaderboards are available for \u003ca href=\"https://labelbox.com/leaderboards/complex-reasoning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eComplex Reasoning\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/multimodal-reasoning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eMultimodal Reasoning\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eImage Generation\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/speech-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSpeech Generation\u003c/u\u003e\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/leaderboards/video-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eVideo Generation\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eOur goal is to go beyond traditional leaderboards and benchmarks by incorporating the following elements:\u003c/p\u003e\u003ch3 id=\"1-multimodal-and-niche-focus\"\u003e\u003cstrong\u003e1. Multimodal and niche focus\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eUnlike leaderboards that primarily focus on text-based large language models, we evaluate a diverse range of AI modalities and specialized applications, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImage generation and analysis\u003c/li\u003e\u003cli\u003eAudio processing and synthesis\u003c/li\u003e\u003cli\u003eVideo creation and manipulation\u003c/li\u003e\u003cli\u003eComplex and multimodal reasoning\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"2-expert-human-evaluation\"\u003e\u003cstrong\u003e2. Expert human evaluation\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFor every evaluation, public or private, it’s critical for the raters to reflect your target audience. We place expert human judgment, using our \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAlignerr\u003c/u\u003e\u003c/a\u003e workforce, at the core of the evaluation process to ensure:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSubjective quality assessment:\u003c/strong\u003e Humans assess aspects like aesthetic appeal, realism, and expressiveness.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContextual understanding:\u003c/strong\u003e Evaluators consider the broader context and intended use.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAlignment with human preferences:\u003c/strong\u003e Raters ensure evaluations reflect criteria that matter to end-users.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResistance to contamination:\u003c/strong\u003e Human evaluations on novel tasks are less prone to data contamination.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"3-reliable-and-transparent-methodology\"\u003e\u003cstrong\u003e3. Reliable and transparent methodology\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWe are committed to performing trustworthy evaluations using a variety of sophisticated metrics. Quantumworks Lab balances privacy with openness by providing detailed feature-level metrics (e.g. prompt alignment, visual appeal, and numerical count for text-image models) and multiple ratings.\u003c/p\u003e\u003cp\u003eIn addition to critical human experts performing the evaluations, our methodology utilizes the \u003ca href=\"https://labelbox.com/product/platform/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLablebox Platform\u003c/a\u003e to generate advanced metrics on both the rater and model performance. We provide the following metrics across our three leaderboards:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eElo rating system:\u003c/strong\u003e Adapted from competitive chess, our Elo system provides a dynamic rating that adjusts based on head-to-head comparisons between models. This allows us to capture relative performance in a way that's responsive to improvements over time.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTrueSkill rating:\u003c/strong\u003e Originally developed for Xbox Live, TrueSkill offers a more nuanced rating that accounts for both a model's performance and the uncertainty in that performance. This is particularly useful for newer models or those with fewer evaluations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank percentages:\u003c/strong\u003e We track how often each model achieves each rank (1st through 5th) in direct comparisons. This provides insight into not just average performance, but consistency of top-tier results.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAverage rating:\u003c/strong\u003e A straightforward metric that gives an overall sense of model performance across all evaluations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to these key metrics, our methodology incorporates the following characteristics to ensure a balanced and fair evaluation:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eExpert evaluators:\u003c/strong\u003e Utilizing skilled professionals from our Alignerr platform to provide nuanced, context-aware assessments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComprehensive and novel datasets:\u003c/strong\u003e Curated to reflect real-world scenarios while minimizing contamination.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTransparent reporting:\u003c/strong\u003e Detailed insights into our methodologies and results without compromising proprietary information.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"4-continuously-updated-evaluations\"\u003e\u003cstrong\u003e4. Continuously updated evaluations\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOur leaderboard isn't static; we plan to regularly update our evaluations to include the latest models and evaluation metrics, ensuring stakeholders have access to current and relevant information.\u003c/p\u003e\u003ch2 id=\"leaderboard-insights-a-glimpse-into-the-image-generation-leaderboard\"\u003e\u003cstrong\u003eLeaderboard insights: A glimpse into the image generation leaderboard\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo illustrate the power of our comprehensive evaluation approach, let's explore the \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eimage generation leaderboard\u003c/a\u003e. For each evaluation of the latest image-generating models, we capture and publish four key pieces of data to help understand capabilities and areas of opportunity for each model.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Elo ratings:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXf_TDIoHT0uGUl1DnLLMI3qkaV4g9M9yrK4GVWRc1Ze495hZYjwXE5Yq0wZefgLQecqsXA8cS7bSmTNz923B8CYgza7d2PkPn25crjiCrd0I3W2MG53hWiTgo-N8BTL8y11b3vuog?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT Image 1 leads with 1069.17, followed by GPT 4.1 at 1039.62 and Recraft v3 at 1039.37\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2) TrueSkill ratings:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdIblItim2rjJ5kSmyMtNqbhzIQZM24C6TwogiUeuNFCwBNyUPydDpZ3vOlYPSimzksITCDgQ_ej4Czavte2eI8aT0OEjCB0FwDDagBBP-yQLgt2T_FV8dTy0DdFqWUfk4cbMSYcg?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT Image 1 again leads with 982.86, with GPT 4.1 following at 979.89\u003c/li\u003e\u003cli\u003eThis indicates high expected performance for GPT Image 1with relatively low uncertainty\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Rank percentages:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe8jjQCJegixK47fNhDT67NDHe8fhN6U0Kwm-mbkrIa9HWWFcwKbIubLDO7uy0_KtVVu1gHOcIk0Zh9VOHGgV6zrBBkGxbaCHW9a5Uu48lSo8L-J_48nAq6Q-bFqD9k-aYwPvY3?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT 4.1 achieves the top rank 60.14% of the time, followed by GPT Image 1 at 59.31%\u003c/li\u003e\u003cli\u003eThis shows GPT 4.1 consistency in achieving top results, but also highlights GPT Image 1 and DALL-E 3’s strong performance\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4) Average rank (lower better):\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemzdbe3m9BuWRyjcZTnpDJMSAn8a4UuMJTaepEJHsQwJ_JocoixeC4UQcftoenMWuZ3y9tToZU7UtSwxRMNCVdLwytjw72Tq_8bMC5MAJOxB0VYMpE4P4-EcVxs-uqZsEPlM6K?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eNote: Lower score is better here so GPT 4.1 leads in average rank.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT 4.1 slightly edges out GPT Image 1 with an average rating of 1.4 vs 1.41 (lower is better)\u003c/li\u003e\u003cli\u003eThis suggests GPT 4.1 performs well in direct comparisons despite lower Elo and TrueSkill ratings\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese metrics provide a multi-faceted view of model performance, allowing users to understand not just which model is \"best\" overall, but which might be most suitable for their specific use case. For instance, while GPT Image 1 and GPT 4.1 leads in most metrics, DALL-E 3’s and Imagen 3’s strong average rating suggests it is a reliable choice for consistent performance across a range of tasks.\u003c/p\u003e\u003ch2 id=\"join-the-revolution-beyond-the-benchmark\"\u003e\u003cstrong\u003eJoin the revolution: Beyond the benchmark\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Quantumworks Lab Leaderboards represent a significant advance in AI evaluation, pushing past traditional leaderboards by incorporating expert human evaluations for subjective generative AI models using comprehensive metrics. We are uniquely able to achieve this thanks to our modern AI data factory that combines human experts and our scalable platform with years of operational excellence evaluating AI models.\u003c/p\u003e\u003cp\u003eWe invite you to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCheck out the \u003ca href=\"http://labelbox.com/leaderboards?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e to explore our latest evaluations across various AI modalities and niche applications.\u003c/li\u003e\u003cli\u003e\u003cu\u003eLet us know\u003c/u\u003e if you have suggestions or want a specific model included in future assessments.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eContact us\u003c/u\u003e\u003c/a\u003e to learn more about how we can help you evaluate and improve your AI models across all modalities.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eReady to go beyond the benchmark? Let's redefine AI evaluation — together—and drive the field toward more meaningful, human-aligned progress that truly captures the capabilities of next-generation AI models.\u003c/p\u003e","comment_id":"684755a1e82e4e00013fe307","feature_image":"https://labelbox-guides.ghost.io/content/images/2025/06/guide_LeaderboxHero2.png","featured":false,"visibility":"public","created_at":"2025-06-09T21:44:01.000+00:00","updated_at":"2025-06-11T17:30:22.000+00:00","published_at":"2025-06-10T23:15:53.000+00:00","custom_excerpt":"Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"664e43b39132db0001f20548","name":"Michael Haag","slug":"michael","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/michael/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"664e43b39132db0001f20548","name":"Michael Haag","slug":"michael","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/michael/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/","excerpt":"Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Quantumworks Lab Leaderboards: Redefining AI Evaluation with Human Experts","meta_description":"Introducing our groundbreaking Quantumworks Lab leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66cceba90693fd000117e1ca","uuid":"baef90d7-868a-4181-b171-0c1bfec81d5c","title":"Programmatically launch human data jobs for RLHF and evaluation","slug":"programmatically-launch-human-data-jobs-for-rlhf-and-evaluation","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eLabelbox’s Python SDK provides AI teams with a powerful approach to orchestrate human data labeling projects. In this guide, we’ll walk through how to harness the Python SDK to manage human data labeling jobs for RLHF and evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.\u003c/p\u003e\u003ch2 id=\"getting-started-set-up-the-labelbox-python-sdk\"\u003eGetting started: Set up the Quantumworks Lab Python SDK\u003c/h2\u003e\u003cp\u003eLet's begin by first setting up the Quantumworks Lab Python SDK in four simple steps:\u003c/p\u003e\u003cp\u003e1) \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCreate an API key\u003c/u\u003e\u003c/a\u003e to start using Quantumworks Lab Python SDK\u003c/p\u003e\u003cp\u003e2) pip install \"Quantumworks Lab[data]\" in terminal or !pip install \"Quantumworks Lab[data]\" in your notebook\u003c/p\u003e\u003cp\u003e3) Authentication can be done by saving your key to an environment variable:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003euser@machine:~$ export LABELBOX_API_KEY=\"\u0026lt;your_api_key\u0026gt;\"\nuser@machine:~$ python3\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e4) Then, import and initialize the API Client. \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab as lb \nclient = lb.Client()\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"importing-your-data-into-labelbox-methods-and-supported-formats\"\u003eImporting your data into Quantumworks Lab: Methods and supported formats\u003c/h2\u003e\u003cp\u003eNow that the SDK has been set up,\u0026nbsp; let's look at an example of uploading LLM response evaluation data for RLHF:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003e# Create a dataset\ndataset = client.create_dataset(\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"RLHF asset upload example\"+str(uuid.uuid4()),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;iam_integration=None\n)\n# Upload assets\ntask = dataset.create_data_rows([\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_1.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;},\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_2.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;},\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_3.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;}\n\u0026nbsp;\u0026nbsp;])\ntask.wait_till_done()\nprint(\"Errors:\",task.errors)\nprint(\"Failed data rows:\", task.failed_data_rows)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLearn more about all supported data types and editors \u003ca href=\"https://docs.labelbox.com/docs/label-data?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003ch2 id=\"creating-an-ontology-using-the-sdk\"\u003eCreating an ontology using the SDK\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWith the data imported, the next step is to create your ontology for the project. The ontology defines the structure and relationships within the data for your labeling process. Below is an example of how to create an ontology using the Quantumworks Lab Python SDK:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003eimport Quantumworks Lab as lb\nontology_builder = lb.OntologyBuilder(\n\u0026nbsp;\u0026nbsp;classifications=[\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.TEXT,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Free form text example\"),\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.CHECKLIST,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Checklist example\",\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"first_checklist_answer\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"second_checklist_answer\")\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;),\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.RADIO,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Radio example\",\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"first_radio_answer\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"second_radio_answer\")\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.RADIO,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Rank #1\", # More ranks can be created like this\u0026nbsp; with N number of options\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;required = True,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 1\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 2\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 3\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;,)\n\u0026nbsp;\u0026nbsp;]\n)\n\n\nontology = client.create_ontology(\"RLHF classification example\", ontology_builder.asdict(), media_type=lb.MediaType.Conversational)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eFor more information about ontology creation, please refer to the \u003ca href=\"https://docs.labelbox.com/reference/ontology-examples?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edocumentation\u003c/u\u003e\u003c/a\u003e for more examples.\u003c/p\u003e\u003ch2 id=\"best-practices-for-ontology-design\"\u003eBest practices for ontology design\u003c/h2\u003e\u003ch3 id=\"leverage-existing-ontologies-wisely\"\u003eLeverage existing ontologies wisely\u003c/h3\u003e\u003cp\u003eLabelbox allows users to reuse ontologies from previous projects, saving time and ensuring consistency across related tasks. However, be cautious when modifying shared ontologies:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCopy existing ontologies: To prevent unintended changes to previous projects, create a copy of an existing ontology. This creates a new schema node while retaining all your classes.\u003c/li\u003e\u003cli\u003eUsers can customize the ontology for their current project. After copying, they can freely modify the ontology to suit the new project's needs without affecting earlier work.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"optimize-object-ordering-for-logical-workflows\"\u003eOptimize object ordering for logical workflows\u003c/h3\u003e\u003cp\u003eThe order of objects in the ontology can significantly impact the labeling process:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePrioritize common objects: Create the most frequently used objects first. They'll appear at the top of the list, making them easily accessible to labelers.\u003c/li\u003e\u003cli\u003eDesign a logical flow: For complex tasks like model response comparisons, structure the ontology to guide labelers through a step-by-step analysis:\u003c/li\u003e\u003c/ul\u003e\u003col\u003e\u003cli\u003eStart with individual model evaluation criteria.\u003c/li\u003e\u003cli\u003ePlace comparative questions (e.g., \"Which model response is best?\") at the end.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis approach ensures labelers have thoroughly analyzed each option before making final comparisons.\u003c/p\u003e\u003ch3 id=\"enhance-visual-clarity-with-color-coding\"\u003eEnhance visual clarity with color coding\u003c/h3\u003e\u003cp\u003eImprove the visual experience for labelers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConsistent color schemes: Assign and edit colors for each object in the ontology.\u003c/li\u003e\u003cli\u003eMaintain color consistency: Use the same colors throughout the project to reduce cognitive load and improve labeling speed and accuracy.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"provide-easy-access-to-labeling-instructions\"\u003eProvide easy access to labeling instructions\u003c/h3\u003e\u003cp\u003eMake sure labelers have all the information they need at their fingertips:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAttach PDF instructions: Upload labeling guidelines as a PDF document.\u003c/li\u003e\u003cli\u003eSide-by-side viewing: Labelers can reference the instructions within Quantumworks Lab, displayed alongside the project for convenient access.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"use-advanced-classification-features\"\u003eUse advanced classification features\u003c/h3\u003e\u003cp\u003eTake advantage of Quantumworks Lab's classification capabilities to create more nuanced and accurate labels:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImplement nested classifications: This allows for more detailed object identification. For example, after drawing a segmentation mask over a tree, labelers can further classify it as healthy or unhealthy.\u003c/li\u003e\u003cli\u003eSet required questions: Ensure critical information is always captured by making certain questions mandatory for each asset.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy following these best practices, users will create more efficient labeling jobs, leading to higher quality data and improved model performance.\u003c/p\u003e\u003ch2 id=\"labelbox-labeling-services\"\u003eLabelbox labeling services\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFor Enterprise plan users, Quantumworks Lab offers data labeling services, connecting them with professional labelers to process large amounts of data quickly and efficiently. Key features include:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Rapid Data Processing\u003c/strong\u003e: Quickly handle large volumes of data without the overhead of hiring additional staff.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Specialized Expertise\u003c/strong\u003e: Access labelers with specialized knowledge, including:\u003c/p\u003e\u003col\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eMedical experts\u003c/li\u003e\u003cli\u003eVarious language specialists\u003c/li\u003e\u003cli\u003eOther certified specialties\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003c/ol\u003e\u003cp\u003e\u003cstrong\u003e3) Flexibility\u003c/strong\u003e: Scale your labeling service up or down based on project needs without long-term commitments.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4) Quality Assurance\u003c/strong\u003e: Professional labelers are trained to maintain high standards of accuracy and consistency.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5) Time and Resource Savings\u003c/strong\u003e: Eliminate the need for recruitment, training, and management of an in-house labeling team.\u003c/p\u003e\u003cp\u003eBy leveraging labeling services, enterprise users can significantly accelerate their data labeling projects, especially when dealing with complex datasets or when requiring domain-specific expertise. This service complements Quantumworks Lab's robust data import and management capabilities, providing a comprehensive solution for large-scale AI and machine learning projects.\u003c/p\u003e\u003cp\u003eTo leverage labeling services, Quantumworks Lab provides programmatic methods to request labeling services as, shown here:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Getting labeling service information:\u003c/strong\u003e Users can retrieve information about the labeling service for a specific project:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003elabeling_service = project.get_labeling_service()\nprint(labeling_service)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e This will return details such as the service ID, project ID, creation date, status, and more.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Requesting labeling services for faster results:\u003c/strong\u003e Once data and an ontology with instructions has been added, users can initiate a boost request:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003elabeling_service.request()\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e This call initiates the labeling services service for your project.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Monitoring your labeling service’s status\u003c/strong\u003e:The Quantumworks Lab labeling service requested can be easily monitored via the UI as shown below:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXekVuCZ2W9OORuo0lpeibkcuoC79eXyUJ1KIU4s1OYsnE_VgNaLcMBLaJ5OHsc57ae_3BfSIM3hv4GacVmkhbOvbPNb-PetLKLK4vUN7fDitccn5y-PIhV_nZj_OU7TeOln9Y-tRbyClHvSN3vXPHVczmk?key=Eb16GHK2ItC9fBn058nPbA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"799\"\u003e\u003c/figure\u003e\u003ch2 id=\"simple-export-via-the-labelbox-sdk\"\u003eSimple export via the Quantumworks Lab SDK\u003c/h2\u003e\u003cp\u003eOnce the labeling project is complete, users can easily export the labels using the SDK, as shown below.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eexport_task = project.export(params=export_params, filters=filters)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis simple command allows users to retrieve labeled data that is ready for use in machine learning pipelines. Please refer to \u003ca href=\"https://docs.labelbox.com/reference/export-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edocumentation\u003c/u\u003e\u003c/a\u003e for flexible ways of exporting a project with filters.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab Python SDK offers teams with a convenient and powerful way to programmatically manage human data labeling projects. By providing control over every aspect of the labeling process - from data import and ontology design to project monitoring and data export - the SDK enables AI teams with the ability to incorporate high-quality labeled data into their workflows seamlessly.\u003c/p\u003e\u003cp\u003eWe hope you found this guide helpful for gaining a deeper understanding of how to capitalize on an SDK-driven approach to simplify complex tasks and enhance productivity. Whether you’re working on small-scale projects or large, distributed labeling efforts, the Quantumworks Lab SDK offers the full-suite of tooling needed to efficiently manage your\u0026nbsp; data labeling needs and accelerate their AI development process.\u003c/p\u003e\u003cp\u003eIf you're interested in implementing an SDK approach to jumpstart your human data jobs for RLHF and model evaluation,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out, or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more.\u003c/p\u003e","comment_id":"66cceba90693fd000117e1ca","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-26-at-2.03.08-PM.png","featured":false,"visibility":"public","created_at":"2024-08-26T20:55:05.000+00:00","updated_at":"2024-08-28T15:34:08.000+00:00","published_at":"2024-08-26T21:03:56.000+00:00","custom_excerpt":"Learn how to harness the SDK to manage human data labeling jobs for RLHF and model evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/programmatically-launch-human-data-jobs-for-rlhf-and-evaluation/","excerpt":"Learn how to harness the SDK to manage human data labeling jobs for RLHF and model evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bfded301c06500016e8b1c","uuid":"a2c40e88-6376-4824-b57f-e0e3673d8b1b","title":"Evaluating leading text-to-speech models","slug":"evaluating-leading-text-to-speech-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in speech generation evaluation, visit \u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe evolution of Text-to-Speech (TTS) models has been groundbreaking, with various companies offering high-quality solutions for a range of applications. Whether it's enhancing accessibility, automating customer service, or generating voiceovers, TTS models play a vital role in today's digital landscape. \u003c/p\u003e\u003cp\u003eWe recently evaluated the performance of six leading TTS models—Google TTS, Cartesia, AWS Polly, OpenAI TTS, Deepgram, and Eleven Labs—using key evaluation metrics such as Word Error Rate (WER) and human preference ratings.\u003c/p\u003e\u003cp\u003eTo provide an in-depth analysis, we evaluated each model on 500 prompts. Each prompt was reviewed by three expert labelers to ensure accuracy and provide diverse perspectives.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"evaluation-process\"\u003e\u003cstrong\u003eEvaluation Process\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate speech outputs across several key criteria:\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer\"\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWER measures the accuracy of the generated speech by calculating the number of insertions, deletions, and substitutions compared to a reference transcription. The lower the WER, the better the model performed in terms of generating accurate and coherent speech.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1120\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"word-errors\"\u003eWord errors\u003c/h3\u003e\u003cp\u003eRaters evaluated the presence of word errors in the generated TTS audio by counting the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Insertion Errors\u003c/strong\u003e: Additional words not present in the original text.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Deletion Errors\u003c/strong\u003e: Words from the original text that were omitted.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Substitution Errors\u003c/strong\u003e: Incorrect replacements for original words.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: No word errors or only minor errors (0-1 error).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some errors (2-3 errors).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Significant errors (4 or more errors).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"speech-naturalness\"\u003eSpeech naturalness\u003c/h3\u003e\u003cp\u003eRaters assessed how human-like the generated speech sounded.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Very human-like, natural flow with appropriate pauses and inflections.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some human qualities but with occasional robotic or awkward elements.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Clearly robotic or artificial, with choppy or monotone speech.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"pronunciation-accuracy\"\u003ePronunciation accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated how clearly and correctly words were pronounced in the TTS output.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: All words are pronounced clearly and correctly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: 1-2 words are mispronounced or unclear.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: 3 or more words are mispronounced or difficult to understand.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"noise\"\u003eNoise\u003c/h3\u003e\u003cp\u003eRaters evaluated the level of background noise or artifacts in the audio.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNone\u003c/strong\u003e: No detectable noise or artifacts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Minor noise, such as a faint hiss, but not distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Noticeable noise or artifacts that may be somewhat distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Significant noise or artifacts that interfere with comprehension.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"context-awareness\"\u003eContext awareness\u003c/h3\u003e\u003cp\u003eRaters assessed the TTS system's ability to adjust to context, including tone, emphasis, and punctuation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Excellent adaptation to context, with clear tonal shifts and pauses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Adequate adaptation but misses some nuances.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Little to no adaptation; reads text monotonously without context cues.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"prosody-accuracy\"\u003eProsody accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated the rhythm, stress, and intonation (prosody) of the generated speech.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Natural-sounding rhythm and intonation that closely mimics human speech.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Mostly natural but with occasional awkwardness in stress or rhythm.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Monotonous or robotic intonation with unnatural pauses and emphasis\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"prompts-curation-process\"\u003ePrompts curation process\u003c/h2\u003e\u003cp\u003eThe diverse set of prompts used in our TTS evaluation provides a comprehensive test of model capabilities across multiple dimensions. \u003c/p\u003e\u003cp\u003eConsider these examples:\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Wow! Did you see the fireworks last night? They were spectacular!\"\u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003e This short, exclamatory sentence tests the model's ability to convey excitement and emphasis. It challenges the TTS system to appropriately modulate tone and pitch to express enthusiasm.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"I wish you were on the right path...I-I-I truly do, but you're not,' he said sadly.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis prompt evaluates the model's handling of emotional nuance, hesitation (indicated by the stuttering \"I-I-I\"), and the use of ellipsis for pauses. It also tests the system's ability to convey sadness as specified in the dialogue tag.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Alright, listen up, team! The mission briefing is as follows: Operation Nighthawk commences at 0300 hours. Rendezvous at coordinates 51°30'26.5\"N 0°07'39.0\"W. Your login credentials for the secure server are: Username: NightOwl_Alpha, Password: 3ch3l0n_X9#. Remember, this is a stealth operation. Radio silence must be maintained except for emergency protocol Sierra-Tango-Seven. If compromised, use the failsafe code: Zulu-Yankee-4-7-2-Bravo. Any questions? No? Then synchronize your watches... now. Good luck, and godspeed.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis complex prompt tests multiple aspects:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical pronunciation of alphanumeric strings and military jargon\u003c/li\u003e\u003cli\u003eConveying urgency and authority\u003c/li\u003e\u003cli\u003eHandling of coordinates and precise numerical information\u003c/li\u003e\u003cli\u003eProper pacing for a lengthy, detailed instruction set\u003c/li\u003e\u003cli\u003eAppropriate pausing, indicated by punctuation and paragraph breaks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese prompts demonstrate how we cover a range of linguistic features, emotional tones, and formatting challenges. They span from simple exclamations to complex, technical instructions, ensuring a thorough assessment of prosody, emphasis, and versatility.\u003c/p\u003e\u003cp\u003eThis diversity in prompts matters because it enables a comprehensive evaluation that reflects real-world applicability. By covering such a broad spectrum of scenarios, we can identify specific strengths and weaknesses in each TTS model, allowing for fair comparisons and pinpointing areas for improvement. Whether it's expressing joy over fireworks, conveying emotional conflict, or delivering a detailed mission briefing, these prompts help us ensure that TTS models can handle the variety and complexity of human communication.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overall-ranking-best-to-worst-model-samples\"\u003eOverall Ranking best to worst model samples\u003c/h2\u003e\u003cp\u003eRaters ranked each TTS sample from best to worst based on performance across all categories above.\u003c/p\u003e\u003ch3 id=\"model-configurations-used-at-test-time\"\u003e\u003cstrong\u003eModel configurations used at test time\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: tts-1-hd\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: alloy\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle Text-to-Speech (TTS)\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel:\u003c/strong\u003e Default\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice: \u003c/strong\u003eDefault with neutral gender voice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eElevenLabs\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: eleven_monolingual_v1 (As of 30th September, eleven_monolingual_v1 is an outdated version and Quantumworks Lab will run a new experiment with\u0026nbsp;eleven_multilingual_v2 in the future)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: pMsXgVXv3BLzUgSXRplE\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: aura-asteria-en\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Default\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAmazon Polly\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Generative\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Ruth\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: sonic-english\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Teacher Lady\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe selected these models for our evaluation as they represent the cutting-edge offerings from leading companies in the TTS space, showcasing the latest advancements in voice synthesis technology.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"results\"\u003eResults\u003c/h1\u003e\u003cp\u003eOur evaluation of 500 diverse and complex prompts provided valuable insights into the capabilities of Google, Cartesia, AWS Polly, OpenAI, Deepgram, and Eleven Labs TTS models. Each prompt was meticulously assessed by three different raters to ensure accurate and diverse perspectives. This rigorous testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of these leading TTS models:\u003c/p\u003e\u003ch3 id=\"rank-distribution-percentages\"\u003eRank distribution percentages\u003cstrong\u003e\u0026nbsp; \u0026nbsp; \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"overall-ranking\"\u003eOverall ranking\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1:\u003c/strong\u003e\u0026nbsp; (Open AI TTS) - occurred 607 times, 42.93%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2:\u003c/strong\u003e\u0026nbsp; (Cartesia) - occurred 460 times, 32.53%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 410 times, 29.00%\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 4:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 338 times, 23.90%. Followed by Eleven labs as a close second with occurrence 335, 23.69%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 5:\u003c/strong\u003e\u0026nbsp; (Deepgram) - occurred 438 times, 30.98%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 6:\u003c/strong\u003e\u0026nbsp; (Google TTS) - occurred 868 times, 61.39%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote: Rank is determined by the most frequently selected model for each position, which is why some AWS Polly appears twice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1122\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003eFig: % of times each model was ranked first.\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer-results\"\u003eWord Error Rate (WER) Results\u003c/h3\u003e\u003cp\u003eThe WER scores offer an objective look at how accurately each model converts text to speech.\u003c/p\u003e\u003cul\u003e\u003cli\u003eDeepgram: 5.67%\u003c/li\u003e\u003cli\u003eOpenAI TTS: 4.19%\u003c/li\u003e\u003cli\u003eGoogle TTS: 3.36%\u003c/li\u003e\u003cli\u003eAWS Polly: 3.18%\u003c/li\u003e\u003cli\u003eCartesia: 3.87%\u003c/li\u003e\u003cli\u003eEleven Labs: 2.83%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs achieved the lowest WER at 2.83%, making it the most accurate model. Deepgram, however, had the highest WER at 5.67%, indicating room for improvement in transcription accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" title=\"Chart\" width=\"2000\" height=\"1126\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1123\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-specific-performance-results\"\u003eModel-Specific Performance Results\u003c/h2\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each TTS model based on key evaluation criteria.\u003c/p\u003e\u003ch3 id=\"openai-tts\"\u003eOpenAI TTS\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 4.1898%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 89.60% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 87.13% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 92.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 63.37% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 64.57% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOpenAI TTS excels at producing natural-sounding speech, making it a top choice for applications requiring lifelike vocal output. It demonstrates exceptional accuracy in pronunciation and performs excellently in producing clean audio with minimal background noise. The model shows strong capabilities in understanding and conveying contextual nuances in speech and is proficient in delivering appropriate intonation and rhythm. Despite ranking high in human preference, it shows moderate accuracy in word reproduction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cm24ur6wwf\" title=\"OpenAI: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gicqvkqc5e\" title=\"OpenAI: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kg9wqqftu4\" title=\"OpenAI: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"cartesia\"\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 58.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 53.11% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 93.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 80.98% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 77.02% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.8707%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCartesia demonstrates good accuracy in word reproduction. While not as consistent as OpenAI TTS in naturalness, it still performs very well in producing natural-sounding speech. It shows excellent accuracy in pronunciation and excels at producing clean audio with minimal background noise. The model displays good capabilities in understanding contextual nuances and performs well in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/3gukhi7z2j\" title=\"Cartesia: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y3lbes0sin\" title=\"Cartesia: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f0lmkto1xk\" title=\"Cartesia: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"aws-polly\"\u003e\u003cstrong\u003eAWS Polly\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 56.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 55.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.39% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 84.72% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Medium in 58.20% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.1770%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAWS Polly shows very good accuracy in word reproduction, ranking second among the models. It struggles more with producing natural-sounding speech compared to top performers but maintains excellent pronunciation accuracy. The model performs well in producing clean audio and shows good capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qawx1dwc9g\" title=\"Polly: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/klmnfcsno5\" title=\"AWS Polly: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ubzrqtjnc9\" title=\"AWS Polly: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"eleven-labs\"\u003e\u003cstrong\u003eEleven Labs\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 2.8302%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: Medium in 44.98% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 81.97% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 80.27% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 44.70% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 46.25% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs demonstrates the best accuracy in word reproduction among all models. It shows mixed results in producing natural-sounding speech but maintains excellent pronunciation accuracy. The model performs reasonably well in producing clean audio but has more noise issues than some competitors. It shows moderate capabilities in understanding contextual nuances and has room for improvement in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/x3qzhkt0ab\" title=\"Eleven Labs: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kl0lc7oqmz\" title=\"Eleven Labs: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bc19r3iqip\" title=\"Eleven Labs: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"deepgram\"\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 5.6743%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: High in 57.78% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 64.43% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 88.83% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 53.18% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 55.52% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDeepgram shows the highest error rate in word reproduction among all models. It performs moderately well in producing natural-sounding speech and demonstrates good accuracy in pronunciation, although lagging behind some competitors. The model performs well in producing clean audio with minimal background noise but shows moderate capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mngnvgtzx3\" title=\"Deepgram: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6h3bgmcg6d\" title=\"Deepgram: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uaj3x9bln8\" title=\"Deepgram: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"google-tts\"\u003e\u003cstrong\u003eGoogle TTS\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.3574%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Low in 78.01% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 77.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.46% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: Medium in 39.25% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: Low in 45.83% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z27ett9q8m\" title=\"Google TTS: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/06pqi5qc13\" title=\"Google TTS: Wow Did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kwotkto0u6\" title=\"Google TTS: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation reveals that while Eleven Labs leads in transcription accuracy with the lowest WER, OpenAI TTS emerges as the top choice when it comes to human preference, particularly for speech naturalness, pronunciation accuracy, and prosody.\u003c/p\u003e\u003cp\u003eConversely, Google TTS, despite a relatively low WER, ranks last in human preference due to poor performance in categories like speech naturalness and context awareness. This demonstrates the importance of a balanced evaluation that considers both quantitative metrics like WER and qualitative factors like user satisfaction.\u003c/p\u003e\u003cp\u003eAs the TTS field continues to evolve, the key to further advancements will lie in combining high transcription accuracy with natural, expressive, and context-aware speech. Models like OpenAI TTS and Eleven Labs are setting the standard, but there remains room for improvement across all models to fully meet the growing demands of diverse TTS applications.\u003c/p\u003e\u003cp\u003eIn summary, while WER is crucial, the broader context of user experience—focusing on factors like speech naturalness, context awareness, and prosody accuracy—is essential in evaluating the overall effectiveness of TTS models.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-speech models presented here represents a significant advancement in assessing AI-generated speech. By leveraging this methodology alongside Quantumworks Lab's cutting-edge platform, AI teams can dramatically accelerate the development and refinement of sophisticated, domain-specific speech generation models.\u003c/p\u003e\u003cp\u003eOur solution offers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eEfficient dataset curation\u003c/li\u003e\u003cli\u003eAutomated evaluation techniques\u003c/li\u003e\u003cli\u003eHuman-in-the-loop quality assurance\u003c/li\u003e\u003cli\u003eCustomizable workflows for TTS model assessment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-speech model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdohXa2KeUawTMTWS7MfKSUYC6fVPCHjtmcFCRt5ycmB7S4k4-JElTwnmRb6ufW2kR7F3Bqu1PwzbvL83LKcqgz8jbkoY02tefklFLp98wvqm2liOKBglEGaRITIk6Wx1tY3fi-43TaCJnpwEwRVCz7q101?key=Qy_gZn4bA_q73M6n7nwEIA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"1024\"\u003e\u003c/figure\u003e","comment_id":"66bfded301c06500016e8b1c","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-12.12.46-PM.png","featured":false,"visibility":"public","created_at":"2024-08-16T23:20:51.000+00:00","updated_at":"2024-11-26T00:11:38.000+00:00","published_at":"2024-08-17T19:13:25.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/evaluating-leading-text-to-speech-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bff4f801c06500016e8bba","uuid":"e5bf7da8-d960-4a06-9050-b06f63a8a4c0","title":"Metrics-based RAG Development with Quantumworks Lab","slug":"metrics-based-rag-development-with-labelbox","html":"\u003ch1 id=\"overview\"\u003e\u003cstrong\u003eOverview\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOne of the biggest challenges with RAG Application development is evaluating the quality of responses. As it currently stands, there are a series of different benchmark metrics, tools, and frameworks to help with RAG application evaluation.\u003c/p\u003e\u003cp\u003eLLM evaluation tools (such as RAGAS and DeepEval) provide a plethora of quality scores designed to evaluate the efficiency of the RAG model, ranging from retrieval to generation. In this guide, we’ll take a metrics based approach to enhance RAG development by focusing on 2 target metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext recall\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eThis measures (on a scale from 0-1) how well the retrieved content aligns with the Ground Truth Answer\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext precision\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eIdeally, the most relevant context chunks should be retrieved first. This metric measures whether the RAG application can return the most relevant chunks (with respect to the Ground Truth) at the top.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"generating-ground-truth-data-and-%E2%80%98synthetic%E2%80%99-data\"\u003e\u003cstrong\u003eGenerating ground truth data and ‘\u003cem\u003esynthetic’\u003c/em\u003e data\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo calculate performance metrics, we have to compare the RAG response with ground truth. Ground truths are factually verified responses to a given user query and must be linked back to a source document.\u003c/p\u003e\u003cul\u003e\u003cli\u003eOn the other hand, ground truth data can be automatically generated by pre-trained models. LLM Frameworks, such as LangChain, have modules that generate Question / Answer pairs based on a source document. While creating Synthetic ground truth is less labor intensive, drawbacks include quality issues, lacking real-world nuances, and inheriting model bias.\u003c/li\u003e\u003cli\u003eGround truth data is typically manually created by experts through the collection and verification of internal source documents. While labor intensive and slower to collect, manual data labeling optimizes for accuracy and quality.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo start off, we’ll use the Quantumworks Lab Platform to orchestrate a ground truth creation workflow with 2 approaches.\u003c/p\u003e\u003col\u003e\u003cli\u003eManual ground truth generation\u003c/li\u003e\u003cli\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"manual-ground-truth-generation\"\u003e\u003cstrong\u003eManual ground truth generation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eBy uploading common user questions (prompts) to the Quantumworks Lab Platform, we can set up an annotation project. Now subject matter experts can craft a thoughtful answer and provide the appropriate source.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"938\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"synthetic-ground-truth-generation-with-human-in-the-loop\"\u003e\u003cstrong\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1154\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeveraging the latest Foundation Models on Quantumworks Lab Foundry, we can combine an automated ground truth generation process with a Human in the Loop component for quality assurance.\u003c/p\u003e\u003cp\u003eTo start off, we will include the source document for each user query (PDF in this case). We can then prompt a foundation model (Google Gemini 1.5 Pro in this case) for each user question and attach the PDF context needed to answer the question.\u003c/p\u003e\u003cp\u003eThe provided prompt in this case is: *For the given text and PDF attachment, answer the following. Given the attached PDF, generate an answer using the information from the attachment. Respond with I don't know if the PDF attachment doesn't contain the answer to the question.*\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"724\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eNext, we can include the response generated from Gemini \u003cstrong\u003eas pre-labels\u003c/strong\u003e to an annotation project.\u0026nbsp;\u003c/p\u003e\u003cp\u003eInstead of starting from scratch while answering the question, human experts can modify and verify the Gemini response along with the corresponding document and user query, optimizing for quality and efficiency.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1480\" height=\"570\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1480w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"improving-rag\"\u003eImproving RAG\u003c/h1\u003e\u003cp\u003eA RAG based application consists of many components. Summarized in a recent research survey by Gao et al. (2023), a few factors that impact RAG system performance include (but not limited to):\u003c/p\u003e\u003cul\u003e\u003cli\u003eImproving the retriever (e.g. Reranking)\u003c/li\u003e\u003cli\u003eFine-tuning the embedding model for source documents\u003c/li\u003e\u003cli\u003eAdding document metadata tagging (for rerouting or semantic search)\u003c/li\u003e\u003cli\u003eQuery rewriting\u003c/li\u003e\u003cli\u003eOptimizing the chunking strategy\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’ll focus on the following strategies for improving metrics based RAG evaluation:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eOptimizing the chunking strategy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-tuning the embedding model\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"chunking-strategy\"\u003e\u003cstrong\u003eChunking strategy\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhile keeping LLM generation model (GPT Turbo 3.5) and Embeddings Model (sentence-transformers/distiluse-base-multilingual-cased-v2) consistent, we will compare the retrieval metrics derived from RAGAs.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will export the question and response pairs generated from Quantumworks Lab to a Python Environment.\u003c/li\u003e\u003cli\u003eNext we will generate the RAG response based on the following chunking strategies:\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRecursive character splitter\u003c/strong\u003e (500 token size with 150 chunk overlap)\u003cul\u003e\u003cli\u003eThis is a heuristic approach and involves dynamically splitting text based on a set of rules (e.g. new lines / tables / page breaks) while maintaining coherence.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpacy text splitter\u003c/strong\u003e\u003cul\u003e\u003cli\u003espaCy uses a rule-based approach combined with machine learning models to perform tokenization.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnce the RAG response and context have been generated, we create a table with the following fields required for RAGAs metrics evaluation: (Question, Ground Truth, RAG Answer, Context)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"432\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-5.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-5.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eBy calculating RAGAs metrics evaluation using the necessary Query, Response, Ground Truth, and Source information, we can derive performance results for the entire dataset and aggregate the findings. The Context Precision and Context Recall metrics for Recursive and Spacy are shown below:\u003c/p\u003e\u003ch3 id=\"recursive-text-chunking\"\u003e\u003cstrong\u003eRecursive text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1008\" height=\"624\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-4.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png 1008w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"spacy-text-chunking\"\u003e\u003cstrong\u003eSpacy text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1054\" height=\"652\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png 1054w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eFrom the results, we observe that Spacy yielded slightly better context precision. Given the volume of our dataset (~100 QA pairs), the difference is likely statistically insignificant. However, we can conclude that choosing the Chunking strategy and parameters will certainly affect the output of the chatbot.\u003c/p\u003e\u003cp\u003eGiven the distribution of our Recall and Precision metrics, both techniques seem to struggle with the same data rows. This indicates that the key semantics are not captured by our default embeddings (\u003cstrong\u003esentence-transformers/distiluse-base-multilingual-cased-v2\u003c/strong\u003e). We’ll test out another embedding model to see if we can improve RAG performance metrics.\u003c/p\u003e\u003ch2 id=\"embeddings-model\"\u003e\u003cstrong\u003eEmbeddings model\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAn Embedding Model is responsible for generating embeddings, which is a vector representation to any piece of information, such as a chunk of text in a RAG application. A solid embeddings model is able to better capture semantic meaning of a user question and the corpus of source documents, therefore returning the most relevant chunks as the context for answer generation.\u003c/p\u003e\u003cp\u003eWhile holding the chunking strategy constant (Recursive), we will change the underlying embeddings model in the RAG workflow before evaluating with RAGAs. The Huggingface\u003ca href=\"https://huggingface.co/spaces/mteb/leaderboard?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003eLeaderboard\u003c/u\u003e\u003c/a\u003e includes performance benchmarks for various embeddings models. Beyond retrieval performance, model size, latency, and context length are also important when choosing an embeddings model.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will generate responses using the \u003cstrong\u003ebge-base-en-v1.5\u003c/strong\u003e model and evaluate with RAGAs\u003cul\u003e\u003cli\u003eWhile ranking 35 on the leaderboards, this model is extremely lightweight (109 million parameters)\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1096\" height=\"632\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png 1096w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eFrom the results, changing the embeddings model (from distiluse-base-multilingual-cased-v2 to bge-base-en-v1.5) significantly improved the retrieval metrics, around ~10 points for both context recall and precision.\u003cul\u003e\u003cli\u003eThis indicates that the retrieved context chunks are more relevant to the ground truth and ranked higher on average as well.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"fine-tuning-an-embeddings-model\"\u003e\u003cstrong\u003eFine-tuning an embeddings model\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eMost embedding models are trained on general knowledge. Specific domains, such as internal organization information, may limit the effectiveness of these models. Therefore, we can choose to fine-tune an embeddings model with the goal of better understanding and generating domain specific information.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo fine tune embeddings, we need labeled examples of positive and negative context chunks.\u003cul\u003e\u003cli\u003eUsing a training dataset, we’ll upload the user query and the top X chunks to Quantumworks Lab\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing Quantumworks Lab Annotate, subject matter experts can determine if each chunk is relevant in answering the given question.\u003c/li\u003e\u003cli\u003e\u003c/li\u003e\u003cli\u003eUsing the Quantumworks Lab SDK we can now export our labels and convert to the following format for fine tuning: {\"query\": str, \"pos\": List[str], \"neg\":List[str]}\u003cul\u003e\u003cli\u003eThe fine tuning process for the BGE family of models is also described\u003ca href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/examples/finetune/README.md?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing the fine tuned embeddings model, we can generate updated responses and evaluate.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom the example in the screenshot, bge tends to incorrectly extract the citations section, which the expert labelers can correct and mark as irrelevant.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1005\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the resulting metrics, we can see that by fine tuning on 50 data rows is able to improve context recall and context precision by 2-3 points each. We expect this to improve as more labels are provided to continuously fine tune the embeddings model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1098\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image.png 1098w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eEvaluating the quality of responses in developing RAG Applications is essential for creating efficient and reliable systems. By incorporating the Quantumworks Lab platform in metrics based RAG development, from ground truth generation to feedback for embedding models, developers can enhance the overall performance and reliability of RAG applications. \u003c/p\u003e","comment_id":"66bff4f801c06500016e8bba","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.55.12-PM.png","featured":false,"visibility":"public","created_at":"2024-08-17T00:55:20.000+00:00","updated_at":"2024-09-03T20:00:44.000+00:00","published_at":"2024-08-17T01:07:55.000+00:00","custom_excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/metrics-based-rag-development-with-labelbox/","excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66b28fe6c2887d000107cdde","uuid":"40792083-45ad-4606-8583-7781bc74c305","title":"Unlocking precision: The \"Needle-in-a-Haystack\" test for LLM evaluation","slug":"unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation","html":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\u003cp\u003eSelecting the optimal large language model (LLM) for specific tasks is crucial for maximizing efficiency and accuracy. One of the key challenges faced by teams is selecting the best models for pre-labeling tasks, especially when dealing with large datasets and complex annotations.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox’s Model Foundry provides a robust platform for evaluation and determining the most suitable model for various applications. To illustrate this, the Quantumworks Lab Labs team conducted an experiment simulating the \"Needle-in-a-Haystack\" test. This test involves identifying specific elements within vast amounts of data, ensuring the model’s precision and reliability.\u003c/p\u003e\u003cp\u003eBy utilizing Quantumworks Lab Model Foundry’s advanced experiments and evaluation tools, teams can compare multiple LLMs to identify the one that delivers the highest accuracy and efficiency for pre-labeling on complex tasks, thus saving time and enhancing the quality of predictions.\u003c/p\u003e\u003cp\u003eIn this blog post, we’ll dive into the intricacies of the \"Needle-in-a-Haystack\", exploring how to leverage Foundry to find the best model for your pre-labeling or data enrichment needs.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"the-needle-in-a-haystack-test\"\u003eThe \"Needle-in-a-Haystack\" test\u003c/h1\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test is a specialized evaluation method designed to gauge the performance of large language models (LLMs) in identifying specific, often infrequent, elements in large datasets.\u0026nbsp;\u003c/p\u003e\u003cp\u003eImagine you have a massive dataset filled with a mix of common and rare pieces of information, similar to a haystack with a few needles hidden inside. The challenge is to determine how effectively a model can find those needles (rare information) without getting distracted by the surrounding hay (common information ).\u0026nbsp; This rare information could be anything from specific keywords in a text document to unique objects in a video.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"669\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/08/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePrevious analysis of Claude-2.1 \u003c/span\u003e\u003ca href=\"https://twitter.com/GregKamradt/status/1722386725635580292?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eperformed by Greg Kamradt\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e, member of the ARC Prize team.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"628\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/08/image-2.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePrevious analysis of GPT-4 \u003c/span\u003e\u003ca href=\"https://twitter.com/GregKamradt/status/1722386725635580292?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eperformed by Greg Kamradt\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e, member of the ARC Prize team.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"why-is-this-a-good-test-for-model-foundry\"\u003e\u003cstrong\u003eWhy is this a good test for Model Foundry?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test is a great fit for Model Foundry for several reasons:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReal-world relevance:\u003c/strong\u003e This test simulates real-world conditions, where critical information is buried in a large dataset. This ensures that models are being simulated in environments that match actual applications.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComprehensive evaluation:\u003c/strong\u003e Quantumworks Lab Model Foundry offers advanced tools that make setting up experiments, running evaluations, and comparing results efficient and easy.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eBetter decision making:\u003c/strong\u003e The insights gained from the Needle in a Haystack test can facilitate stronger decision-making when we are choosing the most suitable LLM for a task. This ensures investment in models that offer the best performance for application.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch1 id=\"creating-the-needle-in-a-haystack-internally\"\u003eCreating the \"Needle-in-a-Haystack\" internally\u003c/h1\u003e\u003cp\u003eThe first step in our experiment was to create a detailed labeling instructions set that we could eventually send to LLMs for pre-labeling. It is important to note that we decided to use Text data for our study. Various other asset types such as Video and Image can also emulate a similar test.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1254\" height=\"698\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 1254w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"instructions-overview\"\u003e\u003cstrong\u003eInstructions overview\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe wanted to build a dataset that consisted of conversations between users and a customer support chatbot, focusing on banking and financial transactions. Each conversation would be categorized into specific issues related to accounts, banking services, and transactions.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs a result, our instruction set would include detailed descriptions of each category, example conversations to guide the labeling process, and clear decision-making guidelines to help annotators distinguish between closely-related issues.\u003c/p\u003e\u003ch2 id=\"ontology\"\u003e\u003cstrong\u003eOntology\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe ontology included categories such as:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eACCOUNT_TERMINATION\u003c/li\u003e\u003cli\u003eACCOUNT_RECOVERY\u003c/li\u003e\u003cli\u003eACCOUNT_SECURITY_BREACH\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_UPDATE_DETAILS\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eBANKING_LOAN_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_FEE_DISPUTE\u003c/li\u003e\u003cli\u003eBANKING_OVERDRAFT_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_WIRE_TRANSFER_HELP\u003c/li\u003e\u003cli\u003eBANKING_SAVINGS_PLANS\u003c/li\u003e\u003cli\u003eBANKING_INVESTMENT_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eBANKING_INSURANCE_PRODUCTS\u003c/li\u003e\u003cli\u003eBANKING_MOBILE_APP_SUPPORT\u003c/li\u003e\u003cli\u003eBANKING_DEBIT_CARD_ACTIVATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eTRANSACTION_DISPUTE\u003c/li\u003e\u003cli\u003eTRANSCATION_REFUND\u003c/li\u003e\u003cli\u003eTRANSACTION_VERIFICATION\u003c/li\u003e\u003cli\u003eTRANSACTION_FRAUD_REPORT\u003c/li\u003e\u003cli\u003eTRANSACTION_LIMIT_INCREASE\u003c/li\u003e\u003cli\u003eTRANSACTION_HISTORY_QUERY\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAs seen, we chose closely-correlated categories and provided precise instructions so that while there were many similarities between subcategories, there were slight differences and nuances that our chosen LLM would have to notice and use to drive the decision-making process.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"building-the-dataset\"\u003e\u003cstrong\u003eBuilding the dataset\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCreating the dataset involved curating and structuring the data row to reflect real-world scenarios that modeled the above ontology. This ensured the dataset was comprehensive and challenging for the models.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"dataset-composition\"\u003e\u003cstrong\u003eDataset composition\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData Row Content: \u003c/strong\u003eEach data row represented a conversation between a user and customer support chatbot.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"sample-data-rows\"\u003e\u003cstrong\u003eSample data rows\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eACCOUNT_TERMIATION: User conversations requesting closure of their account\u003c/li\u003e\u003cli\u003eBANKING_LOAN_SERVICES: Inquiries about applying for or managing loans\u003c/li\u003e\u003cli\u003eTRANSACTION_FRAUD_REPORT: Reports of suspected fraudulent activities\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo view our labeling instructions, click the link \u003ca href=\"https://storage.googleapis.com/labelbox-datasets/lb_rahul/pdfs/Customer%20Support%20Ticket%20LLM%20Instructions.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"model-evaluation-using-foundry-llms\"\u003eModel evaluation using Foundry LLMs\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1246\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 1246w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"model-selection\"\u003e\u003cstrong\u003eModel selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe decided to evaluate our dataset on four leading LLMs currently on the market:\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"gemini-15-pro\"\u003e\u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eReleased by Google as part of the Gemini series;\u003c/li\u003e\u003cli\u003eKnown for its strong multimodal capabilities;\u003c/li\u003e\u003cli\u003eDesigned for complex reasoning and task completion.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gpt-4o\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eDeveloped by OpenAI;\u003c/li\u003e\u003cli\u003eAn advanced iteration of the GPT (Generative Pre-trained Transformer) series;\u003c/li\u003e\u003cli\u003eKnown for its strong natural language understanding and generation;\u003c/li\u003e\u003cli\u003eOptimized for faster response times and efficient computational resource usage.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"claude-35-sonnet\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eCreated by Anthropic;\u003c/li\u003e\u003cli\u003ePart of the Claude 3 model family;\u003c/li\u003e\u003cli\u003eKnown for its strong performance in writing and complex tasks;\u003c/li\u003e\u003cli\u003eCapable of engaging in nuanced conversations and providing detailed explanations.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gemini-15-flash\"\u003e\u003cstrong\u003eGemini 1.5 Flash\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eAnother model in the Google Gemini series;\u003c/li\u003e\u003cli\u003eOptimized for speed and efficiency;\u003c/li\u003e\u003cli\u003eDesigned for tasks requiring quick responses;\u003c/li\u003e\u003cli\u003eSuitable for applications where real-time responses are crucial.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"analysis-and-insights\"\u003e\u003cstrong\u003eAnalysis and insights\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1314\" height=\"712\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 1314w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce we had created Model Foundry Predictions on our dataset for all four LLMs, we placed them into a Model Experiment for model evaluation. Creating an experiment allowed us to dive deeply into the intricacies of each model to determine their overall performance on a needle in a haystack application.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFrom the exhibits above, we can see which models performed best from an precision perspective:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eGemini 1.5 Pro (81.55%)\u003c/li\u003e\u003cli\u003eClaude 3.5 Sonnet (80.98%)\u003c/li\u003e\u003cli\u003eGPT-4o (79.02%)\u003c/li\u003e\u003cli\u003eGemini 1.5 Flash (76.96%)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1220\" height=\"772\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 1220w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eConfusion matrices and Precision graphs are also available in Model Experiment, giving us a better understanding of the above precision scores.\u003c/p\u003e\u003cp\u003eFrom the graphs and further analysis, we can see the categories in the ontology that each model struggled with. Note that a struggle indicates a precision score of less than 0.75.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1306\" height=\"734\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 1306w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"gemini-15-pro-1\"\u003e\u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"claude-35-sonnet-1\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gpt-4o-1\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eBANKING_MISC_QUERY\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gemini-15-flash-1\"\u003e\u003cstrong\u003eGemini 1.5 Flash\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_FEE_DISPUTE\u003c/li\u003e\u003cli\u003eBANKING_INSURANCE_PRODUCTS\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eBANKING_MISC_QUERY\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBased on the performance breakdown, we can draw several insights:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTop performers\u003c/strong\u003e: Gemini 1.5 Pro and Claude 3.5 Sonnet emerge as the leading models for this particular needle in a haystack task, with very similar performance profiles.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCommon challenges\u003c/strong\u003e: All models struggled with certain categories, particularly ACCOUNT_ID_CONFIRMATION and BANKING_CREDIT_CARD_ISSUES. This suggests these categories may be inherently more difficult to classify or may require more specific training data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrecision vs. Speed\u003c/strong\u003e: While Gemini 1.5 Pro achieved the highest accuracy, teams should consider their specific needs. If real-time responses are crucial, Gemini 1.5 Flash might be a better choice despite its lower accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRoom for improvement\u003c/strong\u003e: Even the top-performing models have areas where they struggle. This information can be valuable for fine-tuning models or adjusting the labeling instructions for future iterations.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch1 id=\"leveraging-model-foundry-for-decision-making-and-pre-labeling\"\u003eLeveraging Model Foundry for decision making and pre-labeling\u003c/h1\u003e\u003cp\u003eThe experiment demonstrates the power of Quantumworks Lab Model Foundry in facilitating data-driven decision-making for model selection and optimizing the pre-labeling process. By providing comprehensive evaluation tools and visualizations, Model Foundry enables teams to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompare \u003cstrong\u003emultiple\u003c/strong\u003e models \u003cstrong\u003esimultaneously\u003c/strong\u003e\u003c/li\u003e\u003cli\u003eIdentify \u003cstrong\u003especific\u003c/strong\u003e strengths and weaknesses of \u003cstrong\u003eeach\u003c/strong\u003e \u003cstrong\u003emodel\u003c/strong\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eMake informed decisions based on \u003cstrong\u003eprecision\u003c/strong\u003e, \u003cstrong\u003erecall\u003c/strong\u003e, and \u003cstrong\u003eoverall accuracy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePinpoint\u003c/strong\u003e areas for potential model \u003cstrong\u003eimprovement\u003c/strong\u003e or \u003cstrong\u003efine-tuning\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to model evaluation, Model Foundry significantly enhances the pre-labeling workflow:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEfficient pre-labeling\u003c/strong\u003e: Once the best-performing model is identified, it can be seamlessly integrated into the pre-labeling pipeline, significantly reducing manual labeling efforts\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eQuality assurance\u003c/strong\u003e: By understanding model strengths and weaknesses, teams can strategically allocate human resources to review and correct pre-labels in categories where models struggle\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIterative improvement\u003c/strong\u003e: As more data is labeled and models are retained, teams can continuously evaluate and update their pre-labeling model, ensuring ongoing optimization of the labeling process.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost reduction\u003c/strong\u003e: By selecting the most accurate model for pre-labeling, teams can minimize the need for manual corrections, leading to substantial time and cost savings in large-scale labeling projects.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy leveraging Model Foundry for both decision-making and pre-labeling processes, teams can significantly enhance the efficiency and accuracy of their entire data labeling pipeline.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"next-steps\"\u003eNext Steps\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/2024/08/image-1.png 1000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo further improve model performance and decision-making, consider the following steps:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eFine-tune models on challenging categories.\u003c/li\u003e\u003cli\u003eConduct additional experiments with different data types or industry-specific datasets.\u003c/li\u003e\u003cli\u003eImplement regular evaluations and feedback loops to identify areas for improvement and adapt to changing requirements.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy continually refining your approach and leveraging the insights gained from Model Foundry, you can ensure that your team is always using the most effective LLM for your specific needs, driving efficiency and accuracy in your AI-powered workflows.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test, as implemented through Quantumworks Lab Model Foundry, proves to be an effective method for evaluating LLM performance on complex, nuanced tasks. By simulating real-world scenarios and leveraging Model’s advanced evaluation tools, teams can select the most suitable model for their specific pre-labeling needs.\u003c/p\u003e\u003cp\u003eIn our experiment, Gemini 1.5 Pro and Claude 3.5 Sonnet demonstrated superior performance, but the choice between them (or other models) would depend on the specific requirements of the project, including factors like speed, resource efficiency, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs the field of AI continues to evolve rapidly, tools like Quantumworks Lab Model Foundry become increasingly valuable, enabling teams to stay at the forefront of the space by consistently evaluating and selecting the best models for their unique challengers.\u003c/p\u003e","comment_id":"66b28fe6c2887d000107cdde","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-2.55.05-PM.png","featured":false,"visibility":"public","created_at":"2024-08-06T21:04:38.000+00:00","updated_at":"2024-09-03T20:04:14.000+00:00","published_at":"2024-08-06T22:08:47.000+00:00","custom_excerpt":"Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our \"Needle-in-a-Haystack\" experiment. Learn how to enhance accuracy and efficiency in complex data annotations.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"66b29da3c2887d000107ce2e","name":"Raj Jain","slug":"raj","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/raj/"},{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"66b29da3c2887d000107ce2e","name":"Raj Jain","slug":"raj","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/raj/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation/","excerpt":"Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our \"Needle-in-a-Haystack\" experiment. Learn how to enhance accuracy and efficiency in complex data annotations.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66b1410331089400019364ee","uuid":"2207c60e-9f86-4ef2-9613-168f8707aee5","title":"A comprehensive approach to evaluating text-to-video models","slug":"a-comprehensive-approach-to-evaluating-text-to-video-models","html":"\u003cp\u003eThe emergence of text-to-video AI models has marked a significant milestone in artificial intelligence, with models from Runway ML (Gen-3), Luma Labs, and Pika transforming written descriptions into dynamic and lifelike videos. This technology is reshaping industries from video production to digital marketing, democratizing visual storytelling.\u003c/p\u003e\u003cp\u003eHowever, despite their impressive capabilities, these models often fall short of human expectations, producing results that lack prompt adherence, realism, or fidelity to the input text. To accelerate the development of text-to-video models, it is crucial to establish comprehensive evaluation methodologies to pinpoint areas for improvement.\u003c/p\u003e\u003cp\u003eThis article presents a rigorous approach to assessing the strengths and limitations of Runway ML (Gen-3), Luma Labs, and Pika using human preference ratings. Let’s dive into how we systematically analyzed these leading models.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"human-preference-evaluation\"\u003e\u003cstrong\u003eHuman preference evaluation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate video outputs across several key criteria:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1245\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 2246w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eChecklist for evaluating text-to-video models using human preferences.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"prompt-adherence\"\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRaters assessed how well each generated video matched the given text prompt on a scale of high, medium, or low. For example, in the given prompt: “A peaceful Zen garden with carefully raked sand, bonsai trees, and a small koi pond.” Raters looked to see if there was prompt adherence by looking at the presence of key concepts for the prompt.\u003c/p\u003e\u003cul\u003e\u003cli\u003eIs there a garden?\u003c/li\u003e\u003cli\u003eDoes it look peaceful?\u003c/li\u003e\u003cli\u003eIs the sand present, and is it raked?\u003c/li\u003e\u003cli\u003eAre there bonsai trees?\u003c/li\u003e\u003cli\u003eIs the a small koi pond?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: If all or most of the key concepts are present.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e:\u0026nbsp; If half the key concepts are present.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: If less than half of key concepts are present .\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdENKR1QEaokNDikPODa-kTi209qknyV3PBGvEm5xa4QWqugwI5sx0zmlAZOIH2XrpQRjB9xuAAb4gshoWZfSl7mjgNmNDIWcL_bXxK4pqk0TnzP0-Xn7EG0LTznK4sBhXk1ZxuiG4p_WwoCXqx2d3dj5V_?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"509\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 1: \"A romantic Parisian street scene with couples walking and street musicians playing\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"video-realism\"\u003e\u003cstrong\u003eVideo realism\u003c/strong\u003e \u003c/h4\u003e\u003cp\u003eRaters assessed how closely the video resembled reality.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Realistic lighting, textures, and proportions.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Somewhat realistic but with slight issues in shadows or textures.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Animated or artificial appearance.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd4QhDzWX5l35W_ecjUSerUNd0sTSA_DtOLU2DjwPLenslTtGrRyf4tPjhEcwQdIxFw50JqfnPwk86brTWRtowMUFRHbMdG1hr-dqGZ69-nYFebJ9KVMslxPNvHQdjrBWPA6soRH5LZ3uUDZBggfO_tJSFl?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"504\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 2: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"video-resolution\"\u003e\u003cstrong\u003eVideo resolution\u003c/strong\u003e \u003c/h4\u003e\u003cp\u003eThis criterion evaluates the level of detail and overall clarity in the video.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Fine details visible (e.g., individual leaves, fabric textures).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Major elements are clear, but finer details are somewhat lacking.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Overall blurry or lacking significant detail.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdbrYYgTHolz9LOsLQzJHJVpzFx7TWbyIXrutMv52jbQl6nu_qHjrEV5kFXbCH01iYVLRTgoJV3BDjOIdZKU8TOod8uA_Ev-5QemuPmYTYiymsr2XN7nMs6F2AbWoVph_NnkZ54BRHe2Ldq8-IJK2J1ggNj?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"502\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 3: \"An ancient temple in the jungle with hidden traps and treasures waiting to be discovered\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"artifacts\"\u003e\u003cstrong\u003eArtifacts \u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRaters identified any visible artifacts, distortions, or errors, such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnnatural distortions in objects or backgrounds\u003c/li\u003e\u003cli\u003eMisplaced or floating elements\u003c/li\u003e\u003cli\u003eInconsistent lighting or shadows\u003c/li\u003e\u003cli\u003eUnnatural repeating patterns\u003c/li\u003e\u003cli\u003eUnnatural movements\u003c/li\u003e\u003cli\u003eBlurred or pixelated areas\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: If all or 5 of the errors are present.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: If 2 or 3 errors are present.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: \u0026nbsp;If 1 or 0 errors are present.\u0026nbsp;\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcOz4iP5QSqnQJpBB5F4neKuaS-jElkj40E21sIahJZmgm2qEE2oBxWOhDBTriFa26xOPwHbpQUGPjzUMFIfIjzWxl8I8vI8Z904UjiO-Jut14igYMNzk55VBU0U82is1tavV3FJA7uSU1kXtjCEuAmhB6I?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"502\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 4: \"Cat following a mouse\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"results\"\u003e\u003cstrong\u003eResults\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOur evaluation of 25 diverse set of complex prompts, generated by GPT-4, was stress-tested and provided valuable insights into the capabilities of Runway ML, Luma Labs and Pika. Each prompt was assessed by three different raters to ensure more accurate and diverse perspectives. This rigorous stress testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of the top models: Runway ML (Gen-3), Luma Labs, and Pika.\u003c/p\u003e\u003ch3 id=\"overall-ranking-for-human-preference-evaluations\"\u003e\u003cstrong\u003eOverall ranking for human-preference evaluations\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1\u003c/strong\u003e: Runway ML (Gen-3) ranked 1st in 65.22% of cases, Luma Labs in 18.84% and Pika in 15.94%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2\u003c/strong\u003e: Luma Labs ranked 2nd in 59.42% of cases, Runway ML (Gen-3) in 21.74% and Pika in 18.84%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3\u003c/strong\u003e: Pika ranked 3rd in 65.22% of cases, Luma Labs in 21.74% and\u0026nbsp; Runway ML in 13.04%\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1248\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRank 1 standings: Runway ML (Gen-3), Luma Labs, Pika\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"model-specific-performance-results\"\u003e\u003cstrong\u003eModel-Specific Performance results\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1248\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch4 id=\"runway-ml-gen-3\"\u003e\u003cstrong\u003eRunway ML (Gen-3)\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 59.42% of cases. This model excels at accurately reflecting the input prompts, making it a reliable choice for generating intended content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 17.39% of cases. Although it performs well relative to other models, it still has room for improvement in minimizing artifacts and errors.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 46.38% of cases. Runway ML generates realistic videos nearly half the time, indicating strong capabilities in producing lifelike content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 56.52% of cases. This model is proficient in delivering high-resolution videos, enhancing the viewing experience.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uyngtw0382\" title=\"RUNWAYML_____Gen-3 Alpha 2891026367, A grand fantasy cast Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"576\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRunway ML (Gen-3): \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"luma-labs\"\u003e\u003cstrong\u003eLuma Labs\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 37.68% of cases. While not as consistent as Runway ML, it still performs reasonably well in adhering to prompts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 17.39% of cases. Similar to Runway ML, it needs improvements to reduce visual defects.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 20.29% of cases. This model struggles more with realism, making it less suitable for applications requiring lifelike video content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 30.43% of cases. Luma Labs offers moderate video resolution quality but lags behind Runway ML.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/pyz5tqupa7\" title=\"LUMA_______A_grand_fantasy_castle_surrounded_by_lush_landscapes_and_mythical_creatures__85043b Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"530\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLuma Labs: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"pika\"\u003e\u003cstrong\u003ePika\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 36.23% of cases. Comparable to Luma Labs, Pika maintains a fair level of consistency with input prompts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 43.48% of cases. Pika has the highest occurrence of artifacts and errors, indicating significant areas for enhancement.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 20.29% of cases. Like Luma Labs, Pika also faces challenges in producing realistic videos.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 23.19% of cases. Pika offers the least in terms of video resolution among the three models, suggesting a need for improvement in this aspect.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/k1kvephgc9\" title=\"PIKA____A_grand_fantasy_castle_surrounded_by_lush_landscapes_and_mythical_creatures._seed3125151661828847 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePika: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIt's worth noting that the scope of this study was constrained by two key factors: \u003c/p\u003e\u003cul\u003e\u003cli\u003eThe absence of a public API for large-scale video generation from prompts; \u003c/li\u003e\u003cli\u003eOur deliberate use of a diverse prompt dataset. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis dataset encompassed a wide range of complexity, from simple to intricate descriptions. Additionally, we attempted to use automatic evaluations, such as assessing video quality based on all video frames and evaluations by large language models (LLMs) that support video. However, due to conflicting results, these methods were omitted from the blog post.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation of state-of-the-art text-to-video models reveals a clear preference hierarchy among Runway ML (Gen-3), Luma Labs, and Pika. Runway ML (Gen-3) emerges as the top performer, securing the first rank in 65.22% of cases, thanks to its high prompt adherence and superior video resolution. However, it still exhibits a notable occurrence of artifacts and errors, suggesting room for enhancement.\u003c/p\u003e\u003cp\u003eLuma Labs, while trailing behind Runway ML, demonstrates moderate performance, particularly in maintaining prompt consistency and video resolution. Its primary weakness lies in generating realistic videos, which is crucial for lifelike content applications. On the other hand, Pika, ranking third, shows the highest need for improvement, especially in minimizing artifacts and enhancing video resolution.\u003c/p\u003e\u003cp\u003eWhile each model has its strengths and weaknesses, Runway ML (Gen-3) stands out for its robust performance across most evaluation criteria, making it the preferred choice for generating high-quality, realistic videos. As the field of text-to-video generation continues to evolve, addressing the identified shortcomings will be key to advancing the capabilities of these models.\u003c/p\u003e\u003cp\u003eBy targeting these key areas, we can drive the next wave of innovations in text-to-video technology, creating more sophisticated and versatile text-to-video systems that cater to a broader range of applications and user needs.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-video models presented here represents a significant advance in assessing AI-generated videos. When combined with Quantumworks Lab's platform, AI teams can accelerate the development and refinement of sophisticated, domain-specific video generation models with greater efficiency and quality through our dataset curation, automated evaluation techniques, and human-in-the-loop QA.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-video model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more. \u003c/p\u003e\u003cp\u003eWe'd love to hear from you and discuss how we can support your AI evaluation needs.\u003c/p\u003e","comment_id":"66b1410331089400019364ee","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.13.57-AM.png","featured":false,"visibility":"public","created_at":"2024-08-05T21:15:47.000+00:00","updated_at":"2024-09-03T20:08:05.000+00:00","published_at":"2024-08-05T22:14:00.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/a-comprehensive-approach-to-evaluating-text-to-video-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66a978d931089400019364a5","uuid":"bbe7d436-1e06-4439-bce3-780f72151bdf","title":"A comprehensive approach to evaluating text-to-image models","slug":"a-comprehensive-approach-to-evaluating-text-to-image-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in image generation evaluation, visit\u0026nbsp;\u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAs text-to-image AI models continue to evolve, it's become increasingly important to develop robust evaluation methods that can assess their performance across multiple dimensions. In this post, we'll explore a comprehensive approach to evaluating 3 leading text-to-image models - \u003cem\u003eDALL-E 3, Stable Diffusion, and Imagen 2 \u003c/em\u003e- using both human preference ratings and automated evaluation techniques.\u003c/p\u003e\u003ch2 id=\"the-rise-of-text-to-image-models\"\u003eThe rise of text-to-image models\u003c/h2\u003e\u003cp\u003eText-to-image generation has seen remarkable progress in recent years. Models like \u003cem\u003eDALL-E 3, Stable Diffusion, and Imagen 2\u003c/em\u003e can now produce strikingly realistic and creative images from natural language descriptions. This technology has many useful applications, from graphic design and content creation to scientific visualization and beyond.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe5C5TuOA7TCKRT_7O_PQ5hnqXPPAlM4jOO4KvRk2DR0y0k9OWNYJzNhGIL5rqanTMlYK9reVzCb_pwx__rvW6rTmRkRljBX6aIjnA3DuEH1L_-ahgR0MnJU9JD-vWdQnDi8pZeoRIvpXD0kskzRf3WdSxH?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"723\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemOVe_M8fg0ddRqLYwN1MaKDcuJtPGMXgkP1hlaXtbjU7ZKsbunWQZMVM34ttGlsa8ulDjWoCxW-KVagWUiNG4xdUc3yCYdWMcEKsC1Pl8da6kyk0UgjjL66-qxaua9sYL4IUNTOSNsyggkztxBhXbIur6?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"733\"\u003e\u003c/figure\u003e\u003cp\u003eAs these models become more advanced, having reliable ways to compare their performance and identify areas for improvement are paramount. Let’s next dive into how we developed a two-fold evaluation approach for getting more granularity into their performance.\u003c/p\u003e\u003ch2 id=\"1-human-preference-evaluation\"\u003e[1] Human preference evaluation\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfTKedTbSWhlq5swtSUlaHwVE--84lNrAPDAGpko3p_AnEz0jooz4-lin5Mpg-SwHG3CJktHiRqSHTCJcuavjLCJOau4-GxczB2Odq4mQOXSaH7ijz1wCWZic_o0B_npX0x5qgxAqiHULmNouzCOv2l8nMy?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"902\"\u003e\u003c/figure\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human labelers per data row, allowing us to tap into a network of expert raters to evaluate image outputs across several key criteria:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAlignment with prompt: \u003c/strong\u003eRaters assessed how well each generated image matched the given text prompt on a scale of high, medium, or low. For example, for the prompt \"A red apple on a wooden table\":\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Image shows a clear, realistic red apple on a wooden table\u003c/li\u003e\u003cli\u003eMedium: Image shows an apple, but it's green or the table isn't clearly wooden\u003c/li\u003e\u003cli\u003eLow: Image shows an unrelated scene\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePhotorealism: \u003c/strong\u003eThis criterion evaluates how closely the image resembled a real photograph:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Realistic lighting, textures, and proportions\u003c/li\u003e\u003cli\u003eMedium: Somewhat realistic but with slight issues in shadows or textures\u003c/li\u003e\u003cli\u003eLow: Cartoonish or artificial appearance\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eDetail: \u003c/strong\u003eRaters then determined the level of detail and overall clarity:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Fine details visible (e.g., individual leaves, fabric textures)\u003c/li\u003e\u003cli\u003eMedium: Major elements clear, but finer details somewhat lacking\u003c/li\u003e\u003cli\u003eLow: Overall blurry or lacking significant detail\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eArtifacts: \u003c/strong\u003eFinally, raters identified any visible artifacts, distortions, or errors, such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnnatural distortions in objects or backgrounds\u003c/li\u003e\u003cli\u003eMisplaced or floating elements\u003c/li\u003e\u003cli\u003eInconsistent lighting or shadows \u003c/li\u003e\u003cli\u003eUnnatural repeating patterns\u003c/li\u003e\u003cli\u003eBlurred or pixelated areas\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"2-automated-evaluations\"\u003e[2] Automated evaluations\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXduxNvNrJswCQvT7DGZfSyEoIGUOLdlCVN1c3UTP_Bbz-s-sL-P246muageVr1aPVyH7DlaXtqkmGS6Lf9fm_MuztOdJAddLr2muQLG7MzHCC4BqHfBLlN9YAO7teoaN1Qma8wTq8kRv53xfh-V1iGRVMqO?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"907\"\u003e\u003c/figure\u003e\u003cp\u003eTo complement human ratings, we implemented several automated evaluation techniques. Here’s an example for one \u003ca href=\"https://storage.googleapis.com/text_image_eval/gen_images/014ac7aa527c953fd0a7aeb08e238dea/results.json?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eImage Quality Score: \u003c/strong\u003eWe calculated an objective image quality score based on several key metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSharpness: Using Laplacian variance to assess image clarity.\u003c/li\u003e\u003cli\u003eContrast: Evaluating the range between minimum and maximum pixel values.\u003c/li\u003e\u003cli\u003eNoise Estimation: Employing a filter-based approach to quantify image noise.\u003c/li\u003e\u003cli\u003eStructural Similarity Index (SSIM): Comparing the image to a slightly blurred version to assess structural integrity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese metrics were combined into a comprehensive quality score in order to provide an objective measure of the image's technical attributes.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePrompt adherence: \u003c/strong\u003eWe utilized the CLIP (Contrastive Language-Image Pre-training) model to measure similarity between the text prompt and the generated image in a shared embedding space. This approach provides an automated assessment of how well the image aligns with the given prompt, offering insights into the model's ability to accurately interpret and visualize textual descriptions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDetailed scoring: \u003c/strong\u003eWe employed Claude, an advanced AI model, to provide detailed scoring and analysis of the generated images. This multifaceted evaluation includes several key components.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cu\u003eElement accuracy scores\u003c/u\u003e: For each key element in the prompt, Claude assesses its presence, provides a description, and assigns an accuracy score out of 10 reflecting how well these elements matched the prompt.\u003c/li\u003e\u003cli\u003e\u003cu\u003eCategory scores\u003c/u\u003e: Claude evaluates images across various categories such as objects, colors, spatial relations, activities, and materials. Each category receives a score out of 10, providing a comprehensive view of the image's content accuracy.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eClaude prompt adherence\u003c/u\u003e: Claude assigns an overall similarity score, expressed as a percentage, indicating how closely the entire image matches the given prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eUnexpected elements and inconsistencies\u003c/u\u003e: Claude identifies any unexpected elements or inconsistencies in the image that do not align with the intended prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eOverall impression:\u003c/u\u003e Claude provides an overall impression of the image, summarizing how well it captures the essence of the prompt. This includes a qualitative assessment of the image's strengths and areas for improvement.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"results\"\u003eResults\u0026nbsp;\u003c/h2\u003e\u003cp\u003eOur evaluation of 100 images across a diverse set of complex prompts, generated by GPT-4, stress-tested and provided valuable insights into the capabilities of Stable Diffusion, DALL-E, and Imagen 2.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHere’s a summary of the human-preference evaluations:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eModel rankings and initial findings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eStable Diffusion ranked first in 50.7% of cases, DALL-E in 38%, and Imagen 2 in 11.3%.\u003c/li\u003e\u003cli\u003eStable Diffusion ranked second (37%), followed by DALL-E (33%) and Imagen 2 (30%)\u003c/li\u003e\u003cli\u003eImagen 2 was ranked third (59%), while DALL-E and Stable Diffusion were ranked last less often (29% and 12% respectively\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ePerformance metrics (as percentages of maximum possible scores):\u003c/p\u003e\u003cul\u003e\u003cli\u003eStable Diffusion: 84.3% prompt alignment, 85.3% photorealism, 91.7% detail/clarity.\u003c/li\u003e\u003cli\u003eDALL-E: 84.3% prompt alignment, 58.3% photorealism, 83.7% detail/clarity.\u003c/li\u003e\u003cli\u003eImagen 2: 61.3% prompt alignment, 74.7% photorealism, 71.3% detail/clarity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eHere’s a summary of the detailed auto evaluation metrics:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eStable Diffusion emerged as a consistent performer across various metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003eClaude’s prompt adherence score: 82.27%. More specifically, attributed to accurate/realistic colors (89.40%) and depicting objects (89.30%)\u003c/li\u003e\u003cli\u003eHowever, image quality (34.98%) was relatively low\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDALL-E excelled in prompt interpretation and visualization:\u003c/p\u003e\u003cul\u003e\u003cli\u003eClaude’s prompt adherence score: 87.04%. More specifically, attributed to depicting objects well (91.60%) and displaying moving activities accurately (81.10%)\u003c/li\u003e\u003cli\u003eStrongest in translating textual descriptions into visual elements\u003c/li\u003e\u003cli\u003eHowever, image quality (30.11%) was lowest\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eImagen 2 performed the worst, but had a higher technical quality for images:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLowest Claude prompt adherence score (76.86%) and aspect accuracy (70.92%)\u003c/li\u003e\u003cli\u003eMuch weaker in moving activities (72.20%) and detailed attributes (77.70%)\u003c/li\u003e\u003cli\u003eHigher image quality (55.55%) than the other two models\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eAnalysis and insights:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"831\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eComparing the auto evaluation metrics to the human preference evaluations reveals some additional interesting findings:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStable Diffusion's balanced performance:\u003c/strong\u003e Stable Diffusion emerged as the top performer overall in human evaluations, ranking first in 50.7% of cases. It showed consistent high scores across human-evaluated metrics, particularly excelling in detail/clarity (91.7%) and photorealism (85.3%). However, the auto evaluation revealed a relatively low image quality score (34.98%), suggesting that technical image quality doesn't always correlate with human perception of quality.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDALL-E's strengths and weaknesses:\u003c/strong\u003e While DALL-E ranked first in 38% of human evaluations, it showed a significant weakness in human-perceived photorealism (58.3%). Interestingly, it had the highest Claude prompt adherence score (87.04%) in the auto evaluation, which aligns with its strong performance in human-evaluated prompt alignment (84.3%). This suggests DALL-E excels at interpreting and executing prompts, but may struggle with realistic rendering.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImagen 2's technical quality vs. human preference:\u003c/strong\u003e Imagen 2 consistently ranked lower in human preferences, struggling particularly with prompt alignment (61.3%). However, it had the highest technical image quality score (55.55%) in the auto evaluation. This discrepancy highlights that technical image quality doesn't necessarily translate to human preference or perceived prompt adherence.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrompt alignment discrepancies\u003c/strong\u003e: While human evaluations showed Stable Diffusion and DALL-E tied in prompt alignment (84.3% each), the auto evaluation gave DALL-E a higher score (87.04%) compared to Stable Diffusion (82.27%). This suggests that human and AI perceptions of prompt adherence may differ slightly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePhotorealism and image quality\u003c/strong\u003e: The human-evaluated photorealism scores don't align with the auto-evaluated image quality scores. Stable Diffusion led in human-perceived photorealism (85.3%) but had low technical image quality (34.98%). Conversely, Imagen 2 had the highest technical image quality (55.55%) but ranked second in human-perceived photorealism (74.7%).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDetail and clarity vs. technical metrics\u003c/strong\u003e: Stable Diffusion stood out in human-evaluated detail and clarity (91.7%), which aligns with its high auto-evaluated scores in depicting objects (89.30%) and accurate colors (89.40%). This suggests a correlation between these technical aspects and human perception of detail and clarity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIt's also important to note that this study didn't include Midjourney due to its Discord-only integration, which made it challenging to implement into our evaluation study. While Midjourney is recognized for its high-quality output, its unconventional access method can be a barrier for users seeking traditional API or web-based interactions. Additionally, Google’s Imagen 2 implements strict safety and content filters across a wide range of topics, which did limit versatility and required additional pre-processing. Such factors, alongside the technical and human-based perceptual metrics evaluated in our study, also influence the overall usability and adoption of AI image generation models in real-world scenarios.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eOur comprehensive evaluation of leading text-to-image models, combining human preference ratings with automated metrics, reveals intriguing contrasts between quantitative performance and human perception. Stable Diffusion emerged as the overall top performer in human evaluations, excelling in detail/clarity and photorealism despite a lower technical image quality score. This underscores the complex relationship between technical metrics and human perception of quality. DALL-E demonstrated strength in prompt interpretation and adherence across both human and automated evaluations, although it showed weakness in human-perceived photorealism. Imagen 2, while scoring highest in technical image quality, consistently ranked lower in human preferences, particularly struggling with prompt alignment.\u003c/p\u003e\u003cp\u003eAs these technologies continue to evolve, our results indicate that each model has distinct strengths and areas for improvement. Stable Diffusion offers balanced performance across various criteria, making it suitable for a wide range of applications. DALL-E excels in prompt interpretation and execution, making it ideal for tasks requiring precise visualization of detailed descriptions. Imagen 2's high technical quality suggests it could be particularly useful in applications where image fidelity is prioritized, although improvements in prompt adherence would enhance its overall performance. Future research should focus on bridging the gap between technical metrics and human perception, as well as addressing specific weaknesses identified in each model, such as DALL-E’s photorealism or Imagen 2’s prompt alignment. By refining these aspects, we can push the boundaries of AI-generated imagery and develop more versatile and capable text-to-image systems that better meet the needs of various applications and user preferences.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003eGet started today\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-image models presented here represents a significant advance in assessing AI-generated images. When combined with Quantumworks Lab's platform, AI teams can accelerate the development and refinement of sophisticated, domain-specific image generation models with greater efficiency and quality through our dataset curation, automated evaluation techniques, and human-in-the-loop QA.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-image model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more. We'd love to hear from you and discuss how we can support your AI evaluation needs.\u003c/p\u003e","comment_id":"66a978d931089400019364a5","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-4.35.13-PM.png","featured":false,"visibility":"public","created_at":"2024-07-30T23:35:53.000+00:00","updated_at":"2024-11-26T00:11:57.000+00:00","published_at":"2024-07-31T00:02:09.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"66a9822f31089400019364c9","name":"Krish Maniar","slug":"krish","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/krish/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"66a9822f31089400019364c9","name":"Krish Maniar","slug":"krish","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/krish/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/a-comprehensive-approach-to-evaluating-text-to-image-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"668c2f59878a56000112a4e0","uuid":"93014cdf-3555-4d2b-b480-2b9db686e2db","title":"Using multimodal chat to enhance a customer’s online support experience","slug":"using-multimodal-chat-to-enhance-a-customers-online-support-experience","html":"\u003cp\u003eCustomer service systems are prime targets for multimodal chat as this new technology leverages an intuitive interface for answering user questions\u0026nbsp; while enabling companies to unlock previously hidden insights about customer journeys and sentiment.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can streamline the process of labeling and managing data for various use cases, including intent classification, dialogue retrieval, state tracking, and model evaluation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. We’ll illustrate the various labeling tasks that Quantumworks Lab can accomplish to ensure the chatbot is intelligent and effective, providing customers with relevant and timely answers.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://files.readme.io/ce6e056-Labelbox_2024-06-04_10-37-061.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"678\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eLabelbox offers a \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/docs/live-multimodal-chat-evaluation?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003enumber of editor types\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, including the live multimodal chat editor.\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"labeling-tasks\"\u003e\u003cstrong\u003eLabeling Tasks\u003c/strong\u003e\u003c/h1\u003e\u003ch2 id=\"multimodal-intent-classification-dialogue-retrieval-and-state-tracking\"\u003e\u003cstrong\u003eMultimodal Intent Classification, Dialogue Retrieval and State Tracking\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xs53lmro56\" title=\"Customer Query Resolution Training - Part 1 - Training Data Collection + Using Classifications Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 1: Generate prompt-response pairs for supervised fine-tuning of agentic workflows.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eLabel the customer’s intent and sentiments in a given turn of the chat, so you can identify the appropriate next steps accordingly.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1427\" height=\"1333\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/first-one.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/first-one.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png 1427w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe customer appears frustrated, so the sentiment is labeled as “frustrated.”\u003c/p\u003e\u003cp\u003eFurthermore, the customer appears to be reporting their order was damaged and is requesting a new one, so both of those requests are labeled using checklist classifications.\u003c/p\u003e\u003cp\u003eWith these labels, you know the customer’s intent (get a new piece) and can also record the current state of the conversation (damage reported but no action taken yet). As the conversation proceeds, you can update the state based on what actions are taken and keep checking the intent and tone of the customer to ensure they are satisfied.\u003c/p\u003e\u003cp\u003eLabelbox allows you to label every aspect of every turn-by-turn conversation, enabling you to develop fine-grained data sets of annotated real-world conversations.\u003c/p\u003e\u003cp\u003eYou can use these labels to train your agent to retrieve similar past interactions and follow similar response patterns or simply be inspired by them.\u003c/p\u003e\u003ch2 id=\"model-evaluation-ranking-and-selection\"\u003e\u003cstrong\u003eModel evaluation, ranking and selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eYou trained a few models offline with the previously generated data using supervised fine-tuning or RLHF. How do you know which of the models is the best performing, and how often do they make mistakes?\u003c/p\u003e\u003cp\u003eLabelbox allows you to connect these models hosted behind your custom endpoints. Then, with the Live Multimodal Chat editor, labelers can interact with all of them simultaneously by feeding prompts, comparing and contrasting responses, and even rewriting the best ones to be even better.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1vpskixih4\" title=\"Customer Query Resolution Training - Part 2 - LLM Development \u0026amp; Evaluation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 2: Evaluate a single model, generate multiple responses and write your own gold standard response.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eWhen evaluating a single model, Quantumworks Lab allows you to generate multiple responses and also add your own response. This is very useful for use cases where all of the model’s responses are lacking in quality and a human-written response provides the gold standard for further downstream training. Furthermore, in the course of the labeling conversation, you can choose one of these responses to be passed in as prior context into later turns, allowing you to choose the path of the conversation in real-time for better training data generation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uzesocscxk\" title=\"Customer Query Resolution Training - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 3: Evaluate multiple models at once in “chat arena” style.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/secone-one-1-1-1-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"450\" height=\"640\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox also supports evaluating multiple models in a “chat arena” style setup.\u003c/p\u003e\u003cp\u003eSay you have trained candidate models and now want to compare them against each other and also foundation models like GPT 4o. Connect your own models with Quantumworks Lab as \u003cem\u003ecustom\u003c/em\u003e models, and easily choose any latest and greatest foundation model from Foundry within Labelbox. The chat arena excludes information about which response was generated from which model so as not to bias the labeling process.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWith Quantumworks Lab's multimodal chat functionalities, you can generate high-quality datasets to train, evaluate, and test your custom LLMs or pit off-the-shelf LLMs like GPT4o and Gemini 1.5 against each other in a Chatbot Arena-style competition to see which one suits your needs best.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/b3f423vxob\" title=\"Customer Query Resolution Training - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 4: Survey of all LLM capabilities in Quantumworks Lab\u003c/em\u003e\u003c/p\u003e\u003cp\u003eAs the landscape of LLMs evolves rapidly, Quantumworks Lab will continue to build functionality to ensure our customers can align models to their use cases quickly and reliably. With differentiated data, streamlined operations, and transparent orchestration, you can build and refine foundation models that meet your specific needs.\u003c/p\u003e\u003ch2 id=\"evaluate-multimodal-chat-data-today-for-free\"\u003eEvaluate multimodal chat data today for free\u003c/h2\u003e\u003cp\u003eReady to deliver on the next-generation of generative AI?\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eSign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more and we’d love to hear from you.\u003c/p\u003e","comment_id":"668c2f59878a56000112a4e0","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/mmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp","featured":false,"visibility":"public","created_at":"2024-07-08T18:26:33.000+00:00","updated_at":"2024-07-10T16:09:14.000+00:00","published_at":"2024-07-09T16:06:15.000+00:00","custom_excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-multimodal-chat-to-enhance-a-customers-online-support-experience/","excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6656135f9132db0001f20551","uuid":"f173905b-1c7a-4f87-8064-ac8af0ccdaa7","title":"AI foundations: Understanding embeddings","slug":"ai-foundations-understanding-embeddings","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eEmbeddings transform complex data into meaningful vector representations, enabling powerful applications across various domains.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis post covers the theoretical foundations, practical examples, and demonstrates how embeddings enhance key use cases at Labelbox.\u003c/p\u003e\u003cp\u003eWhether you’re new to the concept or looking to deepen your understanding, this guide will provide valuable insights into the world of embeddings and their practical uses.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"what-is-an-embedding\"\u003eWhat is an embedding?\u003c/h1\u003e\u003cp\u003eAt its core, an embedding is a vector representation of information. This information can be of any modality—video, audio, text, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe process of generating embeddings involves a deep learning model trained on specific data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach index in a vector represents a numerical value, often a floating-point value between 0 and 1.\u003c/p\u003e\u003ch2 id=\"the-relationship-between-dimensions-and-detail\"\u003eThe relationship between dimensions and detail\u003c/h2\u003e\u003cp\u003eThe dimensions of a vector describe the level of detail it can capture. Higher dimensionality means higher detail and accuracy in the similarity score.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, a vector with only two dimensions is unlikely to store all relevant information, leading to simpler but less accurate representations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1362\" height=\"626\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1362w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"vector-representations-of-images\"\u003eVector representations of images\u003c/h2\u003e\u003cp\u003eFor instance, consider an image. The underlying compression algorithm converts this image into a vector representation, preserving all relevant information. These embeddings can then be used to determine the similarity between different pieces of data. For example, images of a cat and a dog would have different embeddings, resulting in a low similarity score.\u003c/p\u003e\u003cp\u003eAnother example: Let's consider a model trained on images of helicopters, planes, and humans. If we input an image of a helicopter, the model generates a vector representation reflecting this. If the image contains both a helicopter and humans, the representation changes accordingly. This process helps in understanding how embeddings are generated and used.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1240\" height=\"618\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1240w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Helicopter or not helicopter?\" captured via embeddings.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"importance-and-applications-of-embeddings\"\u003eImportance and applications of embeddings\u003c/h1\u003e\u003ch2 id=\"why-embeddings-matter\"\u003eWhy embeddings matter\u0026nbsp;\u003c/h2\u003e\u003cp\u003eAs we’ve described in the prior sections, understanding what embeddings are and how they work is crucial for several reasons:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCapturing Semantic Relationships\u003c/strong\u003e: Embeddings help capture complex relationships within data, enabling more accurate predictions and recommendations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Search Capabilities\u003c/strong\u003e: Embeddings facilitate efficient and accurate similarity searches, essential in applications like search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"applications-of-embeddings\"\u003eApplications of embeddings\u003c/h2\u003e\u003cp\u003eEmbeddings are extremely useful in search problems, especially within recommender systems.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHere are a few examples:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImage Similarity\u003c/strong\u003e: Finding images similar to an input image, like searching for all images of dogs based on a sample image.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eText-to-Image Search\u003c/strong\u003e: Using a textual query, such as \"image of a dog,\" to find relevant images.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContent Recommendations\u003c/strong\u003e: Identifying articles or movies similar to ones you've enjoyed, enhancing search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1296\" height=\"874\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1296w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEmbeddings are the essential ingredient to a smooth and enjoyable family movie night via recommendations systems, which can suggest movies similar to ones you've enjoyed in the past.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"how-embeddings-are-generated\"\u003eHow embeddings are generated\u003c/h1\u003e\u003cp\u003eDeep learning models, trained on either generalized or specific datasets, learn patterns from data to generate embeddings. Models trained on specific datasets, like cat images, are good at identifying different breeds of cats but not dogs. Generalized models, trained on diverse data, can identify various entities.\u003c/p\u003e\u003cp\u003eModern models produce embeddings of over 1024 dimensions, increasing accuracy and detail. However, this also raises challenges related to the cost of embedding generation, storage, and computational resources.\u003c/p\u003e\u003cp\u003eSome of the earliest work with creating embedding models included word2vec for NLP and convolutional neural networks for computer vision.\u003c/p\u003e\u003ch2 id=\"the-significance-of-word2vec\"\u003eThe significance of word2vec\u003c/h2\u003e\u003cp\u003eThe introduction of word2vec by Mikolov et al. in 2013 marked a significant milestone in creating word embeddings. Word2vec enabled the capture of semantic relationships between words, such as the famous king-queen and man-woman analogy, demonstrating how vectors can represent complex relationships.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis breakthrough revolutionized natural language processing (NLP), enhancing tasks like machine translation, sentiment analysis, and information retrieval.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWord2vec laid the foundation for subsequent embedding techniques and deep learning models in NLP.\u003c/p\u003e\u003ch2 id=\"equivalent-of-word2vec-for-images\"\u003eEquivalent of word2vec for Images\u003c/h2\u003e\u003cp\u003eFor images, convolutional neural networks (CNNs) serve as the equivalent of word2vec. Models like AlexNet, VGG, and ResNet revolutionized image processing by creating effective image embeddings.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese models convert images into high-dimensional vectors, preserving spatial hierarchies and semantic information.\u0026nbsp;\u003c/p\u003e\u003cp\u003eJust as word2vec transformed text data into meaningful vectors, CNNs transform images into embeddings that capture essential features, enabling tasks like object detection, image classification, and visual similarity search.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k_icon.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"250\" height=\"250\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eImageNet is a large-scale visual database designed for use in visual object recognition research, containing millions of labeled images across thousands of categories. Models like AlexNet, VGG, and ResNet demonstrated the power of CNNs using datasets like ImageNet.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"uploading-custom-embeddings-to-labelbox\"\u003eUploading custom embeddings to Quantumworks Lab\u0026nbsp;\u0026nbsp;\u003c/h2\u003e\u003cp\u003eLabelbox now supports importing and exporting custom embeddings seamlessly, allowing for better integration into workflows. This new feature provides flexibility in how embeddings are utilized, making it easier to leverage embeddings for various applications.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eimport Quantumworks Lab as lb\nimport transformers\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nimport requests\nimport numpy as np\n\n# Add your API key\nAPI_KEY = \"your_api_key_here\"\nclient = lb.Client(API_KEY)\n\n# Get images from a Quantumworks Lab dataset\nDATASET_ID = \"your_dataset_id_here\"\ndataset = client.get_dataset(DATASET_ID)\nexport_task = dataset.export_v2()\nexport_task.wait_till_done()\n\ndata_row_urls = [dr_url['data_row']['row_data'] for dr_url in export_task.result]\n\n# Get ResNet-50 from HuggingFace\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = transformers.ResNetModel.from_pretrained(\"microsoft/resnet-50\")\n\nimg_emb = []\n\nfor url in data_row_urls:\n    response = requests.get(url, stream=True)\n    image = Image.open(response.raw).convert('RGB').resize((224, 224))\n    img_hf = image_processor(image, return_tensors=\"pt\")\n    with torch.no_grad():\n        last_layer = model(**img_hf, output_hidden_states=True).last_hidden_state\n        resnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\n        resnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\n        img_emb.append(resnet_embeddings.cpu().numpy())\n\ndata_rows = []\n\nfor url, embedding in zip(data_row_urls, img_emb):\n    data_rows.append({\n        \"row_data\": url,\n        \"embeddings\": [{\"embedding_id\": new_custom_embedding_id, \"vector\": embedding[0].tolist()}]\n    })\n\ndataset = client.create_dataset(name='image_custom_embedding_resnet', iam_integration=None)\ntask = dataset.create_data_rows(data_rows)\nprint(task.errors)\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eTo compute your ow embeddings and send them to Quantumworks Lab, you can use models from Hugging Face. Here’s a brief example using ResNet-50:\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003cscript src=\"https://gist.github.com/mmbazel-labelbox/edb8efbab1904106009e1ed630199e29.js\"\u003e\u003c/script\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eThis code snippet demonstrates how to create and upload custom embeddings, which can then be used for similarity searches within Labelbox.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1694\" height=\"1176\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1694w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"why-you-should-care-about-custom-embeddings\"\u003eWhy you should care about custom embeddings\u003c/h3\u003e\u003cp\u003eCustom embeddings provide several advantages:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTailored Representations\u003c/strong\u003e: Custom embeddings can be tailored to your specific dataset, improving accuracy in domain-specific tasks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Performance\u003c/strong\u003e: They allow for more precise similarity searches and recommendations by capturing nuances specific to your data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFlexibility\u003c/strong\u003e: Custom embeddings offer flexibility in handling various types of data and use cases, making them a versatile tool in machine learning workflows.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"using-embeddings-for-better-search\"\u003eUsing embeddings for better search\u003c/h1\u003e\u003cp\u003eEarlier we mentioned that search (a core activity of search engines, recommendation systems, data curation, etc) is a common and important application of embeddings. How? By comparing how similar two (or many more) items are to each other.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"measuring-similarity\"\u003eMeasuring similarity\u003c/h2\u003e\u003cp\u003eThe most popular algorithm for measuring similarity between two vectors is cosine similarity. It calculates the angle between two vectors: the smaller the angle, the more similar they are. Cosine similarity scores range from 0 to 1, or -1 to 1 for dissimilarity.\u003c/p\u003e\u003cp\u003eFor example, if vectors W and V are close, their cosine similarity might be 0.79. If they are opposite, the similarity is lower, indicating dissimilarity. This method helps in visually and computationally understanding the similarity between different assets.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J0ha8suIAyD2qrFVEWI9TqDRLb2c8ForgqTmyvKQ8ZKsXLeHf5H96V0tQSMV_bCbqBsLWcBb3ljxSP8vKYDKElnlTv1zHq36uQG7mwzyP9HELFlHgGf7FTZO8AWH_ndg5OAXm3yLmbbEpVPg1Cvp108\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"718\" height=\"521\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOriginal source: \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Similarity-between-vectors-can-be-estimated-by-calculating-the-cosine-of-the-angle-th_fig1_333734049?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Similarity between vectors can be estimated by calculating the cosine of the angle θ between them.\"\u003c/span\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBecause embeddings are essentially vectors, we can apply many of the same operations used to analyze vectors to embeddings.\u003c/p\u003e\u003ch3 id=\"strategies-for-similarity-computation\"\u003eStrategies for similarity computation\u003c/h3\u003e\u003cp\u003eAdditional strategies for performing similarity computation include:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eBrute Force Search\u003c/strong\u003e: Comparing one asset against every other asset. This provides the highest accuracy but requires significant computational resources and time. For instance, running brute force searches on a dataset with millions of entries can lead to significant delays and high computational costs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eApproximate Nearest Neighbors (ANN)\u003c/strong\u003e: Comparing one asset against a subset of assets. This method reduces computational resources and latency while maintaining high accuracy. ANN algorithms like locality-sensitive hashing (LSH) and KD-trees provide faster search times, making them suitable for real-time applications, although they may sacrifice some accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHashing Methods\u003c/strong\u003e: Techniques like MinHash and SimHash provide efficient similarity searches by hashing data into compact representations, allowing quick comparison of hash values to estimate similarity. These methods are particularly useful for text and document similarity.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTree-based methods (KD-trees, R-trees, and VP-trees) and graph-based methods (like Hierarchical Navigable Small World, also known as HNSW) are also options.\u003c/p\u003e\u003ch2 id=\"leveraging-embeddings-for-data-curation-management-and-analysis-in-labelbox\"\u003eLeveraging embeddings for data curation, management, and analysis in Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThrough search, embeddings improve the efficiency and accuracy of data curation, management, and analysis in several ways:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eAutomated Data Organization\u003c/strong\u003e: Embeddings help group similar data points together, making it easier to organize and manage large datasets.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Search and Retrieval\u003c/strong\u003e: By transforming data into embeddings, search and retrieval operations become faster and more accurate, enabling quick access to relevant information.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Data Analysis\u003c/strong\u003e: Embeddings capture underlying patterns and relationships within data, facilitating more effective analysis and insights extraction.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAnd these are exactly the ways we use embeddings in our products at Quantumworks Lab to \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimprove search for data\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, in \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e, users can find images similar to an input image, such as searching for basketball players on a court. Our natural language search allows users to input a textual query, like \"dog,\" and find relevant images.\u003c/p\u003e\u003cp\u003eLabelbox offers several features to streamline embedding workflows, including:\u003c/p\u003e\u003ch3 id=\"data-curation-and-management\"\u003eData curation and management\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/custom-embeddings?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCustom Embeddings\u003c/u\u003e\u003c/a\u003e in Catalog: Surfaces high-impact data, with automatic computation for various data types.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/manage-selections?ref=labelbox-guides.ghost.io#smart-select\"\u003e\u003cu\u003eSmart Select\u003c/u\u003e\u003c/a\u003e in Catalog: Curates data using random, ordered, and cluster-based methods.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/cluster-view?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCluster View\u003c/u\u003e\u003c/a\u003e: Discovers similar data rows and identifies labeling or model mistakes.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1620\" height=\"838\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1620w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Cluster view to explore relationships between data rows, identify edge cases, and select rows for pre-labeling or human review.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eNatural Language\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSimilarity Search\u003c/u\u003e\u003c/a\u003e: Supports video and audio assets.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-metrics?ref=labelbox-guides.ghost.io#automatic-metrics\"\u003e\u003cu\u003ePrediction-level Custom Metrics\u003c/u\u003e\u003c/a\u003e: Filters and sorts by custom metrics, enhancing model error analysis.\u003c/li\u003e\u003cli\u003ePre-label Generation from \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFoundry\u003c/u\u003e\u003c/a\u003e Models: Streamlines project workflows.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/video-annotations?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnhanced Video Player\u003c/u\u003e\u003c/a\u003e: Aids in curating, evaluating, and visualizing video data.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1630\" height=\"904\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1630w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWould a rose smell as sweet if it was merely one of a thousand easily discovered in a flower dataset using a natural language search? Trick question: you can't smell photos. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"embeddings-schema\"\u003eEmbeddings schema\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/working-with-features?ref=labelbox-guides.ghost.io#create-features-from-the-schema-tab\"\u003e\u003cu\u003eSchema Tab\u003c/u\u003e\u003c/a\u003e: Includes an Embeddings subtab for viewing and creating custom embeddings.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"api-and-inferencing\"\u003eAPI and inferencing\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/run-foundry-apps?ref=labelbox-guides.ghost.io#run-app-using-rest-api\"\u003e\u003cu\u003eInferencing Endpoints\u003c/u\u003e\u003c/a\u003e: Generate predictions via REST API without creating data rows or datasets.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"additional-workflow-enhancements\"\u003eAdditional workflow enhancements\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBoost Express: Request additional labelers for data projects.\u003c/li\u003e\u003cli\u003eExport v2 Workflows and Improved SDK: Streamlines data management.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese features ensure that embeddings are utilized effectively, making your workflows more efficient and your models more accurate.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eEmbeddings are a powerful tool in machine learning, enabling efficient and accurate similarity searches and recommendations. We hope this post has clarified what embeddings are and how they can be utilized.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor more on uploading custom embeddings, check out our \u003ca href=\"https://docs.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/page/tutorials?ref=labelbox-guides.ghost.io\"\u003edeveloper guides\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAdditional resources on embeddings can be found here:\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jbwzrxh814\" title=\"Embeddings Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWatch the video: \u003ca href=\"https://labelbox.wistia.com/medias/jbwzrxh814?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eUnderstanding embeddings in machine learning\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eLearn more in our community about:\u003cu\u003e \u003c/u\u003e\u003ca href=\"https://community.labelbox.com/t/how-to-upload-custom-embedding-s-and-why-you-should-care/1169?ref=labelbox-guides.ghost.io#why-would-i-need-embedding-2\"\u003e\u003cu\u003eHow to upload custom embeddings and why you should care\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThanks to our contributors Tomislav Peharda, Paul Tancre, and Mikiko Bazeley! \u003c/p\u003e","comment_id":"6656135f9132db0001f20551","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/05/YT-Thumbnails--23-.jpg","featured":false,"visibility":"public","created_at":"2024-05-28T17:24:47.000+00:00","updated_at":"2024-09-04T18:02:27.000+00:00","published_at":"2024-05-28T20:32:29.000+00:00","custom_excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},"url":"https://labelbox-guides.ghost.io/ai-foundations-understanding-embeddings/","excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"662804d1d733f0000145b1c4","uuid":"3db0aafe-d6b7-470a-8ff0-37835d066fe4","title":"How to harness AI for efficient video labeling","slug":"harnessing-ai-for-efficient-video-labeling","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eWorking in collaboration with numerous leading companies in artificial intelligence, we're observing a surge in enthusiasm for using advanced models to initially label data, integrating human expertise later to refine and tailor previously labor-intensive and time-consuming tasks.\u003c/p\u003e\u003cp\u003eThese AI models are transforming one of the most daunting tasks in machine learning—the creation of high-quality video datasets. Utilizing such models allows machine learning teams to leverage automated tools to pre-label or enrich data, facilitating a range of applications from monitoring driver behavior to detecting objects in manufacturing environments.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis blog post will explore how models like Gemini 1.5 Pro, Grounding DINO, and SAM are redefining the video labeling landscape, while boosting efficiency and speed. \u003c/p\u003e\u003cp\u003eBy automating the labor-intensive labeling tasks, these models not only accelerate the workflow, but also liberate time for users and decrease labeling costs.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"steps\"\u003eSteps\u003c/h2\u003e\u003ch3 id=\"step-1-select-video\"\u003eStep 1: Select video\u0026nbsp;\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eTo select a video: \u003c/p\u003e\u003cul\u003e\u003cli\u003eNarrow in on a subset of data. Users can use Quantumworks Lab Catalog filters, including media attributes, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eUsers can click “Predict with Foundry” once the data of interest is selected.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-choose-a-model-of-interest\"\u003eStep 2: Choose a model of interest\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eTo select a model: \u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, users will be prompted to choose a model of interest for a model run.\u003c/li\u003e\u003cli\u003eThen, select a model from the ‘model gallery’ based on the type of task - such as video classification, video object detection, and video segmentation.\u003c/li\u003e\u003cli\u003eTo locate a specific model, users can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine-learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-configure-model-settings-and-submit-a-model-run\"\u003eStep 3: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eOnce the model of interest is selected, users can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of settings, which can be found in the Advanced model setting.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can generate preview predictions on up to five data rows to understand how the current model settings affect the final predictions.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows users to confirm the configuration settings confidently:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf users are unhappy with the generated preview predictions, they can edit the model settings and continue to generate preview predictions until they're satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eAfter users are satisfied with the preview predictions, a model run can be submitted.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-send-the-images-to-annotate\"\u003eStep 4: Send the images to Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eUsers can transfer the results to a labeling project using the UI via the \"Send to Annotate\" feature. Labelers can then quickly review labels for accuracy.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"pre-labeling-use-cases\"\u003ePre-labeling Use Cases\u003c/h2\u003e\u003ch3 id=\"example-1-segmentation-mask-using-grounding-dino-sam\"\u003eExample 1: Segmentation mask using Grounding DINO + SAM\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eSegmentation masks\u003c/a\u003e are used for autonomous vehicles, medical imagery, retail applications, face recognition and analysis, video surveillance, satellite image analysis, etc. Masks are some of the most time-consuming annotations to make for video. Below, we see an example of how this can be automated with Grounding DINO + SAM so the reviewers can make small edits if needed instead of starting from scratch.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7k184d0h65\" title=\"Pre-labeling Use Case: Segmentation mask using Grounding DINO + SAM Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"example-2-bounding-box-using-grounding-dino\"\u003eExample 2: Bounding box using Grounding DINO\u003c/h3\u003e\u003cp\u003eBounding boxes are utilized in similar scenarios as segmentation masks, but these scenarios demand less precision than those requiring pixel-level (masks) detail. Bounding boxes can be automated using Grounding DINO, as illustrated below with detection of a person in video.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ie9s2r3zff\" title=\"Pre-labeling Use Case: Bounding Box using Grounding DINO Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"example-3-global-classification-using-gemini-15-pro\"\u003eExample 3: Global classification using Gemini 1.5 Pro\u003c/h3\u003e\u003cp\u003eGlobal classification for video is used when the overall classification for video is required like when a driver safety system needs to detect if a driver is distracted. Gemini 1.5 Pro can analyze an hour long video and provide answers about events that took place in the video. This automation reduces the need for human intervention, allowing personnel to focus on reviewing videos only when they are flagged with specific classifications. \u003c/p\u003e\u003ch3 id=\"example-4-frame-based-classification-using-gemini-15-pro\"\u003eExample 4: Frame based classification using Gemini 1.5 Pro\u003c/h3\u003e\u003cp\u003eFrame-based classification is utilized in scenarios similar to segmentation masks. Gemini 1.5 Pro can analyze an hour-long video and identify the specific timestamps for a particular event. \u003c/p\u003e\u003ch2 id=\"additional-considerations\"\u003e\u003cstrong\u003eAdditional considerations\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAdditional considerations as users incorporate Foundry labels into their projects and workflows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo produce frame based binary classifications in Gemini 1.5 Pro, users are recommended to experiment with the prompt and provide as much context as possible to get the best results. \u003cul\u003e\u003cli\u003eFor example, the following yields better results:\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003c!--kg-card-begin: html--\u003e\n“For the given video, what timestamps have a banana and be as thorough as possible about checking each second for a banana. Make sure there is no overlap in timestamps. Return the result as a dictionary without any new lines. {“banana”: “\u003ctimestamps-with-banana\u003e“, “no_banana”:  “\u003ctimestamps-without-banana\u003e“}” than using “For the given video, what timestamps have a banana. Return the result as a dictionary without any new lines. {“banana”: “\u003ctimestamps-with-banana\u003e” }”\n\u003c!--kg-card-end: html--\u003e\n\u003cul\u003e\u003cli\u003eUsers can also \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA/B test different models \u003c/u\u003e\u003c/a\u003efrom Foundry to find the model that best fits the use case using Quantumworks Lab Model.\u003c/li\u003e\u003cli\u003eIf we do not support your use case or any questions arise, feel free to contact our \u003ca href=\"https://docs.labelbox.com/docs/contacting-customer-support?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esupport team\u003c/u\u003e\u003c/a\u003e, as we would love to hear your feedback to improve Foundry.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAnnotating video data has traditionally been a tedious and time-consuming task. The integration of advanced AI models from Quantumworks Lab Foundry into the video labeling process marks a significant transformation in how video data is annotated. By leveraging Foundry's capabilities, users can drastically speed up their video labeling projects. This acceleration not only diminishes the time required to bring products to market but also substantially reduces the costs involved in model development.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our additional resources on how to utilize state-of-the-art AI models in Foundry, including using \u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003emodel distillation\u003c/a\u003e and fine-tuning to leverage the power of foundation models:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-accelerate-labeling-projects/?ref=labelbox-guides.ghost.io#conclusion\"\u003e\u003cu\u003eHow to accelerate labeling projects using GPT–4V in Foundry\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io#what-are-the-benefits-of-using-image-segmentation-for-my-machine-learning-model\"\u003e\u003cu\u003eHow to create high-quality image segmentation masks quickly and easily\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://event.on24.com/wcc/r/4460171/5D5357846BEAB1FD1E12A5ED146DE6E1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomated Labeling With Any Foundation Model\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFine tune LLMs\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"662804d1d733f0000145b1c4","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/image.png","featured":false,"visibility":"public","created_at":"2024-04-23T18:58:25.000+00:00","updated_at":"2024-11-22T23:53:12.000+00:00","published_at":"2024-04-23T23:40:19.000+00:00","custom_excerpt":"Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/harnessing-ai-for-efficient-video-labeling/","excerpt":"Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"tag":{"slug":"build-ai","id":"653aa45d375d13000123d7de","name":"Build AI","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","count":{"posts":74},"url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"slug":"build-ai","currentPage":1},"__N_SSG":true},"page":"/guides/tag/[id]","query":{"id":"build-ai"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/tag/build-ai/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:31:52 GMT -->
</html>