<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/tag/build-ai/page/2/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:17:48 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../../../static/scripts/munchkin.js"></script><script src="../../../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/guides/tag/%5bid%5d/page/%5bpagenum%5d-da4e9ee1c105845a.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../../../index.html"><img width="106" height="24" alt="logo" src="../../../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../../index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Build AI</a><a href="../../../use-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Use AI</a><a href="../../../explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="../../../label-data-for-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Label data for AI</a><a href="../../../train-fine-tune-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../../../mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../turn-langchain-logs-into-conversational-data-with-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2FFrame-3783.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2FFrame-3783.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2FFrame-3783.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2FFrame-3783.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2FFrame-3783.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2FFrame-3783.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2FFrame-3783.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2FFrame-3783.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexb5d3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2FFrame-3783.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../turn-langchain-logs-into-conversational-data-with-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to turn LangSmith logs into conversational data with Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">Use Quantumworks Lab&#x27;s human &amp; AI evaluation capabilities to turn LangSmith chatbot and conversational agent logs into data.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-improve-search-relevance/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index4a80.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-improve-search-relevance/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to use AI to improve website search relevance</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can dramatically improve search relevance for any website or app by using AI and foundation models to better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-automate-medical-imaging-with-ai/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-15.40.15-1.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-15.40.15-1.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-15.40.15-1.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-15.40.15-1.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-15.40.15-1.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-15.40.15-1.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-15.40.15-1.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-15.40.15-1.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index0813.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-15.40.15-1.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-automate-medical-imaging-with-ai/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to automate medical imaging with AI</p><p class="text-base max-w-2xl undefined line-clamp-3">Walk through an end-to-end tutorial on how your team can use Quantumworks Lab to build powerful models to improve medical imaging detection.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-use-ai-to-automate-invoice-and-document-processing/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-29-at-7.54.26-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-29-at-7.54.26-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-29-at-7.54.26-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-29-at-7.54.26-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-29-at-7.54.26-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-29-at-7.54.26-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-29-at-7.54.26-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-29-at-7.54.26-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexf45d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-29-at-7.54.26-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-use-ai-to-automate-invoice-and-document-processing/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to use AI to automate invoice and document processing</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage Quantumworks Lab’s platform to build an AI model to accelerate high-volume invoice and document processing from PDF documents using OCR.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-speed-up-labeling-with-labeling-functions/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-2.12.03-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-2.12.03-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-2.12.03-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-2.12.03-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-2.12.03-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-2.12.03-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-2.12.03-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-2.12.03-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index95f0.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2FScreenshot-2024-03-05-at-2.12.03-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-speed-up-labeling-with-labeling-functions/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to accelerate and automate data labeling with labeling functions</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how teams can accelerate and automate data labeling by using labeling functions with Labelbox.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../end-to-end-workflow-for-knowledge-distillation-with-nlp/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexd0a1.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../end-to-end-workflow-for-knowledge-distillation-with-nlp/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Distilling a faster and smaller custom LLM using Google Gemini</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-build-equipment-detection-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-13-at-11.58.37-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-13-at-11.58.37-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-13-at-11.58.37-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-13-at-11.58.37-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-13-at-11.58.37-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-13-at-11.58.37-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-13-at-11.58.37-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-13-at-11.58.37-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index208a.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FScreenshot-2024-02-13-at-11.58.37-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-build-equipment-detection-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to build equipment detection models to improve worker safety and efficiency</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve safety detection for personal protective equipment using the latest advances in foundation models to automate labeling. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../end-to-end-workflow-with-model-distillation-for-computer-vision/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index98f9.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../end-to-end-workflow-with-model-distillation-for-computer-vision/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">End-to-end workflow with model distillation for computer vision</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-build-defect-detection-models-to-improve-visual-quality-inspection/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-3.34.17-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-3.34.17-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-3.34.17-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-3.34.17-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-3.34.17-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-3.34.17-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-3.34.17-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-3.34.17-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index80ab.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-3.34.17-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-build-defect-detection-models-to-improve-visual-quality-inspection/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to build defect detection models to automate visual quality inspection</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection using image segmentation for visual inspection.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-build-defect-detection-models-to-improve-preventative-maintenance-2/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-12.44.25-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-12.44.25-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-12.44.25-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-12.44.25-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-12.44.25-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-12.44.25-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-12.44.25-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-12.44.25-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexc00f.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-19-at-12.44.25-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-build-defect-detection-models-to-improve-preventative-maintenance-2/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to build defect detection models to improve predictive maintenance</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we’ll walk through an end-to-end tutorial on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection on pipes.</p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8"><a class="mr-9 text-neutral-700 mb-1" href="../1/index.html">&lt;</a>Page 2 of 8<a class="ml-9 text-neutral-700 mb-1" href="../3/index.html">&gt;</a></div></div></div></div></div></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"661fffd84efc960001cf4cac","uuid":"30ac0bda-fee0-4ffa-93bd-357a123fa51a","title":"How to turn LangSmith logs into conversational data with Quantumworks Lab","slug":"turn-langchain-logs-into-conversational-data-with-labelbox","html":"\u003cp\u003eTo design task-specific LLMs, a step-by-step approach that will improve their day-to-day usability needs to be taken while ensuring safety and relevance and obtaining user feedback. The success of these in real-world scenarios depends on the availability of reliable, high-quality training data and with alignment from human preferences.\u003c/p\u003e\u003cp\u003eLangChain, one of the most popular frameworks for building LLM-powered applications, is complemented by LangSmith, a unified developer platform for building, testing, and monitoring LLM applications. Together, LangChain and LangSmith provide the framework and platform for you to manage the entire LLM-powered application lifecycle.\u003c/p\u003e\u003cp\u003eLabelbox offers a comprehensive data-centric AI platform that includes a native labeling editor, model-assisted labeling, human-labeling workflows, human labeling workforce, and diagnostics to align task-specific models and develop intelligent applications across various data modalities, including text, documents, audio, images, and video, while also providing features like data catalog, model automation, and workforce services to optimize labeling operations and scale human evaluation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.24.59-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1252\" height=\"686\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-22-at-8.24.59-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-22-at-8.24.59-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.24.59-PM.png 1252w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this tutorial guide, we’ll walk through combining Quantumworks Lab and LangSmith by getting conversation data created with \u003ca href=\"https://www.langchain.com/langsmith?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLangSmith\u003c/u\u003e\u003c/a\u003e to \u003ca href=\"https://labelbox.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBelow is also a video guide for more details: \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9qrdqetxgs\" title=\"Langsmith to Quantumworks Lab Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"steps\"\u003eSteps\u0026nbsp;\u003c/h1\u003e\u003ch2 id=\"what-you%E2%80%99ll-need\"\u003eWhat you’ll need\u003c/h2\u003e\u003cul\u003e\u003cli\u003eLabelbox API key\u003c/li\u003e\u003cli\u003eLangSmith API key\u003c/li\u003e\u003cli\u003eOpenAI API\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can also follow-along with this \u003ca href=\"https://colab.research.google.com/drive/1tjiTgL5rgcsfgMDOYfzqMJab0IANFpfO?usp=sharing\u0026ref=labelbox-guides.ghost.io#scrollTo=bJCokRW6PRe6\"\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"setup\"\u003eSetup\u003c/h2\u003e\u003cp\u003eUtilizing \u003ca href=\"https://python.langchain.com/docs/langsmith/walkthrough?ref=labelbox-guides.ghost.io#exporting-datasets-and-runs\"\u003e\u003cu\u003eLangSmith Python SDK\u003c/u\u003e\u003c/a\u003e, you can run a variety of LLMs on a test dataset, and example prompts to evaluate a model’s performance. First, you must create a dataset inside \u003ca href=\"https://smith.langchain.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLangSmith\u003c/u\u003e\u003c/a\u003e. Make sure to keep track of the name you give your dataset and set the type as Chat:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1250\" height=\"622\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-22-at-8.25.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-22-at-8.25.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.33-PM.png 1250w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAfter you create your dataset, add a few example prompts that can be used to evaluate your model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.47-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1252\" height=\"626\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-22-at-8.25.47-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-22-at-8.25.47-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.47-PM.png 1252w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.59-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1250\" height=\"594\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-22-at-8.25.59-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-22-at-8.25.59-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.59-PM.png 1250w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"evaluate-model\"\u003eEvaluate Model\u003c/h2\u003e\u003cp\u003eNow that you are set up, you can evaluate your model with your example prompts. The notebook included in this tutorial goes over this process. We will be using the chain_results after running our model on our dataset. Below is an example of what that would look like:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003echain_results = run_on_dataset(\n    dataset_name=LS_DATASET_NAME,\n    llm_or_chain_factory=functools.partial(\n        create_agent, prompt=prompt, llm_with_tools=llm_with_tools\n    ),\n    evaluation=evaluation_config,\n    verbose=True,\n    client=client,\n    project_name=f\"tools-agent-test-5d466cbc-{unique_id}\",\n    # Project metadata communicates the experiment parameters,\n    # Useful for reviewing the test results\n    project_metadata={\n        \"env\": \"testing-notebook\",\n        \"model\": \"gpt-3.5-turbo\",\n        \"prompt\": \"5d466cbc\",\n    },\n)\u003c/code\u003e\u003c/pre\u003e\u003chr\u003e\u003ch2 id=\"create-labelbox-data-rows\"\u003eCreate Quantumworks Lab Data Rows\u003c/h2\u003e\u003cp\u003eWith the chain results obtained above, you can format to Quantumworks Lab conversation data using the function below. Please see the Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/reference/text-conversational?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eImport Conversation Text Data\u003c/u\u003e\u003c/a\u003e developer guides for more information.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003efrom uuid import uuid4\n\ndef import_conversational (chain_results: dict[str:str], user_id_dict: dict[str:str], output_user_name: str) -\u0026gt; dict[str:str]:\n    \"\"\"Turn chain_result dictionary object from Langchain to conversation data for Quantumworks Lab\n\n    Args:\n        chain_results (dict[str:str]): LangChain evaluation results.\n        user_id_dict (dict[str:str]): Dictionary matching chat type of LangChain(user) to Quantumworks Lab type with corresponding Quantumworks Lab UserID and Chat Alignment.\n                                      {\u0026lt;Langchain user\u0026gt;: {id: \u0026lt;Quantumworks Lab userid\u0026gt;, \"alight\", \u0026lt;Quantumworks Lab alignment (right or left)\u0026gt;}}\n        output_user_name (str): LangChain output user type.\n    Returns:\n        list[\u0026lt;Quantumworks Lab data tows\u0026gt;]\n    \"\"\"\n    lb_conversations = []\n    for conversational in chain_results[\"results\"].values():\n        lb_conversation =   {\n        \"row_data\": {\n            \"type\": \"application/vnd.labelbox.conversational\",\n            \"version\": 1,\n            \"messages\": []\n        },\n        \"global_key\": str(uuid4()),\n        \"media_type\": \"CONVERSATIONAL\",\n        }\n        if \"input\" in conversational[\"output\"]:\n            for input in conversational[\"output\"][\"input\"]:\n                lb_conversation[\"row_data\"][\"messages\"].append({\n                    \"content\": input[\"data\"][\"content\"],\n                    \"user\": {\n                        \"userId\": user_id_dict[input[\"type\"]][\"id\"],\n                        \"name\": input[\"type\"]\n                    },\n                    \"canLabel\": True,\n                    \"align\": user_id_dict[input[\"type\"]][\"align\"],\n                    \"messageId\": str(uuid4())\n                })\n        if \"output\" in conversational[\"output\"]:\n            output = conversational[\"output\"][\"output\"]\n            lb_conversation[\"row_data\"][\"messages\"].append({\n                \"content\": output,\n                \"user\": {\n                    \"userId\": user_id_dict[output_user_name][\"id\"],\n                    \"name\": output_user_name\n                },\n                \"canLabel\": True,\n                \"align\": user_id_dict[output_user_name][\"align\"],\n                \"messageId\": str(uuid4())\n            })\n        lb_conversations.append(lb_conversation)\n    return lb_conversations\u003c/code\u003e\u003c/pre\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe function uses a dictionary python object parameter (user_id_dict) to match LangSmith user types to Quantumworks Lab types along with their alignment inside the editor. The messages in this example are all marked as “canLabel: True.” This means that all messages can be annotated inside our editor. Also, the global key is set randomly through the UUID Python library, but you can have the global_key set to the ID associated with the example run to avoid importing duplicate information.\u003c/p\u003e\u003ch2 id=\"import-data-rows-into-labelbox\"\u003eImport Data Rows into Quantumworks Lab\u003c/h2\u003e\u003cp\u003eNow that you have created your data rows with your conversational data, you can import them into Labelbox. Please reference these \u003ca href=\"https://docs.labelbox.com/reference/dataset?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edocuments\u003c/u\u003e\u003c/a\u003e from Quantumworks Lab for more information. The script below demonstrates creating a Quantumworks Lab dataset and importing your data rows.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e\nimport Quantumworks Lab as lb\n\nclient = lb.Client(api_key=\"\u0026lt;YOUR_API_KEY\u0026gt;\")\n\ndataset = client.create_dataset(name='\u0026lt;dataset_name\u0026gt;')\n\ndataset.create_data_rows(\"\u0026lt;data row payload\u0026gt;\")\u003c/code\u003e\u003c/pre\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"use-cases-and-implication\"\u003eUse Cases and Implication\u003c/h1\u003e\u003cp\u003eUtilizing Quantumworks Lab with LangSmith can better develop chatbots by engaging large language models (LLMs) for conversation intent classification. This combination enables constant model tracking and evaluation because LangSmith and Quantumworks Lab ensure the chatbot performance adheres to human preferences through proper training data. Also, alongside a human-in-the-loop system, semantic search and pre-trained models for pre-labeling allow nuanced analysis.\u003c/p\u003e\u003cp\u003eCombining efforts between Quantumworks Lab and LangSmith enhances the performance of RAG-based agents by \u003ca href=\"https://labelbox.com/guides/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/?ref=labelbox-guides.ghost.io\"\u003eimproving reranking models\u003c/a\u003e and preprocessing documents to make them more relevant. Considering a broader collection of retrieved text or incorporating more human-generated labels rather than relying only on a single document can facilitate the generation of high-quality training labels, hence making the \u003ca href=\"https://labelbox.com/guides/how-to-improve-your-task-specific-chatbot-for-safety-relevancy-and-user-feedback/?ref=labelbox-guides.ghost.io\"\u003echatbot responses more relevant and accurate\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe tutorial above demonstrates the combination of LangSmith with Quantumworks Lab, enabling some of the mentioned use cases.\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOnce you have your data rows inside Quantumworks Lab, you can send them to an \u003ca href=\"https://docs.labelbox.com/docs/what-is-a-project?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eannotate project \u003c/u\u003e\u003c/a\u003ewhere you can apply a variety of \u003ca href=\"https://docs.labelbox.com/docs/conversational-annotations?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eannotations\u003c/u\u003e\u003c/a\u003e or have a popular LLM model generate pre-labels that can then be evaluated with a human in the loop. After annotating your data rows, you can utilize the generated labels to train your LLM further. You can then reassess and annotate your predictions using the same workflow.\u003c/p\u003e\u003cp\u003eIf you’re interested in learning more about the capabilities offered by Quantumworks Lab and LangChain for supporting generative AI development, check out our \u003ca href=\"https://labelbox.com/blog/seamless-llm-human-evaluation-with-langsmith-and-labelbox/?ref=labelbox-guides.ghost.io\"\u003epartnership announcement\u003c/a\u003e for additional entails and use cases.\u003c/p\u003e\u003cp\u003eInterested in joining the conversation? Let us know what you think in our original post \u003ca href=\"https://community.labelbox.com/t/how-to-convert-langchain-results-to-labelbox-conversation-data/1091?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eReach out to the \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox sales team\u003c/u\u003e\u003c/a\u003e to start implementing a hybrid evaluation system for your production generative AI applications.\u003c/p\u003e","comment_id":"661fffd84efc960001cf4cac","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/Frame-3783.png","featured":false,"visibility":"public","created_at":"2024-04-17T16:59:04.000+00:00","updated_at":"2024-09-23T15:46:09.000+00:00","published_at":"2024-04-17T18:24:56.000+00:00","custom_excerpt":"Use Quantumworks Lab's human \u0026 AI evaluation capabilities to turn LangSmith chatbot and conversational agent logs into data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/turn-langchain-logs-into-conversational-data-with-labelbox/","excerpt":"Use Quantumworks Lab's human \u0026 AI evaluation capabilities to turn LangSmith chatbot and conversational agent logs into data.","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66183ccc8d5c4a00014061cc","uuid":"2afa1d4e-85b9-48d3-ae01-81c57db2916b","title":"How to use AI to improve website search relevance","slug":"how-to-improve-search-relevance","html":"\u003cp\u003eWith the latest advances in foundation models, organizations can now enhance search relevance for websites by better matching between user intent with product listings. While companies now have access to a wealth of search queries, sifting through all of these search results can be incredibly time-consuming and resource-intensive. By leveraging AI, teams can now analyze search queries and feedback at scale, to gain insights into common topics or customer sentiment. This allows businesses to identify common themes and pinpoint areas of improvement to enhance\u0026nbsp;their overall website experience to maximize for key metrics such as user retention, conversion and revenue.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for search relevance. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eImproving search relevance requires a vast amount of data in the form of search queries and accurate product descriptions. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape: \u003c/strong\u003eThe changing nature and format of customer review data from multiple sources (e.g webpages, apps, social media, etc.) poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their website search relevance for product descriptions and listings. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s assisted data enrichment and flexible training frameworks to quickly build AI systems that uncover actionable insights from customer searches. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-full\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1746\" height=\"952\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1746w\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to dramatically improve search relevance for any website or app. Specifically, this guide will walk through how you can explore and better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-use-ai-to-improve-search-relevance-for-your-website\"\u003eSee it in action: How to use AI to improve search relevance for your website\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below in either: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1-kTALPqchHXGogUdRVnya7GklKHrsWVL?ref=labelbox-guides.ghost.io#scrollTo=Vw_nNPl8baNJ\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1-kTALPqchHXGogUdRVnya7GklKHrsWVL?ref=labelbox-guides.ghost.io#scrollTo=Vw_nNPl8baNJ\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs customer queries and product descriptions across channels proliferate, brands want to learn from customer feedback to build the most user-friendly experience on their website or app. For this use case, we’ll be working with a dataset of e-commerce website queries – with the goal of analyzing the queries to demonstrate how a company could gain insight into how their customers search for products and how to optimize for relevance.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/vpl1wf0vui\" title=\"Search relevance 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003eFor the purpose of this tutorial, we’ve provided a sample open-source \u003ca href=\"https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe?ref=labelbox-guides.ghost.io\"\u003eKaggle dataset\u003c/a\u003e that can be downloaded.  \u003c/p\u003e\u003cp\u003ePlease \u003ca href=\"https://drive.google.com/file/d/1hh2MYzol6-4PSsqX7mbX2YSLLGA4jAYZ/view?usp=drivesdk\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edownload the dataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your text data as a CSV. \u003c/li\u003e\u003cli\u003eIf your text snippets sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your text data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data-by-clustering\"\u003eSearch and curate data by clustering\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"708\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 1538w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Smart select to cluster data and focus your model improvement on specific data rows\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExplore topics of interest\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith your data in Quantumworks Lab, you can begin to leverage Catalog to uncover interesting topics to get a sense of what customers are searching for. \u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize your data –\u0026nbsp; you can click through individual data rows to get a sense for which queries are the most popular.\u003c/li\u003e\u003cli\u003eDrill into specific topics of interest\u003cstrong\u003e \u003c/strong\u003e– leverage natural language search, for example searching “beds, mirrors, etc,” to bring up all related queries related to that topic. You can adjust the confidence threshold of your searches accordingly which can be helpful in gauging the volume of data related to the topic of interest.\u0026nbsp;\u003c/li\u003e\u003cli\u003eEasily find all instances of similar examples to data of interest.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-2-create-an-initial-model-run-of-search-relevance-assessments\"\u003ePart 2: Create an initial model run of search relevance assessments \u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/t3er59wcfk\" title=\"Search relevance 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can proceed to using Quantumworks Lab's Foundry product to model run an initial model run to accelerate search relevance assessments.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, you'll need to set up your ontology for search relevance assessment based on your project's requirements.\u003c/li\u003e\u003cli\u003eAfterwards, you can define the criteria for rating the relevance of search results to each type of query.\u003c/li\u003e\u003cli\u003eNext, you can communicate the business definition of relevance to the models directly into the prompt. You can use Foundry to add context to the prompt, allowing it to rank results as if it were part of your respective business. In this example, we'll include the prompts for what \"good relevance\", \"excellent relevance\", etc and help the model predict what would fit under this criteria. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAs an illustrative example, you can set up \"excellent relevance\" as a result that perfectly matches the search query, including all specific attributes (category, material, color, purpose, etc). This indicates that the term is exactly what the user is searching for. For the query, \"kitchen blender stainless steel\", a result for \"stainless steel countertop blender\" is highly relevant, matching the user's intended category.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"673\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eGenerate an initial preview to assess how well the adjusted prompt performs and you can save the adjusted prompt as an app, including data type (text), ontology, and the original prompt. This allows for easy re-use and the ability to build upon the saved app for future assessments of search relevance criteria. \u003c/p\u003e\u003cp\u003eAfter this has been set up, you can now generate the next preview to ensure quality before submitting the model run for assessments.\u003c/p\u003e\u003ch3 id=\"view-search-relevance-assessments-results\"\u003eView search relevance assessments results \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gq8nnv2ykw\" title=\"Search relevance 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce your model inferencing job has been completed, you can then navigate to the model tab and locate a variety of foundation models (e.g., Claude 3 will be used in this tutorial) to view the completed model run with your rankings.\u003c/li\u003e\u003cli\u003eOptionally, you can add an explanation classification or review the results of the ranking, which includes all 560 items/data rows.\u003c/li\u003e\u003cli\u003eBy adding the results to your project, you can next perform further analytics such as analyzing the distributions of predictions from the Metrics view.\u003c/li\u003e\u003cli\u003eNext, select all items and you can send them to Annotate, and choose \"search relevance assessment\", where you'll then be able to have humans review as an additional quality check.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"further-analyze-and-optimize-your-search-relevance-assessments-results\"\u003eFurther analyze and optimize your search relevance assessments results \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kw7in8bosa\" title=\"Search relevance 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe last part of the walkthrough is to analyze your distribution of relevance categories within your project, noting varying levels of relevance and to review the query class for all 560 data rows to identify trends in relevance. You can do this by using automated approaches to understand query types and relevance patterns, as we show in the video above.\u003c/li\u003e\u003cli\u003eBy filtering your dataset by the search relevance assessment project, you can navigate to the Analytics view to identify trends and examples of excellent relevance and poor relevance within specific query classes (as shown below).\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"883\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample analytics distribution showing excellent relevance on terms like kids wall decor, sectionals and area rugs.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"838\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample analytics distribution showing poor relevance for beds, furniture cushions, mirrors\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAs this time, you can consider adjusting prompts to accurately reflect relevance criteria, or use metadata fields, such as query class, to further analyze relevance.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo further evaluate and enrich the data, teams can also explore incorporating human supervision in the labeling process, with a hybrid or combination approaches: fully automated, half human in the loop, half automated, or all human-in-the-loop.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can improve your data further in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eAlignerr\u003c/a\u003e: Leverage our global network of\u0026nbsp;specialized labelers for a variety of tasks.\u0026nbsp;This community of subject matter experts from several disciplines align AI models by creating high-quality data in their field of expertise. The community spans nearly every major discipline of sciences, industries and languages, worldwide.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eBy tapping into the most recent developments in foundation models, businesses can transform the effectiveness of website searches by refining the alignment between user intent and product offerings. Given the abundance of search queries that a prospective customer may use, the process of sorting through them manually is labor-intensive and time-consuming. \u003c/p\u003e\u003cp\u003eBy harnessing the power of AI, organizations can efficiently examine search queries and feedback on a large scale, uncovering recurring themes and gauging customer sentiment. \u003c/p\u003e\u003cp\u003eThis enables enterprises to detect prevalent trends and target areas for enhancement, allowing them to optimizing the overall website experience to drive key metrics like user retention, conversion rates, and revenue. Remember to optimize the \u003ca href=\"https://www.web4business.com.au/portfolio-item/the-most-important-24-pages-to-include-on-website/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ewebsite content\u003c/a\u003e as well to ensure it's meeting your end user's goals. Give the walkthrough a try and we also recommend checking out our other solution accelerators such as \u003ca href=\"https://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003epersonalized experiences\u003c/a\u003e for retail to improve customer experiences.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful search relevance websites. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"66183ccc8d5c4a00014061cc","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/thumbnail--6-.png","featured":false,"visibility":"public","created_at":"2024-04-11T19:41:00.000+00:00","updated_at":"2024-09-12T23:45:53.000+00:00","published_at":"2024-04-12T17:02:16.000+00:00","custom_excerpt":"Learn how you can dramatically improve search relevance for any website or app by using AI and foundation models to better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-improve-search-relevance/","excerpt":"Learn how you can dramatically improve search relevance for any website or app by using AI and foundation models to better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65e8dd751ac5200001374088","uuid":"eb8d2ad8-e01c-4e9a-b3ad-0021f50d7241","title":"How to automate medical imaging with AI","slug":"how-to-automate-medical-imaging-with-ai","html":"\u003cp\u003eWith AI-powered detection, you can now easily harness the latest advances in foundation models to help accelerate your medical imaging and life sciences operations. AI is being used for a range of features and use cases - from segmentation of medical scans, detection of abnormalities in organs, or classifying cell nuclei for the purpose of predicting diseases like cancer. As the demand for more intelligent monitoring continues to rise, it's essential for teams to improve imaging efficiency, accuracy, and diagnostic capabilities. Quantumworks Lab empowers the world’s largest organizations to leverage AI solutions tailored to their unique medical detection challenges.\u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for large-scale medical imaging detection. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving medical imaging detection requires a vast amount of data in the form of images, videos, and documents. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e\u0026nbsp;The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e\u0026nbsp;Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers organizations to transform their medical imaging operations through advanced computer vision techniques. Instead of relying on time-consuming manual human review, companies can leverage Quantumworks Lab’s AI-assisted data labeling and flexible training frameworks to quickly build task-specific models that uncover actionable insights.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-9.29.44-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1704\" height=\"918\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-9.29.44-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-9.29.44-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-9.29.44-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-9.29.44-AM.png 1704w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through an end-to-end workflow on how your team can use Quantumworks Lab to build a powerful model to improve medical imaging detection. Specifically, this guide will walk through how you can explore and better understand your visual assets to make more data-driven diagnostics on histopathology images. \u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-automate-medical-imaging-with-ai\"\u003eSee it in action: How to automate medical imaging with AI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/q9qnm01eu1\" title=\"Solution Accelerator - Medical Imaging - Intro \u0026amp; Curation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"496\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across\u0026nbsp;\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab%2520pricing\u0026utm_source=house\u0026utm_medium=email\u0026utm_campaign=mar2024\u0026\u0026landingPageAnonymousId=%22b04806c5-ff2b-4a09-9379-a9837b1801a9%22\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data with Catalog and Foundry\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below via:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://drive.google.com/file/d/1d3CZWl0YmBKxOKzYmjabip5fXtz-V8ke?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1d3CZWl0YmBKxOKzYmjabip5fXtz-V8ke?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eDatabricks Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003e\u003cstrong\u003ePart 1: Explore and prepare your data\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in the\u0026nbsp;\u003ca href=\"https://drive.google.com/file/d/1d3CZWl0YmBKxOKzYmjabip5fXtz-V8ke?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, please make a copy of the notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ll be working with a dataset of\u0026nbsp;histopathology images – with the goal of identifying cancerous regions through visual inspections. These images were taken from the open source PanNuke dataset, first published by researchers at Warwick University. The raw image dataset comes in the form of a numpy array of pixels, and first needs to be converted to jpeg stored as objects on your cloud storage. The steps taken to achieve this can be found in the Python notebooks that accompany this demo. \u003c/p\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003ePlease download the\u0026nbsp;dataset and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your data as a CSV.\u003c/li\u003e\u003cli\u003eIf your images sit as individual files in cloud storage, you can reference the URL of these files through our\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your image data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your medical imaging data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this demo, we'll be using Catalog to find relevant images of histopathology images for our dataset by taking samples of tissue cells that are stained and subsequently analyzed under a microscope - with trained pathologists identifying cancerous regions through visual inspections. AI models are increasingly being used to perform the visual inspection in order to accelerate this manual and time consuming activity, as well as reduce observer variability.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can leverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom the data shown, you can toggle over to the Analytics view within Catalog to understand the distribution of your data better and  looking for the number of images relating to each organ. In this specific data set, you can see that \"breast\" and \"colon\" datarows are the most common, making up roughly 30% and 18% respectively. For the purposes of this walkthrough, you will select the \"colon\" datarows, layer multiple filters to arrive at your relevant data, and save this as a slice for annotation, which we'll be covering next.\u003c/p\u003e\u003ch3 id=\"create-your-annotate-project\"\u003eCreate your Annotate project\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/2fncdldnr4\" title=\"Solution Accelerator - Medical Imaging - Create Annotate Project Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"496\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eNow that you have narrowed down your relevant subset of data, the next step is to and set up your Annotate project, which allows you to begin labeling the key objects or classes about an image - in this case it will be the cell nuclei of interest within each image.\u003c/p\u003e\u003cp\u003eIn order to understand the various nuclei categories that are present across the dataset, you can observe academic papers that accompany the PanNuke dataset - in the case of Colon organ images, we will look to annotate 5 main categories: \u003cem\u003eNeoplastic, Connective, Inflammatory, Epithelial, and Dead Cell \u003c/em\u003enuclei.\u003c/p\u003e\u003cp\u003eAfter identifying what we're interested in detecting, you can proceed to create an ontology. When entering your ontology, you can select the segmentation annotation type and then pass data to the project, and attach your ontology and begin tagging. There are various options for segmentation, from pen, to bounding box, to pen and brush.\u003c/p\u003e\u003ch3 id=\"annotate-your-data-rows\"\u003eAnnotate your data rows\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/j3tcc2ti9m\" title=\"Solution Accelerator - Medical Imaging - Annotating Datarows Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"498\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eNow that you have appropriately configured your annotation project with the relevant data and ontology, you can begin to annotate each of your datarows. To do this, a labeler can navigate to the top right and under “Start”, you can select the option to “Start Labeling”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eDoing so will bring up the Quantumworks Lab annotate UI, where on the right you will have the datarow of interest, in this case an image of colon tissue, and on the left you can see each of the objects (or nuclei type) included in your ontology. Your labelers can use this to begin marking up each of the objects of nuclei types present in the image.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.45.52-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"959\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-06-at-4.45.52-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-06-at-4.45.52-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-06-at-4.45.52-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.45.52-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eIn some situations especially in the medical field, you may wish to have your data rows annotated my subject matter experts such as trained pathologists or other medical professionals which may be from within your organization.\u003c/li\u003e\u003cli\u003eHowever, if you do not have an appropriate labeling workforce or technical expertise within your organization, you can utilize Quantumworks Lab’s \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eBoost Workforce\u003c/a\u003e offering. This program offers customers the ability to source labelers through Quantumworks Lab with specialized knowledge and skillset required to annotate the features of relevance across your dataset.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIt is worth exploring a few of the capabilities available within Annotate in order to perform your task. Quantumworks Lab has AI powered solutions embedded in Annotate, that make use of image embeddings to help in suggesting appropriate segment instances\u003c/p\u003e\u003cp\u003eAfter selecting the object of interest, you can see at the top the various options available:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.44.10-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"988\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-06-at-4.44.10-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-06-at-4.44.10-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-06-at-4.44.10-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.44.10-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003col\u003e\u003cli\u003eThe first is the Hover and Click option, which allows us to move our cursor over various part of the image and identify possible segment instances as you can see on the screen in front of you.\u003c/li\u003e\u003cli\u003eThe second option is the Auto Segment Box solution, where we can draw a bounding box around an object of interest, and leverage the segment anything model to automatically generate appropriate segment masks.\u003c/li\u003e\u003cli\u003eThe third option is Brush. This gives you full control control of the segment area by allow us to highlight an area using a larger brush stroke\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThe effectiveness of the Hover and Click and Segment Anything options will depend on several factors such as the quality and resolution of your images, as well as features in the image such as the definition of object boundaries and brightness of the image. However, in this case we can see that the Hover and Click options leads to good results, so you can go ahead and begin annotating accordingly.\u003c/p\u003e\u003ch3 id=\"reviewing-your-annotations\"\u003eReviewing your annotations\u003c/h3\u003e\u003cp\u003eEventually you will reach a stage where you have a sufficient number of labeled data rows to begin reviewing. In this instance, you can label roughly 1000 data rows in your batch in the first instance. Reviewing can alternatively be an ongoing activity that is run in parallel to your initial labeling, and the frequency of review is completely dependent on your labeling operations preferences or business logic. To begin reviewing the labeling workforces annotations, a reviewer can simply navigate to the top right and select start \"Review\". \u003c/p\u003e\u003cp\u003eThere are a few options available to your reviewers: \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.42.38-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"946\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-06-at-4.42.38-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-06-at-4.42.38-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-06-at-4.42.38-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.42.38-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003col\u003e\u003cli\u003eFirst of all, you could mark an issue or comment and then reject the data rule (as shown in the image above). This would move the data rule back into a rework bucket where a labeler can review it and see any issues attached before taking remedial action and resubmitting for a second review. \u003c/li\u003e\u003cli\u003eAlternatively, a labeler may choose to edit the image as part of the review stage themselves directly. \u003c/li\u003e\u003cli\u003eThe third and final option is simply to review the data row, acknowledge that everything seems appropriate and go ahead and approve. This will then subsequently bring up the next data row for review and the processes repeated until all data rows sit within the done bucket. Once in the done bucket, the data can be extracted and used for downstream exercises as training or fine tuning a custom model or deriving business insights from the annotations.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"part-2-using-model-foundry-to-generate-pre-labels\"\u003ePart 2: Using Model \u0026amp; Foundry to generate pre-labels\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/h7c8z0spyy\" title=\"Solution Accelerator - Medical Imaging - Model \u0026amp; Foundry Pre-labels Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"502\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn Part 1 of this walkthrough, we\u0026nbsp;covered how to curate medical images and set up your annotation project for initial labeling and review. The next part of this guide will cover how you can extract your annotations for evaluating and diagnosing your model. There are various ways that this can be done; from exporting the data directly within the Quantumworks Lab UI to using Quantumworks Lab's API to extract the data directly from your project.\u003c/p\u003e\u003cp\u003eWe'll be covering first how to set up a model experiment within the Quantumworks Lab UI. You can navigate to the model tab and see the tab for \"Experiment\" here. By creating a new experiment in the top right as shown below, you can name it something appropriate and add a description and select next. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.25.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"960\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.25.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.25.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.25.55-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.25.55-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAfterwards, select the ontology that you used in your previous annotate project and then subsequently select the project which your labeled data rows sit within. \u003c/p\u003e\u003cp\u003eYou'll see that Quantumworks Lab handles the test train and validate splits and suggests by default 80/10/10. You can tweak these by using the slide at the bottom or by editing the percentages shown in the text box. For the purpose of this demo, you'll keep this as standard and create your first model run (as shown below). \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-10.19.43-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"936\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-10.19.43-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-10.19.43-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-10.19.43-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-10.19.43-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAfter your data has been successfully passed into a model run, you can observe each of your annotations, and on the left hand side, you can see the splits across train, validate, and test. \u003c/p\u003e\u003cp\u003eYou're now in a position to copy your model run ID, and leverage the accompanying notebook in order to extract the data using Quantumworks Lab's API. The export will contain each data row along with the accompanying annotations and  the data split so you know what to train your model on versus validation and test sets. \u003c/p\u003e\u003cp\u003eYou can follow along in the \u003ca href=\"https://drive.google.com/file/d/1d3CZWl0YmBKxOKzYmjabip5fXtz-V8ke?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003enotebook\u003c/a\u003e provided to see how a YOLO model was trained for the purpose of identifying nuclei. The real advantage of model runs is that after you've trained your model, you can pass the predictions for your dataset back into Quantumworks Lab in order to evaluate your model performance. \u003c/p\u003e\u003cp\u003eAfter uploading your model predictions back to your model run, you can then dive into each data row in turn to observe your ground truth, versus your model predictions as shown below. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.28.32-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"939\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.28.32-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.28.32-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.28.32-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.28.32-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou'll see the ground truth annotations as well as the predictions from your model run, including each instance of your classes. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.29.03-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"758\" height=\"564\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.29.03-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.29.03-AM.png 758w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThis gives you a more granular view of how your model is performing. You can also navigate to the metrics tab in the top right. As shown below, you will see a view of various metrics from precision recall F1, and in the case of segmentation and object detection, intersection over union (IoU). \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-10.17.04-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"994\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-10.17.04-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-10.17.04-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-10.17.04-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-10.17.04-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eWe're now in a position to compare model inferences with your ground-truth annotations to see where the model may be underperforming. A disagreement between model predictions and ground truth labels can be due to a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection and additional labeling.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.30.47-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"979\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.30.47-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.30.47-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.30.47-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.30.47-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn some cases, it may be beneficial to go ahead and select a few of these edge case examples as anchors and then leverage the \"Find Similar\" feature in Quantumworks Lab Catalog. This allows you to identify where your model is underperforming on an ongoing basis, surface relevant data from across your repository that matches that type of data, and then to re-batch this data back to an annotate project for further labeling.\u003c/p\u003e\u003cp\u003eThis allows you to be targeted in the type of data that you're surfacing and to improve your overall training data iteratively. In each case, you can retrain a custom model and within the model tab and in your experiment, you can begin to compare model runs to see each time you retrain, whether you see the intended benefits in accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.32.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"994\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.32.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.32.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.32.34-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.32.34-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAs a next step, you can now begin to narrow down your data that was not already annotated in your project, and begin to layer up various filters and filter by colon tissue. You can also narrow down to a subset of our data that may be useful in order to batch label for annotation. \u003c/p\u003e\u003cp\u003eYou can also leverage the first iteration of your model in order to accelerate this process, and deploy your model within Quantumworks Lab's Foundry and use this to generate prelabels, which can be passed to Annotate for a labeler to review and to refine if necessary.\u003c/p\u003e\u003cp\u003e\u003cu\u003eNote\u003c/u\u003e: In this walkthrough, we've already deployed the custom model to Foundry. If you have any custom models within your organization that you're interested in deploying into your workspace, please reach out to your Quantumworks Lab support team.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.36.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1222\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.36.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.36.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.36.55-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.36.55-AM.png 2318w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this case, go ahead and select the sample of four hundred and select \"Predict with Foundry\" as shown above. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.37.33-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"564\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.37.33-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.37.33-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.37.33-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.37.33-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou'll see the custom YOLO-V8 model that we have trained as part of the accompanying notebook. This allows you to proceed to the model run UI where you can configure your ontology, as well as various parameters of our model run. You can also save this as an app for repeatability in the future, or you can generate previews on a sample of five or less to see how your model is performing as shown below. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.39.46-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1182\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.39.46-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.39.46-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.39.46-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.39.46-AM.png 2312w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this case, we're fairly confident that the model will perform as expected for this particular iteration, so you can go ahead and submit this as a full model run.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.41.08-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1205\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.41.08-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.41.08-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.41.08-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.41.08-AM.png 2308w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou'll need to wait a few moments before this is complete. Once your model run is complete, you can navigate back to your dataset within Catalog, and filter for your model run which narrows down to a subset of your data for which nuclei were detected (in this case roughly three hundred). Next, select all of these data rows and you can leverage the \"Send to Annotate\" feature to include these model predictions.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.42.58-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"944\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.42.58-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.42.58-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.42.58-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.42.58-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eA reminder here to do a bit of mapping to ensure that the ontology is properly mapped. To confirm, you'll get a green confirmation and see your model predictions match up with the ontology that's assigned to your project. \u003c/p\u003e\u003cp\u003eAfterwards, you can select the stage of your project that you want to send this to, whether it's straight to an initial review task or an initial labeling task. In this case, you'll want your labeling workforce to both validate and add where any nuclei that may have been missed. You can then navigate back to this particular project, and after a few moments, you can refresh and see your data rows passed into your project.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.45.52-AM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"978\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.45.52-AM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.45.52-AM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.45.52-AM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.45.52-AM-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce your data rows have been successfully passed to your project, they now sit within the \"to label\" tab. When a labeler steps into the labeling UI, they will now see the model predictions, as shown above, outlined as pre-annotations. You can add additional segmentation for any nuclei that have been missed by your initial model. \u003c/p\u003e\u003cp\u003eOnce appropriately annotated, you can go ahead and click \"submit\" and you'll now  have all your pre labels showing over several nuclei of interest which can be simply validated, as shown below. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.48.35-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"971\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.48.35-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.48.35-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.48.35-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.48.35-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThis wraps up part 2 of our guide on how you can leverage Quantumworks Lab Model to evaluate each of your model iterations, as well as speed up each iteration via Quantumworks Lab Foundry to accelerate subsequent batches of data labeling.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eBy automating the ability to analyze high-volumes of medical images with AI, Quantumworks Lab provides valuable human-in-the-loop insights for a variety of diagnostics and detection use cases. This gives leading healthcare and life sciences organizations the ability to dramatically improve medical imaging for better efficiency, accuracy, and diagnostic capabilities.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful computer vision models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab%2520pricing\u0026utm_source=house\u0026utm_medium=email\u0026utm_campaign=mar2024\u0026\u0026landingPageAnonymousId=%22b04806c5-ff2b-4a09-9379-a9837b1801a9%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"65e8dd751ac5200001374088","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-15.40.15-1.png","featured":false,"visibility":"public","created_at":"2024-03-06T21:17:41.000+00:00","updated_at":"2024-03-13T17:03:02.000+00:00","published_at":"2024-03-11T18:56:19.000+00:00","custom_excerpt":"Walk through an end-to-end tutorial on how your team can use Quantumworks Lab to build powerful models to improve medical imaging detection.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa615375d13000123d7fc","name":"Industry: Healthcare \u0026 life science","slug":"industry-healthcare-life-science","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-healthcare-life-science/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-automate-medical-imaging-with-ai/","excerpt":"Walk through an end-to-end tutorial on how your team can use Quantumworks Lab to build powerful models to improve medical imaging detection.","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65df9c591ac5200001373e38","uuid":"8d95e2bd-c28d-4dfa-9adf-a74623c1fb4b","title":"How to use AI to automate invoice and document processing","slug":"how-to-use-ai-to-automate-invoice-and-document-processing","html":"\u003cp\u003eWith AI-powered invoice and document processing, you can now seamlessly integrate the latest advances in foundation models into your core financial and administrative operations. As the demand for better monitoring, reporting and compliance continues to rise, it's essential for teams to ensure accurate, timely, and organized handling of financial transactions. Quantumworks Lab empowers the world’s largest financial services organizations to leverage AI solutions tailored to their unique invoice and document processing challenges.\u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for large-scale invoice \u0026amp; document processing. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving invoice and document processing via OCR analysis requires a vast amount of data in the form of document PDFs and images. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e\u0026nbsp;The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e\u0026nbsp;Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their invoice and document processing through advanced computer vision and OCR techniques. Instead of relying on time-consuming manual human review, companies can leverage Quantumworks Lab’s AI-assisted data enrichment and flexible training frameworks to quickly build task-specific models that uncover actionable insights for faster processing.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-29-at-7.53.46-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1906\" height=\"1060\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-29-at-7.53.46-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-29-at-7.53.46-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-29-at-7.53.46-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-29-at-7.53.46-AM.png 1906w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build an AI model to improve invoice and document processing from images. Specifically, this guide will walk through how you can explore and better understand unstructured data to make more data-driven business decisions.\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-use-ai-to-automate-invoice-and-document-processing\"\u003eSee it in action: How to use AI to automate invoice and document processing\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gtbfqcy3j2\" title=\"Mark demo 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across\u0026nbsp;\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and label your data with Catalog and Foundry\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003e\u003cstrong\u003ePart 1: Explore and prepare your data\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/4j3emzidh0\" title=\"Mark demo 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ll be working with a dataset of image invoices – with the goal of quickly curating data, and using OCR to understand where the text is and to identify what information these invoices contain while finding and correcting model errors. This workflow is very popular with Quantumworks Lab users because it allows teams to have a model do most of the work, while humans (aka subject matter experts) will be able to focus on the task of correcting the model, thereby reducing the amount of manual work.\u003c/p\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you will see your image data rendered in Quantumworks Lab Catalog. You can browse through the invoice dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cp\u003eYou’ll now be able to see your invoice dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore image invoices, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"using-foundry-to-pre-label-invoices\"\u003eUsing Foundry to pre-label invoices\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/i1ujgt5ofc\" title=\"Mark demo 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn this next step, we'll walk through how you can take a human-in-the-loop approach to iterate or modify pre-labels and speed up the annotation process.  \u003c/p\u003e\u003cp\u003eModel Foundry enables teams to choose from a library of models and in this case, we'll be using Amazon's Textract to generate previews and attach them as pre-labels.\u003c/p\u003e\u003cp\u003eWith\u0026nbsp;\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, you can automate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/p\u003e\u003ch3 id=\"send-to-your-annotation-project-for-human-review\"\u003eSend to your Annotation project for human review\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/e71d5d038r\" title=\"Mark demo 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe next step in order to send our annotation project for human review is to to set up your ontology. In this case, we'll call it \"test OCR\" and we'll be using bounding boxes on our images. Note that you can reuse the ontology that you've created previously or you can create a new one. \u003c/p\u003e\u003cp\u003eIn this case, we want a bounding box for text, as well as a sub-classification for the value with the goal of identifying where the text is for bounding boxes. \u003c/p\u003e\u003cp\u003eWe can now include the model predictions that we just completed, and if we're confident that the model is performing well, we can set it to an initial labeling task or as an initial review task. A labeling task means that the labeler will be able to adjust and modify before it goes to a reviewer, and your reviewer will be able to just reject or accept the labels. \u003c/p\u003e\u003cp\u003eIn this case, we have chosen to use Amazon Textract but there are a variety of OCR-specific models that are available within Foundry. Alternatively, we can choose to use your own custom model for OCR invoice detection. The benefits of this approach is that it will allow you to run predictions using your custom model as an end-to-end workflow and more quickly classify parts of interest.\u003c/p\u003e\u003cp\u003e\u003cu\u003eNote\u003c/u\u003e: If this is interesting and you're looking to adopt this method within Quantumworks Lab, please reach out to our support team as we would be happy to assist with deploying your custom model within Foundry.\u003c/p\u003e\u003ch3 id=\"human-in-the-loop-review-for-ocr-invoices\"\u003eHuman-in-the-loop review for OCR invoices\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y7czt6semk\" title=\"Mark demo 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce the initial comparison task is completed, our labelers can now start labeling and we can see how the Amazon Textract model performs on these image invoices with a human-in-the loop come workflow to correct labels for any mistakes. From this example, we can see that the model seems to be performing well so that we can submit these labels for further review and QA. \u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-invoice-processing-model-effectiveness\"\u003eEvaluate and diagnose invoice processing model effectiveness\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/eu7q715hun\" title=\"Mark demo 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe final step involves comparing or A/B testing different OCR models to see which one is the best fit for our specific use case. In this case, we'll be comparing Amazon's Textract with the Tesseract OCR model. A disagreement between model predictions and ground truth labels can be due to a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eVisually compare ground truth labels (in green) to the model predictions (in red).\u003c/li\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model for 10x faster corner-case detection – detect and visualize corner-cases where the model is underperforming. For example, you can drill into cases where ‘empty’ objects are not predicted, where the model might have difficulty identifying specific fields in the image.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\u003cp\u003eBy analyzing high-volumes of documents or images, Quantumworks Lab provides valuable human-in-the-loop insights for invoice and document processing to ensure enable financial services and insurance companies to make data-driven decisions that improve operational efficiency, compliance and revenue.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful task-specific models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"65df9c591ac5200001373e38","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-29-at-7.54.26-AM.png","featured":false,"visibility":"public","created_at":"2024-02-28T20:49:29.000+00:00","updated_at":"2024-02-29T15:58:22.000+00:00","published_at":"2024-02-23T22:24:00.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab’s platform to build an AI model to accelerate high-volume invoice and document processing from PDF documents using OCR.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa608375d13000123d7fa","name":"Industry: Finance \u0026 insurance","slug":"industry-finance-insurance","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-finance-insurance/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},"url":"https://labelbox-guides.ghost.io/how-to-use-ai-to-automate-invoice-and-document-processing/","excerpt":"Learn how to leverage Quantumworks Lab’s platform to build an AI model to accelerate high-volume invoice and document processing from PDF documents using OCR.","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65e754531ac5200001373f08","uuid":"bec8483a-e1a5-4aed-aefb-77189e33fdea","title":"How to accelerate and automate data labeling with labeling functions","slug":"how-to-speed-up-labeling-with-labeling-functions","html":"\u003cp\u003eWhen it comes to data labeling and annotation, an approach that teams like to evaluate is the efficacy of using programmatic labeling via labeling functions to speed up their labeling operations. You can think of labeling functions as a set of rules or instructions that you follow in order to help automatically assign labels or categories to your data. This is especially useful when working with large datasets where manually labeling each piece of data could be resource or time-intensive. In this guide, we'll cover two specific workflows around how teams can perform labeling functions within Labelbox.\u003c/p\u003e\u003cp\u003eWith recent advances in foundation models, teams can now incorporate models such as GPT, Gemini, Claude, etc to kickstart a zero-shot or rules-based approach for labeling at scale. This can work well for regular expressions to identify phone numbers, zip codes, currencies, etc. Without having to train any models from scratch, you're able to simply call an API and have it complete many of these tasks. However, the nature of these generative AI models is that there may still be prone to hallucinations or there may be a desire to include custom business level logic that you may want to supplement in your labeling workflows. Generative AI models are also not inherently meant to address rules-based approaches as typically found in custom business level logic or reg expression. In these situations, having the ability to leverage custom labeling functions is needed.\u003c/p\u003e\u003cp\u003eLet's get started and you can follow along using the Colab notebook \u003ca href=\"https://colab.research.google.com/drive/1pdtB2kuUypYWu1Q8Ot89C-HR1sGkPwN4?usp=sharing\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"part-1-an-sdk-approach-to-creating-labeling-functions\"\u003ePart 1: An SDK-Approach to Creating Labeling Functions\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/5kud55wo9p\" title=\"Kushal labeling functions vid 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou can follow this guide via text or watch the video walkthrough above. To get started, imagine you have a dataset of customer comments for product reviews, and your goal is to run sentiment analysis in order to identify positive, negative or neutral emotions across different comments.  \u003c/p\u003e\u003cp\u003e1) With an SDK-approach, you can first navigate over to your coding environment, and install the appropriate set up steps shown in the notebook \u003ca href=\"https://colab.research.google.com/drive/1pdtB2kuUypYWu1Q8Ot89C-HR1sGkPwN4?usp=sharing\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eprovided\u003c/a\u003e. The first step is to come up with several different rules or labeling functions based on keywords, phrases, heuristics, or knowledge sources that are commonly associated or attributed to your different sentiment categories. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.00.35-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1956\" height=\"928\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.00.35-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.00.35-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.00.35-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.00.35-AM.png 1956w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e2)  An example of a rule that you could use is if your comments data set contains the words \"happy\" or \"excellent\" or \"delighted\". You can use these words as the anchor for your keywords and assign a positive label. Similarly, if the text contains any of the negative words that serves as the basis for negative keywords, then you can  assign this as a negative label. \u003c/p\u003e\u003cp\u003e3) Note that there may be times when a comment is ambiguous or contains both positive and negative words. As an example, this could be something like \"I love how this product looks but I hate how it works\". In such cases, labeling functions can be designed with more nuanced rules to decide the overall sentiment such as considering the context or the number of positive versus negative words. In the example shown, we've used a library like \u003ca href=\"https://textblob.readthedocs.io/en/dev/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eTextBlob\u003c/a\u003e to help with defining several of these labeling functions and returning true if the text meets the criteria and false otherwise. \u003c/p\u003e\u003cp\u003e4) Now that you have all of your initial labeling functions, the next step is to aggregate the outputs from these labeling functions in order to make a more accurate and reliable prediction about the text sentiment. You can choose to use  a variety of aggregation approaches, whether that's majority voting, weighted voting or any other voting based technique.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.00.12-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"871\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.00.12-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.00.12-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.00.12-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.00.12-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe process whereby you're taking several noisy or weak labels and aggregating them to a strong label using an aggregation technique is commonly referred to as \"weak supervision\". Each labeling function can be thought of as an \"expert\" that provides their opinion on how a data point should be labeled, but these experts may not always be right. \u003c/p\u003e\u003cp\u003e5) The goal is to amortize the cost of these potentially noisy or weaker labels by coming up with a strong label. In the example shown, if you were to go with a majority voting-based approach, the idea is that we can define our classes and our aggregation function through a tally up score for your different labels. By going ahead and doing that for positive, negative, or neutral sentiments, you're taking the max score and the max label associated with the max score and assigning that as your final sentiment value. By applying this on you sample piece of text, it returns positive as expected. \u003c/p\u003e\u003cp\u003e6) The next step is to iterate through all of your text assets in your datasets. Using your aggregation function turns your predictions for the sentiment using the rules-based functions that we had seen above, reconstructing the Python annotation and you can use this as the sentiment schema in your ontology, collect your labels, and import it as a label import job to upcert all of these labels into Quantumworks Lab . \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.55.36-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1758\" height=\"1130\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-10.55.36-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-10.55.36-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-10.55.36-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.55.36-AM.png 1758w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this example shown above, we are defining metadata fields for whether or not the text contains a phone number, whether or not the text contains a hashtag, and define your labeling functions. If it meets that criteria then you'll want to set the metadata value to be whatever is returned from that labeling function.  Go ahead and bulk upcert those metadata fields. And once this is all done, you can navigate over to the Quantumworks Lab UI. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.54.08-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"759\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-10.54.08-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-10.54.08-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-10.54.08-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.54.08-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e7) By clicking on the Analytics tab,  you can now see all of your sentiments labels. These are the labels that are a result of running the labeling functions in the notebook. You'll see that roughly 46% of the text has been labeled as neutral,  36% percent is positive, 17% percent is negative. If you want to click into one of the specific classes, you can see the sentiment for negative and there's roughly 10,000 records. Similarly, if you wanted to observe what the metadata distribution looks like, you can see that for the phone numbers and for the hashtags. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.27.32-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1178\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-10.27.32-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-10.27.32-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-10.27.32-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.27.32-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e8) By selecting all the text that contains hashtags, you see around one hundred and sixty four results. This SDK-approach enables teams to quickly labels large datasets, saving time and effort. You can easily add, remove, or adjust rules as you discover new patterns in how sentiments are expressed, especially in instances where over time, you'll observe that there's variations in the way that vernacular or jargon or slang is being used across your text. \u003c/p\u003e\u003cp\u003eThis wraps up the first part of the guide on how you can use labeling functions within the SDK for bulk labeling data and bring that into the Quantumworks Lab UI. In the next part, let's cover how you can perform rules-based processes within the Quantumworks Lab UI for faster annotation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.53.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"830\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-10.53.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-10.53.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-10.53.34-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.53.34-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"part-2-using-the-labelbox-ui-to-create-labeling-functions\"\u003ePart 2: Using the Quantumworks Lab UI to Create Labeling Functions\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/d618qv9z47\" title=\"Kushal Labeling functions vid 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eNow that you've seen how to leverage labeling functions by using the SDK to bulk label your data, let's walk through you can accomplish a similar workflow directly from the Quantumworks Lab UI.\u003c/p\u003e\u003cp\u003e1) One of the first features you can take advantage of in Quantumworks Lab \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eCatalog\u003c/a\u003e is the find text feature, which allows a Quantumworks Lab user to search raw text occurrences of a specific word or sequence of words across their data set. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.42.45-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1256\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.42.45-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.42.45-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.42.45-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.42.45-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAs an example, if you want to find all occurrences of the word \"happy\", you can use this feature to see all of your data rows that contain the words \"happy\". From here, you can save this as a slice and name this as a \"happy\" raw text search and hit save. Any time new data gets added that meets the filtering criteria, it will automatically get added to the slice. This is equivalent to writing a similar Python function that does a substring match or raw text search to match the specific word. \u003c/p\u003e\u003cp\u003e2) You can also take advantage of Similarity Search and select a subset of these data rows and click \"similar to selection\". Quantumworks Lab leverages built-in embeddings that get automatically generated. Instead of looking just for the raw text of \"happy\", you can look for text that has the overall theme or structure of happiness. You can select all of these data rows and toggle by confidence level to filter by an even finer granularity.\u003c/p\u003e\u003cp\u003e3) Next, you can add a pre label or metadata. As an example, let's add metadata by selecting that option, and selecting the emotion category for \"happy\" and hit save. This will apply that metadata to all of my data rows in bulk. Similarly, if you wanted to add additional classifications, you could do that as well. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUse Aggregation Functions to Group Weak Labels into a Strong Label\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOne of the other things shown in Part 1 of the SDK demo was how to leverage aggregation functions to group together several weaker labels into a strong label. Let's cover how you can do this all directly within the Quantumworks Lab UI.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.47.27-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"828\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.47.27-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.47.27-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.47.27-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.47.27-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) First, curate a subset of text records that contain hashtags and emojis with the goal of getting labels for whether or not each of these text records contain hashtags and emojis. Select all of these data rows and hit \"Predict with Foundry\". \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.44.46-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"731\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.44.46-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.44.46-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.44.46-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.44.46-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e2) Next, choose a foundation model of choice (e.g., GPT-4 in this case) and connect that to an ontology, and hit generate preview. Once this is done, GPT-4 will come back with predictions on whether or not the text contains hashtags and emojis. This approach will work with many advanced foundation models including Google  Gemini so let's compare, and you will see that Gemini also has returned with a different set of predictions. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.47.05-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.47.05-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.47.05-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.47.05-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.47.05-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e3) Finally, navigate over to your data set and deselect all of your data rows, and select the different models that you just ran (GPT-4, Gemini, etc). If either of these two models have predicted that it contains hashtag as true. If Gemini has said yes and a GPT four has said yes, and if you notice that your metadata value that was imported earlier from the SDK, also says that it has a hashtag, then you'll have 3 different signals that are telling us this contains a hashtag. By providing 3 different weak labels that you feel pretty confident about, you are now able to aggregate that to be your strong label and use these signals to improve your overall data annotation process. To complete the project, go ahead and select your data, hit classification, choose your project, and set the \"has hashtag\" as \"yes\" and then hit \"Submit\". To wrap up, this approach shows how you can leverage weak labels and weak supervision to come up with a strong label for your data directly by using the Quantumworks Lab UI.\u003c/p\u003e\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\u003cp\u003eIn this guide, we walked through two approaches for creating labeling functions with Quantumworks Lab for speeding up data labeling and annotation. By embracing programmatic labeling approaches through the utilization of labeling functions, teams can enhance the efficiency of their overall labeling operations for a variety of verticals especially in domains such as retail/e-commerce, media and internet, and more.  Give the solution a try using the \u003ca href=\"https://colab.research.google.com/drive/1pdtB2kuUypYWu1Q8Ot89C-HR1sGkPwN4?usp=sharing\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003enotebook\u003c/a\u003e provided, and we'd love to hear your feedback or ideas on how we can help you improve your data annotation workflows via labeling functions.\u003c/p\u003e","comment_id":"65e754531ac5200001373f08","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-2.12.03-PM.png","featured":false,"visibility":"public","created_at":"2024-03-05T17:20:19.000+00:00","updated_at":"2024-03-06T19:30:08.000+00:00","published_at":"2024-02-21T19:54:00.000+00:00","custom_excerpt":"Learn how teams can accelerate and automate data labeling by using labeling functions with Labelbox.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/how-to-speed-up-labeling-with-labeling-functions/","excerpt":"Learn how teams can accelerate and automate data labeling by using labeling functions with Labelbox.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65c97ca417c84d0001395ccb","uuid":"ed465aa9-c5cf-4c1e-bf18-fa38cc037a18","title":"Distilling a faster and smaller custom LLM using Google Gemini","slug":"end-to-end-workflow-for-knowledge-distillation-with-nlp","html":"\u003cp\u003eThe race to both mimic and create competitor models to OpenAI’s GPT3.5 energized the interest in model compression and quantization techniques.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKnowledge distillation, \u003c/strong\u003ealso known as \u003cstrong\u003emodel distillation,\u003c/strong\u003e is one of many techniques that have grown in popularity and importance in enabling small teams to leverage foundation models in developing small (but mighty) custom models, used in intelligent applications.\u003c/p\u003e\u003cp\u003eIn “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, we illustrated some of the conceptual foundations for how model distillation works as well as why we even need smaller models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe also provided an in-depth guide with a worked example in the second part of our series,\u0026nbsp;“\u003ca href=\"https://labelbox.com/guides/end-to-end-workflow-with-model-distillation-for-computer-vision/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnd-to-end workflow with model distillation for computer vision\u003c/u\u003e\u003c/a\u003e”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eNow we turn our attention to demonstrating the flexibility and power of model distillation in another domain and use case, where increased efficiency through supervised training of a smaller model by a foundation model is necessary.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll demonstrate an end-to-end workflow for natural language processing, using model distillation to fine-tune a BERT model with labels created in Model Foundry using \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGoogle Gemini\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll show how easy it is to go from raw data to cutting-edge models, customized to your use case, using a \u003ca href=\"https://huggingface.co/datasets/SetFit/emotion?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esentiment dataset\u003c/u\u003e\u003c/a\u003e (additional public datasets can be found \u003ca href=\"https://labelbox.com/datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e or on sites like Kaggle or HuggingFace).\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn less than 30 min you’ll learn how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIngest\u003c/strong\u003e, \u003cstrong\u003eexplore\u003c/strong\u003e and \u003cstrong\u003eprepare\u003c/strong\u003e the sentiment dataset;\u003c/li\u003e\u003cli\u003ePick and configure \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany text-based foundation model\u003c/u\u003e\u003c/a\u003e to \u003cstrong\u003eautomatically label\u003c/strong\u003e data with just a few clicks;\u003c/li\u003e\u003cli\u003eExport the labeled predictions to a project as a potential set-up for \u003cstrong\u003emanual evaluation\u003c/strong\u003e;\u003c/li\u003e\u003cli\u003eUse the labeled predictions dataset to \u003cstrong\u003efine-tune a student model\u003c/strong\u003e using a cloud-based notebook provider;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEvaluate performance\u003c/strong\u003e of the fine-tuned student model.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAt the end of the tutorial we’ll also discuss advanced considerations in scaling your models up and out, such as automating data ingestion and labeling, and resources for incorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e\u0026nbsp; into your workflows.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-use-model-distillation-to-fine-tune-a-small-task-specific-language-model\"\u003eSee it in action: How to use model distillation to fine-tune a small, task-specific Language Model\u003c/h3\u003e\u003cp\u003e\u003cem\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab\u0026gclid=Cj0KCQiA7OqrBhD9ARIsAK3UXh1OL7LeJadsNvbP_43MnwLSpLlRPD0IkilIwyvf3VwKOzxQ5ioGon8aApXREALw_wcB\u0026referrer_url=https://www.google.com/\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial. You’ll also need to \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003ecreate API keys for accessing the SDK\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eNotebook: \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=1FMXvdze28AZ\"\u003e\u003cu\u003eText Bert Model Distillation\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003ch3 id=\"the-model-distillation-workflow-for-nlp\"\u003eThe Model Distillation Workflow for NLP\u003c/h3\u003e\u003cp\u003eIn our prior post on model distillation concepts, we discussed the different model distillation patterns, based on the following criteria:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe type of \u003cstrong\u003eTeacher\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eStudent\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eknowledge\u003c/strong\u003e being transferred from teacher to student model(s) (\u003cstrong\u003eresponse-based\u003c/strong\u003e,\u003cstrong\u003e feature-based\u003c/strong\u003e, \u003cstrong\u003erelation-based\u003c/strong\u003e knowledge);\u003c/li\u003e\u003cli\u003eThe cadence and scheme for how the student mode is trained (\u003cstrong\u003eoffline\u003c/strong\u003e, \u003cstrong\u003eonline\u003c/strong\u003e, \u003cstrong\u003eself\u003c/strong\u003e).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e“Benefits of Using Model Distillation”\u003c/strong\u003e, Source: “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, Fig \u003cstrong\u003e3.1\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll be demonstrating the most popular and easiest pattern to get started with: \u003cstrong\u003eoffline, response-based model\u003c/strong\u003e (or \u003cstrong\u003eknowledge\u003c/strong\u003e) \u003cstrong\u003edistillation\u003c/strong\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe teacher model we’ll be using to produce the responses is \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGoogle \u003c/u\u003eGemini\u003c/a\u003e and the student model is \u003ca href=\"https://huggingface.co/docs/transformers/en/model_doc/bert?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBERT\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://huggingface.co/distilbert/distilbert-base-uncased?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edistilbert-base-uncased\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs you’ll see, we could have chosen \u003ca href=\"https://labelbox.com/guides/how-to-fine-tune-large-language-models-with-labelbox/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany combination of teacher or student models\u003c/u\u003e\u003c/a\u003e, because the \u003cstrong\u003eoffline, response-based\u003c/strong\u003e pattern of model distillation is incredibly flexible.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen implementing this process for your own use case, it’s important to understand the relative strengths and weaknesses of each model and match them according to your requirements and use cases (whether it’s \u003ca href=\"https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edetecting and removing PII to be GDPR compliant\u003c/u\u003e\u003c/a\u003e or \u003ca href=\"https://labelbox.com/guides/how-to-build-a-content-moderation-model-to-detect-disinformation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edetecting unsavory content\u003c/u\u003e\u003c/a\u003e).\u003c/p\u003e\u003ch3 id=\"using-the-labelbox-platform-to-automate-model-distillation\"\u003eUsing The Quantumworks Lab Platform To Automate Model Distillation\u0026nbsp;\u003c/h3\u003e\u003cp\u003eLabelbox is the data factory for genAI, providing an end-to-end solution for curating, transforming, annotating, evaluating and orchestrating unstructured data for data science and machine learning.\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab platform supports the development of intelligent applications using the model distillation and fine-tuning workflow enabling AI developers to easily:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImport, curate, filter and eventually select the text that will be labeled for use in student model training and fine-tuning with \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e. Additionally you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/model-foundry-automating-data-tasks-with-foundation-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomate the labeling and annotation of the original text dataset using foundation models\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eor any pre-trained model, including custom models\u003c/u\u003e\u003c/a\u003e) in \u003ca href=\"https://labelbox.com/guides/guide-to-using-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e. The ability to leverage a variety of open source or third-party models to accelerate pre-labeling can cut labeling costs by up to 90% for teams with existing labeling initiatives (and kickstart new and budget constrained AI developers operating without\u0026nbsp; manual labeling capabilities).\u003c/li\u003e\u003cli\u003eLeverage \u003ca href=\"https://labelbox.com/customers/intuitive-surgical-customer-story/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman-in-the-loop\u003c/u\u003e\u003c/a\u003e evaluation (and \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e) through the Foundry to Annotate integration, so labels can be reviewed before being used for training or fine-tuning.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIntegrate with some of the most common cloud providers like \u003ca href=\"https://learn.labelbox.com/accelerate-retail-and-ecommerce-ai-roi/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Cloud Platform\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://labelbox.com/company/partnerships/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eDatabricks\u003c/u\u003e\u003c/a\u003e for additional MLOps services.\u003c/li\u003e\u003cli\u003eOrchestrate and schedule future automated labeling and model runs as \u003ca href=\"https://labelbox.com/blog/10x-faster-uploads-with-labelboxs-new-ingestion-upgrades/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enew data flows into the Quantumworks Lab platform\u003c/u\u003e\u003c/a\u003e through Model Apps (including \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emultimodal data labeling\u003c/u\u003e\u003c/a\u003e). \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"introduction-to-data-preparation-for-natural-language-processing-with-catalog\"\u003eIntroduction To Data Preparation for Natural Language Processing With Catalog\u003c/h2\u003e\u003cp\u003eBefore beginning the tutorial:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCreate a free HuggingFace account (in order to access the \u003ca href=\"https://huggingface.co/datasets/SetFit/emotion?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003e\"Setfit/emotion\" dataset\u003c/u\u003e\u003c/a\u003e)\u003cul\u003e\u003cli\u003eDownload the dataset locally\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab\u0026gclid=Cj0KCQiA7OqrBhD9ARIsAK3UXh1OL7LeJadsNvbP_43MnwLSpLlRPD0IkilIwyvf3VwKOzxQ5ioGon8aApXREALw_wcB\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eCreate a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e (in order to \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecreate a free API key\u003c/u\u003e\u003c/a\u003e)\u003cul\u003e\u003cli\u003eNavigate to “\u003cstrong\u003eCatalog\u003c/strong\u003e” in the sidebar\u003c/li\u003e\u003cli\u003eSelect “\u003cstrong\u003e+New\u003c/strong\u003e”\u003c/li\u003e\u003cli\u003eUpload the dataset from Kaggle\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNote\u003c/strong\u003e: The easiest method is to use \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#create-a-dataset-via-the-app-ui\"\u003e\u003cu\u003ethe Quantumworks Lab Web UI\u003c/u\u003e\u003c/a\u003e\u0026nbsp; to manually upload your dataset.\u0026nbsp;\u003cul\u003e\u003cli\u003eIf your goal is to scale the data ingestion process for future labeling or data refreshes, check out our SDK.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’re able to see your dataset in Quantumworks Lab Catalog, you’ll be able to do the following:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e for more granular data curation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003eFor additional details on how to use Catalog to enable data selection for downstream data-centric workflows (such as data labeling, model training, model evaluation, error analysis, and active learning), check out \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour documentation\u003c/u\u003e\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"using-a-large-nlp-model-or-llm-to-generate-and-distill-predictions-for-fine-tuning\"\u003eUsing A Large NLP Model Or LLM To Generate And Distill Predictions For Fine-Tuning\u003c/h2\u003e\u003cp\u003eThe first step of model distillation is to \u003cstrong\u003eidentify an appropriate teacher model\u003c/strong\u003e, which will be used to produce responses that, when combined with the original text, will serve as the \u003cstrong\u003efine-tuning dataset for the student model\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResponse-based\u003c/strong\u003e model distillation is powerful because it can be used even when access to the original model weights is limited (or the model is so big that downloading a copy of the model would take a really long time). Response-based distillation also doesn’t require the user to have trained the model themselves; just that the model was pre-trained.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox allows you to pick any of the \u003ca href=\"https://labelbox.com/blog/6-key-llms-to-power-your-text-based-ai-applications/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecurrently hosted, state-of-the-art models to use\u003c/u\u003e\u003c/a\u003e (as well as upload your own custom models) to use as the teacher model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor now, let’s get started with preparing the text we’ll be labeling, or generating predictions with, using Google Gemini. The combination of text and label pairs will be used for BERT.\u003c/p\u003e\u003ch3 id=\"step-1-select-text-assets-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select text assets and choose a foundation model of interest\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1002\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePreview of text in Quantumworks Lab Catalog\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to your uploaded Emotions dataset in Catalog.\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the text on which the predictions should be made.e.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “\u003cstrong\u003ePredict with Model Foundry\u003c/strong\u003e”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run (in this case \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGemini\u003c/u\u003e\u003c/a\u003e).\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task -\u0026nbsp; such as \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003etext classification, summarization, and text generation\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eTo locate a \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003especific model\u003c/u\u003e\u003c/a\u003e, you can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003eStep 2: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eWhen developing ML based applications, developers need to quickly and iteratively prepare and version training data, launch model experiments, and use the performance metrics to further refine the input data sources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe performance of a model can vary wildly depending on the data used, the quality of the annotations, and even the model architecture itself. A necessary requirement for replicability is being able to see the exact version of all the artifacts used or generated as a result of an experiment.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox will snapshot the experiment, the data artifacts as well as the trained model, as a saved process known as a \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis includes the types of items the model is supposed to identify and label, known as an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eontology\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of settings, which you can find in the Advanced model setting.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce you’ve located Gemini, you can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIn this case, we will enter the following prompt:\u0026nbsp;\u003cul\u003e\u003cli\u003e\u003cem\u003eFor the given text, answer the following. Classify emotions, pick one of the options: [sadness, joy, love, anger, fear, surprise]. Return the result as a JSON object. {\"emotions\" : \"\"}.\u0026nbsp;\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis prompt is designed to facilitate responses from the model with one of the following: \u003cem\u003esadness, joy, love, anger, fear, surprise\u003c/em\u003e.\u003c/p\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-review-predictions-in-the-model-tab\"\u003eStep 3: Review predictions in the Model tab \u003c/h3\u003e\u003cp\u003eBecause each \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run is submitted with a unique name\u003c/u\u003e\u003c/a\u003e, it’s easy to distinguish between each subsequent model run.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/foundry-view-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eView prediction results\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results \u003ca href=\"https://docs.labelbox.com/docs/comparing-model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eacross a variety of model runs\u003c/u\u003e\u003c/a\u003e different models\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-2.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-2.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eView predictions in Model tab for the model run\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-4.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-4.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-4.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eIn this case, Gemini Pro predicted this text to be \"joy\"\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThese generated labels are now ready to be used for fine-tuning the BERT student model.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-enriching-and-evaluating-predictions-using-human-in-the-loop-and-annotate\"\u003eStep 4: Enriching and evaluating predictions using human-in-the-loop and Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAlthough fine-tuning a foundation model requires less data than pre-training a large foundation model from scratch, the data (specifically the labels) need to be high-quality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEven big, powerful foundation models make mistakes or miss edge cases.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou might also find that there are additional categories that the parent model didn’t identify correctly because the ontology was incomplete.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce a parent model like Gemini has been used for the initial model-assisted labeling run, those predictions \u003ca href=\"https://docs.labelbox.com/docs/foundry-annotate-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then be sent\u003c/u\u003e\u003c/a\u003e to a \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eproject\u003c/u\u003e\u003c/a\u003e, a container \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhere all your labeling processes happen\u003c/u\u003e\u003c/a\u003e. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bpsrmes4da\" title=\"Distilling a faster and smaller LLM using BERT and Gemini_SendToAnnotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eIn this case, we feel fairly confident\u003cstrong\u003e \u003c/strong\u003ein how well Gemini performed so we’ll send the inferences to the corresponding Quantumworks Lab project and treat them as the ground truth that the student model will be fine-tuned on. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"fine-tuning-the-student-model-bert\"\u003eFine-Tuning The Student Model (BERT)\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWe’ve shown the first half of the model distillation to fine-tuning workflow.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe identified the categories we wanted the parent model (Gemini) to detect and label in the form of an ontology.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe used Gemini to automatically label the texts as \u003cem\u003e\"sadness\"\u003c/em\u003e or \u003cem\u003e\"fear\"\u003c/em\u003e (for example).\u003c/li\u003e\u003cli\u003eWe exported the generated labels to a Quantumworks Lab project, at which point we could review the labels manually and enrich them further using the Quantumworks Lab editor.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe next step is to use the generated labels, along with the original texts, to fine-tune a student model in Colab.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: You’ll now need the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=cclwx_KmJr3Z\u0026line=3\u0026uniqifier=1\"\u003e\u003cem\u003e\u003cu\u003eAPI keys from earlier\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to follow along with the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=1FMXvdze28AZ\"\u003e\u003cem\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003ch3 id=\"step-5-fetch-the-ground-truth-labels-from-the-project-via-labelbox-sdk\"\u003eStep 5: Fetch the ground truth labels from the project via Quantumworks Lab SDK\u003c/h3\u003e\u003cp\u003eFor brevity, we’ve omitted the surrounding code samples but you can copy or run the corresponding blocks in the provided notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our documentation to find out all the ways you can automate the model lifecycle (\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eincluding labeling\u003c/u\u003e\u003c/a\u003e) using our SDK. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore fine-tuning the BERT student model, we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=Jq-1tQs2QBrj\"\u003e\u003cu\u003eneed to fetch the generated labels\u003c/u\u003e\u003c/a\u003e from the Quantumworks Lab project using the Quantumworks Lab SDK as well as the \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=jKMysiHvUI1D\"\u003e\u003cu\u003eground truth\u003c/u\u003e\u003c/a\u003e.\u0026nbsp; \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-6-additionally-text-data-processing\"\u003eStep 6: Additionally Text Data Processing \u003c/h3\u003e\u003cp\u003eThere's additional processing that needs to happen, which we walk through below.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNext we’ll ensure the labels are \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=HFiQMbcSQSks\"\u003e\u003cu\u003eexported into a .csv file\u003c/u\u003e\u003c/a\u003e that contains two columns, the original ‘text’ and the generated ‘label’.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=B2_znE_XHbvG\u0026line=2\u0026uniqifier=1\"\u003e\u003cu\u003eread the csv file into a pandas dataframe\u003c/u\u003e\u003c/a\u003e, perform a series of aggregation operations to help us splits the text into train and test sets based on the category count.\u003c/li\u003e\u003cli\u003eWe’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=k9eNSPuLRAJx\"\u003e\u003cu\u003einitialize a tokenizer\u003c/u\u003e\u003c/a\u003e and encode the train and test texts.\u003c/li\u003e\u003cli\u003eFinally we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=PnHQrghbQ7tD\" rel=\"noreferrer\"\u003efinish creating the training \u0026amp; validation dataset\u003c/a\u003e. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNote\u003c/strong\u003e: Expand the “Export labels into .CSV file” block in the Colab notebook for the full code sample.\u0026nbsp; \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-7-fine-tune-student-bert-model-using-labels-generated-by-google-gemini\"\u003eStep 7: Fine tune student BERT model using labels generated by Google Gemini\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn Colab we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=gjMPaBgAIAGd\u0026line=2\u0026uniqifier=1\" rel=\"noreferrer\"\u003einstantiate a BERT model\u003c/a\u003e and \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=cmbMl5wXIB3I\u0026line=6\u0026uniqifier=1\" rel=\"noreferrer\"\u003etrain it using the data\u003c/a\u003e, which includes both text and labels. Specifically we'll fine-tune a text classifier model called \u003cem\u003e“distilbert-base-uncased”\u003c/em\u003e to classify text as one of the following categories in the ontology: \u003cem\u003esadness, joy, love, anger, fear, surprise\u003c/em\u003e. \u003c/li\u003e\u003cli\u003eWe’ll also \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=VzlNe83KRh98\"\u003esave the model\u003c/a\u003e and \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=RbPplpJdRuXL\"\u003etest the prediction\u003c/a\u003e.\u003cul\u003e\u003cli\u003eBy saving the model (or every model we create) we have the option of A/B testing models and using the models for downstream use cases (as well as share the models with other key stakeholders through a model registry, like MLFlow). \u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1339\" height=\"716\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 1339w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1262\" height=\"344\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 1262w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-8-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 8: Create a model run with predictions and ground truth\u003c/h3\u003e\u003cp\u003eOftentimes the initial training or fine-tuning step isn’t the final stop on the journey of developing a model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOne of the biggest differences between the traditional method of training models in the classroom versus the real-world is how much control you have over the quality of your data, and consequently the quality of the model produced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we mentioned earlier, developers can upload predictions and use the Model product to \u003ca href=\"https://labelbox.com/blog/how-to-confidently-compare-test-and-evaluate-models-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ediagnose performance issues with models and compare them across multiple experiments\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDoing so automatically populates model metrics that make it easy to evaluate the model’s performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e \u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, you’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=mjDaoyV3L3l1\"\u003egrab the model’s ID\u003c/a\u003e to \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=HsDr_g3qqUFo\"\u003ecreate a new model run\u003c/a\u003e (if needed).\u003c/li\u003e\u003cli\u003eThen you’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=K8D7lE9nrOLG\"\u003eget the ground truth from your project\u003c/a\u003e via the export as well as the \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=-cv2GJ6zrU5A\"\u003elabel IDs from ground truth\u003c/a\u003e. \u003c/li\u003e\u003cli\u003eNext you’ll create the predictions by \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=qxwq7kKZrfzL\"\u003erunning the fine-tuned BERT model on the original text assets\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eYou \u003ca href=\"https://docs.labelbox.com/docs/import-ground-truth?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then upload the ground truth labels\u003c/u\u003e\u003c/a\u003e (the labels generated by Gemini and used as the fine-tuning dataset) and the \u003ca href=\"https://docs.labelbox.com/docs/upload-model-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003epredictions\u003c/u\u003e\u003c/a\u003e from the fine-tuned BERT model \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=i7Xq-fldsiRY\"\u003eto the corresponding Quantumworks Lab model\u003c/a\u003e. \u003c/li\u003e\u003cli\u003eYou can see an example of how model metrics are automatically populated by Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"997\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-10.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-10.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-10.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-10.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAuto generated metrics\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"evaluating-model-performance\"\u003eEvaluating Model Performance\u003c/h2\u003e\u003cp\u003eThere’s no single metric to rule them all \u003ca href=\"https://labelbox.com/blog/gpt4-vs-palm-assessing-performance-of-llm-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhen evaluating how your fine-tuned LLM performs\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBoth qualitative and quantitative measures must be considered, combined with sampling and manual review.\u003c/p\u003e\u003cp\u003eWith that being said, Model offers a number of the most common out-of-the-box. With the \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io#metrics-view\"\u003e\u003cu\u003e‘Metrics view’\u003c/u\u003e\u003c/a\u003e users can drill into crucial model metrics, such as \u003ca href=\"https://docs.labelbox.com/docs/upload-model-metrics-1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econfusion matrix, precision, recall, F1 score, false positive, and more\u003c/u\u003e\u003c/a\u003e, to surface model errors.\u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive, which means you can click on any chart or metric to immediately open up the gallery view of the model run and see corresponding examples, as well as visually compare model predictions between multiple model runs. \u003c/p\u003e\u003ch3 id=\"step-9-evaluate-predictions-from-different-bert-model-runs-in-labelbox-model\"\u003eStep 9: Evaluate predictions from different BERT model runs in Quantumworks Lab Model \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBERT fine tuned on labels created by Gemini Pro vs ground truth labels \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to “\u003cstrong\u003eModel\u003c/strong\u003e”\u003c/li\u003e\u003cli\u003eIn this case, we fine-tuned two models, one using 1000 ground truth labels and the other with 1000 labels generated by the Gemini model. We see very similar results and leveraging an off the shelf model is almost as good as using ground truth labels.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"examples-of-predictions-from-fine-tuned-bert-model\"\u003e\u003cstrong\u003eExamples of predictions from fine tuned BERT model\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eHow does our fine-tuned model perform? \u003c/p\u003e\u003cp\u003eLet's manually inspect a few examples of predictions from the fine-tuned BERT model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-12.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-12.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-12.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-12.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “anger”.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-11.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-11.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-11.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-11.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “joy”.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “fear”.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"advanced-considerations\"\u003eAdvanced Considerations\u003c/h2\u003e\u003cp\u003eIn this step-by-step walkthrough, we’ve shown how anyone with any text-based dataset can leverage an LLM to label, fine-tune and analyze a smaller but mighty custom model.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"610\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e“Leveraging FMOps To Develop intelligent Applications”, Source: “\u003c/span\u003e\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eA pragmatic introduction to model distillation for AI developers\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e”, Fig 5.2.4\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAdditional considerations users should address for scaling similar projects include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCollecting feedback \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efrom users \u0026amp; human SME’s to improve\u003c/u\u003e\u003c/a\u003e the fine-tuning dataset quality on a continuous basis, including error analysis and \u003ca href=\"https://docs.labelbox.com/docs/llm-human-preference?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman preference modeling\u003c/u\u003e\u003c/a\u003e;\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003eStrategic planning for incorporating multiple data modalities besides text, including image, audio, and video;\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomating future data ingestion, curation, enrichment, and labeling when the fine-tuned model needs to be retrained due to drift via a \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erobust SDK\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eEasy-to-use, user interface that can be customized for various modalities of data when multiple users are involved (as well as \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM data generation\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eIn this tutorial we demonstrated an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language processing\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a BERT model with labels created in Model Foundry using \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Gemini\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHopefully you were able to see how easy it is to go from raw data to cutting-edge custom models in less than 30 min.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou learned how the Quantumworks Lab platform enables model distillation by allowing developers to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIngest, explore and prepare text-based datasets using \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eUse any \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM\u003c/u\u003e\u003c/a\u003e to automatically label data using \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e as well as how to incorporate human-in-the-loop evaluation using \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eExport these labeled predictions to a cloud-based training environment for fine-tuning;\u003c/li\u003e\u003cli\u003eAutomate the various workflows using the \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox SDK\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eEvaluate model performance and analyze model errors using \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox Model\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you’re interested in learning more about model distillation, check out the previous posts in this series: “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, “\u003ca href=\"https://labelbox.com/guides/end-to-end-workflow-with-model-distillation-for-computer-vision/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnd-to-end workflow with model distillation for computer vision\u003c/u\u003e\u003c/a\u003e”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLooking to implement a production-ready model distillation and fine-tuning in your organization but not sure how to get started leveraging your unstructured data?\u0026nbsp;\u003c/p\u003e\u003cp\u003eAsk \u003ca href=\"https://community.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour community\u003c/u\u003e\u003c/a\u003e or reach out to \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour solutions engineers\u003c/u\u003e\u003c/a\u003e!\u003c/p\u003e","comment_id":"65c97ca417c84d0001395ccb","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/distilling.png","featured":false,"visibility":"public","created_at":"2024-02-12T02:04:20.000+00:00","updated_at":"2024-11-20T23:09:07.000+00:00","published_at":"2024-02-15T19:25:02.000+00:00","custom_excerpt":"Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"681516a56e02120001d5ab84","name":"#mikiko","slug":"hash-mikiko","description":null,"feature_image":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/404/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"65bd4ff882e5680001e078f9","name":"Model Distillation","slug":"model-distillation","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/tag/model-distillation/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/end-to-end-workflow-for-knowledge-distillation-with-nlp/","excerpt":"Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. ","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65cbb3b7643f030001d22310","uuid":"2de9eae7-61eb-4935-94ab-91dedf3f59be","title":"How to build equipment detection models to improve worker safety and efficiency","slug":"how-to-build-equipment-detection-models","html":"\u003cp\u003eWith AI-powered object detection, you can now seamlessly integrate the latest advances in foundation models into your warehouse and construction site safety operations. As the demand for better safety monitoring continues to rise, it's essential for teams to maximize protective equipment use to mitigate potential hazards. Quantumworks Lab empowers the world’s largest organizations to leverage AI solutions tailored to their unique safety detection challenges.\u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for large-scale safety detection. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving safety detection requires a vast amount of data in the form of images and videos. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e\u0026nbsp;The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e\u0026nbsp;Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their safety detection through advanced computer vision techniques. Instead of relying on time-consuming manual human review, companies can leverage Quantumworks Lab’s AI-assisted data enrichment and flexible training frameworks to quickly build task-specific models that uncover actionable insights for the prevention of supply chain mistakes.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-13-at-11.11.42-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1778\" height=\"996\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-13-at-11.11.42-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-13-at-11.11.42-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-13-at-11.11.42-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-13-at-11.11.42-AM.png 1778w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through an end-to-end workflow on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve safety detection using personal protective equipment as an example. Specifically, this guide will walk through how you can explore and better understand your visual assets to make more data-driven business decisions for worker safety.\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-build-equipment-detection-models-to-improve-worker-safety-and-efficiency\"\u003eSee it in action: How to build equipment detection models to improve worker safety and efficiency\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/97k6ufmyas\" title=\"WD 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across\u0026nbsp;\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data with Catalog and Foundry\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below via:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/195NPiEDxpzYxuzBNI3aJ-Z-MzzgIo0Ds?ref=labelbox-guides.ghost.io#scrollTo=SBBJzrCYQ9gd\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003e\u003cstrong\u003ePart 1: Explore and prepare your data\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in the \u003ca href=\"https://colab.research.google.com/drive/195NPiEDxpzYxuzBNI3aJ-Z-MzzgIo0Ds?ref=labelbox-guides.ghost.io#scrollTo=SBBJzrCYQ9gd\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, please make a copy of the notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ll be working with a dataset that detects workers wearing safety equipment\u0026nbsp; – with the goal of quickly curating data and finding protective equipment (e..g, helmets, goggles, reflective vests, gloves, masks, etc) from high-volumes of images. \u003c/p\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003ePlease download the \u003ca href=\"https://colab.research.google.com/drive/1W_d3Gq_-2o3gOT8gvF7Zomamg7eSiGYj?ref=labelbox-guides.ghost.io#scrollTo=dOHormwXvTwB\" rel=\"noreferrer\"\u003edataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your data as a CSV.\u003c/li\u003e\u003cli\u003eIf your images sit as individual files in cloud storage, you can reference the URL of these files through our\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your image data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this demo, we'll be using Catalog to find relevant images of equipment for our dataset with the goal of annotating bounding boxes for the personal protective equipment using foundation models.\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"using-foundry-to-pre-label-bounding-boxes\"\u003eUsing Foundry to pre-label bounding boxes\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/o9k033g1ii\" title=\"WD 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn this next step, we'll walk through how you can take a human-in-the-loop approach to iterate or modify pre-labels and speed up the annotation process.  \u003c/p\u003e\u003cp\u003eModel Foundry enables teams to choose from a library of models and in this case, we'll be using GroundingDINO to generate previews and attach them as pre-labels.\u003c/p\u003e\u003cp\u003eWith\u0026nbsp;\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, you can automate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6jofnqbzb3\" title=\"WD - 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eReview initial inference results and send to annotate\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/vldocd1w58\" title=\"WD 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eCreate model experiment and create a model run\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"part-2-train-your-model-and-generate-predictions\"\u003e\u003cstrong\u003ePart 2: \u003c/strong\u003eTrain your model and generate predictions\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/fewiasx1he\" title=\"WD - 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSee predictions overlayed on top of annotations\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe next step is to take all of our labeled data and train our model on it. This allows us to make predictions on this data and for Quantumworks Lab to calculate evaluation metrics so that we can see where the model is going wrong and improve model performance.\u003c/p\u003e\u003ch3 id=\"view-model-predictions-within-the-labelbox-ui-to-evaluate-and-diagnose-model-effectiveness\"\u003eView model predictions within the Quantumworks Lab UI to evaluate and diagnose model effectiveness\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/o7um6x4md1\" title=\"WD 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a last step, let's compare model inferences with ground-truth annotations to see where the model may be underperforming. A disagreement between model predictions and ground truth labels is typically due to either a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eAfter running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/li\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model for 10x faster corner-case detection – detect and visualize corner-cases where the model is underperforming. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection and additional labeling.\u003c/p\u003e\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\u003cp\u003eBy analyzing high-volumes of images and videos using foundation models and human alignment, Quantumworks Lab provides teams with the ability to inject valuable  insights for delivering better protective equipment detection models for warehouses and construction sites that allow you to improve operational efficiency, compliance and overall worker safety.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful task-specific models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"65cbb3b7643f030001d22310","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-13-at-11.58.37-AM.png","featured":false,"visibility":"public","created_at":"2024-02-13T18:23:51.000+00:00","updated_at":"2024-02-14T17:46:29.000+00:00","published_at":"2024-02-13T18:54:05.000+00:00","custom_excerpt":"Learn how you can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve safety detection for personal protective equipment using the latest advances in foundation models to automate labeling. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},"url":"https://labelbox-guides.ghost.io/how-to-build-equipment-detection-models/","excerpt":"Learn how you can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve safety detection for personal protective equipment using the latest advances in foundation models to automate labeling. ","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65bd22d782e5680001e07877","uuid":"3e5a16b6-f3f5-494e-bb8b-4162abb3e830","title":"End-to-end workflow with model distillation for computer vision","slug":"end-to-end-workflow-with-model-distillation-for-computer-vision","html":"\u003cp\u003eModel distillation, also known as \u003cstrong\u003eknowledge distillation\u003c/strong\u003e, is a technique that focuses on creating efficient models by transferring knowledge from large, complex models to smaller, deployable ones.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel distillation is one of many techniques that have grown in popularity and importance in enabling small teams to leverage foundation models in developing small (but mighty) custom models, used in intelligent applications.\u003c/p\u003e\u003cp\u003eIn \u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e“A Pragmatic Introduction to Model Distillation for AI Developers\u003c/u\u003e\u003c/a\u003e”, we illustrated some of the conceptual foundations for how model distillation works.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe described how distillation can be leveraged in any domain or data modality requiring efficiency and model optimization, whether the use case is \u003cu\u003ecomputer vision or NLP related\u003c/u\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll demonstrate an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003ecomputer vision\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a YOLO model with labels created in Model Foundry using Amazon Rekognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll show how easy it is to go from raw data to cutting-edge models, customized to your use case, using a \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efashion products dataset\u003c/u\u003e\u003c/a\u003e (additional public datasets can be found \u003ca href=\"https://labelbox.com/datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e or on sites like Kaggle).\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn less than 30 min you’ll learn how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIngest\u003c/strong\u003e, \u003cstrong\u003eexplore\u003c/strong\u003e and \u003cstrong\u003eprepare\u003c/strong\u003e the products fashion dataset;\u003c/li\u003e\u003cli\u003ePick and configure\u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e any image based foundation model\u003c/u\u003e\u003c/a\u003e to \u003cstrong\u003eautomatically label\u003c/strong\u003e data with just a few clicks;\u003c/li\u003e\u003cli\u003eExport the labeled predictions to a project as a potential set-up for \u003cstrong\u003emanual evaluation\u003c/strong\u003e;\u003c/li\u003e\u003cli\u003eUse the labeled predictions dataset to \u003cstrong\u003efine-tune a student model\u003c/strong\u003e using a cloud-based notebook provider;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEvaluate performance\u003c/strong\u003e of the fine-tuned student model.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAt the end of the tutorial we’ll also discuss advanced considerations in scaling your models up and out, such as automating data ingestion and labeling, and resources for incorporating \u003ca href=\"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e\u0026nbsp; into your workflows.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-use-model-distillation-to-fine-tune-a-small-task-specific-computer-vision-model\"\u003eSee it in action: How to use model distillation to fine-tune a small, task-specific Computer Vision Model\u003c/h3\u003e\u003cp\u003e\u003cem\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial. You’ll also need to \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003ecreate API keys for accessing the SDK\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNotebook\u003c/strong\u003e: \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cu\u003eCV YOLO Model Distillation\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003ch3 id=\"the-model-distillation-workflow-for-computer-vision\"\u003eThe Model Distillation Workflow for Computer Vision\u003c/h3\u003e\u003cp\u003eIn our prior post on model distillation concepts, we discussed the different model distillation patterns, based on the following criteria:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe type of \u003cstrong\u003eTeacher\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eStudent\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eknowledge\u003c/strong\u003e being transferred from teacher to student model(s) (\u003cstrong\u003eresponse-based\u003c/strong\u003e,\u003cstrong\u003e feature-based\u003c/strong\u003e, \u003cstrong\u003erelation-based\u003c/strong\u003e knowledge);\u003c/li\u003e\u003cli\u003eThe cadence and scheme for how the student mode is trained (\u003cstrong\u003eoffline\u003c/strong\u003e, \u003cstrong\u003eonline\u003c/strong\u003e, \u003cstrong\u003eself\u003c/strong\u003e).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this tutorial we’ll be demonstrating the most popular and easiest pattern to get started with: \u003cstrong\u003eoffline, response-based model\u003c/strong\u003e (or \u003cstrong\u003eknowledge\u003c/strong\u003e) \u003cstrong\u003edistillation\u003c/strong\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe teacher model we’ll be using to produce the responses is Amazon Rekognition and the student model is \u003ca href=\"https://labelbox.com/product/model/foundry-models/yolov8-object-detection/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eYOLOv8 Object Detection\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs you’ll see, we could have chosen \u003ca href=\"https://labelbox.com/blog/6-cutting-edge-foundation-models-for-computer-vision-and-how-to-use-them/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany combination of teacher or student models\u003c/u\u003e\u003c/a\u003e, because the \u003cstrong\u003eoffline, response-based\u003c/strong\u003e pattern of model distillation is incredibly flexible.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen implementing this process for your own use case, it’s important to understand the relative strengths and weaknesses of each model and match them according to your requirements.\u003c/p\u003e\u003ch3 id=\"using-the-labelbox-platform-to-automate-model-distillation\"\u003eUsing The Quantumworks Lab Platform To Automate Model Distillation\u0026nbsp;\u003c/h3\u003e\u003cp\u003eLabelbox is the leading data-centric AI platform, providing an end-to-end platform for curating, transforming, annotating, evaluating and orchestrating unstructured data for data science, machine learning, and generative AI.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab platform supports the development of intelligent applications using the model distillation and fine-tuning workflow. \u003c/p\u003e\u003cp\u003eAI developers are enabled to easily:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImport, curate, filter and eventually select the images that will be labeled for use in student model training and fine-tuning with \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e. Additionally you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/model-foundry-automating-data-tasks-with-foundation-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomate the labeling and annotation of the original image dataset using foundation models\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eor any pre-trained model, including custom models\u003c/u\u003e\u003c/a\u003e) in \u003ca href=\"https://labelbox.com/guides/guide-to-using-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e. The ability to leverage a variety of open source or third-party models to accelerate pre-labeling can cut labeling costs by up to 90% for teams with existing labeling initiatives (and kickstart new and budget constrained AI developers operating without\u0026nbsp; manual labeling capabilities).\u003c/li\u003e\u003cli\u003eLeverage \u003ca href=\"https://labelbox.com/customers/intuitive-surgical-customer-story/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman-in-the-loop\u003c/u\u003e\u003c/a\u003e evaluation (and \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e) through the native Foundry to Annotate integration, so labels can be reviewed before being used for training or fine-tuning.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIntegrate with some of the most common cloud providers like \u003ca href=\"https://learn.labelbox.com/accelerate-retail-and-ecommerce-ai-roi/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Cloud Platform\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://labelbox.com/company/partnerships/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eDatabricks\u003c/u\u003e\u003c/a\u003e for additional MLOps services.\u003c/li\u003e\u003cli\u003eOrchestrate and schedule future automated labeling and model runs as \u003ca href=\"https://labelbox.com/blog/10x-faster-uploads-with-labelboxs-new-ingestion-upgrades/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enew data flows into the Quantumworks Lab platform\u003c/u\u003e\u003c/a\u003e through Model Apps (including \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emultimodal data labeling\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"introduction-to-data-preparation-for-computer-vision-with-catalog\"\u003eIntroduction To Data Preparation for Computer Vision With Catalog\u003c/h2\u003e\u003cp\u003eBefore beginning the tutorial:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eCreate a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e (in order to \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecreate a free API key\u003c/u\u003e\u003c/a\u003e)\u003c/li\u003e\u003cli\u003eCheck that you can access an \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eexisting fashion products dataset\u003c/u\u003e\u003c/a\u003e in Catalog\u003cul\u003e\u003cli\u003eIf not, check out sites like Kaggle for similar datasets.\u003c/li\u003e\u003cli\u003eDownload the images and their metadata.\u003c/li\u003e\u003cli\u003eChoose whether to upload data via \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#create-a-dataset-via-the-app-ui\"\u003e\u003cu\u003ethe Quantumworks Lab Web UI\u003c/u\u003e\u003c/a\u003e (preferred method) or the SDK.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’re able to see your dataset in Quantumworks Lab Catalog, you’ll be able to do the following:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e for more granular data curation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor additional details on how to use Catalog to enable data selection for downstream data-centric workflows (such as data labeling, model training, model evaluation, error analysis, and active learning), check out \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour documentation\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"using-a-large-computer-vision-model-to-generate-and-distill-predictions\"\u003eUsing A Large Computer Vision Model To Generate And Distill Predictions\u003c/h2\u003e\u003cp\u003eThe first step of model distillation is to \u003cstrong\u003eidentify an appropriate teacher model\u003c/strong\u003e, which will be used to produce responses that, when combined with the original images, will serve as the \u003cstrong\u003efine-tuning dataset for the student model\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResponse-based\u003c/strong\u003e model distillation is powerful because it can be used even when access to the original model weights is limited (or the model is so big that downloading a copy of the model would take a really long time). Response-based distillation also doesn’t require the user to have trained the model themselves; just that the model was pre-trained.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox allows you to pick any of the \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecurrently hosted, state-of-the-art models to use\u003c/u\u003e\u003c/a\u003e (as well as upload your own custom models) to use as the teacher model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor now, let’s get started with preparing the images we’ll be labeling, or generating predictions with, using Amazon Rekognition. The combination of image and label pairs will be used for YOLO.\u003c/p\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select images and choose a foundation model of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to the \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efashion products dataset\u003c/u\u003e\u003c/a\u003e in Catalog.\u003c/li\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “\u003cstrong\u003ePredict with Model Foundry\u003c/strong\u003e”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run (in this case Rekognition).\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task - such as \u003ca href=\"https://labelbox.com/usecases/computer-vision/image-classification/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eimage classification\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/usecases/computer-vision/object-detection/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eobject detection\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/guides/how-to-build-generative-captioning-using-foundation-models-for-product-listings/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage captioning\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003eStep 2: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eWhen developing ML based applications, developers need to quickly and iteratively prepare and version training data, launch model experiments, and use the performance metrics to further refine the input data sources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe performance of a model can vary wildly depending on the data used, the quality of the annotations, and even the model architecture itself. A necessary requirement for replicability is being able to see the exact version of all the artifacts used or generated as a result of an experiment.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox will snapshot the experiment, the data artifacts as well as the trained model, as a saved process known as a \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis includes the types of items the model is supposed to identify and label, known as an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eontology\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of settings, which you can find in the Advanced model setting.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce you’ve located Rekognition, you can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIn this case, we set the ontology to detect “Jacket” and we can see a preview of running the model on this ontology above.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo get an idea of how your current model settings affect the final predictions, you can generate preview predictions on up to five data rows. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-review-predictions-in-the-model-tab\"\u003eStep 3: Review predictions in the Model tab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eBecause each \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run is submitted with a unique name\u003c/u\u003e\u003c/a\u003e, it’s easy to distinguish between each subsequent model run.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/foundry-view-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eView prediction results\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results \u003ca href=\"https://docs.labelbox.com/docs/comparing-model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eacross a variety of model runs\u003c/u\u003e\u003c/a\u003e different models\u003c/li\u003e\u003cli\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe can see that for the most part, “jackets” were correctly identified and labeled.\u003c/li\u003e\u003cli\u003eThese generated labels are now ready to be used for fine-tuning the student model YOLO.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-enriching-and-evaluating-predictions-using-human-in-the-loop-and-annotate\"\u003eStep 4: Enriching and evaluating predictions using human-in-the-loop and Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAlthough fine-tuning a foundation model requires less data than pre-training a large foundation model from scratch, the data (specifically the labels) need to be high-quality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEven big, powerful foundation models make mistakes or miss edge cases.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou might also find that there are additional objects that the parent model didn’t identify because the items in the image weren’t initially identified as being important in the ontology (for example, “boots” or “ski hats”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce a parent model like Rekognition has been used for the initial model-assisted labeling run, those predictions \u003ca href=\"https://docs.labelbox.com/docs/foundry-annotate-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then be sent\u003c/u\u003e\u003c/a\u003e to a Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eproject\u003c/u\u003e\u003c/a\u003e, a container \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhere all your labeling processes happen\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/la022y1i78\" title=\"End-to-End Workflow for Model Distillation with Computer Vision - Send to annotate Jacket YOLO Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"484\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn this case, we feel fairly confident\u003cstrong\u003e \u003c/strong\u003ein how well Rekognition performed so we’ll send the inferences to the corresponding Quantumworks Lab project and treat them as the ground truth that the student model will be fine-tuned on. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"fine-tuning-the-student-model-yolo\"\u003eFine-Tuning The Student Model (YOLO)\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWe’ve shown the first half of the model distillation to fine-tuning workflow.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe identified the items we wanted the parent model (Rekognition) to detect and label in the form of an ontology.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe used Rekognition to automatically label items like “jackets”.\u003c/li\u003e\u003cli\u003eWe exported the generated labels to a Quantumworks Lab project, at which point we could review the labels manually and enrich them further using the Quantumworks Lab editor.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe next step is to use the generated labels, along with the original image, to fine-tune a student model in Colab.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: You’ll now need the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=PgdwI9SR5HHd\"\u003e\u003cem\u003e\u003cu\u003eAPI keys from earlier\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to follow along with the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cem\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003ch3 id=\"step-5-fetch-the-ground-truth-labels-from-the-project-via-labelbox-sdk-and-convert-the-images-into-the-relevant-format\"\u003eStep 5: Fetch the ground truth labels from the project via Quantumworks Lab SDK and convert the images into the relevant format.\u003c/h3\u003e\u003cp\u003eFor brevity, we’ve omitted the relevant code samples but you can copy or run the corresponding blocks in the provided notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our documentation to find out all the ways you can automate the model lifecycle (\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eincluding labeling\u003c/u\u003e\u003c/a\u003e) using our SDK.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"977\" height=\"542\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png 977w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore fine-tuning the YOLO student model, we’ll \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=7hs-oTEXOQKx\"\u003e\u003cu\u003eneed to fetch the generated labels\u003c/u\u003e\u003c/a\u003e from the Quantumworks Lab project using the Quantumworks Lab SDK as well as the \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=jKMysiHvUI1D\"\u003e\u003cu\u003eground truth\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe’ll also need to \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cu\u003econvert the images and ensure they’re in the right format\u003c/u\u003e\u003c/a\u003e for fine-tuning, specifically the COCO format.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-6-fine-tune-student-yolo-model-using-labels-generated-by-amazon-rekognition\"\u003eStep 6: Fine-tune student YOLO model using labels generated by Amazon Rekognition\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"608\" height=\"109\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png 608w\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn Colab we’ll \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=Becr0BZQO_Ze\u0026line=1\u0026uniqifier=1\" rel=\"noreferrer\"\u003einstantiate a YOLO model and train it using the data\u003c/a\u003e, which includes both images and labels.\u003c/li\u003e\u003cli\u003eWe’ll then \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=_quHpcfihY7s\" rel=\"noreferrer\"\u003erun the fine-tuned student YOLO model\u003c/a\u003e on the images to generate the predictions for analysis.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSee the example notebook for omitted code.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"923\" height=\"307\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png 923w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-7-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 7: Create a model run with predictions and ground truth\u0026nbsp;\u003c/h3\u003e\u003cp\u003eOftentimes the initial training or fine-tuning step isn’t the final stop on the journey of developing a model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOne of the biggest differences between the traditional method of training models in the classroom versus the real-world is how much control you have over the quality of your data, and consequently the quality of the model produced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we mentioned earlier, developers can upload predictions and use the Model product to \u003ca href=\"https://labelbox.com/blog/how-to-confidently-compare-test-and-evaluate-models-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ediagnose performance issues with models and compare them across multiple experiments\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDoing so automatically populates model metrics that make it easy to evaluate the model’s performance.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"857\" height=\"266\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png 857w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eHere we show how you \u003ca href=\"https://docs.labelbox.com/docs/import-ground-truth?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan upload the ground truth labels\u003c/u\u003e\u003c/a\u003e (the labels generated by Rekognition and used as the fine-tuning dataset) and the \u003ca href=\"https://docs.labelbox.com/docs/upload-model-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003epredictions\u003c/u\u003e\u003c/a\u003e from the fine-tuned YOLO model to the corresponding Quantumworks Lab model. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"evaluating-model-performance\"\u003eEvaluating Model Performance\u003c/h2\u003e\u003cp\u003eThere’s no single metric to rule them all when evaluating your \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecomputer vision model performance\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith that being said, Model offers a number of the most common out-of-the-box. With the \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io#metrics-view\"\u003e\u003cu\u003e‘Metrics view’\u003c/u\u003e\u003c/a\u003e users can drill into crucial model metrics, such as \u003ca href=\"https://docs.labelbox.com/docs/upload-model-metrics-1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econfusion matrix, precision, recall, F1 score, false positive, and more\u003c/u\u003e\u003c/a\u003e, to surface model errors.\u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive, which means you can click on any chart or metric to immediately open up the gallery view of the model run and see corresponding examples, as well as visually compare model predictions between multiple model runs.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-7-evaluate-predictions-from-different-yolo-model-runs-in-labelbox-model\"\u003eStep 7: Evaluate predictions from different YOLO model runs in Quantumworks Lab Model\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to “Model”\u003c/li\u003e\u003cli\u003eBy fine-tuning the Yolo v8 model with approximately 1000 images annotated using Amazon Rekognition, we can achieve performance similar to the Rekognition model within roughly one hour.\u003c/li\u003e\u003cli\u003eWe can now manually inspect examples of predictions from the fine-tuned YOLO model\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"advanced-considerations\"\u003eAdvanced Considerations\u003c/h2\u003e\u003cp\u003eIn this step-by-step walkthrough, we’ve shown how anyone with any image-based dataset can leverage an image-based foundation model to label, fine-tune and analyze a smaller but mighty custom model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAdditional considerations users should address for scaling similar projects include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAutomating future data ingestion, curation, enrichment, and labeling when the fine-tuned model needs to be retrained due to drift.\u003c/li\u003e\u003cli\u003eIncorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman evaluators and human-in-the-loop\u003c/u\u003e\u003c/a\u003e, as well as error analysis for identifying and addressing edge cases.\u0026nbsp;\u003c/li\u003e\u003cli\u003eEasy-to-use, user interface that can be customized for various modalities of data when multiple users are involved.\u0026nbsp;\u003c/li\u003e\u003cli\u003eA \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erobust SDK\u003c/u\u003e\u003c/a\u003e for integrating with any MLOps solutions provider, especially when incorporating model monitoring and complex deployment patterns.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEven with fine-tuned models, there’s no such thing as “setting and forgetting”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAll models eventually need to be retrained, with the data refreshed to account for changes.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eIn this tutorial, we demonstrated an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003ecomputer vision\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a YOLO model with labels created in Model Foundry using Amazon Rekognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHopefully you were able to see how easy it is to go from raw data to cutting-edge custom models in less than 30 min.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou learned how the Quantumworks Lab platform enables model distillation by allowing developers to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIngest, explore and prepare image-based datasets using Catalog;\u003c/li\u003e\u003cli\u003eUse any \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage based foundation model\u003c/u\u003e\u003c/a\u003e to automatically label data using Model Foundry as well as how to incorporate human-in-the-loop evaluation using Annotate;\u003c/li\u003e\u003cli\u003eExport these labeled predictions to a cloud-based training environment for fine-tuning;\u003c/li\u003e\u003cli\u003eAutomate the various workflows using the Quantumworks Lab SDK;\u003c/li\u003e\u003cli\u003eEvaluate model performance and analyze model errors using Quantumworks Lab Model.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn the next part of this series we replicate a very similar workflow for NLP, using model distillation to fine-tune a BERT model with labels created in Model Foundry using PaLM2. \u003c/p\u003e","comment_id":"65bd22d782e5680001e07877","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Social-Cards-Example--1-.jpg","featured":false,"visibility":"public","created_at":"2024-02-02T17:13:59.000+00:00","updated_at":"2024-10-02T00:02:14.000+00:00","published_at":"2024-02-01T20:23:00.000+00:00","custom_excerpt":"Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"681516a56e02120001d5ab84","name":"#mikiko","slug":"hash-mikiko","description":null,"feature_image":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/404/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"65bd4ff882e5680001e078f9","name":"Model Distillation","slug":"model-distillation","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/tag/model-distillation/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/end-to-end-workflow-with-model-distillation-for-computer-vision/","excerpt":"Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. ","reading_time":11,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65aadec23c5a9d00013a702d","uuid":"3b49686d-0ba6-4e2e-ba83-03a007d34b03","title":"How to build defect detection models to automate visual quality inspection","slug":"how-to-build-defect-detection-models-to-improve-visual-quality-inspection","html":"\u003cp\u003eWith AI-powered defect detection, you can now easily harness the latest advances in automation and computer vision into your quality inspection models. As the demand for defect-free manufacturing continues to rise, it's important for teams to deliver the highest-quality products for their assembly lines and minimize operational and quality-related costs. Quantumworks Lab empowers the world’s largest manufacturers to leverage AI solutions tailored to their unique defect detection challenges in order to more quickly build intelligent applications.\u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for large-scale defect detection for manufacturing use cases. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving defect detection requires a vast amount of data in the form of images and videos. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e\u0026nbsp;The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e\u0026nbsp;Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to automate their visual inspection through advanced computer vision techniques. Instead of relying on time-consuming manual human review, companies can now leverage AI-assisted data enrichment and flexible training frameworks to quickly build task-specific models that uncover actionable insights for visual inspection faster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1654\" height=\"912\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1654w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through a workflow on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific models to improve defect detection using image segmentation. Specifically, this guide will walk through how you can explore and better understand your visual assets to make more data-driven business decisions for minimizing defects.\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-build-defect-detection-models-to-automate-visual-quality-inspection\"\u003eSee it in action: How to build defect detection models to automate visual quality inspection\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/er234bmdq6\" title=\"Solution Accelerator - Defect Detection - Exploration \u0026amp; Annotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across\u0026nbsp;\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data with Foundry\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below via:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://drive.google.com/file/d/1B1qPR8u9Wp2AkRdWjo-WSM0nsW_nQzdP/view?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://drive.google.com/file/d/1nw_dQGQI-bhYdyZCX_49DwVSDvt4DkWu/view?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eDatabricks Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"explore-and-prepare-your-data\"\u003e\u003cstrong\u003eExplore and prepare your data\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in the \u003ca href=\"https://drive.google.com/file/d/1B1qPR8u9Wp2AkRdWjo-WSM0nsW_nQzdP/view?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, please make a copy of the notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ll be working with a dataset that shows submersible pump impeller images for a defect detection use case\u0026nbsp; – with the goal of quickly curating data and segmenting for chips and frayed edges in order to detect broken parts.\u003c/p\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003ePlease download the dataset and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your data as a CSV.\u003c/li\u003e\u003cli\u003eIf your images sit as individual files in cloud storage, you can reference the URL of these files through our\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your image data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/er234bmdq6\" title=\"Solution Accelerator - Defect Detection - Exploration \u0026amp; Annotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this demo, we'll be using Catalog to find relevant images of submersible pump impellers for our dataset with the goal of annotating broken parts using foundation models.\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"manually-label-a-subset-of-data-to-train-a-first-model-with-auto-segment\"\u003eManually label a subset of data to train a first model with Auto-Segment \u003c/h3\u003e\u003cp\u003eThe next step is to manually label a subset of our data using the Segment Anything Model within Annotate. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-9.13.12-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1033\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-25-at-9.13.12-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-25-at-9.13.12-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-25-at-9.13.12-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-9.13.12-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Let's first start off with two hundred images and include the project that we've just set up within Annotate. Once the data rows have been loaded into our Annotate project, we can begin the labeling process. \u003c/p\u003e\u003cp\u003e2) By navigating to the labeling editor, we can set up and use the ontology that we defined earlier with each of the objects of interest, in this case, chips and frayed edged. \u003c/p\u003e\u003cp\u003e3) Next, let's use the auto-segment tool powered by Facebook's Segment Anything Model to accelerate the overall labeling. This makes use of powerful AI in the back end of the editor and allows the user to simply draw a bounding box around objects of interest before the model automatically identifies the segment mask for chips and edges. \u003c/p\u003e\u003cp\u003e4) An additional aspect you can make use here are keyboard hotkeys to save us time, which is shown as the number next to our object of interest. By simply selecting one of the keys, this allows us to activate the chip annotation. Hovering over the auto-segment tool, we can see the key \"R\" is shown. By selecting \"R\" on our keyboard, we can activate the auto-segment tool on and off. So having tagged this one chip, we may want to select the second one. \u003c/p\u003e\u003cp\u003e5) You can continue through in this manner until you reach an appropriate data set for training your first iteration of your model. In the example shown, once we reach two hundred images, we're now in a position to train the first version of our model and upload our model predictions.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/enbva2ynh1\" title=\"Solution Accelerator - Defect Detection - Model Evaluation (1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"640\" height=\"412\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eHaving uploaded our model predictions, we can step back into the Quantumworks Lab platform to begin to explore our model run and understand how we performed in terms of predictions versus ground truth. \u003c/p\u003e\u003cp\u003eTo do so, you can navigate to the Model tab, find the experiment of interest, in this case defect detection and make sure that we're selecting the run that we have just created. We can begin to explore visually how our predictions compare to the ground truth data.\u003c/p\u003e\u003cp\u003eWe can see that there's some good overlap when it comes to chips, but the frayed edges are performing poorer in this case. Aside from just simply visualizing and exploring in this manner here, we can also step into the Analytics tab which provides various views of performance. You'll see that in the overall object metrics, (which provides a traditional precision recall F1 score) is performing fairly well in terms of where there is an instance of a object be that afraid edge or a chip. \u003c/p\u003e\u003cp\u003eHowever, the intersection over union (IoU) score is still low, which suggests that while we're detecting some objects, we are not correctly detecting the full segmentation area of the objects. This is where you can deep dive into the confusion matrix and see that on the whole chip detection performing significantly better than frayed edges. \u003c/p\u003e\u003cp\u003eNow that we have our first iteration of the model, we can see that there's room for improvement. In the next step, let's use our initial model's predictions as part of a model assisted labeling pipeline to accelerate the overall annotations for our second iteration of the training data.\u003c/p\u003e\u003ch3 id=\"leveraging-model-assisted-labelling-to-accelerate-labeling-and-improve-model-performance\"\u003eLeveraging Model Assisted Labelling to accelerate labeling and improve model\u0026nbsp;performance\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/0vbn9n2n0t\" title=\"Solution Accelerator - Defect Detection - Model Assisted Labelling Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"554\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eHaving trained the first iteration of our model, we can now run inference over the remaining dataset and upload these predictions into our Annotate project as part of a model-assisted labeling pipeline. You can do this by following through the code provided in the \u003ca href=\"https://drive.google.com/file/d/1B1qPR8u9Wp2AkRdWjo-WSM0nsW_nQzdP/view?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e that accompanies this demo.\u003c/p\u003e\u003cp\u003eOnce you've uploaded your predictions to your Annotate project, you can step back into Quantumworks Lab, find the appropriate project and iterate or modify pre-labels and speed up the annotation process.  \u003c/p\u003e\u003cp\u003eYou can continue through with this process, accelerating the time it takes for labeling until you arrive at a suitable sized dataset for training your second iteration of your model. This process will continue until you have reached a model that is of production quality as assessed through the Model tab we have seen previously.\u003c/p\u003e\u003ch3 id=\"set-up-a-second-model-run-and-train-an-improved-model\"\u003eSet up a second Model run and train an improved model\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kufuomb75p\" title=\"Solution Accelerator - Defect Detection - Model Run 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter having progressed through a second round of labeling, we'll arrive at a point where the labeled dataset is sufficiently large for training a second iteration of our model. \u003c/p\u003e\u003cp\u003eIn this case, we have a thousand datarows, and we're now in a position to create our second model run. To do so, we navigate to the model experiment that we created previously.\u003c/p\u003e\u003cp\u003eLet's keep the splits between the train, validate, and test set as they were before and create a new Model run. Once this is done, so we can select Model run and see the splits between training, validation, and test sets. By refreshing this view, we will be able to visualize our labeled data, and continue with the second training of a new model.\u003c/p\u003e\u003ch3 id=\"upload-second-model-results-and-compare-performance\"\u003eUpload second model results and compare performance\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y31qcxz9rm\" title=\"Solution Accelerator - Defect Detection - Model Evaluate 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a next step, after having uploaded our second model runs predictions is  the to  evaluate whether or not we have seen the expected improvement between our two models. \u003c/p\u003e\u003cp\u003eLet's navigate back to our Experiments tab within Labelbox. For our first model we had a particularly low intersection over union (IoU) score. If we're interested in understanding whether or not we've seen improvement in this metric, we can scroll down to the appropriate graphic that demonstrates our intersection over union performance and in the top left, we can select the option to compare against our second model run. \u003c/p\u003e\u003cp\u003eLooking at this graph allows us to compare the intersection over union (IoU) scores between our two classes across both model runs. We can see that while there's been a small improvement in performance for a chip class, there's been a larger performance improvement for frayed edges. This shows that we're heading in the right direction with the overall performance of our second model run being better than that of our first. \u003c/p\u003e\u003cp\u003eIn a real-world setting, a decision point would have to come as to whether or not our second model is ready for production. In the event that it is not, we recommend training a third iteration using our second model to perform model-assisted labeling and accelerate the time for labeling a larger proportion of our dataset. \u003c/p\u003e\u003cp\u003eTeams can repeat this iterative process at each stage, evaluating whether or not your model performance is improving with each iteration. In conclusion, this workflow should be able to help you leverage Quantumworks Lab at each stage from evaluating models to helping speed up labeling and accelerating your time to value.\u003c/p\u003e\u003ch2 id=\"summary\"\u003eSummary\u003cbr\u003e\u003c/h2\u003e\u003cp\u003eBy analyzing high-volumes of images using automation and model-assisted labeling, Quantumworks Lab provides teams with the ability to inject valuable human-in-the-loop insights for delivering better models that allow you to detect defects faster that can help improve throughput, quality assurance and overall visual inspection.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful task-specific models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"65aadec23c5a9d00013a702d","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-19-at-3.34.17-PM.png","featured":false,"visibility":"public","created_at":"2024-01-19T20:42:42.000+00:00","updated_at":"2024-03-01T21:14:26.000+00:00","published_at":"2024-01-22T17:38:30.000+00:00","custom_excerpt":"Learn how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection using image segmentation for visual inspection.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},"url":"https://labelbox-guides.ghost.io/how-to-build-defect-detection-models-to-improve-visual-quality-inspection/","excerpt":"Learn how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection using image segmentation for visual inspection.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65a881b23c5a9d00013a6f62","uuid":"8ae52e1c-9e63-40ed-8b21-ef8098acb112","title":"How to build defect detection models to improve predictive maintenance","slug":"how-to-build-defect-detection-models-to-improve-preventative-maintenance-2","html":"\u003cp\u003eWith AI-powered defect detection, you can now seamlessly integrate the latest advances in foundation models into your equipment maintenance and QA operations. As the demand for better monitoring continues to rise, it's essential for teams to maximize the lifespan of their critical assets and minimize operational and quality-related costs. Quantumworks Lab empowers the world’s largest organizations to leverage AI solutions tailored to their unique object detection challenges.\u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for large-scale defect detection. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving defect detection requires a vast amount of data in the form of images and videos. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e\u0026nbsp;The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e\u0026nbsp;Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their predictive maintenance through advanced computer vision techniques. Instead of relying on time-consuming manual human review, companies can leverage Quantumworks Lab’s AI-assisted data enrichment and flexible training frameworks to quickly build task-specific models that uncover actionable insights for defects faster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1654\" height=\"912\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1654w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through an end-to-end workflow on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection on pipes. Specifically, this guide will walk through how you can explore and better understand your assets to make more data-driven business decisions for predictive maintenance.\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-build-defect-detection-models-to-improve-predictive-maintenance\"\u003eSee it in action: How to build defect detection models to improve predictive maintenance\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xofopzl9cm\" title=\"Set up project - 1 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across\u0026nbsp;\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data with Foundry\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below via:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1s4UZoQfrxrB_WlrQVmuc068sWQst6aSM?usp=sharing\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003e\u003cstrong\u003ePart 1: Explore and prepare your data\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in the \u003ca href=\"https://colab.research.google.com/drive/1s4UZoQfrxrB_WlrQVmuc068sWQst6aSM?usp=sharing\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, please make a copy of the notebook.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ocyptoa2t2\" title=\"Add data - 2 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ll be working with a dataset that shows multiple parts of equipment for pipes for a defect detection use case\u0026nbsp; – with the goal of quickly curating data and finding 3 specific parts (i.e., pipe, flange, elbow) from high-volumes of images to detect corrosion and broken parts.\u003c/p\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003ePlease download the dataset and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your data as a CSV.\u003c/li\u003e\u003cli\u003eIf your images sit as individual files in cloud storage, you can reference the URL of these files through our\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your image data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/c10qgqqpoq\" title=\"Curate and prioritize - 3 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this demo, we'll be using Catalog to find relevant images of pipes for our dataset with the goal of annotating bounding boxes for the pipe parts using foundation models.\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"using-foundry-to-pre-label-bounding-boxes\"\u003eUsing Foundry to pre-label bounding boxes\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/8b0nlr5afz\" title=\"Human in the loop - 4 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn this next step, we'll walk through how you can take a human-in-the-loop approach to iterate or modify pre-labels and speed up the annotation process.  \u003c/p\u003e\u003cp\u003eModel Foundry enables teams to choose from a library of models and in this case, we'll be comparing the effectiveness of two object detection models (Grounding DINO vs. OWL-VT) to generate previews and attach them as pre-labels.\u003c/p\u003e\u003cp\u003eWith\u0026nbsp;\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, you can automate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/p\u003e\u003ch2 id=\"part-2-train-a-yolov8-model-and-generate-predictions\"\u003e\u003cstrong\u003ePart 2: \u003c/strong\u003eTrain a YOLOv8 model and generate predictions\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/8cl9s78sj4\" title=\"Experiments - 5 - Defect Detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe next step is to take all of this labeled data and train our model on it, and then  make predictions on this data which allows Quantumworks Lab to calculate evaluation metrics, and see where the model is going wrong.\u003c/p\u003e\u003cp\u003eAs an additional visual method, you can navigate\u0026nbsp;to\u0026nbsp;the\u0026nbsp;Quantumworks Lab projector\u0026nbsp;view to visualize\u0026nbsp;different\u0026nbsp;groups\u0026nbsp;or\u0026nbsp;clusters\u0026nbsp;of\u0026nbsp;different\u0026nbsp;classes.\u0026nbsp;You'll see that there\u0026nbsp;are\u0026nbsp;three\u0026nbsp;different\u0026nbsp;clusters,\u0026nbsp;which\u0026nbsp;aligns\u0026nbsp;with\u0026nbsp;our\u0026nbsp;expectations because\u0026nbsp;we\u0026nbsp;have\u0026nbsp;three\u0026nbsp;different\u0026nbsp;classes\u0026nbsp;across\u0026nbsp;pipes,\u0026nbsp;elbows,\u0026nbsp;and\u0026nbsp;flanges. This allows you to  find outliers in the clusters to provide an initial review.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/8akbbpbmpb\" title=\"Train Yolo v8 model - 7- defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/lr4zqg10ax\" title=\"Run inference - 8 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRun inference from trained model on unlabeled data from your Databricks notebook\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/feuc47q3b6\" title=\"alternative - 9 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOne alternative method to running the inference workflow in the previously shown step is to take model weights and deploy them directly within Quantumworks Lab Foundry as a custom model. The benefits of this is that it will allow you to run predictions using your custom model as an end-to-end workflow and more quickly classify parts of interest (i.e., elbows, pipes, and flanges). \u003c/p\u003e\u003cp\u003e\u003cu\u003eNote\u003c/u\u003e: If this is interesting and you're looking to adopt this method within Quantumworks Lab, please reach out to our support team as we would be happy to assist with deploying your custom model within Foundry.\u003c/p\u003e\u003ch3 id=\"view-model-predictions-within-the-labelbox-ui-to-evaluate-and-diagnose-model-effectiveness\"\u003eView model predictions within the Quantumworks Lab UI to evaluate and diagnose model effectiveness\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jxi7miwgs6\" title=\"final step - 10 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a last step, let's compare model inferences with ground-truth annotations to see where the model may be underperforming. A disagreement between model predictions and ground truth labels can be due to a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eAfter running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/li\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model for 10x faster corner-case detection – detect and visualize corner-cases where the model is underperforming. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection and additional labeling.\u003c/p\u003e\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f78ql0djrc\" title=\"Summary - 11 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBy analyzing high-volumes of images and videos using foundation models and human alignment, Quantumworks Lab provides teams with the ability to inject valuable human-in-the-loop insights for delivering better models that allow you to detect defects that can help improve operational efficiency, quality assurance and overall equipment lifespan.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful task-specific models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"65a881b23c5a9d00013a6f62","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-19-at-12.44.25-PM.png","featured":false,"visibility":"public","created_at":"2024-01-18T01:41:06.000+00:00","updated_at":"2024-03-01T21:10:16.000+00:00","published_at":"2024-01-18T18:06:14.000+00:00","custom_excerpt":"In this guide, we’ll walk through an end-to-end tutorial on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection on pipes.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-build-defect-detection-models-to-improve-preventative-maintenance-2/","excerpt":"In this guide, we’ll walk through an end-to-end tutorial on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection on pipes.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":74,"tag":{"slug":"build-ai","id":"653aa45d375d13000123d7de","name":"Build AI","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","count":{"posts":74},"url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"slug":"build-ai","currentPage":"2"},"__N_SSG":true},"page":"/guides/tag/[id]/page/[pagenum]","query":{"id":"build-ai","pagenum":"2"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/tag/build-ai/page/2/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:17:57 GMT -->
</html>