<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/tag/build-ai/page/4/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 13:06:03 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../../../static/scripts/munchkin.js"></script><script src="../../../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/guides/tag/%5bid%5d/page/%5bpagenum%5d-da4e9ee1c105845a.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../../../index.html"><img width="106" height="24" alt="logo" src="../../../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../../index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Build AI</a><a href="../../../use-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Use AI</a><a href="../../../explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="../../../label-data-for-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Label data for AI</a><a href="../../../train-fine-tune-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../../../mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../a-guide-to-the-data-i-o-process-in-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index9d23.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../a-guide-to-the-data-i-o-process-in-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">A guide to the Data I/O process in Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-uncover-model-errors-with-labelbox-model/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexdb1e.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-uncover-model-errors-with-labelbox-model/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to uncover model errors with Quantumworks Lab Model</p><p class="text-base max-w-2xl undefined line-clamp-3">Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index38a9.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Meta&#x27;s Segment Anything (SAM) model on video with Quantumworks Lab&#x27;s model-assisted labeling</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../leveraging-yolo-and-labelbox-to-make-videos-queryable-by-content/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FScreen-Shot-2023-05-24-at-1.45.31-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FScreen-Shot-2023-05-24-at-1.45.31-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FScreen-Shot-2023-05-24-at-1.45.31-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FScreen-Shot-2023-05-24-at-1.45.31-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FScreen-Shot-2023-05-24-at-1.45.31-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FScreen-Shot-2023-05-24-at-1.45.31-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FScreen-Shot-2023-05-24-at-1.45.31-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FScreen-Shot-2023-05-24-at-1.45.31-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexedfb.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FScreen-Shot-2023-05-24-at-1.45.31-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../leveraging-yolo-and-labelbox-to-make-videos-queryable-by-content/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Leveraging YOLO and Quantumworks Lab to make videos queryable by content</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we&#x27;ll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexc8e8.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-accelerate-image-text-pair-generation-with-blip-2/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexf5d3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-accelerate-image-text-pair-generation-with-blip-2/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to accelerate image-text pair generation with BLIP-2</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../automatically-label-text-with-96-accuracy-using-foundation-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexf3e3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../automatically-label-text-with-96-accuracy-using-foundation-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Automatically label text with 96%+ accuracy using foundation models</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab&#x27;s search capabilities, bulk classification, and foundation models.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../automatically-label-images-with-99-accuracy-using-foundation-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index200d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../automatically-label-images-with-99-accuracy-using-foundation-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Automatically label images with 99% accuracy using foundation models</p><p class="text-base max-w-2xl undefined line-clamp-3">Automatically label images with 99% accuracy leveraging Quantumworks Lab&#x27;s search capabilities, bulk classification, and foundation models. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-export-your-data-with-more-granular-control/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FFrame-2299--3-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FFrame-2299--3-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FFrame-2299--3-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FFrame-2299--3-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FFrame-2299--3-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FFrame-2299--3-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FFrame-2299--3-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FFrame-2299--3-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index8519.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FFrame-2299--3-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-export-your-data-with-more-granular-control/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Introducing Export V2: How to export data with more granular control</p><p class="text-base max-w-2xl undefined line-clamp-3">With Export V2, you can export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-create-and-label-text-layers-from-pdf-documents-for-ai/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index408d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-create-and-label-text-layers-from-pdf-documents-for-ai/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to create and label text layers from PDF documents for AI</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn about the different types of PDF layers and how to import annotations to build robust AI models with contextual information from PDF documents.</p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8"><a class="mr-9 text-neutral-700 mb-1" href="../3/index.html">&lt;</a>Page 4 of 8<a class="ml-9 text-neutral-700 mb-1" href="../5/index.html">&gt;</a></div></div></div></div></div></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"64cab907306044000155752f","uuid":"baf1efed-584a-42c0-975b-78b8368fad3d","title":"A guide to the Data I/O process in Quantumworks Lab","slug":"a-guide-to-the-data-i-o-process-in-labelbox","html":"\u003cp\u003eAs you navigate the world of intelligent application creation, one element remains pivotal - your data. At Quantumworks Lab, we recognize the value of your data and its role in driving your operations, which is why we focus on simplifying the Data In and Out (I/O) process. In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with our platform.\u003c/p\u003e\u003cp\u003eWhether your goal is to store data in a cloud-hosted table, a ML training pipeline, a database, or even a production environment, our aim is to equip you with the knowledge and tools needed for a flexible, robust, and effective data management system.\u003c/p\u003e\u003ch2 id=\"understanding-data-io\"\u003eUnderstanding Data I/O\u003c/h2\u003e\u003cp\u003eAt its core, Data I/O refers to the import and export of data in your Quantumworks Lab workflow. As simple as it sounds, the process can be quite intricate, given the variety of data formats and storage locations. To manage data effectively, Quantumworks Lab uses a structured approach that tackles each data type - \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io\"\u003edata rows\u003c/a\u003e, \u003ca href=\"https://docs.labelbox.com/docs/import-metadata?ref=labelbox-guides.ghost.io\"\u003emetadata\u003c/a\u003e (including embeddings), \u003ca href=\"https://docs.labelbox.com/reference/attachments?ref=labelbox-guides.ghost.io\"\u003eattachments\u003c/a\u003e, and \u003ca href=\"https://docs.labelbox.com/reference/feature?ref=labelbox-guides.ghost.io\"\u003eannotations\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"data-in\"\u003eData In\u003c/h2\u003e\u003ch3 id=\"data-rows\"\u003eData Rows\u003c/h3\u003e\u003cp\u003eData rows typically exist in cloud storage like Amazon Web Services (AWS), Google Cloud Storage (GCS), or Microsoft Azure (Azure). However, data rows can also exist as local files, offering you flexibility in how you access and utilize your data.\u003c/p\u003e\u003ch3 id=\"setting-up-data-in\"\u003eSetting Up Data In\u003c/h3\u003e\u003cp\u003eFor those using cloud storage, setting up \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eDelegated Access\u003c/a\u003e is the first step. This involves granting Quantumworks Lab permission to securely gain read access to your unlabeled data as hosted in your preferred cloud storage provider while providing Quantumworks Lab with the limited access necessary to display and label your data. Once access is granted, you need to identify where your metadata (including embeddings) and attachments are stored.\u003c/p\u003e\u003cp\u003eRefer to the below links to learn more about setting up Delegated Access with the below cloud storage providers:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/import-aws-s3-data?ref=labelbox-guides.ghost.io\"\u003eAmazon S3\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/using-google-cloud-storage?ref=labelbox-guides.ghost.io\"\u003eGoogle Cloud\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/microsoft-azure-blob-storage?ref=labelbox-guides.ghost.io\"\u003eMicrosoft Azure\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"attachments-and-metadata\"\u003eAttachments and Metadata\u003c/h3\u003e\u003cp\u003eAttachments and metadata traditionally exist in a table (Databricks, Excel, BigQuery, CSV, etc.) alongside or separately from the data rows. In an attempt to maintain consistency and streamline the process, Quantumworks Lab encourages users to upload metadata and attachments directly with the data rows.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Metadata\u003c/strong\u003e is any information known about an asset pre-Labelbox that could be useful in data filtering and selection Metadata also includes embeddings - these are representations of your data in a vector space. These vectors capture the essential features of your data and represent them in a form that can be processed by machine learning algorithms.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/metadata?ref=labelbox-guides.ghost.io\"\u003eDeveloper guide on metadata\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2) Attachments\u003c/strong\u003e on the other hand could be any additional files or information that supplement your data and assist in the creation of high quality human labels. This could include anything from text documents with descriptive data, additional images, audio files, or any other data type that provides more context to your main data row.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/attachments?ref=labelbox-guides.ghost.io\"\u003eDeveloper guide on attachments\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Embeddings \u003c/strong\u003ecan improve your data exploration and allow you to make use of similarity search within Catalog. Lablebox computes off-the-shelf embeddings using neural networks trained on publicly available data. Off-the-shelf embeddings provide a useful starting point to explore your data, but to get the most out of similarity search you’ll want to experiment with different embeddings to power your selection based on your particular data. \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/custom-embeddings?ref=labelbox-guides.ghost.io\"\u003eDeveloper guide on embeddings\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHere are a few pointers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf your data is in a BigQuery table, you can refer to this \u003ca href=\"https://github.com/Quantumworks Lab/labelbox-bigquery/tree/main?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eIf your data is in a Databricks table, check out this \u003ca href=\"https://github.com/Quantumworks Lab/labelspark?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eFor data in a CSV format, use this \u003ca href=\"https://github.com/Quantumworks Lab/labelpandas?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eFor unique data formats, our comprehensive Quantumworks Lab documentation will be your guide\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"annotations\"\u003eAnnotations\u003c/h3\u003e\u003cp\u003eAs an output from machine learning models, annotations typically exist in JSON files and can be stored either locally or in cloud storage. The beauty of annotations lies in their versatility; they come in various forms including bounding box, mask, radio classification, and others, giving you the freedom to choose what best suits your application.\u003c/p\u003e\u003cp\u003eWhen it comes to Quantumworks Lab, if you want to upload pre-labels or submitted labels (labels made elsewhere), the process involves a few crucial steps:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Setting up a Quantumworks Lab Project and Ontology:\u003c/strong\u003e An ontology serves as a blueprint for your labeling project. It defines the labels and the structure for the annotation data you are handling. Setting up an ontology in Quantumworks Lab that aligns with your annotation data is an essential step to ensure that your pre-labels or submitted labels can be properly read and processed by the system. You can learn more about how to set up ontologies in our \u003ca href=\"https://chat.openai.com/c/link?ref=labelbox-guides.ghost.io\"\u003edeveloper guide on ontologies\u003c/a\u003e.\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Understanding the Annotation Data Format:\u003c/strong\u003e Quantumworks Lab supports a wide range of data formats including JSON, CSV, and others. It is important to understand the format of your annotation data to ensure compatibility with the Quantumworks Lab platform.\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Identifying the Annotation Types:\u003c/strong\u003e The type of annotation used is dependent on your use case. It could be a bounding box annotation for object detection tasks, a mask annotation for semantic segmentation tasks, or a radio classification for multi-choice tasks. By identifying the annotation types that your use case requires, you can ensure that your data is appropriately annotated for your model.\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4) Determining the Annotation Format:\u003c/strong\u003e Annotation formats can either be customer-specific or adhere to industry standards like COCO. Understanding this will help you prepare your data in a way that fits the requirements of Quantumworks Lab and aids in efficient data processing.\u003c/p\u003e\u003cp\u003eBy paying close attention to these steps, you can maximize the utility of your annotations, thereby boosting the effectiveness of your labeling projects and the performance of your machine learning models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5) Converting Annotations to Quantumworks Lab Format: \u003c/strong\u003eLabelbox supports two formats for importing annotations: NDJSON and Python annotation type. How these annotations are uploaded depends on your media type, see the links below for further information.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-image-annotations?ref=labelbox-guides.ghost.io\"\u003eImport image annotations \u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-video-annotations?ref=labelbox-guides.ghost.io\"\u003eImport video annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-text-annotations?ref=labelbox-guides.ghost.io\"\u003eImport text annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-geospatial-annotations?ref=labelbox-guides.ghost.io\"\u003eImport geospaital annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-document-annotations?ref=labelbox-guides.ghost.io\"\u003eImport document annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-conversational-text-annotations?ref=labelbox-guides.ghost.io\"\u003eImport conversational text annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-audio-annotations?ref=labelbox-guides.ghost.io\"\u003eImport audio conversations\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy paying close attention to these steps, you can maximize the utility of your annotations, thereby boosting the effectiveness of your labeling projects and the performance of your machine learning models.\u003c/p\u003e\u003ch2 id=\"data-out\"\u003e\u003cbr\u003eData Out\u003c/h2\u003e\u003cp\u003eContrary to data in, the data out process focuses on how data is extracted from Labelbox. Extracting data from Quantumworks Lab involves exporting labeled data rows, along with their associated metadata, attachments, and annotations. Data exported from Quantumworks Lab can be used for a variety of purposes, such as model training or data enrichment. Given that every model has unique input requirements and organizations have unique data storage formats, the export process is often more unique than the import.\u003c/p\u003e\u003ch3 id=\"streamlining-data-out\"\u003eStreamlining Data Out\u003c/h3\u003e\u003cp\u003eThe Quantumworks Lab platform supports a variety of data formats and storage solutions to ensure that your data is exported in a format that suits your needs and is compatible with your storage system. Whether you're using BigQuery, Databricks, CSV, or other formats, Quantumworks Lab has a solution for you. Here's how you can navigate the process:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Choose an Export Format:\u003c/strong\u003e Determine the export format that suits your needs and is compatible with your storage system. Quantumworks Lab supports a wide variety of formats including JSON, CSV, and others. This gives you the flexibility to choose a format that best aligns with your downstream workflows.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Select a Connector:\u003c/strong\u003e Just as with data in, Quantumworks Lab offers connectors to aid in exporting data. These connectors are designed to seamlessly bridge the gap between Quantumworks Lab and your storage system, making the data out process efficient and hassle-free.  Quantumworks Lab offers connectors to aid in exporting data:\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor BigQuery users, this \u003ca href=\"https://github.com/Quantumworks Lab/labelbox-bigquery/tree/main?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e will come in handy\u003c/li\u003e\u003cli\u003eDatabricks users can refer to this \u003ca href=\"https://github.com/Quantumworks Lab/labelspark?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eFor CSV-formatted data, this \u003ca href=\"https://github.com/Quantumworks Lab/labelpandas?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e is useful\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Export your Data:\u003c/strong\u003e Initiate the export process via your chosen connector. During the export, Quantumworks Lab compiles your labeled data rows, associated metadata, attachments, and annotations, and organizes them in your chosen export format.\u003c/p\u003e\u003cp\u003eFor a deeper understanding of the process, our \u003ca href=\"https://docs.labelbox.com/reference/export-v2-glossary?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e provides a wealth of information.\u003c/p\u003e\u003cp\u003eIn essence, the Data I/O process is a crucial aspect of your interaction with Quantumworks Lab, designed to make your data management effortless. As you become more familiar with the process, you'll find it an essential tool in creating intelligent applications with Labelbox.\u003c/p\u003e\u003ch3 id=\"utilizing-your-exported-data\"\u003eUtilizing Your Exported Data\u003c/h3\u003e\u003cp\u003eOnce your data is exported, it is ready to be utilized for a variety of purposes:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Model Training:\u003c/strong\u003e The primary use case of exported data is to feed it into your machine learning models. The labeled data serves as the training data, guiding your models to recognize patterns and make predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Data Analysis \u0026amp; Enrichment:\u003c/strong\u003e The exported data, especially metadata and annotations, can provide valuable insights when analyzed. Additionally, it can enrich your existing data sets, enhancing the accuracy and detail of your data and leading to more effective models and analytics. This could guide decision-making processes and strategies within your organization.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Iterative Improvements:\u003c/strong\u003e In some cases, exported data can also be fed back into your annotation process for iterative improvements, creating a feedback loop that continually enhances your data quality and model performance.\u003c/p\u003e\u003cp\u003eIn conclusion, the Data I/O process in Quantumworks Lab is an integral component to successful application creation. This guide provides an in-depth understanding of the process, allowing you to accurately import and export data, whether that be data rows, metadata, attachments, or annotations. The process ensures compatibility with various data formats and storage systems, while facilitating efficient data management for your machine learning projects. \u003c/p\u003e\u003cp\u003eFor detailed instructions or further understanding, our comprehensive \u003ca href=\"https://docs.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e is always available.\u003c/p\u003e","comment_id":"64cab907306044000155752f","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078.png","featured":false,"visibility":"public","created_at":"2023-08-02T20:13:59.000+00:00","updated_at":"2023-10-27T17:13:05.000+00:00","published_at":"2023-08-03T13:44:19.000+00:00","custom_excerpt":"In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guide/a-guide-to-the-data-i-o-process-in-labelbox","tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/a-guide-to-the-data-i-o-process-in-labelbox/","excerpt":"In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"A guide to the Data I/O process in Quantumworks Lab","meta_description":"In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"648a3024f45029000168a9d5","uuid":"df001e73-f4a2-4a45-8bc7-8ebbaa093778","title":"How to uncover model errors with Quantumworks Lab Model","slug":"how-to-uncover-model-errors-with-labelbox-model","html":"\u003cp\u003eMachine learning models are only as good as the quality of their predictions. But how do you ensure that your model is making accurate predictions, and more importantly, how do you identify and address the errors your model might be making? In this guide, we will be exploring how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.\u003c/p\u003e\u003cp\u003eIn the examples below, we’ll start by uploading predictions from YOLOv8 along with confidence scores to Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003emodel run\u003c/a\u003e. We’ll also use an open source dataset to provide a diverse dataset for testing the model. You can get instructions on how to upload model inferences to Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/page/tutorials?ref=labelbox-guides.ghost.io#upload-predictions-to-a-model-run\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"gallery-view-identifying-disagreements\"\u003eGallery View: Identifying disagreements\u003c/h2\u003e\u003cp\u003eGallery View is a powerful tool for identifying disagreements between model predictions and ground truth labels. Here's how to use it for error analysis:\u003c/p\u003e\u003cp\u003e1) Go to the Gallery View within a model run. You may choose to focus on the Validation or Test split if you prefer.\u003c/p\u003e\u003cp\u003e2) Apply a Metrics filter to identify images with metrics that could indicate disagreements between model predictions and ground truth annotations. Users can sort the assets based on any combination of IOU, confidence, recall, false negative, and false positive etc.\u003c/p\u003e\u003cp\u003e3) Sort the data rows either by increasing metrics or increasing order of confidence. This can help surface rows where the model is least confident or is likely to be erroneous.\u003c/p\u003e\u003cp\u003e4) Inspect the surfaced data rows in detail to identify patterns of edge cases where the model is struggling. This may involve manually inspecting hundreds of data rows.\u003c/p\u003e\u003cp\u003eThe video below shows an uncertainty sampling based on a low-confidence example.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7mair0juaw\" title=\"[Uncover model errors] MEA low confidence Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe video below shows how to find model errors for a particular class by sorting in ascending order for IOU. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/tlgntztpau\" title=\"[Uncover model errors] IOU ascending Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eIdentifying incorrect model prediction based on low intersection over union\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-metrics-identifying-struggling-classes\"\u003eModel Metrics: Identifying struggling classes\u003c/h2\u003e\u003cp\u003eThe Metrics View provides a comprehensive overview of how your model is performing. You can leverage this view to quickly identify classes that your model might be struggling with by:\u003c/p\u003e\u003col\u003e\u003cli\u003eInspecting the metrics in the Metrics View. In this example, we’re finding predictions that are false negatives and have recall value for a person between .2 to .3.\u003c/li\u003e\u003cli\u003eClicking on the recall value for person between .2 to .3 will open the Gallery View, which will have filtering and sorting activated to show assets associated with the particular class.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xumtw94n2i\" title=\"[Uncover model errors] Metrics view Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch2 id=\"fix-model-errors\"\u003eFix model errors\u003c/h2\u003e\u003cp\u003eOnce you have identified a pattern of incorrect model predictions, you can find similar assets that the model will also struggle with and send them to be labeled before retraining your model.\u003c/p\u003e\u003col\u003e\u003cli\u003eSelect data rows on which your model is struggling.\u003c/li\u003e\u003cli\u003eOpen the selected data rows in Catalog by clicking on [n] selected \u0026gt; View in Catalog. You will then be redirected to a filtered view of your Catalog showing only the previously selected data rows.\u003c/li\u003e\u003cli\u003eUse \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003esimilarity search\u003c/a\u003e to surface data similar to this pattern of model failures among all of the data in your Catalog. Optionally, you could create a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e that will automatically collect any similar data uploaded in the future.\u003c/li\u003e\u003cli\u003eNext, you could filter on Annotation \u0026gt; is none to surface only unlabeled data rows. Labeling this high-impact data and then re-training your model is a powerful way to boost model performance. Create a \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003ebatch\u003c/a\u003e and send it to a labeling project.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/htzhdxduhh\" title=\"[Uncover model errors] Curating a batch Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLabelbox offers powerful tools and workflows to not only uncover model errors but also to improve the performance of your machine learning models over time. By leveraging the Gallery View, Model Metrics, and Projector View, you can identify where model might be struggling. Additionally, with the ability to fix these errors through data-centric iterations, you can ensure that your model becomes more accurate and reliable with each iteration.\u003c/p\u003e","comment_id":"648a3024f45029000168a9d5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3065--2-.png","featured":false,"visibility":"public","created_at":"2023-06-14T21:24:52.000+00:00","updated_at":"2023-10-27T17:14:26.000+00:00","published_at":"2023-06-15T17:22:00.000+00:00","custom_excerpt":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-uncover-model-errors-with-labelbox-model/","excerpt":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":"How to uncover model errors with Gallery View and Model Metrics","og_description":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","twitter_image":null,"twitter_title":"How to uncover model errors with Gallery View and Model Metrics","twitter_description":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","meta_title":"How to uncover model errors with Gallery View and Model Metrics","meta_description":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6478fbf1f06ecd0001d70bc5","uuid":"506ee668-bb83-4bc8-80b1-8c9b4c76e759","title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling","slug":"using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWhile Yolov8 is no longer supported on Quantumworks Lab, this blog remains relevant if you are working with other object detection models. Alternatives such as OWL-ViT, Rekognition, GroundingDINO, and GroundingDINO + SAM can still be found and used on Quantumworks Lab’s platform.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this guide, we will demonstrate the application of foundation models, such as Meta’s Segment Anything and YOLOv8, to automatically detect, classify and draw masks on objects of interest in a video. This is a follow-up to earlier guide: \u003ca href=\"https://labelbox.com/guides/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/?ref=labelbox-guides.ghost.io\"\u003eUsing Meta’s Segment Anything with YOLOv8 to Automatically Classify Masks\u003c/a\u003e. In this guide, we’ll automatically detect and segment objects in a video.\u003c/p\u003e\u003cp\u003eVideos have many frames and are tedious to label. Segmentation masks are even more time consuming to label as they vary ever so slightly frame-by-frame, requiring manual fine-tuning each time. With foundation models, you can automate and significantly speed up the labeling process to label more video data, in less time. This allows you to focus valuable time on review, simply correcting the AI models’ output.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"498\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we will be walking through a simple semantic segmentation task: drawing masks around a person as they skateboard.\u003c/p\u003e\u003cp\u003eHere’s a high-level summary of the process that we will be walking through step-by-step below, with code:\u003c/p\u003e\u003cp\u003e1) Load YOLOv8, SAM and Quantumworks Lab Python SDK\u003c/p\u003e\u003cp\u003e2) For each frame of the video:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun an object detector to generate bounding boxes with classifications for specified classes\u003c/li\u003e\u003cli\u003eFeed the bounding boxes as inputs to Meta’s Segment Anything model which will produce segmentation masks\u003c/li\u003e\u003cli\u003ePrepare mask predictions in a format that Quantumworks Lab Python SDK expects\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e3) Upload all frames at once to Quantumworks Lab via prediction import\u003c/p\u003e\u003cp\u003e4) Open up video editor and review or modify the pre-labels as you usually do\u003c/p\u003e\u003cp\u003eYou can run all of the above out-of-the-box on your video(s) using our \u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/sam/meta_sam_labelbox_video.ipynb?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e. Simply load in your video and get automatically segmented masks, with classes in Quantumworks Lab, in minutes!\u003c/p\u003e\u003cp\u003eFor this guide, we will use the following video:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-orig.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-orig.gif 600w\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-load-yolov8\"\u003eStep 1: Load YOLOv8\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.ultralytics.com/?ref=labelbox-guides.ghost.io\"\u003eYOLOv8\u003c/a\u003e is a state-of-the-art object detector that produces bounding boxes and classes around common objects. It's the latest iteration of the YOLO (You Only Look Once) family of models, and it boasts some impressive features. YOLOv8 is known for its speed and accuracy, making it an invaluable tool for a wide range of applications. Here, we use YOLOv8 to automatically detect and localize the person skateboarding in the video.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport ultralytics\nultralytics.checks()\nfrom ultralytics import YOLO\nmodel = YOLO(f'{HOME}/yolov8n.pt')\n\n# each class id is assigned a different color\ncolors = np.random.randint(0, 256, size=(len(model.names), 3))\nprint(model.names)\n\n# Specify which classes you care about. The rest of classes will be filtered out.\nchosen_class_ids = [0] # 0 refers to person, as per model.names\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-2-load-sam\"\u003eStep 2: Load SAM\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eMeta's SAM model\u003c/a\u003e is a state-of-the-art computer vision model that is designed to accurately segment images and videos into distinct objects. Using advanced deep learning techniques, Segment Anything is able to identify and segment objects in images, making it a powerful tool for a wide range of applications. The SAM model is able to generate segmentation masks based on prompts, including bounding box prompts, which we will use in the code below.\u003c/p\u003e\u003cp\u003e\u003cbr\u003eFor an in-editor experience of SAM, please read our other blog post \u003ca href=\"https://labelbox.com/blog/coming-soon-auto-segment-powered-by-sam/?ref=labelbox-guides.ghost.io\"\u003eAuto-Segment 2.0 powered by Meta’s Segment Anything Model\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport torch\nimport matplotlib.pyplot as plt\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nsam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\nmask_predictor = SamPredictor(sam)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-3-load-labelboxs-python-sdk\"\u003eStep 3: Load Quantumworks Lab's Python SDK\u003c/h2\u003e\u003cp\u003eLabelbox’s Python SDK gives you easy methods to create ontologies, projects and datasets, and upload masks to a video.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab as lb\nimport labelbox.types as lb_types\n\n# Create a Quantumworks Lab API key for your account by following the instructions here:\n# https://docs.labelbox.com/reference/create-api-key\n# Then, fill it in here\nAPI_KEY = \"\"\nclient = lb.Client(API_KEY)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-4-run-yolov8-and-sam-per-frame\"\u003eStep 4: Run YOLOv8 and SAM per-frame\u003c/h2\u003e\u003cp\u003eHere we run the models on each frame and generate masks automatically.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003ecap = cv2.VideoCapture(VIDEO_PATH)\n\n# This will contain the resulting mask predictions for upload to Quantumworks Lab\nmask_frames = []\n\nframe_num = 1\nwhile cap.isOpened():\n  ret, frame = cap.read()\n  if not ret:\n    break\n\n  # Run frame through YOLOv8 to get detections\n  detections = model.predict(frame, conf=0.7)\n \n  # Run frame and detections through SAM to get masks\n  transformed_boxes = mask_predictor.transform.apply_boxes_torch(detections[0].boxes.xyxy, list(get_video_dimensions(cap)))\n  mask_predictor.set_image(frame)\n  masks, scores, logits = mask_predictor.predict_torch(\n    boxes = transformed_boxes,\n    multimask_output=False,\n    point_coords=None,\n    point_labels=None\n  )\n\n  # Combine mask predictions into a single mask, each with a different color\n  class_ids = detections[0].boxes.cpu().cls\n  merged_with_colors = add_color_to_mask(masks[0][0], colors[int(class_ids[0])]).astype(np.uint8)\n  for i in range(1, len(masks)):\n    curr_mask_with_colors = add_color_to_mask(masks[i][0], colors[int(class_ids[i])])\n    merged_with_colors = np.bitwise_or(merged_with_colors, curr_mask_with_colors)\n\n  # Upload multi-colored combined mask to temp location\n  # to get temp instance uri\n  instance_uri = get_instance_uri(client, global_key, merged_colored_mask)\n\n  # Create MaskFrame object to be uploaded to Quantumworks Lab\n  mask_frame = lb_types.MaskFrame(index=frame_num, instance_uri=instance_uri)\n  mask_frames.append(mask_frame)\n\n  frame_num += 1\n\ncap.release()\n\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-bboxes.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-bboxes.gif 600w\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-masks.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-masks.gif 600w\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-5-upload-the-predicted-masks-as-pre-labels-onto-labelbox\"\u003eStep 5: Upload the predicted masks as pre-labels onto Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThe predicted masks can be easily and seamlessly integrated into Quantumworks Lab via our SDK.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Create MaskInstance per unique class predicted / chosen\ninstances = []\n for cid in chosen_class_ids:\n   color = get_color(colors[int(cid)])\n   name = model.names[int(cid)]\n   instances.append(lb_types.MaskInstance(color_rgb=color, name=name))\n\n# Create list of VideoMaskAnnotation objects, one for each unique class\nannotations = []\nfor instance in instances:\n  video_mask_annotation = lb_types.VideoMaskAnnotation(\n       frames=mask_frames,\n       instances=[instance]\n   )\n  annotations.append(video_mask_annotation)\n\n# Create Label object\nlabels = [\nlb_types.Label(data=lb_types.VideoData(global_key=global_key),\n                  annotations=annotations))\n]\n\n# Run import job\nupload_job = lb.MALPredictionImport.create_from_objects(\n   client=client,\n   project_id=project.uid,\n   name=\"mal_import_job\" + str(uuid.uuid4()),\n   predictions=labels\n)\nupload_job.wait_until_done()\n\nprint(f\"Errors: {upload_job.errors}\", )\nprint(f\"Status of uploads: {upload_job.statuses}\")\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-LB.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"374\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-LB.gif 600w\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eCreating segmentation masks on video data can be tedious and time-consuming. Using the power of foundation models in Quantumworks Lab, you can easily generate masks with classifications in a matter of minutes. Rather than spending hours labeling video data, you now have a way to accelerate video labeling and not only reduce time to market, but also the cost of developing your models.\u003c/p\u003e","comment_id":"6478fbf1f06ecd0001d70bc5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074.png","featured":false,"visibility":"public","created_at":"2023-06-01T20:13:37.000+00:00","updated_at":"2024-11-25T21:18:27.000+00:00","published_at":"2023-06-01T22:14:48.000+00:00","custom_excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/","excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074-1.png","og_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","og_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074-2.png","twitter_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","twitter_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","meta_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","meta_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"646f88789568720001240642","uuid":"b8289ca0-7d94-41c4-84df-a5e0541ba9bb","title":"Leveraging YOLO and Quantumworks Lab to make videos queryable by content","slug":"leveraging-yolo-and-labelbox-to-make-videos-queryable-by-content","html":"\u003cp\u003eVideo data can offer a wealth of information for AI use cases, but because of their dense and dynamic nature, manually extracting insights can be a tedious task. To accelerate this process, leading AI teams often use off-the-shelf, pre-trained models like \u003ca href=\"https://ultralytics.com/yolov8?ref=labelbox-guides.ghost.io\"\u003eYOLO\u003c/a\u003e (short for You Only Look Once) to take a quick first pass at detecting the contents of a video dataset. \u003c/p\u003e\u003cp\u003eYOLO is a real-time object detection model. Unlike traditional models, which scan an image multiple times at different scales, YOLO looks at the entire image only once, making it particularly well-suited for video object detection. It can detect various objects and provide a bounding box for each detected object. Using this model to enrich video data can make it easier for AI teams to understand their data.\u003c/p\u003e\u003cp\u003eHowever, the real magic lies in going a step further by organizing and categorizing this information, making the video content searchable. In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.\u003c/p\u003e\u003ch2 id=\"how-to-make-videos-queryable\"\u003eHow to make videos queryable\u003c/h2\u003e\u003cp\u003eTo make a video queryable, the first step is to run the YOLO model on the video to detect the objects in each frame. Once objects are detected, they are classified, and a bounding box is added to the objects in Labelbox. You can then search your video content based on these annotations with Catalog. Want to find all instances where a \"bowl\" appears in your video? Simply search for \"bowl\" in the Quantumworks Lab UI. Catalog will return all the videos that contain this annotation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"633\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/YOLOGuide_1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/YOLOGuide_1.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_1.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOnce YOLO has added annotations to your video data, you can simply search for specific content within that dataset using Quantumworks Lab Catalog.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can also create a custom workflow in Quantumworks Lab to create a bespoke review process, where you can create tasks that only have assets containing a specific annotation in the video, such as vehicles or other objects of interest. You can let YOLO take the first pass at labeling your video data, and then have human labelers review and/or add annotations in frames that contain specific objects of interest, reducing the time and costs required to label high-quality training data.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"961\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/YOLOGuide_2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/YOLOGuide_2.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_2.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eYou can create a custom labeling workflow that targets specific data in Quantumworks Lab to save labeling time and costs while ensuring high labeling quality.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRead on for step-by-step instructions on using YOLO and Quantumworks Lab to make your videos queryable.\u003c/p\u003e\u003ch2 id=\"part-1-create-project-and-ontology\"\u003ePart 1: Create project and ontology\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eproject = client.create_project(name=\"Video project with YOLO\",\n                                   media_type=lb.MediaType.Video)\n\n\n#connect ontology to your project\nproject.setup_editor(ontology)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"part-2-queue-assets-to-project-based-on-global-keys\"\u003ePart 2: Queue assets to project based on global keys\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003ebatch = project.create_batch(\n \"first-batch-video\"+str(uuid.uuid4()), # Each batch in a project must have a unique name\n global_keys= global_keys, # A paginated collection of data row objects, a list of data rows or global keys\n priority=5 # priority between 1(Highest) - 5(lowest)\n)\nprint(\"Batch: \", batch)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"part-3-load-yolo-model\"\u003ePart 3: Load YOLO model\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003emodel = YOLO(f'{HOME}/yolov8n.pt')\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"part-4-create-predictions-and-upload-them-to-labelbox\"\u003ePart 4: Create predictions and upload them to Quantumworks Lab\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Export queued data rows from the project\nqueued_data_rows = project.export_queued_data_rows()\n\n\n# Initialize an empty list to store labels\nlabels = []\n\n\n# Loop over each data row. Here, it's only looping over the first data row.\nfor data_row in queued_data_rows[:1]:\n\n\n # Extract the URL of the video from the data row.\n video_url = data_row[\"rowData\"]\n\n\n # Extract the global key from the data row.\n global_key = data_row[\"globalKey\"]\n\n\n # Make a GET request to the video URL.\n response = requests.get(video_url)\n\n\n # Open a file in write-binary mode and write the content of the response to it.\n # This is downloading the video and saving it as 'sample_video.mp4'.\n with open('sample_video.mp4', 'wb') as f:\n     f.write(response.content)\n\n\n # Create a VideoCapture object to read frames from the downloaded video.\n cap = cv2.VideoCapture(\"sample_video.mp4\")\n\n\n # Set up the VideoWriter for the output video. The 'mp4v' argument specifies the codec to be used.\n fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n\n # Initialize a counter for the frame number.\n frame_number = 0\n\n\n # Start a loop to read frames from the video.\n while True:\n     # Read the next frame from the video.\n     ret, frame = cap.read()\n\n\n     # If the frame could not be read (i.e., we're at the end of the video), break the loop.\n     if not ret:\n         break\n\n\n     # Increment the frame number.\n     frame_number += 1\n\n\n     # Use the model to predict objects in the current frame. The confidence threshold is set to 0.5.\n     results = model.predict(frame, conf=0.50)\n\n\n     # Loop over each predicted class.\n     for c in results[0].boxes.cls:\n       # Loop over each bounding box predicted for the current class.\n       for idx, box in enumerate(results[0].boxes.xyxy):\n         # Get the class number from the model's class names.\n         class_number = model.names[int(c)]\n\n\n         # Get the coordinates of the bounding box.\n         xmin, ymin, xmax, ymax  = float(box[0]), float(box[1]), float(box[2]), float(box[3])\n\n\n         # Create an annotation for the bounding box.\n         bbox_annotation = [\n           lb_types.VideoObjectAnnotation(\n             name = class_number,\n             keyframe= True,\n             frame=frame_number,\n             segment_index=0,\n            \n             # Define the bounding box as a rectangle with a start and end point.\n             value = lb_types.Rectangle(\n                   start=lb_types.Point(x=xmin, y=ymin), # x = left, y = top\n                   end=lb_types.Point(x= xmax, y=ymax)))] # x= left + width , y = top + height\n        \n         # Append a new Label object to the labels list. Each Label represents one detected object in one frame.\n         labels.append(\n             Label(\n                 data=lb_types.VideoData(global_key=global_key),\n                 annotations = bbox_annotation\n             )\n         )\n\n\n # Release the VideoCapture object.\n cap.release()\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"923\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/YOLOGuide_3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/YOLOGuide_3.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_3.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of Quantumworks Lab video editor with YOLO predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1182\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/YOLOGuide_4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/YOLOGuide_4.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_4.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAnalytics view of a dataset with YOLO predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Large-GIF--1466x882-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1466\" height=\"882\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Large-GIF--1466x882-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Large-GIF--1466x882-.gif 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/Large-GIF--1466x882-.gif 1466w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eVideo is enriched with annotations from YOLO, and the contents can be queried in Quantumworks Lab Catalog.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eCombining the power of the YOLO — or any off-the-shelf AI model that suits your needs — with Quantumworks Lab opens up exciting possibilities for video content. It not only makes the video content queryable, but also helps bring a new level of understanding to what's inside the videos. This combination can be especially beneficial for use cases like surveillance, content creation, content moderation, and advertising, where insights from video content are crucial.\u003c/p\u003e\u003cp\u003eUsing an AI model as a first pass enables users to search videos based on their content, enabling AI teams to learn how many annotations of each object exist within their dataset and what types of annotations the training dataset might lack. This further reduces friction when it comes to finding the next set of assets that will improve the model's performance by doing active learning. Try enriching your videos using YOLO or any other AI model using this \u003ca href=\"https://colab.research.google.com/drive/1vOVo4MtsoUxNJ-tIdmI-GKWjVFo9RrQe?ref=labelbox-guides.ghost.io#scrollTo=rJXXOJdpWD48\"\u003escript\u003c/a\u003e to discover exactly what content already exists in your troves of unstructured videos and find specific videos quickly and easily.\u003c/p\u003e","comment_id":"646f88789568720001240642","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Screen-Shot-2023-05-24-at-1.45.31-PM.png","featured":false,"visibility":"public","created_at":"2023-05-25T16:10:32.000+00:00","updated_at":"2023-10-27T16:58:02.000+00:00","published_at":"2023-05-25T16:33:51.000+00:00","custom_excerpt":"In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/leveraging-yolo-and-labelbox-to-make-videos-queryable-by-content/","excerpt":"In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":"Leveraging YOLO and Quantumworks Lab to make videos queryable by content","og_description":"In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.","twitter_image":null,"twitter_title":"Leveraging YOLO and Quantumworks Lab to make videos queryable by content","twitter_description":"In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.","meta_title":"Leveraging YOLO and Quantumworks Lab to make videos queryable by content","meta_description":"In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"632ccf4016e912003d39b2a7","uuid":"fc620d97-ea51-468e-ae87-a99b87ca45b8","title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","slug":"using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWhile YOLOv8 is no longer supported on Quantumworks Lab, this blog remains relevant if you are working with other object detection models. Alternatives such as OWL-ViT, Rekognition, GroundingDINO, and GroundingDINO + SAM are fully supported on the Quantumworks Lab platform.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this guide, you will learn how to chain \u003ca href=\"https://labelbox.com/solutions/computer-vision/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision\u003c/a\u003e foundation models together to automatically populate pre-labels with class in Quantumworks Lab very quickly. We will be walking through a simple semantic segmentation task: drawing masks around all objects of a particular class in an image.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1086\" height=\"332\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png 1086w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeveraging chained foundation models in Quantumworks Lab will greatly reduce the time it takes you or your team to draw segmentation masks; by augmenting the recently-released \u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eSAM\u003c/a\u003e model with classifications, you will automate the task of assigning classes. Rather than performing a tedious labeling effort, you can focus your valuable efforts reviewing, verifying, and correcting labels drawn by AI models.\u003c/p\u003e\u003cp\u003eHere’s a high-level summary of the process that we will be walk through step-by-step below:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun an object detector on the image to generate bounding boxes with classifications for specified classes\u003c/li\u003e\u003cli\u003eFeed the bounding boxes as inputs to Meta’s Segment Anything model which will produce segmentation masks for each one\u003c/li\u003e\u003cli\u003eUpload the mask predictions onto Quantumworks Lab as pre-labels\u003c/li\u003e\u003cli\u003eOpen up image editor and review or modify the pre-labels as you usually do\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can run all of the above out-of-the-box on your image(s) using our \u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/develop/examples/integrations/sam/meta_sam_labelbox.ipynb?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e. Simply load the images and automatically get segmented masks, with classes, in just a few minutes. \u003c/p\u003e\u003cp\u003eFor this guide, we will use the following image of a lot of colorful chairs: \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"896\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-run-yolov8-on-the-image\"\u003eStep 1: Run YOLOv8 on the image\u003c/h2\u003e\u003cp\u003eThe latest iteration of the YOLO (You Only Look Once) family of models, \u003ca href=\"https://docs.ultralytics.com/?ref=labelbox-guides.ghost.io\"\u003eYOLOv8\u003c/a\u003e is an object detector that produces bounding boxes and classes around common objects. Known for its speed and accuracy, YOLOv8 boasts some impressive features – making it an invaluable tool for a wide range of applications. Here, we use YOLOv8 to automatically detect and localize all the chairs in the image.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# load the YOLOv8 model\nmodel = YOLO(f'{HOME}/yolov8n.pt')\n\n# run the model on the image\nresults = model.predict(source='chairs.jpg', conf=0.25)\npredicted_boxes = results[0].boxes.xyxy\n\n\n# read in the image for visualization\nimage_bgr = cv2.imread(IMAGE_PATH, cv2.IMREAD_COLOR)\n\n# use cv2 to visualize the bounding boxes on the image\nfor box in predicted_boxes:\n cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\ncv2.imshow(\"YOLOv8 predictions\", image_bgr)\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"896\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-2-feed-bounding-boxes-as-inputs-to-meta%E2%80%99s-sam-model\"\u003eStep 2: Feed bounding boxes as inputs to Meta’s SAM model\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eSAM (Segment Anything Model)\u003c/a\u003e – recently released by Meta AI, is an advanced computer vision model designed to accurately segment images and videos into distinct objects. Using advanced deep learning techniques, SAM is able to identify and segment objects in images, making it a powerful tool for a wide range of applications. The SAM model is able to generate segmentation masks based on prompts, including bounding box prompts, which we will use in the code below.\u003c/p\u003e\u003cp\u003eTo see an in-editor experience of SAM, please check out our blog post \u003ca href=\"https://labelbox.com/blog/coming-soon-auto-segment-powered-by-sam/?ref=labelbox-guides.ghost.io\"\u003eAuto-Segment 2.0 powered by Meta’s Segment Anything Model\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# load the SAM model\nsam = sam_model_registry[\"vit_h\"](checkpoint=\"/sam_vit_h_4b8939.pth\n\").to(device=torch.device('cuda:0'))\n\nmask_predictor = SamPredictor(sam)\n\n# transform the YOLOv8 predicted boxes to match input format expected by SAM model\ntransformed_boxes = mask_predictor.transform.apply_boxes_torch(predicted_boxes, image_bgr.shape[:2])\n\n\n# run SAM model on all the boxes\nmask_predictor.set_image(image_bgr)\nmasks, scores, logits = mask_predictor.predict_torch(\n   boxes = transformed_boxes,\n   multimask_output=False,\n   point_coords=None,\n   point_labels=None\n)\n\n# combine all masks into one for easy visualization\nfinal_mask = None\nfor i in range(len(masks) - 1):\n  if final_mask is None:\n    final_mask = np.bitwise_or(masks[i][0], masks[i+1][0])\n  else:\n    final_mask = np.bitwise_or(final_mask, masks[i+1][0])\n\n# visualize the predicted masks\nplt.figure(figsize=(10, 10))\nplt.imshow(image_rgb)\nplt.imshow(final_mask, cmap='gray', alpha=0.7)\nplt.show()\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1daa38c9-1485-4ca8-8c03-57f6a87b9cc7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"794\" height=\"454\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-1daa38c9-1485-4ca8-8c03-57f6a87b9cc7.png 600w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1daa38c9-1485-4ca8-8c03-57f6a87b9cc7.png 794w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-3-upload-the-predicted-masks-as-pre-labels-onto-labelbox\"\u003eStep 3: Upload the predicted masks as pre-labels onto Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThe predicted masks can be easily and seamlessly integrated into Quantumworks Lab via our SDK. The upload is just a few lines of code that run in less than a minute.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass_names = []\nfor c in results[0].boxes.cls:\n class_names.append(model.names[int(c)])\n\nannotations = []\nfor mask in masks:\n  # convert a 2D array to 3D array\n  mask_data = lb_types.MaskData.from_2D_arr(np.asarray(mask[0], dtype=\"uint8\"))\n  mask_annotation = lb_types.ObjectAnnotation(\n    name = class_names[idx], # assign class from Step 1\n    value=lb_types.Mask(mask=mask_data, color=color),\n  )\n  annotations.append(mask_annotation)\n\nlabels = [\nlb_types.Label(data=lb_types.ImageData(global_key=\"image_name\"),annotations=annotations)\n]\nupload_job = lb.MALPredictionImport.create_from_objects(\n   client=client,\n   project_id=project.uid,\n   name=\"mal_job\" + str(uuid.uuid4()),\n   predictions=labels\n)\nupload_job.wait_until_done()\n\nprint(f\"Errors: {upload_job.errors}\", )\nprint(f\"Status of uploads: {upload_job.statuses}\")\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"815\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"final-thoughts-on-using-meta%E2%80%99s-sam-model-with-yolov8-to-automatically-classify-masks\"\u003eFinal thoughts on using Meta’s SAM model with YOLOv8 to automatically classify masks\u003c/h2\u003e\u003cp\u003eWhile Meta’s AI's SAM is really powerful at segmentation, it leaves out the crucial task of classification. In this guide, we demonstrated how you can use YOLOv8 (or another object detector) to generate bounding boxes with classes and then automatically apply those classes to the masks generated by SAM. We also showed how this seamlessly integrates with the Quantumworks Lab Model Assisted Labeling SDK.\u003c/p\u003e\u003cp\u003eIf you are interested in applying SAM on images through our image editor, you can \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003esign up\u003c/a\u003e for a Quantumworks Lab account and give it a try today. \u003c/p\u003e","comment_id":"632ccf4016e912003d39b2a7","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3062.png","featured":false,"visibility":"public","created_at":"2022-09-22T21:10:24.000+00:00","updated_at":"2024-11-26T18:38:02.000+00:00","published_at":"2023-05-09T18:26:38.000+00:00","custom_excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/","tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/","excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","og_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3062-2.png","twitter_title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","twitter_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","meta_title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","meta_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"645a7494225a9a00012fcfae","uuid":"e07975bd-27c6-4c00-bf8c-bc96c772ce56","title":"How to accelerate image-text pair generation with BLIP-2","slug":"how-to-accelerate-image-text-pair-generation-with-blip-2","html":"\u003cp\u003e\u003ca href=\"https://labelbox.com/solutions/generative-ai/?ref=labelbox-guides.ghost.io\"\u003eGenerative AI\u003c/a\u003e has taken the world by storm, opening doors to a plethora of applications, from creating realistic images and videos to generating novel text and music. The success of these applications often hinges on the \u003ca href=\"https://labelbox.com/blog/data-quality-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003equality and quantity of data used to train the underlying machine learning models\u003c/a\u003e, the production of which is often time consuming and costly. As a result, leading AI teams have been innovating on ways to streamline the caption creation process and empower human annotators to work more efficiently without sacrificing quality.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2301.12597?ref=labelbox-guides.ghost.io\"\u003eBLIP-2\u003c/a\u003e (Bootstrapping Language-Image Pre-training) is an AI model that can perform various multi-modal tasks like visual question answering, image-text retrieval (image-text matching) and image captioning. It can analyze an image, understand its content, and generate a relevant and concise caption. BLIP-2 helps language models understand images without changing their original structure. It does this by using querying transformer (q-former) that acts as a bridge between the image and the language model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"527\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSee the original flowchart as published in the \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2301.12597.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBLIP-2 research paper\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBLIP-2 achieves state-of-the-art performance on various vision-language tasks while being more compute efficient than existing methods. Powered by \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox-guides.ghost.io\"\u003eLarge Language Models (LLMs)\u003c/a\u003e, it can perform \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot image-to-text generation\u003c/a\u003e based on natural language instructions, enabling capabilities like visual knowledge reasoning and visual conversation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"898\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExamples of images and their BLIP 2 captions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, we'll explore how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions. Additionally, you can use any model to make pre-labels in Quantumworks Lab as shown \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e. Quantumworks Lab customers using \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003emodel-assisted labeling\u003c/a\u003e have seen 50-70% reductions in labeling costs driven by dramatic reductions in labeling time and complexity. Therefore, using a model like BLIP-2 will further reduce labeling time.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/BLIP-2-lucidchart--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1340\" height=\"420\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/BLIP-2-lucidchart--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/BLIP-2-lucidchart--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/BLIP-2-lucidchart--1-.png 1340w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHow to use BLIP-2 with Quantumworks Lab\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"step-1-create-a-project-and-attach-an-ontology\"\u003eStep 1: Create a project and attach an ontology.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eproject = client.create_project(name = \"BLIP project\", media_type=labelbox.MediaType.Image)\nproject.setup_editor(ontology)\nontology_from_project = labelbox.OntologyBuilder.from_project(project)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-initialize-and-load-a-pre-trained-blip-2-model\"\u003eStep 2: Initialize and load a pre-trained BLIP-2 model.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003edevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n   \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n)\nmodel.to(device)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-3-collect-inferences-to-be-used-as-pre-labels\"\u003eStep 3: Collect inferences to be used as pre-labels.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003equeued_data_rows = project.export_queued_data_rows()\nground_truth_list = list()\n\n\nfor data_row in queued_data_rows:\n url = data_row[\"rowData\"]\n image = Image.open(requests.get(url, stream=True).raw)\n inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n generated_ids = model.generate(**inputs, max_new_tokens=30)\n generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n print(generated_text) \n  text_annotation = labelbox.data.annotation_types.ClassificationAnnotation(\n     name=\"BLIP model prediction\",\n     value=labelbox.data.annotation_types.Text(answer = generated_text)\n   )\n  ground_truth_list.append(Label(\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-4-upload-pre-labels-to-your-project\"\u003eStep 4: Upload pre-labels to your project.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eupload_task = labelbox.MALPredictionImport.create_from_objects(client, project.uid, str(uuid.uuid4()), ground_truth_list)\nupload_task.wait_until_done()\nprint(upload_task.errors)\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1132\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of asset with a pre-label and no human-generated caption\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/unknown.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"893\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/unknown.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/unknown.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/unknown.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of an asset with a pre-label and a human-generated caption\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"review-labels\"\u003eReview labels\u003c/h2\u003e\u003cp\u003eAfter the labels have been annotated, you can use \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e to create highly customizable, multi-step review pipelines, making your review process more efficient and automated. Workflows offer granular control over how your data rows get reviewed, saving you both time and resources. You can create tasks that enable you to filter based on who created the label, what annotations exist and when the label was created.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/unknown--1--2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"804\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/unknown--1--2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/unknown--1--2.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/unknown--1--2.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLabelbox lets you customize labeling and review workflows to your exact requirements.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"export-labels\"\u003eExport labels\u003c/h2\u003e\u003cp\u003eAfter you are done reviewing the labels, you can easily export the annotations as show \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-JSON\"\u003e         \"annotations\": {\n             \"objects\": [],\n             \"classifications\": [\n               {\n                 \"feature_id\": \"clhdn79ae0ent076c4h579rxu\",\n                 \"name\": \"BLIP model prediction\",\n                 \"text_answer\": {\n                   \"content\": \"a yellow flower with a green background\"\n                 }\n               }\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eBy using captions generated by BLIP-2 or inferences by AI models as pre-labels, AI teams can significantly reduce labeling time and costs. To learn more, explore this \u003ca href=\"https://colab.research.google.com/drive/1vnD4gVBu8uAE3dn44h8qIXx3R_DYv5E2?ref=labelbox-guides.ghost.io#scrollTo=PcfsMaUu1GrV\"\u003efull script\u003c/a\u003e for using the BLIP-2 model to generate pre-labels. These captions can then be amended or approved in Quantumworks Lab by labelers. You can also learn more about the BLIP-2 model \u003ca href=\"https://huggingface.co/docs/transformers/main/model_doc/blip-2?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e and model-assisted labeling \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"645a7494225a9a00012fcfae","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Guide_BLIP-2.png","featured":false,"visibility":"public","created_at":"2023-05-09T16:28:04.000+00:00","updated_at":"2023-10-27T17:11:22.000+00:00","published_at":"2023-05-09T17:09:47.000+00:00","custom_excerpt":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-accelerate-image-text-pair-generation-with-blip-2/","tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-accelerate-image-text-pair-generation-with-blip-2/","excerpt":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"How to accelerate image-text pair generation with BLIP-2","og_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","twitter_image":null,"twitter_title":"How to accelerate image-text pair generation with BLIP-2","twitter_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","meta_title":"How to accelerate image-text pair generation with BLIP-2 | Quantumworks Lab","meta_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so a specialized workforce can further improve the image captions.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64590b32638f810001454877","uuid":"25f18d3c-fa4c-4206-a4c2-db2f9c6ba390","title":"Automatically label text with 96%+ accuracy using foundation models","slug":"automatically-label-text-with-96-accuracy-using-foundation-models","html":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, you’ll learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab and foundation models. We will be walking through a \u003ca href=\"https://labelbox.com/product/annotate/text/?ref=labelbox-guides.ghost.io\"\u003etext classification\u003c/a\u003e task: identifying news articles that talk about sports. \u003c/p\u003e\u003cp\u003eHere's a high-level summary of the process:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConnect your data to Quantumworks Lab with just a few lines of code.\u003c/li\u003e\u003cli\u003eLabelbox leverages foundation models to automatically enrich your data.\u003c/li\u003e\u003cli\u003eUse the powerful search capabilities of Quantumworks Lab to quickly find articles with similar traits and classify them in one click. With the help of foundation models, you can instantaneously label large amounts of data. \u003cem\u003ePro tip: Combine a variety of search techniques, such as a similarity search, natural language search, keyword search, and investigate clusters of similar data, to boost your results.\u003c/em\u003e\u003c/li\u003e\u003cli\u003eWhile foundation models are a helpful starting point, they might not always correctly classify data, especially on challenging or rare data points. In this case, utilize human-in-the-loop labeling and QA by pre-labeling data using foundation models and sending it for your internal or external labeling team to review.\u003c/li\u003e\u003cli\u003eAutomatically apply these rules to all new incoming data by creating a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e in Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, let’s take a look at how we can do the above in Labelbox. As a sneak peek into the process, by leveraging foundation models, we managed to \u003cstrong\u003eclassify 88% of our news articles in minutes with a 96.5% accuracy rate\u003c/strong\u003e. An additional 15% of our news articles were successfully pre-labeled using foundation models, with 85% accuracy, and sent for human review. This left us with only 493 data points that were missed by foundation models – a massive efficiency gain for any labeling team.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"step-1-connect-your-data-to-labelbox-with-a-few-lines-of-code\"\u003eStep 1: Connect your data to Quantumworks Lab with a few lines of code\u003c/h2\u003e\u003cp\u003eSince this is a classification task, our goal is to correctly have the model identify news articles about sports. We will be using the following Hugging Face dataset: \u003ca href=\"https://huggingface.co/datasets/ag_news?ref=labelbox-guides.ghost.io\"\u003eag_news\u003c/a\u003e which contains 120,000 articles, including 30,000 about sports, for our analysis.\u003c/p\u003e\u003cp\u003eTo begin, let's connect this data to Labelbox. Simply retrieve the dataset from Hugging Face and integrate it with Quantumworks Lab in just a few lines of code.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom datasets import load_dataset\ndataset = load_dataset(\"ag_news\",split='train')\n\n# iterate over the data\npayload_imgs = []\ncounter = 0\n\n\n# iterate over the data\npayloads = []\nglobal_keys = []\ncounter = 0\n\nfor data in dataset:\n\n  text = data['text']\n  label = data['label']\n  global_key = \"ag_news_\" + str(counter)\n  global_keys.append(global_key)\n\n  # create payload for texts\n  payloads.append({\n    \"row_data\": text, \n    \"global_key\": global_key,\n  })\n\n  counter += 1\n\n\n# create dataset in Quantumworks Lab\nlb_dataset = client.create_dataset(name=\"ag_news\") \n\n# add data in Quantumworks Lab\ntask = lb_dataset.create_data_rows(payloads)  \ntask.wait_till_done() # async\nprint(task.errors) # check errors \u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-leverage-foundation-models-to-instantly-enhance-your-data\"\u003eStep 2: Leverage foundation models to instantly enhance your data\u003c/h2\u003e\u003cp\u003eLabelbox will automatically compute and store MPNet embeddings for your data. We are using \u003ca href=\"https://huggingface.co/sentence-transformers/all-mpnet-base-v2?ref=labelbox-guides.ghost.io\"\u003ethis implementation\u003c/a\u003e available through Hugging Face. \u003c/p\u003e\u003cp\u003eOnce your data has been uploaded, watch as Quantumworks Lab enriches your data with foundation model embeddings. These embeddings are powerful in that they can be harnessed to automatically label, or pre-label, your data.  \u003c/p\u003e\u003cp\u003eIf you don’t want to use the default embeddings by Quantumworks Lab, you can also \u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003eupload custom embeddings\u003c/a\u003e from any other foundation model, with up to 100 custom embeddings for each data point. \u003c/p\u003e\u003cp\u003eWhether you’re using the default or custom embeddings, embeddings are helpful in curating and finding subsets of data that share similar characteristics. For instance, embeddings power \u003ca href=\"https://labelbox.com/blog/how-vector-similarity-search-works/?ref=labelbox-guides.ghost.io\"\u003eLabelbox’s similarity search\u003c/a\u003e, natural language search, and 2D projector view. You can search and explore all of your data with tools that help you powerfully surface specific subsets of similar texts.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"927\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-3-use-powerful-search-capabilities-to-quickly-find-data\"\u003eStep 3: Use powerful search capabilities to quickly find data\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities in Quantumworks Lab, you can easily find and classify data that share similar characteristics. This is a special case of \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot and few-shot learning\u003c/a\u003e: the challenge is to find all examples of sports articles, based on zero (or a few) examples. With the help of foundation models, and minimal human signal, you can quickly label a lot of data in just a few clicks. The following are tools in Quantumworks Lab that help provide labeling signal to make it easy to automatically classify your data:\u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-projector-view-for-classification\"\u003eZero-shot Labeling: Projector View for Classification\u003c/h3\u003e\u003cp\u003eLabelbox allows you to visualize data clusters in 2D. For this example, we can see distinct clusters. By inspecting a few examples, we discover that some of the data clusters correspond to sports news articles. We manually select each cluster and tag it with \"UMAP: sports. We intentionally leave out data points situated between clusters, as these represent challenging data points. This is expected since each labeling function won’t be perfect in isolation, and some data points are difficult and challenging.\u003c/p\u003e\u003cp\u003eWe then repeat the process with t-SNE instead of UMAP and tag each sports cluster with \"t-SNE: sports\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"569\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUMAP: a cluster of news articles about sports\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"705\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003et-SNE: a subcluster of news articles about basketball\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"zero-shot-labeling-keyword-search\"\u003eZero-shot Labeling: Keyword search\u003c/h3\u003e\u003cp\u003eLabelbox enables you to search all data points that contain some keywords. We filter all data points that contain the following keywords\u003cem\u003e: sport, sports, basketball, baseball, soccer, football, tennis, hockey\u003c/em\u003e. And tag these 5,990 texts as “Keyword search: sports”. \u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-natural-language-search-for-classification\"\u003eZero-shot Labeling: Natural language search for classification\u003c/h3\u003e\u003cp\u003eLabelbox enables you to conduct natural language searches on text. For example you can type in “news articles about sports” to surface all pieces of text about sports. Adjusting the similarity threshold will narrow the search to only relevant articles. For this use case, we filter for a similarity score higher than 0.85 and tag all of the 6,468 texts as “Natural language search: sports”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"922\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA natural language search will surface thousands of news articles about sports. By adjusting the similarity score, we can keep the most confident zero-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"few-shot-labeling-similarity-search-for-classification\"\u003eFew-shot Labeling: Similarity search for classification\u003c/h3\u003e\u003cp\u003eLabelbox also streamlines few-shot labeling. Quickly browse all your data in Catalog to surface 5 news articles about sports. For each of them, run a similarity search and tag the top results (e.g with a similarity score higher than 0.85) as “Similarity search: sports”. This provides us with 5 new labeling functions that surface sports news articles.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"967\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA similarity search example with an anchor article about college basketball. We can filter to keep the most confident few-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"combining-different-sources-of-signal-weak-labeling\"\u003eCombining different sources of signal: weak labeling\u003c/h3\u003e\u003cp\u003eWhile each of these labeling signals is powerful on its own, you can combine multiple sources in Labelbox. This allows you to apply simple rules in a weak supervision fashion to further enhance your results. Integrate different labeling signals, such as similarity searches, natural language searches, and data clusters, to boost your outcomes. You can \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003ecombine various filters\u003c/a\u003e by using the AND and OR functions.\u003c/p\u003e\u003ch2 id=\"step-4-automatically-classify-data-with-foundation-models-and-use-human-in-the-loop-qa-for-challenging-cases\"\u003eStep 4: Automatically classify data with foundation models and use human-in-the-loop QA for challenging cases\u003c/h2\u003e\u003ch3 id=\"high-confidence-data-points-direct-classification\"\u003eHigh confidence data points: direct classification\u003c/h3\u003e\u003cp\u003eFoundation models are highly confident about most data points. So much so that we can directly classify data points leveraging Quantumworks Lab’s \u003ca href=\"https://labelbox.com/blog/pre-label-and-enrich-your-data-with-bulk-classifications/?ref=labelbox-guides.ghost.io\"\u003ebulk classification feature\u003c/a\u003e. With this new feature, you can specify and send your texts to a specific step of the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox.ghost.io#how-it-works\"\u003elabeling and review workflow\u003c/a\u003e. We can directly move these high-confidence data points straight to the “Done” task.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe classify thousands of texts in bulk, and send them to the “Done” task of our labeling project, in just a click, since foundation models are confident on those.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these high-confident data points are those that belong to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe sports clusters, both with UMAP and t-SNE. But just how accurate are these predictions? To answer this question, we looked at the Hugging Face ground truths. 704 out of the 21,560 sports predictions are incorrect.\u003c/li\u003e\u003cli\u003eOr, the similarity search score to two or more anchors is higher than 0.85. This results in 3,548 sports classifications, all of which are accurate except 185.\u003c/li\u003e\u003cli\u003eThe sports cluster in UMAP or t-SNE and a natural language search higher than 0.85. This results in 1,219 sports classifications, all of which are accurate except 58. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis method of surfacing high-confident data points enables us to directly classify 26,427 pieces of text - with only 947 errors - achieving an \u003cstrong\u003eaccuracy of 96.5%.\u003c/strong\u003e Since 26,427 out of 30,000 sports articles have been classified directly by foundation models, the \u003cstrong\u003ecoverage is 88%.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhat about the 947 errors? Upon closer inspection of these errors, it turns out that they are all related to sports, but in the context of News, World, or Science, and hence have been labeled on Hugging Face according to those categories instead of Sports.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"952\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFoundation models failed on 947 news articles. It turns out that these articles are all related to sports, but are classified on HuggingFace as World news, or Business news, or Science \u0026amp; Tech news.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eNow, let’s move on to classify the remaining 12% of data rows, on which foundation models are less confident.\u003c/p\u003e\u003ch3 id=\"low-confidence-data-points-human-in-the-loop-labeling\"\u003eLow confidence data points: Human-in-the-Loop labeling\u003c/h3\u003e\u003cp\u003eFor some pieces of text, foundation models exhibit low confidence. We can bulk classify these data points in Quantumworks Lab, but move them to the “To Review” task. This will ensure a human is looped in and will review the classifications coming from foundation models.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe pre-label thousands of data points in bulk, and send them to “Review” in our labeling project, in just a click, since foundation models are moderately confident on these pieces of text.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these data points are those that belong to the sports cluster, with UMAP or t-SNE, and that haven’t been classified yet.\u003c/p\u003e\u003cp\u003eUsing this approach, we managed to classify 4,723 additional data rows, with an \u003cstrong\u003eaccuracy of 85%\u003c/strong\u003e (696 errors). We can send these low-confident data rows for Human-in-the-Loop review. \u003c/p\u003e\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eDirect classification with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eHuman in the loop with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eSports articles missed by foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of data rows classified\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e26,427\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e4,723\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e493\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of errors\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e947\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e696\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e-\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAccuracy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e96.5%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e85%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e-\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eFraction of sports articles\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e88%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e15%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e1.8%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 26,427 pieces of text (88%) in minutes, with \u003cstrong\u003e96.5% accuracy\u003c/strong\u003e thanks to foundation models. An additional 4,723 data points (13.5%) have been pre-labeled with foundation models, with \u003cstrong\u003e85% accuracy\u003c/strong\u003e, and sent for human review. This leaves only 493 data points talking about sports, missed by foundation models.\u003c/p\u003e\u003ch2 id=\"step-5-set-it-and-forget-it-%E2%80%93-automatically-apply-these-rules-to-fresh-incoming-data\"\u003eStep 5: Set it and forget it – automatically apply these rules to fresh, incoming data\u003c/h2\u003e\u003cp\u003eWith Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e, we automatically classify fresh, incoming news articles about sports. \u003c/p\u003e\u003cp\u003eFor example, we can set up a slice that automatically surfaces all new pieces of text, that have been connected to Quantumworks Lab in the past week, that haven’t been classified yet. We can set the slice's criteria to include only text data rows where the natural language search for the prompt is \"news articles about sports\" and is higher than 0.85 (since we know that these data rows are very likely to be on sports). \u003c/p\u003e\u003cp\u003eWith slices, you can easily surface and inspect any new, high-impact data that gets added to your data lake.\u003c/p\u003e\u003cp\u003eFrom there, it only takes one click to classify all of these text data rows as sports articles. \u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 26,427 pieces of text (88%) in minutes, with \u003cstrong\u003e96.5% accuracy\u003c/strong\u003e thanks to foundation models. An additional 4,723 data points (15%) have been pre-labeled with foundation models, with \u003cstrong\u003e85% accuracy\u003c/strong\u003e, and sent for human review. This leaves only 493 data points talking about sports, missed by foundation models.\u003c/p\u003e\u003chr\u003e\u003cp\u003eIf you’re a current Quantumworks Lab user who wants to leverage any foundation model to supercharge your data labeling process in just a few clicks,\u003ca href=\"https://app.labelbox.com/catalog?ref=labelbox-guides.ghost.io\"\u003e try our bulk classification feature today\u003c/a\u003e or \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started with a free Quantumworks Lab account\u003c/a\u003e.\u003cbr\u003eIf you’re interested in seeing how quickly you can label images leveraging foundation models, check out our guide on \u003ca href=\"https://labelbox.com/guides/automatically-label-images-with-99-accuracy-using-foundation-models/?ref=labelbox-guides.ghost.io\"\u003ehow to automatically label images with 99% accuracy.\u003c/a\u003e\u003c/p\u003e","comment_id":"64590b32638f810001454877","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1-.png","featured":false,"visibility":"public","created_at":"2023-05-08T14:46:10.000+00:00","updated_at":"2023-10-27T17:11:48.000+00:00","published_at":"2023-05-08T20:43:24.000+00:00","custom_excerpt":"Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/automatically-label-text-with-96-accuracy-using-foundation-models","tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/automatically-label-text-with-96-accuracy-using-foundation-models/","excerpt":"Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","reading_time":9,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1--2.png","og_title":"Automatically label text with 96%+ accuracy using foundation models","og_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1--1.png","twitter_title":"Automatically label text with 96%+ accuracy using foundation models","twitter_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","meta_title":"Automatically label text with 96%+ accuracy using foundation models","meta_description":"Automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","email_subject":null,"frontmatter":null,"feature_image_alt":"Automatically label text with 96%+ accuracy using foundation models","feature_image_caption":null},{"id":"6452689090e75e0001b3546c","uuid":"5a55750c-5b23-4ce7-b075-117a24a4f392","title":"Automatically label images with 99% accuracy using foundation models","slug":"automatically-label-images-with-99-accuracy-using-foundation-models","html":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, you’ll learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab and foundation models. We will be walking through a sample \u003ca href=\"https://labelbox.com/guides/image-annotation/?ref=labelbox-guides.ghost.io\"\u003eimage classification task\u003c/a\u003e: figuring out if images contain cats or dogs.\u003c/p\u003e\u003cp\u003eHere's a high-level summary of the process that we'll be walking through step-by-step below:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConnect your data to Quantumworks Lab with just a few lines of code.\u003c/li\u003e\u003cli\u003eLabelbox leverages foundation models to magically enrich your data.\u003c/li\u003e\u003cli\u003eUse the powerful search capabilities of Quantumworks Lab to quickly find data with similar traits and classify them in one click. With the help of foundation models, you can instantaneously label large amounts of data. \u003cem\u003ePro tip: Combine a variety of search techniques, such as a similarity search, natural language search, and investigate clusters of similar data, to boost your results.\u003c/em\u003e\u003c/li\u003e\u003cli\u003eWhile foundation models are a helpful starting point, they might not always correctly classify data, especially on challenging or rare data points. In this case, utilize human-in-the-loop labeling and QA by pre-labeling data using foundation models and sending it for your internal or external labeling team to review.\u003c/li\u003e\u003cli\u003eAutomatically apply these rules to all new incoming data by creating a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e in Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, let’s take a look at how we can do the above in Labelbox. As a sneak peek into the process, by leveraging foundation models, we managed to \u003cstrong\u003eclassify 86% of our images in minutes with a 99.9% accuracy rate\u003c/strong\u003e. An additional 13.5% of our images were successfully pre-labeled using foundation models, with 98% accuracy, and were sent for human review. This left us with less than 0.5% of images to manually label – a massive efficiency gain for any labeling team.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"step-1-connect-your-data-to-labelbox-with-a-few-lines-of-code\"\u003eStep 1: Connect your data to Quantumworks Lab with a few lines of code\u003c/h2\u003e\u003cp\u003eSince this is a classification task, our goal is to correctly have the model identify cats and dogs in images. We will be using the following Hugging Face dataset - \u003ca href=\"https://huggingface.co/datasets/cats_vs_dogs?ref=labelbox-guides.ghost.io\"\u003ecats_vs_dogs\u003c/a\u003e, containing 18,699 images for our analysis.\u003c/p\u003e\u003cp\u003eTo begin, let's connect this data to Labelbox. Simply retrieve the dataset from Hugging Face and integrate it with Quantumworks Lab in just a few lines of code.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom datasets import load_dataset\ndataset = load_dataset(\"cats_vs_dogs\",split='train')\n\n# iterate over the data\npayload_imgs = []\ncounter = 0\n\nfor data in dataset:\n  image = data['image']\n  label = data['labels'] # 0 is cat, 1 is dog\n  global_key = \"cat_vs_dog_\" + str(counter)\n\n  # save image locally\n  path = \"/content/images/\"+global_key+\".jpg\"\n  image.save(path) \n\n  # create payload for images\n  payload_imgs.append({\"row_data\": path, \"global_key\": global_key})\n  counter += 1\n\n# create dataset in Quantumworks Lab\nlb_dataset = client.create_dataset(name=\"Cat_vs_dog\") \n\n\n# add data in Quantumworks Lab\ntask = lb_dataset.create_data_rows(payload_imgs[i:i+1000])  task.wait_till_done() # async\nprint(task.errors) # check errors \u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-leverage-foundation-models-to-instantly-enhance-your-data\"\u003eStep 2: Leverage foundation models to instantly enhance your data\u003c/h2\u003e\u003cp\u003eLabelbox will automatically compute and store CLIP embeddings for your data. Our CLIP model leverages \u003ca href=\"https://openai.com/research/clip?ref=labelbox-guides.ghost.io\"\u003eOpenAI\u003c/a\u003e and we are using \u003ca href=\"https://huggingface.co/sentence-transformers/clip-ViT-B-32?ref=labelbox-guides.ghost.io\"\u003ethis implementation\u003c/a\u003e available through Hugging Face. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"922\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce your data has been uploaded, you can enrich your data with foundation model embeddings. These embeddings are powerful in that they can be harnessed to automatically label, or pre-label, your data.  \u003c/p\u003e\u003cp\u003eIf you don’t want to use the default embeddings by Quantumworks Lab, you can also \u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003eupload custom embeddings\u003c/a\u003e from any other foundation model, with up to 100 custom embeddings for each data point. \u003c/p\u003e\u003cp\u003eWhether you’re using the default or custom embeddings, embeddings are helpful in curating and finding subsets of data that share similar characteristics. For instance, embeddings power Quantumworks Lab’s \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003esimilarity search\u003c/a\u003e, natural language search, and 2D projector view. You can search and explore all of your data with tools that help you powerfully surface specific subsets of data.\u003c/p\u003e\u003ch2 id=\"step-3-use-powerful-search-capabilities-to-quickly-find-data\"\u003eStep 3: Use powerful search capabilities to quickly find data\u003c/h2\u003e\u003cp\u003eWith \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003epowerful search capabilities\u003c/a\u003e in Quantumworks Lab, you can easily find and classify data that share similar characteristics. This is a special case of \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot and few-shot learning\u003c/a\u003e: the challenge is to find all examples of cats and dogs, based on zero (or a few) examples from each class. \u003c/p\u003e\u003cp\u003eWith the help of foundation models, and minimal human signal, you can quickly label a lot of data in just a few clicks. The following are tools in Quantumworks Lab that help provide labeling signal to make it easy to automatically classify your data:\u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-projector-view-for-classification\"\u003eZero-shot Labeling: Projector View for classification\u003c/h3\u003e\u003cp\u003eLabelbox allows you to visualize data clusters in 2D. For this example, we can see two distinct clusters: one for cats and another for dogs. By inspecting a few examples, we can ensure the data clustering is accurate. We then manually select each cluster and tag it with \"UMAP: cats: high confidence\" and \"UMAP: dogs: high confidence\". We intentionally left out data points situated between clusters, as these represent challenging data points. This is expected since each labeling function won’t be perfect in isolation and some data points are difficult and challenging.\u003c/p\u003e\u003cp\u003eWe then repeat the process with t-SNE, instead of UMAP, and tag each cluster with \"t-SNE: cats: high confidence\" and \"t-SNE: dogs: high confidence\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"924\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUMAP: a cluster of cats with high confidence\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"923\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUMAP: a sub-cluster of images with two cats\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003et-SNE: the same sub-cluster of images with two cats\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"zero-shot-labeling-natural-language-search-for-classification\"\u003eZero-shot Labeling: Natural language search for classification\u003c/h3\u003e\u003cp\u003eLabelbox enables you to conduct \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003enatural language searches\u003c/a\u003e, for example you can type in “photos of cats” to surface all cat images. Adjusting the similarity threshold will narrow the search parameters to show only the images that contain cats. For this use case, we can filter for a similarity score higher than 0.61 and tag all of the 7,125 images as “Natural language search: Cats (high confidence)”. If we adjust the similarity score to be between 0.6 and 0.61, we can tag the 1,299 images as “Natural language search: Cats (low confidence)”. \u003c/p\u003e\u003cp\u003eWe take the same approach for images containing dogs. Using the same technique above, we tag 7,738 images of dogs as “Natural language search: Dogs (high confidence)” and surface and tag 2,750 images as “Natural language search: Dogs (low confidence)”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA natural language search will surface thousands of images of cats. By adjusting the similarity score, we can keep the most confident zero-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"few-shot-labeling-similarity-search-for-classification\"\u003eFew-shot Labeling: Similarity search for classification\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eLabelbox also streamlines few-shot labeling. Quickly browse all your data in Catalog to surface 5 images of cats and 5 images of dogs. Perform a similarity search in one click using these 10 images as anchors. For each anchor image, run a similarity search and tag the top results (e.g  with a similarity score of higher than 0.895) as “Labeling function: similarity search (cats: high confidence)”. This provides us with 10 new labeling functions that surface images similar to the anchor images.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"925\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA similarity search example with anchor images of dogs. We can filter to keep the most confident few-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"weak-labeling-combining-different-sources-of-signal\"\u003eWeak Labeling: Combining different sources of signal\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eWhile each of these labeling signals is powerful on its own, you can combine multiple sources in Labelbox. This allows you to apply simple rules in a weak supervision fashion to further enhance your results. Integrate different labeling signals, such as similarity searches, natural language searches, and data clusters, to boost your outcomes. You can \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003ecombine various filters\u003c/a\u003e by using the AND and OR functions.\u003c/p\u003e\u003ch2 id=\"step-4-automatically-classify-data-with-foundation-models-and-use-human-in-the-loop-qa-for-challenging-cases\"\u003eStep 4: Automatically classify data with foundation models and use human-in-the-loop QA for challenging cases\u003c/h2\u003e\u003ch3 id=\"high-confidence-data-points-direct-classification\"\u003eHigh confidence data points: direct classification\u003c/h3\u003e\u003cp\u003eFoundation models are highly confident about most data points. So much so that we can directly classify data points leveraging Quantumworks Lab’s \u003ca href=\"https://labelbox.com/blog/pre-label-and-enrich-your-data-with-bulk-classifications/?ref=labelbox-guides.ghost.io\"\u003ebulk classification feature\u003c/a\u003e. With this new feature, you can specify and send your data rows to a specific step of the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox.ghost.io#how-it-works\"\u003elabeling and review workflow\u003c/a\u003e. We can directly move these high-confidence data points straight to the “Done” task.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"927\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe can classify thousands of data points in bulk, and send them to the “Done” task of our labeling project, in just a click, since foundation models are confident on those.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these high-confident data points are those that belong to the cat or dog cluster, both with UMAP and t-SNE, and where the natural language score is higher than 0.61. This results in 7,627 dog classifications. But just how accurate are these classification predictions? \u003c/p\u003e\u003cp\u003eTo answer this question, we looked at the Hugging Face ground truths. On the surface, 10 out of the 7,627 dog predictions are incorrect (0.13%). However, upon closer inspection, it turns out that the Hugging Face dataset contains a few labeling mistakes and only 6 out of the 7,627 predictions (0.078%) of the foundation model’s predictions are actually incorrect. Similarly, there were 6,587 cat classifications. Only 6 out of the 6,587 cat predictions (0.09%) of the foundation model’s predictions are incorrect. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"761\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOut of 7,627 images, foundation models failed on these 10 by predicting dogs instead of cats.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eHigh-confident data points are also those that are found with a similarity search proximity to 2 or more anchors with a 0.895 or higher score. There were 907 dog classifications that fit this criteria, all of which were accurate except 1, and 1,022 cat classifications, all of which were accurate except 6. \u003c/p\u003e\u003cp\u003eBy leveraging the above methods, we were able to classify 16,143 data rows - with only 19 errors - achieving an \u003cstrong\u003eaccuracy of 99.9%. \u003c/strong\u003eSince 16,143 out of 18,699 data rows have been classified directly by foundation models, the \u003cstrong\u003ecoverage is 86%.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eNow, let’s move on to classify the remaining 14% of data rows, on which the foundation model appears to be less confident.\u003c/p\u003e\u003ch3 id=\"medium-confidence-data-points-human-in-the-loop-labeling\"\u003eMedium confidence data points: Human-in-the-Loop labeling\u003c/h3\u003e\u003cp\u003eFor some data points, foundation models exhibit moderate confidence. We can bulk classify these data points in Quantumworks Lab, but move them to the “To Review” task. This will ensure a human is looped in and will review the classifications coming from foundation models.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"922\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe can pre-label thousands of data points in bulk and send them to “Review” in our labeling project, since foundation models are moderately confident on these images.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these data points are those that belong to the cat or dog cluster, with UMAP or t-SNE, and that we hadn’t classified before:\u003c/p\u003e\u003cul\u003e\u003cli\u003e1,622 dogs classifications, which turn out to be all accurate except 10.\u003c/li\u003e\u003cli\u003e907 cat classifications, which turn out to be all accurate except 38. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eUsing this approach, we manage to classify 2,529 data rows, with an \u003cstrong\u003eaccuracy of 98%\u003c/strong\u003e (48 errors). Good that we send them to humans for review! So far, we’ve classified all data rows except 27, so the coverage is \u003cstrong\u003e99.85%\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eNow, let’s move on to classify the remaining 27 images.\u003c/p\u003e\u003ch3 id=\"low-confidence-data-points-manual-labeling\"\u003eLow confidence data points: Manual labeling\u003c/h3\u003e\u003cp\u003eAfter applying these rules, 18,672 data rows out of 18,699 (99.85%) have been labeled, leaving only 27 data rows unclassified. Foundation models lack the confidence to label these remaining data points.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1135\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSome data rows, 27 in our case, were not classified through foundation models. This includes images that are blurry, where animals are turning their backs or are barely visible behind a cage.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThese 27 data points will require manual labeling by humans, which represents only 0.14% of data rows - a massive efficiency gain in labeling effort and speed!\u003c/p\u003e\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eDirect classification with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eHuman in the loop with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eManual classification\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of data rows classified\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e16,143\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e2,529\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e27\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of errors\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e19\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e48\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e0\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAccuracy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e99.9%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e98%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e100%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eCoverage\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e86%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e13.5%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e0.15%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eCumulative coverage\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e86%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e99.8%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e100%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 16,143 images (86%) in minutes, with \u003cstrong\u003e99.9% accuracy\u003c/strong\u003e thanks to foundation models. An additional 2,529 data points (13.5%) have been pre-labeled with foundation models, with \u003cstrong\u003e98% accuracy\u003c/strong\u003e, and sent for human review. This leaves with only 27 very challenging images to label manually!\u003c/p\u003e\u003ch2 id=\"step-5-set-it-and-forget-it-%E2%80%93-automatically-apply-these-rules-to-fresh-incoming-data\"\u003eStep 5: Set it and forget it – automatically apply these rules to fresh, incoming data\u003c/h2\u003e\u003cp\u003eWith Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e, we can automatically classify fresh, incoming data as cats or dogs. \u003c/p\u003e\u003cp\u003eFor example, we can set up a slice that automatically surfaces all new images that have been connected to Quantumworks Lab in the past week, that haven’t been classified yet. We can set the slice’s criteria to include only images where the natural language search for the prompt “photo of a cat” is higher than 0.61 (since we know that these images are very likely to contain cats). \u003c/p\u003e\u003cp\u003eWith slices, you can easily surface and inspect any new and high-impact data that gets added to your data lake. \u003c/p\u003e\u003cp\u003eFrom there, it only takes one click to classify all of these images as cats.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1216\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWith slices and the ability to automatically surface high-impact data, we surfaced 7,000+ new images of cats that were connected to Quantumworks Lab in the past week. We can easily add a cat classification to all these images in one click.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can learn more about how to bulk classify data in our \u003ca href=\"https://docs.labelbox.com/docs/bulk-classification?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e or in our \u003ca href=\"https://labelbox.com/blog/pre-label-and-enrich-your-data-with-bulk-classifications/?ref=labelbox-guides.ghost.io\"\u003erecent blog post\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities, the bulk classification feature, and foundation models, we managed to classify 16,143 images (86%) in minutes, with \u003cstrong\u003e99.9% accuracy\u003c/strong\u003e. An additional 2,529 data points (13.5%) have been pre-labeled with \u003cstrong\u003e98% accuracy\u003c/strong\u003e and sent for human review. This only left us with 27 very challenging images that we needed to label manually. \u003c/p\u003e\u003chr\u003e\u003cp\u003eIf you’re a current Quantumworks Lab user who wants to leverage any foundation model to supercharge your data labeling process in just a few clicks,\u003ca href=\"https://app.labelbox.com/catalog?ref=labelbox-guides.ghost.io\"\u003e try our bulk classification feature today\u003c/a\u003e or \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started with a free Quantumworks Lab account\u003c/a\u003e.\u003c/p\u003e","comment_id":"6452689090e75e0001b3546c","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3058--2-.png","featured":false,"visibility":"public","created_at":"2023-05-03T13:58:40.000+00:00","updated_at":"2023-10-27T17:01:07.000+00:00","published_at":"2023-05-03T17:28:24.000+00:00","custom_excerpt":"Automatically label images with 99% accuracy leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/automatically-label-images-with-99-accuracy-using-foundation-models","tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/automatically-label-images-with-99-accuracy-using-foundation-models/","excerpt":"Automatically label images with 99% accuracy leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","reading_time":10,"access":true,"comments":false,"og_image":null,"og_title":"Automatically label images with 99% accuracy using foundation models","og_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","twitter_image":null,"twitter_title":"Automatically label images with 99% accuracy using foundation models","twitter_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","meta_title":"Automatically label images with 99% accuracy using foundation models","meta_description":"Automatically label images with 99% accuracy leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"645007f474911d003db0cb11","uuid":"3d216637-f0d9-4995-bfd5-d289ba48b7d0","title":"Introducing Export V2: How to export data with more granular control","slug":"how-to-export-your-data-with-more-granular-control","html":"\u003cp\u003eIf you already leverage Quantumworks Lab to enrich and label your unstructured data, you know how important it is to export your data insights in the right format and connect it with your downstream data workflow. Whether you want to store your data in a database, a cloud-hosted table, an ML training pipeline, or a production environment, you need a flexible and powerful export system that can handle your specific needs.\u003c/p\u003e\u003cp\u003eThat’s why \u003cstrong\u003ewe’re excited to introduce a new way to export your data\u003c/strong\u003e. This new system gives you more granular control over your data exports across the Quantumworks Lab platform and SDK. With this new way to export, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003eExport the exact Data Rows you are interested in. You can use various filters in the Catalog and Data row tabs, or hand-select the data rows to export. For example, you can grab only the data rows that have received new labels, metadata, or issues updates within the last 24 hours.\u003c/li\u003e\u003cli\u003eConfigure the export to include exactly the right information you need. You can build a custom export payload that meets your specific data workflow needs with much faster performance. Learn more about these improvements \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io#required--optional-fields\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eOrganize your assets into a data-row-centric framework that’s easy to structure and analyze.\u003c/li\u003e\u003cli\u003eLeverage\u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io\"\u003e improved annotation formats\u003c/a\u003e to rapidly export annotations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhile we will continue supporting the Export v1 system until December 31, 2023, we encourage you to gradually start migrating all of your export workflows to this updated export workflow (Export v2). The Export v1 and Export v2 workflows may be used in tandem until Export v1 is sunset on December 31st.\u003c/strong\u003e \u003c/p\u003e\u003cp\u003ePlease refer to our documentation to learn more about export specifications and compare the old Export v1 and new Export v2 systems: \u003ca href=\"https://docs.labelbox.com/reference/export-image-annotations?ref=labelbox-guides.ghost.io\"\u003eimage\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-video-annotations?ref=labelbox-guides.ghost.io\"\u003evideo\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-text-annotations?ref=labelbox-guides.ghost.io\"\u003etext\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-document-annotations?ref=labelbox-guides.ghost.io\"\u003edocuments\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-geospatial-annotations?ref=labelbox-guides.ghost.io\"\u003egeospatial/tiled imagery\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-audio-annotations?ref=labelbox-guides.ghost.io\"\u003eaudio\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-conversational-text-annotations?ref=labelbox-guides.ghost.io\"\u003econversational text\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-html-annotations?ref=labelbox-guides.ghost.io\"\u003eHTML\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-dicom-annotations?ref=labelbox-guides.ghost.io\"\u003eDICOM\u003c/a\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2 id=\"what%E2%80%99s-new-in-export-v2\"\u003eWhat’s new in Export v2\u003c/h2\u003e\u003ch3 id=\"data-row-centric-asynchronous-exports\"\u003eData row-centric asynchronous exports\u003c/h3\u003e\u003cp\u003eThe previous export system (Export v1) relied upon a less flexible label-centric export that limited access to all the information you might need about a Data Row. Within Export v2’s Data Row-centric context, you can access much more information — including fields like:\u003c/p\u003e\u003cul\u003e\u003cli\u003emetadata,\u003c/li\u003e\u003cli\u003eattachment,\u003c/li\u003e\u003cli\u003eworkflow history,\u003c/li\u003e\u003cli\u003emodel predictions,\u003c/li\u003e\u003cli\u003emedia attributes\u003c/li\u003e\u003cli\u003ebatch id,\u003c/li\u003e\u003cli\u003eand issues, alongside with the labels.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy reframing exports based on Data Rows, we’ve made it much more intuitive for you to integrate with your data tables that are organized around your team’s unique assets and data rows. \u003c/p\u003e\u003cul\u003e\u003cli\u003eAdditionally , now when you trigger an export job, it will occur asynchronously in the background, unblocking your workflow so you can avoid waiting. \u003c/li\u003e\u003cli\u003eIf you are using the Quantumworks Lab UI, you can access the task status in the notification center. \u003c/li\u003e\u003cli\u003eIf you are using SDK, you can query the task for status and results. \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"export-from-a-dataset-or-a-slice-with-the-option-to-grab-labels-from-multiple-projects-and-model-runs\"\u003eExport from a dataset or a slice, with the option to grab labels from multiple projects and model runs.\u003c/h3\u003e\u003cp\u003eA data row can have labels from multiple projects, or have predictions from multiple model runs. Using this new way to export through Catalog, or through the SDK, you can easily grab all the information about a Data Row. \u003c/p\u003e\u003cp\u003eUse data row filters to select a subset of data rows for export:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn the UI, you can build your \u003ca href=\"https://docs.labelbox.com/docs/data-rows-activity?ref=labelbox-guides.ghost.io#filter-data-rows\"\u003efilters within a project in the Data Rows tab\u003c/a\u003e, build \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003efilters in a dataset / slice in Catalog\u003c/a\u003e, or build \u003ca href=\"https://docs.labelbox.com/docs/filtering-and-sorting?ref=labelbox-guides.ghost.io\"\u003efilters in Model using model run filters\u003c/a\u003e. You can then choose to export only the filtered data rows. \u003c/li\u003e\u003cli\u003eIn the UI and SDK, you can use the \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io#filters\"\u003e'last_activity_at' filter\u003c/a\u003e to export only the data rows that have the creation and modification of labels, metadata, status, comments and reviews in a user-specified time range. This applies to a project in Annotate and a dataset or slice export. \u003c/li\u003e\u003cli\u003eIn the UI and SDK, we added a support for a \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io#filters\"\u003e'label_created_at' filter\u003c/a\u003e for you to export only the data rows that have the creation of labels in a user-specified time range. This applies to a project in Annotate and a dataset or slice export.\u003c/li\u003e\u003cli\u003eIn the UI, you can hand-pick data rows for export. Similarly in SDK, we added support for \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io#filters\"\u003e'data_row_ids'\u003c/a\u003e filter to export only the data rows that you are interested in.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"configure-exports-to-selectively-include-or-exclude-certain-information-on-a-data-row\"\u003eConfigure exports to selectively include or exclude certain information on a data row\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWe recognize different teams have unique needs around what they need from exports. For example, labeling team would love to understand the performance and consensus scores of a labeling project, whereas a developer would like to know the metadata and media attributes on a data row.\u003c/li\u003e\u003cli\u003eExport v2 now not only covers all possible fields on a data row, but also makes it configurable so that you can grab only the necessary payload of export information with faster performance. See \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io#required--optional-fields\"\u003ethis table\u003c/a\u003e to check all available option fields that you can include/exclude in your exports. \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"remove-the-caching-for-exports\"\u003eRemove the caching for exports\u003c/h3\u003e\u003cp\u003eExport v1 used to cache exports for 30 minutes. In Export v2, you will always get a fresh export and you can run one export asynchronous task on a project at a time. \u003c/p\u003e\u003ch3 id=\"simplified-and-improved-export-payloads\"\u003eSimplified and improved export payloads\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWe've converted all fields in the export payload into snake case.\u003c/li\u003e\u003cli\u003eRather than isolating them into another JSON file, we've improved the video and DICOM exports to contain all frame annotations in the export ndjson file. and Export v2 provides three representations of objects in frames: “frames”, “segments”, and “key_frame_feature_map” to facilitate your different needs of downstream workflows. See examples in \u003ca href=\"https://docs.labelbox.com/reference/export-video-annotations?ref=labelbox-guides.ghost.io#annotation-export-formats\"\u003eVideo\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/reference/export-dicom-annotations?ref=labelbox-guides.ghost.io#annotation-export-formats\"\u003eDICOM\u003c/a\u003e exports.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"exporting-via-ui\"\u003eExporting via UI\u003c/h2\u003e\u003cp\u003eFrom Quantumworks Lab's UI, you can access the export function through the drop-down menu after selecting a subset of data rows. You can export the entire project, model, dataset, or slice from a set of filters or a selection of data rows within them. \u003c/p\u003e\u003cp\u003eBelow are some examples of Export v2 in action. For more detailed information, please refer to our \u003ca href=\"https://docs.labelbox.com/docs/export-labels?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"annotate-a-labeling-project\"\u003eAnnotate (A labeling project)\u003cbr\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-5691075b-e1cf-448b-9fa1-fa21e739f71f.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"884\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSelect data rows from filteres in the Data Rows tab.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-9e91c4d6-8d20-49fa-a1fb-739a4247487a.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"882\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHand-select specific data rows.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"model-a-model-run\"\u003eModel (A model run)\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-be94fe0b-4fe4-40ba-a93d-a8268afcfe70.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"787\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSelect from metrics filters.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"catalog-a-dataset-and-slices\"\u003eCatalog (A dataset and slices)\u003cbr\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-51102232-fd36-41f0-a343-0f063d17c719.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"850\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExport a slice with labels from multiple projects.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"exporting-data-via-sdk\"\u003eExporting data via SDK\u003c/h2\u003e\u003cp\u003eFor developers that would like to programmatically feed exports directly into downstream data workflows or build automatic workflows to retrieve fresh data exports on a regular basis, we recommend Export v2 SDK. It provides flexibility to control what data you want to export. \u003c/p\u003e\u003ch3 id=\"project-export-v2\"\u003eProject Export v2\u003c/h3\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Set the export params to include/exclude certain fields. Make sure each of these fields are correctly grabbed \nexport_params= {\n  \"attachments\": True,\n  \"metadata_fields\": True,\n  \"data_row_details\": True,\n  \"project_details\": True,\n  \"performance_details\": True\n}\n\n# You can set the range for last_activity_at and label_created_at. You can also set a list of data \n# row ids to export. \n# For context, last_activity_at captures the creation and modification of labels, metadata, status, comments and reviews.\n\n# Note: This is an AND logic between the filters, so usually using one filter is sufficient.\nfilters= {\n  \"last_activity_at\": [\"2000-01-01 00:00:00\", \"2050-01-01 00:00:00\"],\n  \"label_created_at\": [\"2000-01-01 00:00:00\", \"2050-01-01 00:00:00\"],\n  \"data_row_ids\": [\"data_row_id_1\", \"data_row_id_2\"] \n}\n\nexport_task = project.export_v2(params=export_params, filters=filters)\nexport_task.wait_till_done()\n\nif export_task.errors:\n  print(export_task.errors)\n\nexport_json = export_task.result\nprint(\"results: \", export_json)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou can check out SDK examples of exporting from datasets, slices, and model runs in this \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"receiving-updates-via-webhooks\"\u003eReceiving updates via Webhooks\u003cbr\u003e\u003c/h2\u003e\u003cp\u003eFor teams that would like to get near real-time updates for each change on a data row, we recommend webhooks as a better option. Export v2 format can now be used for webhooks to receive the following events from project:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLABEL_CREATED\u003c/li\u003e\u003cli\u003eLABEL_UPDATED\u003c/li\u003e\u003cli\u003eLABEL_DELETED\u003c/li\u003e\u003cli\u003eREVIEW_CREATED\u003c/li\u003e\u003cli\u003eREVIEW_UPDATED\u003c/li\u003e\u003cli\u003eREVIEW_DELETED\u003c/li\u003e\u003cli\u003eWORKFLOW\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can configure a webhook that returns Export v2 in Project whenever an event is triggered. See more details in this \u003ca href=\"https://docs.labelbox.com/reference/webhook?ref=labelbox-guides.ghost.io\"\u003eWebhook Guide\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eFor example, you can use ngrok to expose a local port.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003engrok http 3001\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis will generate an address that looks like ` \u003ca href=\"https://887d-2601-645-8000-3a90-9cb4-7d1b-d9b4-6714.ngrok.io/?ref=labelbox-guides.ghost.io\"\u003ehttps://887d-2601-645-8000-3a90-9cb4-7d1b-d9b4-6714.ngrok.io\u003c/a\u003e` and it will forward all requests to your localhost:3001. \u003c/p\u003e\u003cp\u003eIn your terminal, create a python file that contains the following code to receive webhook payload. Make sure to change your secret.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom flask import Flask, request\nimport hmac, hashlib\nimport json\nimport threading\nfrom werkzeug.serving import run_simple\n\n\n# This can be any secret that matches your webhook config (we will set later)\nsecret = b\"CHANGE-ME\"\n\n\n# Example for server-side code to receive webhook events\napp = Flask(__name__)\n\n\n@app.route(\"/webhook-endpoint\", methods=[\"POST\"])\ndef print_webhook_info():\n   payload = request.data\n   computed_signature = hmac.new(secret, msg=payload,\n                                 digestmod=hashlib.sha1).hexdigest()\n   if request.headers[\"X-Hub-Signature\"] != \"sha1=\" + computed_signature:\n       print(\n           \"Error: computed_signature does not match signature provided in the headers\"\n       )\n       return \"Error\", 500, 200\n\n\n   print(\"=========== New Webhook Delivery ============\")\n   print(\"Delivery ID: %s\" % request.headers[\"X-Labelbox-Id\"])\n   print(\"Event: %s\" % request.headers[\"X-Labelbox-Event\"])\n   print(\"Payload: %s\" %\n         json.dumps(json.loads(payload.decode(\"utf8\")), indent=4))\n   return \"Success\"\n\n\n\n\nthread = threading.Thread(target=lambda: run_simple(\"0.0.0.0\", 3001, app))\nthread.start()\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen run this script to start receiving requests from the ngrok address:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003engrok http 3001\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow, you can configure a webhook in Quantumworks Lab's Project setting. \u003c/p\u003e\u003cul\u003e\u003cli\u003eClick \u003cem\u003eSet up webhook\u003c/em\u003e\u003c/li\u003e\u003cli\u003eChoose V2 as the version of the webhook\u003c/li\u003e\u003cli\u003ePaste in the ngrok address plus /webhook-endpoint. You will need to write the secret to match the secret you specified in your app.py script \u003c/li\u003e\u003cli\u003eFinally, select the topics you want to subscribe to\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-122ef535-e118-494c-b065-4981a49f313d.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"876\" height=\"1110\"\u003e\u003c/figure\u003e\u003cp\u003eNow that you've created a webhook, everytime there is a new event triggered (such as updating a label), you will receieve the payload at / webhook-endpoint. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-bfd67059-8a6a-4e42-a232-ab9621d473a1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"927\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003eThe improved and datarow-centric format of Export v2 empowers you to export with more granularity by including or excluding variables based on your project’s unique needs. Offering a more seamless user experience, the new export format more consistently mirrors our import format and aligns with annotation schema available in the platform.\u003c/p\u003e\u003cp\u003eAs you migrate from Export v1 to Export v2 workflows, please refer to our \u003ca href=\"https://docs.labelbox.com/reference/export-v2-glossary?ref=labelbox-guides.ghost.io\"\u003edocumentation \u003c/a\u003efor more detailed instructions on how to export your data through the UI or through the Python SDK.\u003c/p\u003e","comment_id":"645007f474911d003db0cb11","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Frame-2299--3-.png","featured":false,"visibility":"public","created_at":"2023-05-01T18:41:56.000+00:00","updated_at":"2023-10-27T17:01:36.000+00:00","published_at":"2023-05-01T23:08:32.000+00:00","custom_excerpt":"With Export V2, you can export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-export-your-data-with-more-granular-control","tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-export-your-data-with-more-granular-control/","excerpt":"With Export V2, you can export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. ","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":"Introducing Export V2: How to export data with more granular control","og_description":"Learn how to export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. ","twitter_image":null,"twitter_title":"Introducing Export V2: How to export data with more granular control","twitter_description":"Learn how to export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. ","meta_title":"Introducing Export V2: How to export data with more granular control","meta_description":"Learn how to export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6446b44974911d003db0cad0","uuid":"1b76d2d0-679c-4198-a77b-ab4acdb50498","title":"How to create and label text layers from PDF documents for AI","slug":"how-to-create-and-label-text-layers-from-pdf-documents-for-ai","html":"\u003cp\u003ePDF documents are one of the toughest data types to handle when it comes to building AI models. While most other data types usually contain information in one format, PDFs can comprise multiple types of data within them, such as photos, graphs and charts, text formatted in paragraphs, outlines, lists, links, and more. To add to the complexity, PDF documents can be hundreds of pages long, so a single data point can amount to hundreds of hours of work to label for training a model.\u003c/p\u003e\u003cp\u003eThat's why many AI teams working with this data type use PDF layers to separate and organize different information formats within a document, making them easier to view, edit, and enrich with labels. As most PDF documents contain primarily text, creating a text layer is often essential to the process of building AI on PDF data.\u003c/p\u003e\u003cp\u003eIn this post, we’ll take a look at the different types of PDF layers and discuss various methods for creating PDF text layers, such as using PDF creation software, OCR software, or Python scripts with libraries like reportlab or PyPDF2. Additionally, we will highlight the usefulness of PDF text layers in applications such as Quantumworks Lab, where they can be imported for annotation and used to build robust AI models with contextual information from PDF documents.\u003c/p\u003e\u003ch3 id=\"what-are-pdf-layers\"\u003eWhat are PDF layers?\u003c/h3\u003e\u003cp\u003eA PDF layer comprises a specific type of data contained within a PDF document. Separating a document into layers enables AI builders to process the different types of information within it. \u003c/p\u003e\u003cp\u003eTypes of layers include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eText layer: Refers to the selectable and searchable text contained within a PDF document. The text layer is an essential component of the PDF format, as it enables users to interact with the document's content by copying, searching, and editing text.\u003c/li\u003e\u003cli\u003eImage and graphics layers: Contain images, illustrations, and graphical elements.\u003c/li\u003e\u003cli\u003eBackground layer: Contains the background color or images, usually placed behind the main content.\u003c/li\u003e\u003cli\u003eAnnotations and comments layer: Contains any added annotations, comments, or interactive elements like form fields.\u003c/li\u003e\u003cli\u003eWatermarks or overlays: Contains watermarks, stamps, or other overlay elements that appear over the main content.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"how-to-create-a-pdf-text-layer\"\u003eHow to create a PDF text layer\u003c/h3\u003e\u003cp\u003ePDF text layers can be created in several ways, depending on the source of the document and the tools available. Here are a few methods for creating a PDF text layer:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eUsing PDF creation software.\u003c/strong\u003e There are dedicated PDF creation tools, such as Adobe Acrobat, that allow you to create a PDF document from various file formats, including images, Word files, and other document types. These tools often have built-in OCR (Optical Character Recognition) capabilities that can detect and create text layers from scanned or image-based documents.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOCR software.\u003c/strong\u003e If you have a scanned or image-based document without a text layer, you can use OCR software to recognize the text and create a new PDF with a text layer. There are many OCR tools available, such as Adobe Acrobat, AWS Textract, and Tesseract.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePython script.\u003c/strong\u003e You can separate a document into layers using libraries like reportlab or PyPDF2.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"why-is-a-pdf-text-layer-useful\"\u003eWhy is a PDF text layer useful?\u003c/h3\u003e\u003cp\u003ePDF text layers enable AI teams to annotate text data on its own, and then later add context to that information by combining it with the other data types within the document. Quantumworks Lab enables you to import your PDF and text layer so that you can annotate on the text layer and then export the labeled text with photos, graphics, and other information from the document to build robust AI models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to convert your PDF into Quantumworks Lab PDF text layer format\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eYou can convert your PDF into the Quantumworks Lab PDF text layer format using the CLI shown \u003ca href=\"https://github.com/Quantumworks Lab/PDF-OCR-Transform-CLI?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eFirst, you will need to have AWS CLI installed globally and configured for your AWS user with permissions to S3 and Textract. The CLI will upload your PDF to S3 and save the Quantumworks Lab formatted PDF text layer JSON file in the specified folder.\u003c/p\u003e\u003cp\u003eThere is a configuration file named config.json at the root level of the directory that must be updated before first running the CLI.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-CLI\"\u003e{\n  // The name of the bucket in your cloud provider that pdfs will be uploaded to\n  \"bucketName\": \"\u0026lt;name_of_s3_bucket\u0026gt;\"\n}\nconvert - Run OCR on all pdfs contained in the input folder and convert the result into Quantumworks Lab's text layer JSON.\n\nconvert --inputFolder \u0026lt;input_folder_containing_pdfs\u0026gt; --format \u0026lt;aws-textract\u0026gt; --outputFolder \u0026lt;output_folder\u0026gt; --concurrency 10\n\n--inputFolder The input folder containing the pdfs\n\n--format The OCR format to use (aws-textract, google-cloud-vision)\n\n--outputFolder The output folder to place the generated text layer json files\n\n--concurrency How many pdfs to process at the same time. CAUTION: Setting this value too high can result in rate limits being reached.\n\nExample (Mac)\n\n./textlayer-macos convert --inputFolder input --format aws-textract --outputFolder output --concurrency 10\nvalidate - Validates the provided text layer json\n\nvalidate --textLayerFilepath \u0026lt;text_layer_filepath\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOnce you have the PDF text layer in the Quantumworks Lab format, you can upload it and the PDF file to Quantumworks Lab as shown \u003ca href=\"https://docs.labelbox.com/reference/documents?ref=labelbox-guides.ghost.io#import-format\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eYou can also convert GCP OCR JSON or Adobe OCR JSON formats into Quantumworks Lab's format as shown \u003ca href=\"https://github.com/Quantumworks Lab/PDF-OCR-Transform-CLI?ref=labelbox-guides.ghost.io#ad-hoc-transform-scripts\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"view-and-label-pdf-text-layers\"\u003e\u003cbr\u003eView and label PDF text layers\u003c/h3\u003e\u003cp\u003eSee how you can visualize and annotate PDF text layers for your AI projects using Quantumworks Lab by selecting \"Show text layer\" (illustrated below).\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/80w2yj6yqi\" title=\"text layer Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eShow text layers within Quantumworks Lab\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"627\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample view of a text layer within the Quantumworks Lab UI\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe hope what we covered in this post helps inspire you to better work with the different ways to create PDF text layers, from using software to OCR or Python scripts. These text layers can come in handy when building AI models with PDF context. Feel free to give it a try for yourself and we'd love to hear your feedback and what else you'd like to see when it comes to creating and labeling text layers for your ML use cases.\u003c/p\u003e\u003cp\u003eLearn more about:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHow to \u003ca href=\"https://docs.labelbox.com/reference/documents?ref=labelbox-guides.ghost.io#import-format\"\u003eupload\u003c/a\u003e text layers to Quantumworks Lab\u003c/li\u003e\u003cli\u003eHow to natively \u003ca href=\"https://labelbox.com/guides/how-to-natively-annotate-a-pdf-document/?ref=labelbox-guides.ghost.io\"\u003eannotate\u003c/a\u003e a PDF document in Quantumworks Lab\u003c/li\u003e\u003cli\u003eWhat types of \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#import-document-data\"\u003eDocument\u003c/a\u003e annotation tasks can be used\u003c/li\u003e\u003c/ul\u003e","comment_id":"6446b44974911d003db0cad0","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/04/Screen-Shot-2023-04-24-at-9.16.14-AM.png","featured":false,"visibility":"public","created_at":"2023-04-24T16:54:33.000+00:00","updated_at":"2023-10-26T18:10:32.000+00:00","published_at":"2023-04-20T16:54:00.000+00:00","custom_excerpt":"Learn about the different types of PDF layers and how to import annotations to build robust AI models with contextual information from PDF documents.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa608375d13000123d7fa","name":"Industry: Finance \u0026 insurance","slug":"industry-finance-insurance","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-finance-insurance/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-create-and-label-text-layers-from-pdf-documents-for-ai/","excerpt":"Learn about the different types of PDF layers and how to import annotations to build robust AI models with contextual information from PDF documents.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":74,"tag":{"slug":"build-ai","id":"653aa45d375d13000123d7de","name":"Build AI","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","count":{"posts":74},"url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"slug":"build-ai","currentPage":"4"},"__N_SSG":true},"page":"/guides/tag/[id]/page/[pagenum]","query":{"id":"build-ai","pagenum":"4"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/tag/build-ai/page/4/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 13:06:03 GMT -->
</html>