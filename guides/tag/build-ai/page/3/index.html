<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/tag/build-ai/page/3/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:49:44 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../../../static/scripts/munchkin.js"></script><script src="../../../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/guides/tag/%5bid%5d/page/%5bpagenum%5d-da4e9ee1c105845a.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../../../index.html"><img width="106" height="24" alt="logo" src="../../../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../../index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Build AI</a><a href="../../../use-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Use AI</a><a href="../../../explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="../../../label-data-for-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Label data for AI</a><a href="../../../train-fine-tune-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../../../mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-visually-assess-damage-detection-and-improve-claims-automation-with-ai/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-16-at-2.48.33-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-16-at-2.48.33-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-16-at-2.48.33-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-16-at-2.48.33-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-16-at-2.48.33-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-16-at-2.48.33-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-16-at-2.48.33-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-16-at-2.48.33-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index776a.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F01%2FScreenshot-2024-01-16-at-2.48.33-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-visually-assess-damage-detection-and-improve-claims-automation-with-ai/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to build damage classification models with aerial imagery to improve claims automation</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how your team can leverage Quantumworks Lab’s platform to build a task-specific model to improve building damage detection. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-automatically-ingest-data-from-databricks-into-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FDatabricksLabelboxIngestionPipeline_Guide_Header.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FDatabricksLabelboxIngestionPipeline_Guide_Header.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FDatabricksLabelboxIngestionPipeline_Guide_Header.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FDatabricksLabelboxIngestionPipeline_Guide_Header.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FDatabricksLabelboxIngestionPipeline_Guide_Header.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FDatabricksLabelboxIngestionPipeline_Guide_Header.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FDatabricksLabelboxIngestionPipeline_Guide_Header.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FDatabricksLabelboxIngestionPipeline_Guide_Header.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index8e0a.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FDatabricksLabelboxIngestionPipeline_Guide_Header.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-automatically-ingest-data-from-databricks-into-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to automatically ingest data from Databricks into Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-generate-data-for-model-comparison-and-rlhf/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3091.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3091.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3091.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3091.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3091.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3091.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3091.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3091.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexdbc4.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3091.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-generate-data-for-model-comparison-and-rlhf/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to generate data for model comparison and RLHF</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index0a79.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to analyze customer reviews and improve customer care with NLP</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage Quantumworks Lab&#x27;s data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-build-a-content-moderation-model-to-detect-disinformation/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index0db2.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-build-a-content-moderation-model-to-detect-disinformation/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to build a content moderation model to detect disinformation</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust &amp; safety applications. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../zero-shot-learning-few-shot-learning-fine-tuning/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index109f.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../zero-shot-learning-few-shot-learning-fine-tuning/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Zero-Shot Learning vs. Few-Shot Learning vs. Fine-Tuning: A technical walkthrough using OpenAI&#x27;s APIs &amp; models</p><p class="text-base max-w-2xl undefined line-clamp-3">With large language models (LLMs) gaining popularity, new techniques have emerged for applying them to NLP tasks. Three techniques in particular — zero-shot learning, few-shot learning, and fine-tuning — take different approaches to leveraging LLMs. In this guide, we’ll walk through the key difference between these techniques and how to implement them. 

We’ll walk through a case study of extracting airline names from tweets to compare the techniques. Using an entity extraction dataset, we’ll be</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-boost-retail-profitability-with-ai-powered-shelf-object-detection/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3082.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3082.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3082.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3082.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3082.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3082.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3082.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3082.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexeaa4.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3082.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-boost-retail-profitability-with-ai-powered-shelf-object-detection/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to boost retail profitability with AI-powered shelf object detection</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexfcac.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-fine-tune-vertex-ai-models-with-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index32b3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-fine-tune-vertex-ai-models-with-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to fine-tune Vertex AI LLMs with Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-build-a-powerful-product-recommendation-system-for-retail/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexe6a1.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-build-a-powerful-product-recommendation-system-for-retail/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to build a powerful product recommendation system for retail</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage Quantumworks Lab&#x27;s data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. </p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8"><a class="mr-9 text-neutral-700 mb-1" href="../2/index.html">&lt;</a>Page 3 of 8<a class="ml-9 text-neutral-700 mb-1" href="../4/index.html">&gt;</a></div></div></div></div></div></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"65a6bf0b3c5a9d00013a6ea2","uuid":"e42b9d02-59c8-4914-9b59-88f6c01c9ff4","title":"How to build damage classification models with aerial imagery to improve claims automation","slug":"how-to-visually-assess-damage-detection-and-improve-claims-automation-with-ai","html":"\u003cp\u003e\u003cbr\u003eWith AI-powered claims automation, you can now seamlessly integrate the latest advances in foundation models into your damage detection and disaster assessment models. As the demand for real-time intelligence into understanding residential and commercial properties grows, it's essential for teams to maximize compliance and minimize operational costs. \u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for damage detection. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving damage detection requires a vast amount of data in the form of images and videos. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their claims automation through advanced computer vision techniques. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s AI-assisted data enrichment and flexible training frameworks to quickly build task-specific models that uncover actionable insights from damage assessment.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-16-at-3.48.51-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1714\" height=\"936\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-16-at-3.48.51-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-16-at-3.48.51-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-16-at-3.48.51-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-16-at-3.48.51-PM.png 1714w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build a task-specific model to improve building damage detection via aerial imagery. Specifically, this guide will walk through how you can explore and better understand unstructured data to make more data-driven business decisions around damage detection initiatives.\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-visually-assess-damage-and-improve-claims-automation-with-ai\"\u003eSee it in action: How to visually assess damage and improve claims automation with AI\u003c/h3\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below via:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1v9_arRsnr0od8VlLSk6fnTcTRsaVgOu_?usp=drive_link\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003e\u003cstrong\u003ePart 1: Explore and prepare your data\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1v9_arRsnr0od8VlLSk6fnTcTRsaVgOu_?usp=drive_link\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, please make a copy of the notebook.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/b3hi000zyz\" title=\"SA Imagery -- Part 1. Data Ingestion Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eFor this use case, we’ll be working with an open dataset from the Hurricane Maria aerial assessment dataset\u0026nbsp; – with the goal of quickly curating data and finding building damage from high-volumes of images.\u003c/p\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003ePlease download the dataset and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your data as a CSV.\u003c/li\u003e\u003cli\u003eIf your images sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your image data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mk64mj4bw4\" title=\"SA Imagery -- Part 2. Catalog Data Discovery Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this demo, we'll be using Catalog to find relevant images of buildings for our dataset with the goal of annotating footprints using foundation models. \u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"use-foundry-to-generate-building-footprints\"\u003eUse Foundry to Generate Building Footprints\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/0lnyh3ue3z\" title=\"SA Imagery Part 3. Generate Building Footprints w_ Model Foundry Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe next step is to use foundation models to detect as many building footprints as possible.\u003c/p\u003e\u003cp\u003eModel Foundry enables teams to choose from a library of models and in this case, we'll be using an object detection model (Grounding DINO) to generate previews and attach them as pre-labels.\u003c/p\u003e\u003cp\u003eWith\u0026nbsp;\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, you can automate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/p\u003e\u003ch3 id=\"set-up-your-annotation-project\"\u003eSet up your annotation project\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/i05gqcznas\" title=\"SA Imagery Part 4. Setting Up Annotation Project Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBy pre-labeling building footprints, we have significantly sped up our labeling efficiency and the next step is to set up our annotation project.\u003c/p\u003e\u003cp\u003e1) Set up annotation project and ontology.\u003c/p\u003e\u003cp\u003e2) Ensure ontology includes sub classification for damage severity (e.g., Low, Medium, High).\u003c/p\u003e\u003cp\u003e3) Include bounding box model predictions from Foundry pre-labels.\u003c/p\u003e\u003cp\u003e4) Zoom-in to determine the level of damage and select the appropriate level of damage or draw additional bounding boxes as needed.\u003c/p\u003e\u003ch3 id=\"optional-send-ground-truth-data-to-annotate\"\u003e[Optional] Send ground-truth data to Annotate\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/achdmpcq69\" title=\"SA Imagery Part 5. MAL Upload (Optional) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThese next series of steps are optional and will help you bypass the manual component by uploading ground-truth directly by using the Annotation project ID.\u003c/p\u003e\u003cp\u003e1) Reference a cloud bucket that references annotations and corresponding damage classifications.\u003c/p\u003e\u003cp\u003e2) Send annotation data type of annotation project so that you have ground-truth data.\u003c/p\u003e\u003cp\u003e3) By uploading annotation data directly via the Python SDK, you can access ground-truth data to send directly to your models for fine-tuning and refinement.\u003c/p\u003e\u003ch2 id=\"part-2-create-a-model-run-and-and-evaluate-model-performance\"\u003ePart 2: Create a model run and and evaluate model performance\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this\u0026nbsp;\u003ca href=\"https://colab.research.google.com/drive/1NSxJRK_I8TZFSDRlp_b8cHMdyx0kJjn1?usp=drive_link\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along,\u0026nbsp;\u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCreate a model run\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/pkyquongd8\" title=\"SA Imagery Part 6. Model Run _ Model Tuning _ Active Learning Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you have your labeled data in your project in Annotate, you’re ready to move on to creating a model run in\u0026nbsp;\u003ca href=\"https://app.labelbox.com/mea?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\" rel=\"noreferrer\"\u003eLabelbox Model\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eWe’ll be using this\u0026nbsp;\u003ca href=\"https://colab.research.google.com/drive/1NSxJRK_I8TZFSDRlp_b8cHMdyx0kJjn1?usp=drive_link\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to train a model on the training dataset and bring back inferences from the trained model for evaluation and diagnosis.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou’ll be able to view the model in the ‘Experiments’ tab in Quantumworks Lab Model – you can view ground truth predictions in green and predictions in red.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness\"\u003eEvaluate and diagnose model effectiveness\u003c/h3\u003e\u003cp\u003eA disagreement between model predictions and ground truth labels can be due to a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eAfter running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/li\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model for 10x faster corner-case detection – detect and visualize corner-cases where the model is underperforming. For example, you can drill into cases where ‘empty’ objects are not predicted, where the model might have difficulty identifying empty spaces on shelves where there is a wire mesh material present.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select ‘Find similar in Catalog’ from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled images that you can send to your model for labeling, you can filter on the ‘Annotation is’ filter and select ‘none’. This will only show unlabeled images that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all images that apply and send them as a batch to your original labeling project. Labeling these in priority will help improve model performance.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCompare model runs across iterations\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eImprove model development by up to 90% by leveraging Quantumworks Lab Model to compare model runs across iterations to track and quantify how model performance has improved over time.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model using the same steps with the Colab notebook on this improved data set.\u0026nbsp;You can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003cp\u003eBy analyzing high-volumes of images, videos and text, Quantumworks Lab provides valuable human-in-the-loop insights for damage detection processes to ensure underwriting risks, enabling insurance companies to make data-driven decisions that improve operational efficiency, compliance and revenue.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful task-specific models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"65a6bf0b3c5a9d00013a6ea2","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-16-at-2.48.33-PM.png","featured":false,"visibility":"public","created_at":"2024-01-16T17:38:19.000+00:00","updated_at":"2024-02-28T22:32:48.000+00:00","published_at":"2024-01-16T18:30:45.000+00:00","custom_excerpt":"Learn how your team can leverage Quantumworks Lab’s platform to build a task-specific model to improve building damage detection. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa608375d13000123d7fa","name":"Industry: Finance \u0026 insurance","slug":"industry-finance-insurance","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-finance-insurance/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-visually-assess-damage-detection-and-improve-claims-automation-with-ai/","excerpt":"Learn how your team can leverage Quantumworks Lab’s platform to build a task-specific model to improve building damage detection. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"656e6d5472e557000141cf9d","uuid":"44473dbc-7bfa-43a0-a5d6-7e061877d134","title":"How to automatically ingest data from Databricks into Quantumworks Lab","slug":"how-to-automatically-ingest-data-from-databricks-into-labelbox","html":"\u003cp\u003eMoving data seamlessly through your MLOps pipeline is essential to building successful AI products. In this guide, learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cjp13xg9no\" title=\"Databricks Ingestion Pipeline Demo Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1.\u0026nbsp; Navigate to the \u003ca href=\"https://huggingface.co/spaces/Quantumworks Lab/databricks_upload?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003epipeline creator webpage\u003c/u\u003e\u003c/a\u003e and enter your Databricks domain. You can find this by going to your Databricks environment. The domain will be in the URL, so you can copy and paste it into the pipeline creator.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_1_Domain.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1734\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_1_Domain.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_1_Domain.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_1_Domain.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_1_Domain.png 1734w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eYour Databricks domain is in the URL whenever you access your Databricks environment.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e2. Now select the cloud environment that your Databricks workspace runs in. This information is usually also in the Databricks domain that you just copy/pasted.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3. If you don’t already have a Databricks API, create one by going to your Databricks domain. Go to the user tab in the top right, then go to \u003cstrong\u003eUser settings \u0026gt; Developer \u0026gt; Access Tokens\u003c/strong\u003e, and create an access token. Paste your Databricks API key into the pipeline creator.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_2_AccessToken.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1734\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_2_AccessToken.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_2_AccessToken.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_2_AccessToken.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_2_AccessToken.png 1734w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eYou can create an access token for the Databricks API by going to the user tab, then to User Settings \u0026gt; Developer \u0026gt; Access tokens.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e4. Next, you’ll need your Quantumworks Lab API key. If you don’t already have one, you can create one from within your Quantumworks Lab environment by going to \u003cstrong\u003eWorkspace Settings \u0026gt; API \u0026gt; Create a new key\u003c/strong\u003e. Paste this key into the creator pipeline. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_3_LBAPIKey.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1734\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_3_LBAPIKey.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_3_LBAPIKey.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_3_LBAPIKey.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_3_LBAPIKey.png 1734w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eCreate an API key in Quantumworks Lab from the Workspace Settings section of your environment.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e5. Next, the pipeline creator will give you the option of making a new dataset or appending an existing one. Be sure to give the dataset a relevant name, as it will appear under than name within Quantumworks Lab Catalog once the dataset has been created and the data ingested from Databricks. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_4_NameDataset.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1738\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_4_NameDataset.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_4_NameDataset.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_4_NameDataset.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_4_NameDataset.png 1738w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eName your dataset within the pipeline creator webpage.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e6. Select a cluster from within your Databricks environment on the pipeline creator page.\u0026nbsp;\u003c/p\u003e\u003cp\u003e7. Once the cluster is ready, you’ll see the option to select a run frequency, or the cadence with which the workflow is going to execute. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_5_RunFrequency.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"713\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_5_RunFrequency.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_5_RunFrequency.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_5_RunFrequency.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eChoose how often you want this workflow to run.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e8. Next, choose the relevant table and database in Databricks from which you want to pull data. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_6_SelectTable.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"713\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_6_SelectTable.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_6_SelectTable.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_6_SelectTable.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eChoose the database and table you want to pull data from.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e9. The page will show you a sample of data rows within the selected table. The row data column signifies the URL by which you want to pool the data from. This can either be a public URL or objects hosted within your cloud storage.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_7_RowData.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1738\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_7_RowData.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_7_RowData.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_7_RowData.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_7_RowData.png 1738w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eA sample of the data rows in the selected dataset will appear on the pipeline creator page.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e10. Choose the data row column pointing to the object that you want to render. You also have an additional option of choosing the global key, which will specify the unique identifier assigned to each data row once they’re ingested into Labelbox.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_8_SelectRowDataGlobalKey.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"753\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_8_SelectRowDataGlobalKey.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_8_SelectRowDataGlobalKey.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_8_SelectRowDataGlobalKey.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eSelect your row data column and if needed, select global key.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e11. Click on \u003cstrong\u003eDeploy pipeline. \u003c/strong\u003eAfter executing for a few seconds, you’ll see a confirmation that your pipeline has been deployed. Now you can navigate to your Databricks environment, go to the workflows tab on the left, and see the new upload workflow that you’ve just created. Once the workflow has run, you’ll be able to see the ingested data in Quantumworks Lab Catalog.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_8_NewUploadWorkflow.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1738\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_8_NewUploadWorkflow.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_8_NewUploadWorkflow.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_8_NewUploadWorkflow.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_8_NewUploadWorkflow.png 1738w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eSee the new upload workflow within your Databricks environment by navigating to the Workflows tab.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYour new workflow will now ingest the specified data into Quantumworks Lab at the cadence you chose. \u003ca href=\"https://labelbox.com/blog/seamlessly-integrate-databricks-data-pipelines-with-labelbox/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRead this blog post\u003c/u\u003e\u003c/a\u003e to learn more about how you can integrate Databricks and Quantumworks Lab into a seamless data engine for AI.\u0026nbsp;\u003c/p\u003e","comment_id":"656e6d5472e557000141cf9d","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/12/DatabricksLabelboxIngestionPipeline_Guide_Header.png","featured":false,"visibility":"public","created_at":"2023-12-05T00:22:44.000+00:00","updated_at":"2023-12-05T17:38:22.000+00:00","published_at":"2023-12-05T17:38:22.000+00:00","custom_excerpt":"Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-automatically-ingest-data-from-databricks-into-labelbox/","excerpt":"Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"How to automatically ingest data from Databricks into Quantumworks Lab","og_description":"Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.","twitter_image":null,"twitter_title":"How to automatically ingest data from Databricks into Quantumworks Lab","twitter_description":"Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.","meta_title":"How to automatically ingest data from Databricks into Quantumworks Lab","meta_description":"Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6567b04d72e557000141cf3c","uuid":"fba5c2d3-75df-445a-87ea-ff02a1a28db0","title":"How to generate data for model comparison and RLHF","slug":"how-to-generate-data-for-model-comparison-and-rlhf","html":"\u003cp\u003eLarge language models (LLMs) have disrupted the way teams build and train intelligent applications. Trained on massive text datasets\u0026nbsp; containing millions or even billions of data points, LLMs have become increasingly important for various AI applications such as chatbots, personalized recommendation systems, and more. These models have shattered the barriers of what was once thought possible in natural language understanding and generation.\u003c/p\u003e\u003cp\u003eWhile large language models have enormous potential to revolutionize intelligent applications, effectively deploying them requires adapting LLMs to align with human preferences to mitigate risks. Successfully adopting LLMs involves continually fine-tuning and evaluating the model’s performance. This requires assessing subjective qualities like tone, fluency, and propriety through a combination of automated techniques and human-in-the-loop validation. \u003c/p\u003e\u003ch2 id=\"the-importance-of-human-preference-datasets-and-rlhf\"\u003eThe importance of human preference datasets and RLHF\u0026nbsp;\u003c/h2\u003e\u003cp\u003eAs LLMs rapidly advance in capability, properly directing them for the benefit of humanity should be a top priority. Human preference datasets and reinforcement learning from human feedback (RLHF) provide critical approaches for promoting the safe and ethical alignment of these influential AI systems.\u003c/p\u003e\u003cp\u003eA human preference dataset comprises numerous examples where humans indicate their preferences between model outputs, such as judging which content could be more harmful or which summary appears more accurate. By training models with this data, we can instill more ethical and beneficial behaviors aligned to human values.\u0026nbsp;\u003c/p\u003e\u003cp\u003eReinforcement learning from human feedback (RLHF) goes one step further by actively querying humans within a feedback loop to continuously improve models by correcting mistakes and reinforcing positive behaviors. You can learn more about the importance of RLHF in \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ethis blog post.\u003c/u\u003e\u003c/a\u003e \u003c/p\u003e\u003ch2 id=\"where-does-labelbox-come-in\"\u003eWhere does Quantumworks Lab come in? \u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform that can help your team navigate the future of large language models with human-centric evaluation. To ensure trustworthy, reliable, and safe AI aligned with human values, Quantumworks Lab allows teams to generate high-quality data for alignment and confidently ship LLMs with human experts to validate model outcomes.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith the new LLM human preference editor, you can create human preference data for model comparison or RLHF (reinforcement learning with human feedback). You can compare model outputs side-by-side and select the most favorable model output on a conversational text thread by assigning the model output a classification. This editor solves two important problems that are critical to ensuring responsible AI aligned with human preferences:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eModel comparison: \u003c/strong\u003eConduct the evaluation and comparison of model configurations for a given use case and decide which one to pick.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRLHF: \u003c/strong\u003eCreate preference data for training a reward model for RLHF based on multiple outputs from a single model.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThe \u003ca href=\"https://docs.labelbox.com/docs/llm-human-preference?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003enew LLM human preference editor\u003c/a\u003e and the previously released \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM data generation editor\u003c/u\u003e\u003c/a\u003e support:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eMarkdown rendering and markdown editing capabilities\u0026nbsp;\u003c/li\u003e\u003cli\u003eThe ability to import model predictions and ground truth through model-assisted labeling\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWith Quantumworks Lab, you can tap into both reinforcement learning from human feedback (RLHF) with your own internal team and skillful data labeling services with expertise in RLHF, evaluation, and red teaming. This human-centered approach is the key to developing reliable, responsible AI systems scaled for your business.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-generate-data-for-model-comparison-and-rlhf\"\u003eSee it in action: How to generate data for model comparison and RLHF\u003c/h2\u003e\u003cp\u003e\u003cem\u003eWe recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_keyword=Quantumworks Lab\u0026utm_source=organic\u0026utm_medium=website\u0026utm_campaign=boost\u0026\u0026attr=intercom\u0026referrer_url=https://connect.labelbox.co/\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/wckevgl1sc\" title=\"How to generate data for model comparison and RLHF - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eFor this tutorial, we will utilize conversational text data in Quantumworks Lab to train an AI assistant for a shopping app. This assistant will be designed to aid customers at various stages, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProduct discovery:\u003c/strong\u003e Help browse items and make recommendations based on the customer's needs and interests\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePre-purchase guidance\u003c/strong\u003e: Answer questions and provide details to assist in the buying decision process\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePost-purchase support:\u003c/strong\u003e Provide helpful information about orders, shipping, returns, or other purchase-related needs\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-1-data-upload-and-setup\"\u003eStep 1: Data upload and setup\u003c/h3\u003e\u003cp\u003eThe data that we will be using for this tutorial are three conversational text datasets based on the above scenarios:\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eConversation #1:\u003c/strong\u003e The customer has a budget in mind and is searching for an affordable vacuum cleaner option that meets their needs.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eConversation #2:\u003c/strong\u003e A customer's existing vacuum cleaner is having technical issues, so they require assistance troubleshooting the problems or finding a suitable replacement.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eConversation #3: \u003c/strong\u003eA customer wants gift recommendations for a family member and asks the shopping assistant bot what it would suggest based on the situation.\u003c/p\u003e\u003cp\u003eThe first step is to upload these conversations into Labelbox. To follow along with this tutorial, we’ve provided the 3 sample conversational text datasets in this \u003ca href=\"https://console.cloud.google.com/storage/browser/labelbox-developer-testing-assets/Pranoy;tab=objects?authuser=0\u0026prefix=\u0026forceOnObjectsSortingFiltering=false\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Cloud folder\u003c/u\u003e\u003c/a\u003e available for download.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dasjgw2aat\" title=\"How to generate data for model comparison and RLHF - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWhen inspecting the conversation JSON in VS code, you’ll notice it has 2 parts:\u0026nbsp;\u003c/p\u003e\u003col\u003e\u003cli\u003eThe conversation itself\u0026nbsp;\u003c/li\u003e\u003cli\u003eModel outputs - containing:\u0026nbsp;\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli\u003eTitle of the content (e.g “Response A”)\u0026nbsp;\u003c/li\u003e\u003cli\u003eThe actual content (e.g \"I have 2 options for you…”)\u003c/li\u003e\u003cli\u003eModel configuration (e.g “modelConfigName\": \"GPT-3.5 with temperature 0\") - this allows you to save the metadata around the models that are saved\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this JSON, the content is formatted in markdown since it is an important part of the experience when comparing detailed, nuanced conversations. \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e{\n      \"title\": \"Response A\",\n      \"content\": \"I have 2 options for you:\\n- The Dyson V15 [Product page](https://www.dyson.com/vacuum-cleaners/cordless/v15)\\n- The Shark Stratos [Product page](https://www.sharkclean.com/products/shark-stratos-cordless-vacuum-with-free-steam-mop-zidIZ862HB)\",\n      \"modelConfigName\": \"GPT-3.5 with temperature 0\"\n    }\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSince these data rows are stored in a cloud bucket, we can import them as a pairwise comparison dataset to Labelbox. You can learn more about how to upload a JSON to Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ethrough the UI\u003c/u\u003e\u003c/a\u003e or through a \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecloud storage integration\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-2-explore-and-curate-data-in-catalog\"\u003eStep 2: Explore and curate data in Catalog\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dm1sca15sk\" title=\"How to generate data for model comparison and RLHF - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve successfully uploaded the conversations to Quantumworks Lab, you’ll be able to view them in Catalog. Since we’ve imported them as a pairwise comparison dataset, you can click into each conversation and view the conversation and outputs. Filtering by metadata makes it easy to locate these datasets – for example, you can set up \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ea slice\u003c/u\u003e\u003c/a\u003e to capture all incoming comparison data in a single place.\u0026nbsp;\u003c/p\u003e\u003cp\u003eNot only can you click in to view the outputs, you can switch to \"markdown mode\" and see all of the information with necessary links and formatting in markdown.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-3-create-a-labeling-project-with-the-llm-human-preference-editor\"\u003eStep 3: Create a labeling project with the LLM Human Preference editor\u0026nbsp;\u003c/h3\u003e\u003cp\u003eNow that we have our conversation data in Catalog, we can create an ontology and a labeling project with the LLM Human Preference editor.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cwhf2v5o1p\" title=\"How to generate data for model comparison and RLHF - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eCreate an ontology\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can create our ontology. \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eOntologies\u003c/u\u003e\u003c/a\u003e can be reused across different projects and they are required for data labeling, model training, and evaluation.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cu\u003eTo create a new ontology:\u003c/u\u003e\u003c/p\u003e\u003cp\u003e1) Navigate to the \"Schema\" tab\u003c/p\u003e\u003cp\u003e2) Hit \"Create new ontology\"\u003c/p\u003e\u003cp\u003e3) Select the media type that you wish to work with — for this use case it would be \"Conversational text\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Give your ontology a name\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Add objects and classifications based on you use case\u003c/p\u003e\u003cp\u003eFor this use case, we’ll create two classifications:\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Choose the best response (Radio classification) with options as:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eResponse A\u0026nbsp;\u003c/li\u003e\u003cli\u003eResponse B\u0026nbsp;\u003c/li\u003e\u003cli\u003eTie\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe strongly recommend doing pairwise comparison, especially for more nuanced use cases where alignment using RLHF or robust evaluation can have a significant impact on the quality of the model.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Provide a reason for your choice (Free form text) \u003c/p\u003e\u003cp\u003eAfter this, you can save your ontology. \u003c/p\u003e\u003cp\u003eAfter creating an ontology, you can create a labeling project and begin labeling your data.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCreate a labeling project\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e1) Navigate to the Annotate tab\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Click \"Create new project\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Select the \"LLM human preference\" under LLM alignment\u003c/p\u003e\u003cp\u003e4) Name your project\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Select your quality mode between \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ebenchmark\u003c/u\u003e\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econsensus\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u003c/p\u003e\u003cp\u003e6) Save your project \u003c/p\u003e\u003cp\u003eAttach your previously created ontology to your new project to complete project setup.\u003c/p\u003e\u003ch3 id=\"step-4-import-model-predictions\"\u003eStep 4: Import model predictions\u0026nbsp;\u003c/h3\u003e\u003cp\u003eTo accelerate the labeling process, oftentimes you might want to leverage pre-labels or model predictions to automate the labeling process. This allows your team of labelers or an external team of experts to focus their valuable time on human review rather than spending time starting to label from scratch.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor this use case, we can import model predictions through Quantumworks Lab’s model-assisted labeling (MAL) to do exactly just that. With \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel-assisted labeling\u003c/u\u003e\u003c/a\u003e, you can import computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo demonstrate, we’ve uploaded a choice of Response B as a model-generated label on one of these data rows, which will show up in the editor as a default response. From there, in the editor, a human could verify whether Response B is in fact the best option and provide an explanation for it. \u003c/p\u003e\u003cp\u003eYou can follow this \u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/llm_asset_import/conversational_MAL_GT.ipynb?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e to upload MAL predictions.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-5-label-data\"\u003eStep 5: Label data\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/o279ezt54h\" title=\"How to generate data for model comparison and RLHF - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWith our data, ontology, and labeling project setup, we can begin reviewing the pre-labels and create ground truth data. You can select \"Start labeling\" and see both the conversation and the model outputs for comparison. The model outputs can be viewed both in markdown formatting and as raw text. \u003c/p\u003e\u003cp\u003eDepending on the conversation, you can select the appropriate response and provide a reason for your choice. You can continue to do this for all conversations and compare outputs to decide the best output based on human preference.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can learn more about the new LLM human preference editor in our \u003ca href=\"https://docs.labelbox.com/docs/llm-human-preference?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAs large language models continue to rapidly advance, maintaining reliable and ethical systems aligned to human values is essential. Techniques like human preference learning and reinforcement learning from human feedback are mechanisms for this.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we build increasingly capable AI systems, maintaining human oversight is key. With Quantumworks Lab, you can enable reinforcement learning from human feedback (RLHF) by leveraging your internal team or external labeling services with specialized expertise in areas like evaluation and red teaming. This human-centered approach provides the ability to create reliable, responsible AI aligned with human values and tailored to your business use cases at scale. \u003c/p\u003e","comment_id":"6567b04d72e557000141cf3c","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-3091.png","featured":false,"visibility":"public","created_at":"2023-11-29T21:42:37.000+00:00","updated_at":"2024-01-17T19:06:23.000+00:00","published_at":"2023-12-01T14:36:04.000+00:00","custom_excerpt":"Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-generate-data-for-model-comparison-and-rlhf","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-generate-data-for-model-comparison-and-rlhf/","excerpt":"Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. ","reading_time":7,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-3091-1.png","og_title":"How to generate data for model comparison and RLHF","og_description":"Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-3091-2.png","twitter_title":"How to generate data for model comparison and RLHF","twitter_description":"Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. ","meta_title":"How to generate data for model comparison and RLHF","meta_description":"Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"654bbab4016f5100016579c3","uuid":"6c33e4d1-fcaa-44de-b994-0fe65fce3dcc","title":"How to analyze customer reviews and improve customer care with NLP","slug":"how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp","html":"\u003cp\u003eCustomer reviews have become a critical tool for businesses looking to improve their products, services, and customer satisfaction. In today’s digital world, review sites like Yelp and social media make it easier than ever for customers to share their experiences with the world. Customer care can range in the services and support that businesses provide to their customers before, during, and after purchase. Great customer care can create positive brand experiences that lead to greater loyalty and customer satisfaction. In the ever-evolving world of retail, it also helps keep your business competitive and at the forefront of your customer’s sentiment and desires.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile companies now have access to a wealth of customer feedback data, sifting through all of these reviews can be incredibly time-consuming and manual. By leveraging AI, teams can analyze \u003ca href=\"https://birdeye.com/blog/review-management/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ecustomer reviews and feedback\u003c/a\u003e at scale, to gain insights into common review topics or customer sentiment. This allows businesses to identify common themes and pinpoint areas of improvement to enhance\u0026nbsp; the customer experience.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for customer care. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eImproving customer care requires a vast amount of data in the form of customer reviews. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape: \u003c/strong\u003eThe changing nature and format of customer review data from multiple sources (e.g webpages, apps, social media, etc.) poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their customer care through advanced natural language processing. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s assisted data enrichment and flexible training frameworks to quickly build AI systems that uncover actionable insights from customer reviews. Tackle unique customer care challenges with AI-driven insights to create more thoughtful and strategic customer interactions. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1444\" height=\"784\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 1444w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build an NLP model to improve customer care. Specifically, this guide will walk through how you can explore and better understand review topics and classify review sentiment to make more data-driven business decisions around customer care initiatives.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-accelerate-and-train-an-nlp-model-to-improve-customer-care\"\u003eSee it in action: How to accelerate and train an NLP model to improve customer care\u0026nbsp;\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below in either: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e or \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs customer reviews and feedback across channels proliferate, brands want to learn from customer feedback to foster positive experiences. For this use case, we’ll be working with a dataset of customer hotel reviews – with the goal of analyzing the reviews to demonstrate how a hospitality company could gain insight into how their customers feel about the quality of service they receive.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/q4dqjyg9xf\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"498\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003eFor the purpose of this tutorial, we’ve provided a sample open-source \u003ca href=\"https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe?ref=labelbox-guides.ghost.io\"\u003eKaggle dataset\u003c/a\u003e that can be downloaded.  \u003c/p\u003e\u003cp\u003ePlease \u003ca href=\"https://drive.google.com/file/d/1hh2MYzol6-4PSsqX7mbX2YSLLGA4jAYZ/view?usp=drivesdk\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edownload the dataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your text data as a CSV. \u003c/li\u003e\u003cli\u003eIf your text snippets sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your text data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xn3sj0uc8j\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExplore topics of interest\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith your data in Quantumworks Lab, you can begin to leverage Catalog to uncover interesting topics to get a sense of what customers are talking about from hotel reviews.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize your data –\u0026nbsp; you can click through individual data rows to get a sense for what customers are writing reviews on\u0026nbsp;\u003c/li\u003e\u003cli\u003eDrill into specific topics of interest\u003cstrong\u003e \u003c/strong\u003e– leverage a natural language search, for example searching “interior design,” to bring up all related reviews related to interior design. You can adjust the confidence threshold of your searches accordingly (this can be helpful in gauging the volume of data related to the topic of interest)\u0026nbsp;\u003c/li\u003e\u003cli\u003eBegin to surface subtopics or trends within your initial search – for example is the interior design review related to the style of design, attention to detail, or the type of environment created from the interior design\u003c/li\u003e\u003cli\u003eEasily find all instances of similar examples to data of interest\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate and save data slices\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf you have a search query that you’re interested in saving or reusing in the future, you can save it as \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003ea slice\u003c/a\u003e. You can construct a slice by using one or more filters to curate a collection of data rows. Users often combine filters to surface high-impact data and then save the results as a slice.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this example, we’ve surfaced reviews on the topic of breakfast that all talk about the value and price of the hotel’s breakfast. We can save this as a slice for future reference (“Breakfast_value”) and as we ingest more data that matches the slice’s criteria, they will automatically get filed into the slice.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-an-ontology\"\u003eCreate an ontology\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can create our ontology. \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eOntologies\u003c/a\u003e can be reused across different projects and they are required for data labeling, model training, and evaluation.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gdczmynqjt\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a new ontology:\u003c/p\u003e\u003cp\u003e1) Navigate to the ‘Schema’ tab\u003c/p\u003e\u003cp\u003e2) Hit ‘Create new ontology’\u003c/p\u003e\u003cp\u003e3) Select the media type that you wish to work with – for this use case ‘Text’\u003c/p\u003e\u003cp\u003e4) Give your ontology a name\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Add objects and classifications based on you use case\u003c/p\u003e\u003cp\u003e6) Objects are named entities\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003ePerson’s name\u0026nbsp;\u003c/li\u003e\u003cli\u003eLocation\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003e7) Classifications\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eReview sentiment such as positive or negative (radio)\u0026nbsp;\u003c/li\u003e\u003cli\u003eReview topics such as breakfast, dinner, location, staff, interior design (checklist)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAdd sub-classifications as desired\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003e8) Save and create your ontology\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter creating an ontology, you can begin labeling your data to fine-tune or train a model.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"label-data-of-interest\"\u003eLabel data of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003eWith Quantumworks Lab, you can label your data in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e: leverage our global network of\u0026nbsp; specialized labelers for a variety of tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWorkforce Boost provides a collaborative platform for labeling services in a self-serve manner — this is great for teams that don’t have the technical expertise to build a machine learning system yet are looking for an easy-to-use technology to get a quick turnaround on quality training data. You can learn more about our Boost offerings \u003ca href=\"https://docs.labelbox.com/docs/using-data-labeling-service?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create pre-labels with foundation models\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn addition to creating pre-labels for classification projects, you have the ability to send model predictions as pre-labels to your labeling project. This can be done in one of two ways:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel-assisted labeling\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eImport computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel Foundry\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eAutomate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePre-label data with Model Foundry\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModel Foundry\u003c/a\u003e acts as the copilot to create your training data –\u0026nbsp; instead of going into unstructured text datasets blindly, you can use pre-existing LLMs to pre-label data or pre-tag parts of it, reducing manual labeling efforts and cost.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9zspjgoau7\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1) Select data you wish to label in Catalog\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Hit \"Predict with Model Foundry\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Choose a foundation model\u003c/p\u003e\u003cul\u003e\u003cli\u003eYou can select a foundation model based on your use case to have the model take a first pass at labeling your data\u003c/li\u003e\u003cli\u003eThese pre-labels can be verified with human-in-the-loop review in Quantumworks Lab Annotate\u003c/li\u003e\u003cli\u003eFor this use case, we’ve selected the GPT-4 model\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e4) Configure the model’s settings\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect the previously created ontology in the earlier part of the tutorial\u0026nbsp;\u003c/li\u003e\u003cli\u003eLabelbox will auto-generate a prompt based on your ontology and use case – in this case we wish to classify the sentiment (positive or negative) and classify a topic with one or more options (breakfast, dinner, location, staff, room, facilities, value for money, or interior design)\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e5) Generate preview predictions\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore submitting the model run, you can generate prediction previews to understand how the model will perform\u003c/li\u003e\u003cli\u003eIt is recommended that you preview some predictions to confirm the model parameters are configured as desired\u003c/li\u003e\u003cli\u003eBased on the preview, you can then make any adjustments to the settings or choose to submit the model run as-is\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e6) Name and submit the model run\u0026nbsp;\u003c/p\u003e\u003cp\u003e7) View the model run in the Model tab to explore results\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce your model run is complete, you navigate to the Model tab\u003c/li\u003e\u003cli\u003eExplore the model’s results and click into each data row to dig deeper into the model’s predictions\u003c/li\u003e\u003cli\u003eFor this example, we can see that there are instances where GPT-4 has correctly tagged named entities and identified sentiment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve evaluated and are satisfied with GPT-4’s predictions, you can send them to a labeling project in Quantumworks Lab Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAdd a batch to a labeling project as pre-labels\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eBefore you can send these model predictions to a labeling project as pre-labels, you need to create a labeling project. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7zgl76n76w\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eCreate a new labeling project\u003c/em\u003e\u003c/p\u003e\u003cp\u003e1) Navigate to the Annotate tab\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Create a ‘New project’\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Select the project type – in this case we want to create a ‘Text’ project\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Name your project\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Attach your model’s ontology (created in a previous step)\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce you’ve created your labeling project and configured the ontology, head back to the Model tab to send your batch of data with pre-labels to that labeling project.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Highlight all data rows of interest\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Select ‘Manage selection’ \u0026gt; ‘Add batch to project’\u003c/p\u003e\u003cp\u003e3) Select the appropriate project that you created in the above step\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) You can give the batch a priority (from 1-5)\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Select the appropriate model run of the predictions you wish to send\u0026nbsp;\u003c/p\u003e\u003cp\u003e6) You can explore and select the various tags that have been applied and uncheck those that aren’t of interest\u0026nbsp;\u003c/p\u003e\u003cp\u003e7) Submit the batch\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can now navigate back to your project in Annotate and hit ‘Start labeling’.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"verify-data-quality-with-custom-workflows\"\u003eVerify data quality with custom workflows\u003c/h3\u003e\u003cp\u003eRather than starting from scratch, your internal or external team of labelers can now see predictions from the Model Foundry run. From here, you can validate or edit predictions as necessary and submit data rows to create ground truth labels.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/56bratjais\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs you begin to progress through your data rows, you’ll notice data rows that are initially marked up and reviewed by labelers in the ‘Initial review’ task (for your reviewers to verify and approve), with all submitted data rows falling into ‘Done’.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can create customizable, multi-step review and rework pipelines to drive efficiency and automation for your review tasks. Set a review task based on specific parameters that are unique to your labeling team or desired outcome.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eInitial labeling task: reserved for all data rows that have been queued for labeling\u003c/li\u003e\u003cli\u003eInitial review task: first review task for data rows with submitted labels\u003c/li\u003e\u003cli\u003eRework task: reserved for data rows that have been rejected\u003c/li\u003e\u003cli\u003eDone task: reserved for data rows that have a) moved through their qualified tasks in the workflow or b) did not qualify for any of the tasks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce all data rows have been reviewed and moved to the ‘Done’ step, you can begin the model training process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn Part 1 of this tutorial, we have looked at how we can leverage Catalog to understand the topics that exist within your dataset and construct an appropriate ontology. To accelerate our initial labeling job, we leveraged Model Foundry as part of our model-assisted labeling pipeline to use pre-labels from GPT-4 to our labeling workforce for validation. Those initial annotations can be exported via a model run and can be used to train or fine-tune a model outside of Labelbox.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"part-2-train-or-fine-tune-a-model-and-evaluate-model-performance\"\u003ePart 2: Train or fine-tune a model and evaluate model performance\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e or \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"train-a-custom-model-on-a-subset-of-data-outside-of-labelbox\"\u003eTrain a custom model on a subset of data outside of Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/07yc0p652w\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn the previous step, we leveraged Model Foundry to create pre-labels that were passed through Annotate for review with human-in-the-loop validation. Now that we have our appropriate annotation data, we can train a series of initial models on sentiment, topic classification, and named entity recognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eYou can reference \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io#scrollTo=EDmBNjYP_0u7\"\u003ethis step\u003c/a\u003e (Databricks) or \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io#scrollTo=EDmBNjYP_0u7\" rel=\"noreferrer\"\u003ethis step\u003c/a\u003e (Google Colab) in either notebook.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBring the trained model’s predictions back into a model run\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce the model has been trained, you can create an inference pipeline that leverages each model to classify different attributes for review. We can then leverage this for two things:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun inference on the model run dataset and upload it to Quantumworks Lab for evaluation\u003c/li\u003e\u003cli\u003eRun inference on our remaining dataset and use the predictions for model-assisted labeling, to be refined in the platform and used to accelerate labeling efforts\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ePlease follow \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io#scrollTo=B9_dYR_D_0vE\"\u003ethis step\u003c/a\u003e (Databricks) or \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io#scrollTo=B9_dYR_D_0vE\" rel=\"noreferrer\"\u003ethis step\u003c/a\u003e (Google Colab) to create an inference pipeline and to upload predictions to the model run and evaluate it against ground truth.\u003c/p\u003e\u003cp\u003eAfter following the notebook, you’ll be able to compare ground truth (green) versus the model’s predictions (red) for sentiment and topic.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness\"\u003eEvaluate and diagnose model effectiveness\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dqzdzj1seb\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 8 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eDiagnose model performance with model metrics\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn addition to visualizing the difference between model predictions and ground truth, you can click into the ‘Metrics’ view to get a better sense of how your model is performing.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the \"Metrics view\" to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor example, we can click into false negatives or false positives to narrow down situations where there might be false positives – where ‘negative’ sentiment is predicted whereas ground truth sentiment is ‘positive’.\u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select \"Find similar in Catalog\" from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled reviews that you can send to your model for labeling, you can filter on the \"Annotation is\" filter and select \"none.\" This will only show unlabeled text reviews that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all reviews that apply and select \"Add batch to project\"\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eUse model predictions as model-assisted labeling pipeline\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/r4p2h6iklg\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 9 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve filtered for and have selected reviews that you wish to label you can \"Add batch to project\" to send them to your labeling project in Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Name your batch\u003c/p\u003e\u003cp\u003e2) Select your labeling project from the dropdown\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Include model predictions (from your model run) – this will perform better than the initial GPT-4 run with Model Foundry since it has been trained on your custom data\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Select or uncheck any predictions as desired\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Submit the batch\u003c/p\u003e\u003cp\u003eWhen you return to Quantumworks Lab Annotate, you will now see the original batch that we added at the start of the project, as well as the newly added batch ready for labeling.\u0026nbsp;\u003c/p\u003e\u003cp\u003eRather than starting from scratch, similar to the predictions created by GPT-4 in Model Foundry, your labelers will now see the custom model predictions and validate them with human-in-the-loop review in the same manner. This workflow helps accelerate model iterations, allowing your team to bring in the latest model prediction as pre-labels for your project to reduce the amount of human labeling effort required to create ground truth labels.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model and can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003eCustomer reviews and feedback data represent an invaluable yet untapped opportunity for businesses. Manually analyzing this growing mountain of data is no longer practical. Instead, forward-thinking companies are turning to AI to efficiently sift through and extract actionable insights from reviews.\u003c/p\u003e\u003cp\u003eNatural language processing can help identify customer sentiment, pain points, and unmet needs. By leveraging AI to tap into this feedback treasure trove, businesses can drive measurable improvements in customer satisfaction, retention, and advocacy. They can refine products, enhance user experiences, and preemptively address concerns.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"654bbab4016f5100016579c3","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-2457--1-.png","featured":false,"visibility":"public","created_at":"2023-11-08T16:43:32.000+00:00","updated_at":"2024-06-25T16:28:21.000+00:00","published_at":"2023-11-08T21:47:13.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/","excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","reading_time":12,"access":true,"comments":false,"og_image":null,"og_title":"How to analyze customer reviews and improve customer care with NLP","og_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-2457--1--1.png","twitter_title":"How to analyze customer reviews and improve customer care with NLP","twitter_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","meta_title":"How to analyze customer reviews and improve customer care with NLP","meta_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"654407cdd96ee80001d8c876","uuid":"e29733c1-b966-4d1e-858d-f96deab4850e","title":"How to build a content moderation model to detect disinformation","slug":"how-to-build-a-content-moderation-model-to-detect-disinformation","html":"\u003cp\u003eAs user-generated content increases and the amount of data grows, trust and safety on digital platforms is becoming increasingly critical. Content that goes unmoderated can not only directly hurt brand reputation, but it can directly impact a businesses bottom line through lost users, advertisers, and revenue. Regulators worldwide are also implementing \u003ca href=\"https://insightplus.bakermckenzie.com/bm/data-technology/united-states-now-is-the-time-to-evaluate-your-online-content-moderation-program?ref=labelbox-guides.ghost.io\"\u003estricter rules\u003c/a\u003e around content moderation, online safety, misinformation, and disinformation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo address these growing risks, more businesses are looking to AI and machine learning as part of robust trust and safety strategies. State-of-the-art AI solutions enable unprecedented scale, nuance, consistency, and efficiency in identifying and taking action on high-risk user content.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for trust and safety. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDynamic content landscape: \u003c/strong\u003eModels are only as good as the data they are trained on. As new trends or content emerges, AI models need constant retraining on compelling diverse, unbiased, and large labeled datasets to reinforce content moderation.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEthical risks \u0026amp; biases: \u003c/strong\u003eWithout careful design, machine learning models risk exacerbating biases and are prone to \u003ca href=\"https://labelbox.com/blog/what-does-it-mean-when-an-llm-hallucinates/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ehallucination\u003c/a\u003e. Teams need a way to monitor and evaluate model training with ethical oversight.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that enables businesses to build state-of-the-art AI solutions for enhanced controls, transparency, efficiency in content moderation, and greater brand safety. Rather than spending valuable time building an in-house solution or relying on disparate systems, businesses can explore data, use foundation models for assisted-enrichment, and evaluate models to quickly build more accurate AI systems for analyzing user behavior, detecting disinformation, and enhancing ad-targeting. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1430\" height=\"786\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png 1430w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build a model for content moderation, such as detecting and classifying disinformation, allowing you to elevate brand trust and improve the trust and safety of your applications.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-build-a-content-moderation-model-to-detect-disinformation\"\u003eSee it in action: How to build a content moderation model to detect disinformation\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data: \u003ca href=\"https://colab.research.google.com/drive/1bTIKkQUHiccIy1adgKqQ_CVCm2KAxiHP?ref=labelbox-guides.ghost.io\"\u003eGoogle Colab Notebook\u0026nbsp;\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2: \u003c/strong\u003eCreate a model run, fine-tune an LLM, and evaluate model performance: \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1bTIKkQUHiccIy1adgKqQ_CVCm2KAxiHP?ref=labelbox-guides.ghost.io\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eWith the growing amount of user-generated content, businesses want to ensure that there is no inappropriate content or disinformation happening on their platform. To implement content moderation at scale, teams can leverage AI to analyze and detect harmful content and classify disinformation from existing data stored in a cloud bucket or a local folder.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9e76g8llbu\" title=\"How to enhance brand safety and content moderation with AI - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo upload a sample of your content to Quantumworks Lab for labeling, you have a few options:\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUpload a dataset through the SDK\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eUsing the \u003ca href=\"https://colab.research.google.com/drive/1bTIKkQUHiccIy1adgKqQ_CVCm2KAxiHP?ref=labelbox-guides.ghost.io\"\u003eGoogle Colab notebook\u003c/a\u003e, upload the sample dataset into Quantumworks Lab or use it to import data from various sources like Bigquery, Databricks, or Snowflake.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this notebook, we’re going to bring in two libraries of interest:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/sdk-fundamental-concepts-1?ref=labelbox-guides.ghost.io\"\u003eLabelbox SDK\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/Quantumworks Lab/labelpandas?ref=labelbox-guides.ghost.io\"\u003eLabelpandas\u003c/a\u003e (for bringing tabular data into Quantumworks Lab)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou’ll need your Quantumworks Lab API key to initiate the Quantumworks Lab Client and create a dataset. For this guide, we’ll be using a dataset stored in a Google Cloud bucket as a CSV and we can use Labelpandas to bring this data in.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe provided sample dataset includes:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAn article with a corresponding headline\u003c/li\u003e\u003cli\u003eWhen it was retrieved\u003c/li\u003e\u003cli\u003eMetadata (sorted by source)\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003ePre-labels based on if the article contains “disinformation” or not\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eUpload a dataset through the UI \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf you have a dataset from your local file, you can upload it through the Quantumworks Lab UI by clicking \"new dataset\" in Catalog.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce you’ve successfully uploaded your text, you can browse the dataset in Catalog — along its metadata. You can visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/vl9jr5463n\" title=\"How to enhance brand safety and content moderation with AI - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003eSearch across datasets\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003efind similar data\u003c/a\u003e in seconds with off-the-shelf embeddings\u0026nbsp;\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003enatural language\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003elayer structured and unstructured filters\u003c/a\u003e for more granular data curation\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate and save data slices\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ksmc9w7acz\" title=\"How to enhance brand safety and content moderation with AI - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIf you have a search query that you’re interested in saving or reusing in the future, you can save it as \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003ea slice\u003c/a\u003e. You can construct a slice by using one or more filters to curate a collection of data rows. Users often combine filters to surface high-impact data and then save the results as a slice.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this example, we are interested in saving the surfaced data rows as “Climate Articles” so that this filtered dataset can easily be surfaced later on for annotation or data discovery purposes.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-a-labeling-project-in-annotate\"\u003eCreate a labeling project in Annotate\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uxspsczkpn\" title=\"How to enhance brand safety and content moderation with AI - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1) Create a text project in \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Sample and send your uploaded dataset as a \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003ebatch\u003c/a\u003e to your newly created project. In this case we can send the two dataset slices that we created: “Climate related articles” and “Non-climate related articles”\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eontology\u003c/a\u003e to determine how to structure your data. If you have a previous ontology you’d like to use, you can do so. If not, you’ll need to create a new ontology. For this use case, our ontology consists of two classifications:\u003c/p\u003e\u003cul\u003e\u003cli\u003e“Does the article contain disinformation?” with two options\u003c/li\u003e\u003cli\u003e“Is the article climate related?” with two options\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e4) If you’re relying on an external team of labelers or want to provide your internal labeling team with more instructions, you can upload instructions as a PDF for your labelers during the ontology creation process.\u003c/p\u003e\u003ch3 id=\"label-the-data-of-interest\"\u003eLabel the data of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003eNow that we have a project with our data set up in Annotate, we’ll need to label this training data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eSince this project is a classification use case, we can also leverage \u003ca href=\"https://docs.labelbox.com/docs/bulk-classification?ref=labelbox-guides.ghost.io\"\u003ebulk classification\u003c/a\u003e to speed up our labeling process and maximize labeling efficiency. Teams who have used bulk classification in Quantumworks Lab have seen labeling time decrease from a full quarter to a few days. Since we’ve leveraged filters in Catalog to identify “Climate related articles,” we can send these articles to our newly created labeling project with pre-labels.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/eh5mwisy6q\" title=\"How to enhance brand safety and content moderation with AI - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo bulk classify and pre-label data rows, you can:\u003c/p\u003e\u003cp\u003e1) Highlight any data rows of interest, in our use case these would be data rows in the slice \"Climate related articles\",\u0026nbsp; and select \"Manage selection\" \u0026gt; \"Add classifications\"\u003c/p\u003e\u003cp\u003e2) Select the labeling project that you made in the previous step and determine a step of the project’s review workflow that you would like to send the classifications to. In the above demo, we are sending these to the \"Initial labeling task\" because we want to have a labeler verify that these are indeed all climate related articles\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Select the desired classification — in this case it would be \"Climate related\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) You can determine the batch’s data row priority (from 1-5) and submit the bulk classification job \u003c/p\u003e\u003cp\u003eRather than labeling from scratch, a team of labelers can now simply verify or correct the pre-labels used during this bulk classification step.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can label your data in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e: leverage our global network of\u0026nbsp; specialized labelers for a variety of tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWorkforce Boost provides a collaborative platform for labeling services in a self-serve manner — this is great for teams that don’t have the technical expertise to build a machine learning system yet are looking for an easy-to-use technology to get a quick turnaround on quality training data. You can learn more about our Boost offerings \u003ca href=\"https://docs.labelbox.com/docs/using-data-labeling-service?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create pre-labels with foundation models\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn addition to creating pre-labels for classification projects, you have the ability to send model predictions as pre-labels to your labeling project. This can be done in one of two ways:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel-assisted labeling\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eImport computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel Foundry\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eAutomate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"verify-data-quality-with-custom-workflows\"\u003eVerify data quality with custom workflows\u003c/h3\u003e\u003cp\u003eContent moderation relies heavily on training the model on accurate and verified data. To ensure that you’re producing the most reliable and high-quality training datasets, you can customize your \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003elabeling review workflow\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/lqa22gzj7o\" title=\"How to enhance brand safety and content moderation with AI - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou can create customizable, multi-step review and rework pipelines to drive efficiency and automation for your review tasks. Set a review task based on specific parameters that are unique to your labeling team or desired outcome.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eInitial labeling task: reserved for all data rows that have been queued for labeling\u003c/li\u003e\u003cli\u003eInitial review task: first review task for data rows with submitted labels\u003c/li\u003e\u003cli\u003eRework task: reserved for data rows that have been rejected\u003c/li\u003e\u003cli\u003eDone task: reserved for data rows that have a) moved through their qualified tasks in the workflow or b) did not qualify for any of the tasks\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-2-create-a-model-run-fine-tune-an-llm-and-evaluate-model-performance\"\u003ePart 2: Create a model run, fine-tune an LLM, and evaluate model performance\u0026nbsp;\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this part of the tutorial, we’ll be taking the ground truth labels created in Part 1 to fine-tune a large language model (LLM). From there, we’ll evaluate model performance in Quantumworks Lab Model to diagnose model strengths and weaknesses and look to continuously boost and improve model performance.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-a-new-model\"\u003eCreate a new model\u0026nbsp;\u003c/h3\u003e\u003cp\u003eOnce you have your labeled data in your project in Annotate, you’re ready to move on to creating a model run in \u003ca href=\"https://app.labelbox.com/mea?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003eLabelbox Model\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z31zew9hbd\" title=\"How to enhance brand safety and content moderation with AI - Part 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a new model, you’ll need to:\u003c/p\u003e\u003cp\u003e1) Navigate to the \"Experiments\" tab in Model. The \"Experiments\" tab will be where you can find all model experiments across iterations.\u003c/p\u003e\u003cp\u003e2) Create a new model by selecting the \"New model\" button.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eProvide a model name\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect the model ontology — in this case we will select the same ontology we used to create our labeling project that contains the corresponding ground truth data.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSubmit and create a model — before creating a model run, you will also be able to see and verify the number of data rows that are being submitted.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate a model run\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve created a new model, we will need to create a new \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003emodel run\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eA model run is a model training experiment — each model run provides a versioned data snapshot of the data rows, annotations, and \u003ca href=\"https://docs.labelbox.com/docs/curate-data-splits?ref=labelbox-guides.ghost.io\"\u003edata splits\u003c/a\u003e for that model run. You can upload predictions to the model run and compare its performance against other model runs in a model directory.\u003c/p\u003e\u003cp\u003eThe model run we create will be the initial model run for our LLM fine-tuning experiment. To add a new model run:\u003c/p\u003e\u003cp\u003e1) Select \"New model run\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Give the model run a name (e.g “model run #1”)\u003c/p\u003e\u003cp\u003e3) Set data splits for the model run (for train, validate, and test)\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Create the model run\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter creating a model run, you’ll be able to see the corresponding data rows with ground truth populated into the appropriate train, validate, and test splits. This model run will be the gateway for us to export ground truth data to fine-tune a large language model.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/bObh0Kje6BWPfrIPWcd3V0TFt09svuX0-7Wka4IQI9j-bKdmhJAEjTWsWPWOmdFUg-CgU9fLQC-p_vFdafFXv4nYhMZupffw7Bl6TN8Z-2j771nF4riavmSL-xiDAmjU8E32deblRc4eEmNjptF3GpI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"327\"\u003e\u003c/figure\u003e\u003ch3 id=\"export-ground-truth-from-the-model-run-experiment-for-fine-tuning\"\u003eExport ground truth from the model run experiment for fine-tuning\u0026nbsp;\u003c/h3\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/s2gzlnehyy\" title=\"How to enhance brand safety and content moderation with AI - Part 8 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWe’ll be using this \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e to fine-tune a model and bring back inferences from the fine-tuned model for evaluation and diagnosis.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor this step, you will need:\u003c/p\u003e\u003cul\u003e\u003cli\u003eYour API Key\u0026nbsp;\u003c/li\u003e\u003cli\u003eYour Model Run ID to export the corresponding ground truth and articles from the model run\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExport ground truth from the model run experiment\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLabelbox will return the ground truth export in a JSON format. With the provided \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e, we can visualize the exported JSON into a DataFrame format for us to view corresponding ground truth for each article.\u0026nbsp;\u003c/p\u003e\u003cp\u003eGiven that we want to fine-tune a Google Vertex model with this data, we’ll need to convert the ground truth export to a GCP vertex tuning format (JSONL):\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# build LLM prompt and convert to GCP vertex tuning format (jsonl)\n\nprompt = 'Given the following headline and content, determine whether the article is related to climate change or similar topics. Also determine whether the article contains inaccurate or disinformation. Answer in the following format with Yes/No Answers: [climate related? / disinformation?]'\ndf['input_text'] = prompt + df['content']\ndf['output_text'] = 'climate related: ' + df['climate_related'] + ' disinformation: ' + df['disinformation_flag']\n\n\nwith open('modelPrompt_GCP.jsonl', 'w') as file:\nfor _, row in df[['input_text', 'output_text']].iterrows():\njson_line = row.to_json()\nfile.write(json_line + '\\n')\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"fine-tune-an-llm-with-google-vertex-ai\"\u003eFine-tune an LLM with Google Vertex AI \u003c/h3\u003e\u003cp\u003eFine-tuning is a technique whereby we take an off-the-shelf open-source or proprietary model and retrain it on a variety of concrete examples, and save the updated weights as a new model checkpoint. You can learn more about other techniques to leverage LLMs \u003ca href=\"https://labelbox.com/guides/zero-shot-learning-few-shot-learning-fine-tuning/?ref=labelbox-guides.ghost.io#zero-shot-learning-few-shot-learning-and-fine-tuning-in-action\"\u003ein this guide\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/um0f8w1rzn\" title=\"How to enhance brand safety and content moderation with AI - Part 9 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eFor this use case, we’ll be using \u003ca href=\"https://cloud.google.com/vertex-ai?ref=labelbox-guides.ghost.io\"\u003eGoogle Vertex AI \u003c/a\u003eto fine-tune an LLM with the ground truth from Part 1 of this tutorial. Once in the Vertex AI console, we’ll want to create a tuned model:\u003c/p\u003e\u003cul\u003e\u003cli\u003eChoose a supervised learning task\u0026nbsp;\u003c/li\u003e\u003cli\u003eEnter additional model parameters (e.g model name)\u0026nbsp;\u003c/li\u003e\u003cli\u003eUpload the JSONL file from the previous step\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, we can start the model tuning process. Once the model fine-tuning job has been completed, we can head over to the Google Vertex sandbox and give the newly tuned model a prompt.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, we can ask if the article is climate related and if it contains disinformation and it will provide a response based on the training dataset we provided.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-inferences-with-the-tuned-model-and-evaluate-model-effectiveness-in-labelbox\"\u003eCreate inferences with the tuned model and evaluate model effectiveness in Quantumworks Lab\u003c/h3\u003e\u003cp\u003eNow that we’ve fine-tuned a model, we can use it to make predictions on the initial dataset and compare it with our ground truth data to assess the fine-tuned model’s performance.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cjbrcpktcr\" title=\"How to enhance brand safety and content moderation with AI - Part 10 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eCreate inferences with the tuned model\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWe’ll need to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eInstall Google Vertex and Google Cloud SDK\u0026nbsp;\u003c/li\u003e\u003cli\u003eProvide the endpoint ID for the tuned model\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe can then start creating model inferences and predictions from the tuned model on our news articles. Use Pandas to clean up the responses, to remove corresponding prompts, and save them as a DataFrame — this will return the model’s initial headline and the client’s response if the data row is climate related or contains disinformation. \u003c/p\u003e\u003cp\u003eOnce we have model inferences, we can send the inferences back to a model run in Quantumworks Lab for further evaluation and analysis.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eEvaluate and diagnose model effectiveness\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo evaluate the effectiveness of the fine-tuned model in Quantumworks Lab, we’ll need to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSpecify the model run ID\u0026nbsp;\u003c/li\u003e\u003cli\u003eUpload the list of model inferences for each specific data row\u0026nbsp;\u003c/li\u003e\u003cli\u003eAttach each list of data rows and submit it to a model run in Quantumworks Lab as an upload job via the SDK\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce that’s complete, you can hop back to the original Quantumworks Lab model run and view the corresponding ground truth data and model inferences on each data row. You can visually compare the effectiveness of the fine-tuned model predictions (in red) with ground truth (in green).\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the \"Metrics view\" to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor this use case, our goal is to minimize the spread of disinformation, so we can take a look at the metric that shows corresponding articles that are considered \"disinformation\" by labelers, but where the model incorrectly predicted articles as \"not disinformation\". \u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003ch3 id=\"curate-high-impact-data-to-drastically-improve-model-performance\"\u003eCurate high-impact data to drastically improve model performance\u003c/h3\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7podbew17q\" title=\"How to enhance brand safety and content moderation with AI - Part 11 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select \"Find similar in Catalog\" from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled articles that you can send to your model for labeling, you can filter on the \"Annotation is\" filter and select \"none\". This will only show unlabeled text articles that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all articles that apply and send them as a batch to your original labeling project. Labeling these in priority will help improve model performance. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model and can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eUnmoderated content poses mounting risks to businesses with the risk of spreading misinformation, disinformation, and an unsafe online environment. With responsible implementation, businesses can leverage AI for trust and safety to efficiently and consistently identify high-risk content at scale. This not only helps create an online environment that is safe for users, but also helps protect brand reputation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"654407cdd96ee80001d8c876","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Frame-2299--2-.png","featured":false,"visibility":"public","created_at":"2023-11-02T20:34:21.000+00:00","updated_at":"2024-07-17T20:55:32.000+00:00","published_at":"2023-11-02T21:18:18.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-build-a-content-moderation-model-to-detect-disinformation","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},"url":"https://labelbox-guides.ghost.io/how-to-build-a-content-moderation-model-to-detect-disinformation/","excerpt":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","reading_time":12,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Frame-2299--2--2.png","og_title":"How to build a content moderation model to detect disinformation","og_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Frame-2299--2--1.png","twitter_title":"How to build a content moderation model to detect disinformation","twitter_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","meta_title":"How to build a content moderation model to detect disinformation","meta_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6537fb91375d13000123d799","uuid":"6609e3d3-fc0e-4183-91ef-a83c801c7c6d","title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine-Tuning: A technical walkthrough using OpenAI's APIs \u0026 models","slug":"zero-shot-learning-few-shot-learning-fine-tuning","html":"\u003cp\u003eWith large language models (LLMs) gaining popularity, new techniques have emerged for applying them to NLP tasks. Three techniques in particular — zero-shot learning, few-shot learning, and fine-tuning — take different approaches to leveraging LLMs. In this guide, we’ll walk through the key difference between these techniques and how to implement them.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll walk through a case study of extracting airline names from tweets to compare the techniques. Using an entity extraction dataset, we’ll benchmark performance starting with zero-shot prompts, then experiment with few-shot learning, and finally fine-tune a model. By analyzing the results, we can better understand when to use zero-shot, few-shot, or fine-tuning with LLMs. You’ll also pick up tips for constructing effective prompts and setting up LLM experiments.\u003c/p\u003e\u003cp\u003eThe goal of this guide is to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompare the quantitative results among zero-shot learning, few-shot learning, and fine-tuning on an NER use case\u003c/li\u003e\u003cli\u003eExplore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026amp; Quantumworks Lab Model\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"zero-shot-learning-few-shot-learning-and-fine-tuning-in-action\"\u003eZero-shot learning, few-shot learning, and fine-tuning in action\u003c/h2\u003e\u003cp\u003eFor the purposes of this case study, we will be walking through an example of entity extraction featured in \u003ca href=\"https://colab.research.google.com/drive/1OCD8ivCtPS84cEhtjXkIWNAQX9oyKfFe?usp=sharing\u0026ref=labelbox-guides.ghost.io\"\u003ethis Google Colab Notebook\u003c/a\u003e. Specifically, we have a dataset of tweets about major airlines, and the task is to use an API to extract all airlines names that appear in the tweets.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe full dataset can be found on Kaggle \u003ca href=\"https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eHere is an example data row:\u003c/p\u003e\u003cp\u003e\"@AmericanAir do you have the phone number of a supervisor i can speak to regarding my travels today,['American Airlines’]\u003c/p\u003e\u003cp\u003ewhere the:\u003c/p\u003e\u003cp\u003eTWEET: @AmericanAir do you have the phone number of a supervisor i can speak to regarding my travels today\u003c/p\u003e\u003cp\u003eLABEL: [‘American Airlines']\"\u003c/p\u003e\u003cp\u003eBefore delving into the details, a few key technical definitions to keep in mind:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eZero-shot learning\u003c/strong\u003e — a technique whereby we prompt an LLM without any examples, attempting to take advantage of the reasoning patterns it has gleaned (i.e. a generalist LLM)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFew-shot learning\u003c/strong\u003e — a technique whereby we prompt an LLM with several concrete examples of task performance\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-tuning\u003c/strong\u003e — a technique whereby we take an off-the-shelf open-source or proprietary model, re-train it on a variety of concrete examples, and save the updated weights as a new model checkpoint\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"establishing-a-benchmark-baseline-with-zero-shot-learning\"\u003eEstablishing a benchmark baseline with zero-shot learning\u003c/h3\u003e\u003cp\u003eAs with any scientific or machine learning experiment, it is important to establish a benchmark baseline. For this case study, we will use zero-shot learning as the baseline by experimenting with various zero-shot prompts and evaluating the performance (precision, recall, f1-score, accuracy) of these prompts against the test set.\u003c/p\u003e\u003cp\u003eThe demo video below shows how we can leverage the \u003cstrong\u003e‘Humans Generate Prompt’ \u003c/strong\u003eoption of the \u003ca href=\"https://www.google.com/url?q=https://docs.labelbox.com/docs/llm-data-generation\u0026sa=D\u0026source=docs\u0026ust=1732675975664445\u0026usg=AOvVaw1fM_zT8IUOw2SSFmqUnUcd\" rel=\"noreferrer\"\u003ePrompt and Response Editor\u003c/a\u003e within Quantumworks Lab Annotate to create various zero-shot prompts using in-house and/or external teams to scale out our annotation operations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/44tuzazslq\" title=\"[Guide] Create Annotate Project \u0026amp; Airline Prompts in LLM Editor Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"534\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce we have constructed our dataset of zero-shot prompts within Quantumworks Lab using a prompt engineering workforce, we can export them from the Quantumworks Lab UI  the Quantumworks Lab Python SDK and use them in our script, as shown in the video below.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kryaxyhdx3\" title=\"[Guide] Exporting Prompts from LB Annotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eUsing gpt-3.5-turbo, we see the results of the prompts, along with their performance metrics.\u003c/p\u003e\u003cp\u003eFrom these results, we observe that prompts that fared best provided a clear and structured instruction to the model. These prompts explicitly mention the expected format for identifying airline names. Furthermore, these prompt structures introduce a pattern that the model can recognize and follow. In the case of \"Detect airline references...\" the model is prompted to look for references in a specific format (e.g. hashtags), which may be a common pattern in tweets mentioning airlines.\u003c/p\u003e\u003cp\u003ePrompts that fared worst had a theme: ambiguity in the prompt response format. Examples like \"What are the airlines in this tweet?\" and \"Find all airline mentions in the tweet\" are open- ended and do not provide a specific structure, making it harder for the model to interpret the task.\u003c/p\u003e\u003cp\u003eWhat’s interesting to note is that the prompt “Identify airlines like this - [#AIRLINE_NAME_1]:'{tweet}’” did NOT perform well even though it provided an example response format. This underscores the profound impact that punctuation and grammatical structure can have for prompt engineering. Instead of interpreting “[#AIRLINE_NAME_1]\" as the LLM-output format, the LLM instead interpreted it as a pattern-matching task to identify all airlines within a tweet that contain this specific format [#AIRLINE_NAME_1], of which there are none (hence, 0% across the evaluation metrics).\u003c/p\u003e\u003cp\u003eIn addition to testing different prompts, we can run various experiments to evaluate the efficacy of the task and evaluate the impact of those experiments in Quantumworks Lab Model. One such experiment could be to compare different models (GPT-4 vs. GPT-3.5, etc.). The video below shows how we can compare GPT-4 vs. GPT-3.5 on how each model performs on extracting airlines from the prompts we created above.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/13yquhzp4o\" title=\"[Guide] Airline extraction model comparison Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"534\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eZero-shot learning netted us the following baseline benchmark on the test set: 19%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"few-shot-learning\"\u003eFew-shot learning\u0026nbsp;\u003c/h3\u003e\u003cp\u003eTo build upon this benchmark, we used one of the prompts that performed well in zero-shot learning in tandem with few-shot learning — a technique whereby along with the prompt, we also feed an LLM concrete examples of task performance. These concrete examples are chosen from our training dataset (found in \u003cstrong\u003eairline_train.csv\u003c/strong\u003e).\u003c/p\u003e\u003cp\u003eThis is an example few-shot prompt that we passed to the LLM:\u003c/p\u003e\u003cp\u003eGiven the following tweets and their corresponding airlines, separated by new lines:\u003c/p\u003e\u003cp\u003e1) SouthwestAir bags fly free..just not to where you're going.,['Southwest Airlines']\u003c/p\u003e\u003cp\u003e2) Jet Blue I don't know- no one would tell me where they were coming from,['JetBlue Airways']\u003c/p\u003e\u003cp\u003ePlease extract the airline(s) from the following tweet:\u003c/p\u003e\u003cp\u003e\"SouthwestAir Just got companion pass and trying to add companion flg. Help!\"\u003c/p\u003e\u003cp\u003eUsing the following format - ['#AIRLINE_NAME_1] for one airline or ['#AIRLINE_NAME_1, #AIRLINE_NAME_2...] for multiple airlines.\u003c/p\u003e\u003cp\u003eEvaluating our model (gpt-3.5-turbo) on the test set via few-shot learning, we achieved an accuracy of 96.66%! There were 7 total misclassifications. Further inspection into these misclassifications actually reveals that there may be issues within the ground-truth dataset.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFew-shot learning netted us the following accuracy benchmark on the test set: 97%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"fine-tuning-with-a-training-dataset\"\u003eFine-tuning with a training dataset\u003c/h3\u003e\u003cp\u003eLastly, we seek to determine whether fine-tuning would improve our results. To ensure parity across our experiments, we used the same 100 randomly generated examples from the training dataset for few-shot learning as our control variable for the fine-tuning task.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFine-tuning netted us the following accuracy benchmark on the test set: 97%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"comparing-results\"\u003e\u003cstrong\u003eComparing results\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThese were the final results:\u003c/p\u003e\u003cp\u003e1) Zero-shot learning netted us the following accuracy baseline benchmark on the test set: 19%.\u003c/p\u003e\u003cp\u003e2) Few-shot learning netted us the following accuracy benchmark on the test set: 97%.\u003c/p\u003e\u003cp\u003e3) Fine tuning netted us the following accuracy benchmark on the test set: 91%.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKey takeaways\u003c/strong\u003e\u003c/p\u003e\u003cp\u003ePrompt engineering in tandem with few-shot learning versus fine-tuning both yield similar results for the task of extracting airline references from tweets. The ultimate consideration between the two boils down to achieving economies of scale. If teams find themselves needing to execute a prompt 100,000 times, the effort invested in terms of both human hours and GPU usage (or, token costs, if utilizing an API) can be justified when fine-tuning, as the cumulative savings in prompt tokens and the potential for improved output quality can accumulate significantly. Conversely, if we’re only using the prompt ten times and it's already effective, there's no rationale for fine-tuning it.\u003c/p\u003e\u003cp\u003eRegardless of which option you choose, we’ve seen how \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Annotate\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e can help achieve both outcomes.\u003c/p\u003e\u003cp\u003eWe can also improve results by ensuring variability within the few-shot learning examples. Currently, we are using a naive approach by selecting 100 randomly generated examples from the training dataset, so that it fits within the context window of gpt-3.5-turbo. Despite leveraging ~10% of our training data (100 rows / 900 possible training data rows), it’s less about data quantity and more about data quality. If we can curate a few-shot learning dataset of only 100 rows that has enough variance to be representative of the larger 900 rows, then that would be most ideal, from a token-sizing and, subsequently, cost perspective in addition to an engineering perspective (achieving more with less).\u003c/p\u003e\u003cp\u003eExamples of variance include tweet structure as well as a healthy mix of tweets with multiple airlines referenced. For tweet structure, we can use regular expressions to create patterns to capture mentions (@username), hashtags (#hashtag), airline stock ticker symbols, or emojis. In our use case, hashtags and ticker symbols would be super beneficial, since we see them scattered throughout our training dataset (e.g. #LUVisbetter, #jetblue, #UnitedAirlines, AA, etc.).\u003c/p\u003e\u003cp\u003eUsing \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e, we can version our few-shot learning and fine-tuning experiments by tying each experiment to a corresponding model run. Comparing evaluation metrics across different model runs allows us to either:\u003c/p\u003e\u003cul\u003e\u003cli\u003eChoose the few-shot learning prompt with the best precision, accuracy, and/or recall metric\u003c/li\u003e\u003cli\u003eChoose the fine-tuned model with the best precision, accuracy, and/or recall metric\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"get-started-today\"\u003eGet started today \u003c/h2\u003e\u003cp\u003eThrough our case study extracting airline names from tweets, we've explored the key differences between zero-shot learning, few-shot learning, and fine-tuning for applying large language models to NLP tasks.\u003c/p\u003e\u003cp\u003eThe best technique depends on your use case and available resources. The key is understanding how different data inputs affect an LLM's behavior. With the right approach, you can take advantage of LLMs' reasoning skills for a wide range of NLP applications. You can leverage each of these learning techniques with Quantumworks Lab’s LLM data generation editor and Quantumworks Lab Model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox is a data factory that empowers teams to generate high-quality data and accelerate the development of differentiated genAI solutions. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026gclid=Cj0KCQiA6Ou5BhCrARIsAPoTxrBVxRVcyUPeK8y7Mn2rtcM2M257gsxyxejwhFES91vcP7xYdoij4OQaAjUwEALw_wcB\u0026attr=arcade\u0026landingPageAnonymousId=%22ce322bab-56ea-43b8-a113-000851a3ebaf%22\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"6537fb91375d13000123d799","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083.png","featured":false,"visibility":"public","created_at":"2023-10-24T17:14:57.000+00:00","updated_at":"2024-11-27T02:11:35.000+00:00","published_at":"2023-10-24T20:34:32.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/zero-shot-learning-few-shot-learning-fine-tuning","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},"url":"https://labelbox-guides.ghost.io/zero-shot-learning-few-shot-learning-fine-tuning/","excerpt":"With large language models (LLMs) gaining popularity, new techniques have emerged for applying them to NLP tasks. Three techniques in particular — zero-shot learning, few-shot learning, and fine-tuning — take different approaches to leveraging LLMs. In this guide, we’ll walk through the key difference between these techniques and how to implement them. \n\nWe’ll walk through a case study of extracting airline names from tweets to compare the techniques. Using an entity extraction dataset, we’ll be","reading_time":6,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083-2.png","og_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","og_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083-1.png","twitter_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","twitter_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","meta_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","meta_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"652ddccebe381f00016e8e79","uuid":"6ce142af-2859-45cb-9b97-010b3461c209","title":"How to boost retail profitability with AI-powered shelf object detection","slug":"how-to-boost-retail-profitability-with-ai-powered-shelf-object-detection","html":"\u003cp\u003eFor today’s retailers facing intense competition and thin margins, optimizing shelves is a high-impact yet often overlooked opportunity to directly boost sales, enhance in-store experience, and enable better data-driven decisions across the organization. However, manually monitoring and checking shelves can be an inefficient process that limits productivity and prevents real-time insights across stores nationwide.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBy applying AI and computer vision for automated shelf monitoring, retailers can unlock the benefits of real-time shelf optimization at scale. With data-driven insights into inventory levels, pricing, product placement, and more, they can make better decisions around merchandising, improve customer satisfaction, optimize supply chain operations, and ultimately drive higher productivity and profitability. With real-time insights and efficiency gains, retailers can look to improve sales by millions of dollars per year leveraging AI.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, building and scaling an effective AI-powered shelf detection solution can pose the following key challenges for retailers:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eBuilding accurate computer vision models for a shelf optimization use case requires vast amounts of high-quality labeled images – being able to capture shelves with a diverse product range, from various distances and angles. Collecting, organizing, and labeling this data can be expensive and time-consuming for retailers.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeploying models across hundreds or thousands of retail locations can require significant investment. As shelf inventory rapidly changes, keeping models accurately trained and scaled can incur costs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLack of machine learning expertise: \u003c/strong\u003eMany brick and mortar retailers might not have access to engineering or machine learning talent. Without the right expertise, it can be difficult to develop automated shelf-monitoring solutions needed to drive value for shelf management.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that enables retailers to rapidly develop accurate computer vision models for automated shelf monitoring allowing for real-time monitoring across stores. Rather than spending valuable time building an in-house solution or relying on disparate systems, retailers can leverage Quantumworks Lab's end-to-end platform to label high-impact data, ensure model accuracy, and continue to evaluate the effectiveness of models in production. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.37.14-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"782\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-20-at-11.37.14-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-20-at-11.37.14-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.37.14-AM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage \u003ca href=\"https://labelbox.com/product/platform/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox’s platform\u003c/a\u003e to build a shelf object detection model, empowering you to make more precise merchandising decisions, enhance customer satisfaction, optimize supply chain operations, and in turn, boost productivity and profitability.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-build-a-shelf-object-detection-model\"\u003eSee it in action: How to build a shelf object detection model\u0026nbsp;\u003c/h2\u003e\u003cp\u003e\u003cem\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial.\u003c/em\u003e\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data: \u003ca href=\"https://colab.research.google.com/drive/1jfD1bBIyi_o6SUFdCVRYiDHko8GzYHXm?ref=labelbox-guides.ghost.io#scrollTo=7Gd-ZMpf0trE\"\u003e\u003cu\u003eGoogle Colab Notebook\u0026nbsp;\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance: \u003ca href=\"https://colab.research.google.com/drive/120ar420ZGmrh7LJ-6cvISpfHv5wnGMXG?ref=labelbox-guides.ghost.io#scrollTo=RI-8G2LxbqV2\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1jfD1bBIyi_o6SUFdCVRYiDHko8GzYHXm?ref=labelbox-guides.ghost.io#scrollTo=7Gd-ZMpf0trE\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook. \u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uo2jmabchf\" title=\"How to boost retail profitability with AI-powered shelf object detection (Part 1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a retailer, you might have hundreds of images captured on shelves from various stores stored in a cloud bucket or as a local folder of images. In order to upload the desired images of shelves to Quantumworks Lab, you have the option of:\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Using the \u003ca href=\"https://colab.research.google.com/drive/1jfD1bBIyi_o6SUFdCVRYiDHko8GzYHXm?ref=labelbox-guides.ghost.io#scrollTo=7Gd-ZMpf0trE\"\u003e\u003cu\u003eGoogle Colab notebook\u003c/u\u003e\u003c/a\u003e, upload the CSV of images and metadata to Quantumworks Lab\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eInput your Quantumworks Lab API key into the provided \u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003e\u003cu\u003eGoogle Colab notebook\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eSelect ‘Runtime’ in the navigation bar and hit ‘Run all’ to bring the selected amount of data rows into your \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox Catalog \u003c/u\u003e\u003c/a\u003e— where you can browse, explore, and curate the data for insights and model development.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e2) If you have a dataset from your local file, you can upload it through the Quantumworks Lab UI by clicking ‘new dataset’ in Catalog.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/10/Screenshot-2023-10-16-at-9.03.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"436\" height=\"279\"\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve successfully uploaded your images, you can browse them in Catalog - along with image metadata such as Date, Time, Store ID, Camera View, and more. You can visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"create-a-labeling-project-in-annotate\"\u003eCreate a labeling project in Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003e1) Create an image project in \u003ca href=\"https://app.labelbox.com/projects?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Annotate\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e2) Sample and send your uploaded dataset as a batch to your newly created project.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eontology\u003c/a\u003e to determine how to structure your data. If you have a previous ontology you’d like to use, you can do so. If not, you’ll need to create a new ontology. To create a new ontology:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eAdd a class. For this use case we’ll want to add bounding boxes\u003c/li\u003e\u003cli\u003ePick a name for the class. For this use case we’ll be detecting empty spaces on shelves, so we can create two bounding box classes named ‘empty’ and ‘product’ respectively\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e4) If you’re relying on an external team of labelers or want to provide your internal labeling team with more instructions, you can upload instructions as a PDF for your labelers during the ontology creation process. \u003c/p\u003e\u003ch3 id=\"label-the-data-of-interest\"\u003eLabel the data of interest\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jhlkuc8pym\" title=\"How to boost retail profitability with AI-powered shelf object detection (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eNow that we have a project with our data set up in Annotate, we’ll need to label this training data. With Quantumworks Lab, you can label your data in the following ways:\u003c/p\u003e\u003cp\u003e1) \u003cstrong\u003eInternal team of labelers:\u003c/strong\u003e your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) \u003cstrong\u003eExternal team of expert labelers\u003c/strong\u003e with \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Boost\u003c/a\u003e: leverage our global network of\u0026nbsp; specialized labelers for a variety of tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWorkforce Boost provides a collaborative platform for labeling services in a self-serve manner – this is great for teams that don’t have the technical expertise to build a machine learning system yet are looking for an easy-to-use technology to get a quick turnaround on quality training data. You can learn more about our Boost offerings \u003ca href=\"https://docs.labelbox.com/docs/using-data-labeling-service?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) \u003cstrong\u003eCreate pre-labels with foundation models\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you have the ability to send model predictions as pre-labels to your labeling project. This can be done in one of two ways:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel-assisted labeling: \u003c/strong\u003eWith \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel-assisted labeling\u003c/u\u003e\u003c/a\u003e, you can import computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eModel Foundry: \u003c/strong\u003eWith \u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, you can automate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%. \u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-2-create-a-model-run-and-evaluate-model-performance\"\u003ePart 2: Create a model run and evaluate model performance\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/120ar420ZGmrh7LJ-6cvISpfHv5wnGMXG?ref=labelbox-guides.ghost.io#scrollTo=RI-8G2LxbqV2\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"create-a-model-run\"\u003eCreate a model run\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/hub0lc15ue\" title=\"How to boost retail profitability with AI-powered shelf object detection (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you have your labeled data in your project in Annotate, you’re ready to move on to creating a model run in \u003ca href=\"https://app.labelbox.com/mea?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Model\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eWe’ll be using this \u003ca href=\"https://colab.research.google.com/drive/120ar420ZGmrh7LJ-6cvISpfHv5wnGMXG?ref=labelbox-guides.ghost.io#scrollTo=RI-8G2LxbqV2\"\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/a\u003e to train a model on the training dataset and bring back inferences from the trained model for evaluation and diagnosis.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor this step, you will need:\u003c/p\u003e\u003cul\u003e\u003cli\u003eYour Project ID (located in the URL)\u0026nbsp;\u003c/li\u003e\u003cli\u003eYour API key\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe Colab notebook will modify the output layer of a YOLOv5 algorithm and train data on the provided project’s labels. To do so, you’ll need to:\u003c/p\u003e\u003cp\u003e1) Input your API key and project ID into the notebook\u003c/p\u003e\u003cp\u003e2) Name your model\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Ensure that the ‘MEA’ field is ‘true’ and ‘MAL_upload’ and ‘Label_import’ fields are ‘false’\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce those are inputted, you can select ‘Runtime’ in the navigation bar and hit ‘Run all’ – the notebook will pull all of the labels into this environment, fine-tune YOLOv5 on your use case and import those predictions back into Quantumworks Lab Model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou’ll be able to view the model in the ‘Experiments’ tab in Quantumworks Lab Model – you can view ground truth predictions in green and predictions in red.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness\"\u003eEvaluate and diagnose model effectiveness\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qnm905nupl\" title=\"How to boost retail profitability with AI-powered shelf object detection (Part 5) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eDiagnose model performance with model metrics\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eA disagreement between model predictions and ground truth labels can be due to a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eAfter running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/li\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model for 10x faster corner-case detection – detect and visualize corner-cases where the model is underperforming. For example, you can drill into cases where ‘empty’ objects are not predicted, where the model might have difficulty identifying empty spaces on shelves where there is a wire mesh material present. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select ‘Find similar in Catalog’ from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled images that you can send to your model for labeling, you can filter on the ‘Annotation is’ filter and select ‘none’. This will only show unlabeled images that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all images that apply and send them as a batch to your original labeling project. Labeling these in priority will help improve model performance.\u0026nbsp; \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCompare model runs across iterations\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eImprove model development by up to 90% by leveraging Quantumworks Lab Model to compare model runs across iterations to track and quantify how model performance has improved over time.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model using the same steps with the Colab notebook on this improved data set.\u0026nbsp;You can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003eAI-powered computer vision enables real-time, scalable shelf optimization for retailers. By automating insights into inventory, product placement, and more, retailers can make smarter merchandising decisions, boost customer satisfaction, and increase sales by millions per year. AI shelf optimization unlocks greater efficiency, agility, and profitability.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"652ddccebe381f00016e8e79","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3082.png","featured":false,"visibility":"public","created_at":"2023-10-17T01:01:02.000+00:00","updated_at":"2024-01-17T22:04:19.000+00:00","published_at":"2023-10-17T01:16:44.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-boost-retail-profitability-with-ai-powered-shelf-object-detection","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},"url":"https://labelbox-guides.ghost.io/how-to-boost-retail-profitability-with-ai-powered-shelf-object-detection/","excerpt":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. ","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":"How to boost retail profitability with AI-powered shelf object detection","og_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3082-1.png","twitter_title":"How to boost retail profitability with AI-powered shelf object detection","twitter_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. ","meta_title":"How to boost retail profitability with AI-powered shelf object detection","meta_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64ff69ca935e200001ee0180","uuid":"89463a23-02d8-47e3-98db-49448eb94d96","title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","slug":"how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox","html":"\u003cp\u003eIn machine learning, fine-tuning pre-trained models is a powerful technique that adapts models to new tasks and datasets. Fine-tuning takes a model that has already learned representations on a large dataset, such as a large language model, and leverages prior knowledge to efficiently “teach” the model a new task. \u003c/p\u003e\u003cp\u003eThe key benefit of fine-tuning is that it allows you to take advantage of transfer learning. Rather than training a model from scratch, which requires massive datasets and compute resources, you can start with an existing model and specialize it to your use case with much less data and resources. Fine-tuning allows ML teams to efficiently adapt powerful models to new tasks with limited data and compute. It is essential for applying state-of-the-art models to real-world applications of AI. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning?ref=labelbox-guides.ghost.io\"\u003eOpen AI’s fine-tuning API\u003c/a\u003e allows teams to fine-tune the following models:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGPT-3.5 -turbo-0613 (recommended)\u003c/li\u003e\u003cli\u003eBabbage-002\u003c/li\u003e\u003cli\u003eDavinci-002\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we’ll cover how to leverage \u003ca href=\"https://www.ssw.com.au/rules/what-is-gpt/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eOpen AI’s GPT-3.5\u003c/a\u003e and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe goal of model fine-tuning is to improve the model’s performance against a specific task. Over other techniques to optimize model output, such as prompt design, fine-tuning can help achieve:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigher quality results: \u003c/strong\u003eFine-tuning allows the model to learn from a much larger and more diverse dataset than can fit into a prompt. The model can learn more granular patterns and semantics that are relevant to your use case through extensive fine-tuning training. Prompts are limited in how much task-specific context they can provide, while fine-tuning teaches the model your specific domain.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eToken savings: \u003c/strong\u003eFine-tuned models require less prompting to produce quality outputs. With fine-tuning, you can leverage a shorter, more general prompt since the model has learned your domain – saving prompt engineering effort and tokens. Whereas highly-specific prompts can often hit token limits.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLower latency: \u003c/strong\u003eHeavily engineered prompts can increase latency as they require more processing. As fine-tuned models are optimized for your specific task, they allow faster inference and can quickly retrieve knowledge for your domain.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFine-tuning is especially beneficial for adapting models to your specific use case and business needs. There are several common scenarios where fine-tuning really can help models capture the nuances required for an application:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStyle, tone, or format customization:\u003c/strong\u003e Fine-tuning allows you to adapt models to match the specific style or tone required for a use case, whether it be a particular brand voice or difference in tone for speaking to various audiences.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDesired output structure: \u003c/strong\u003eFine-tuning can teach models to follow a required structure or schema in outputs. For example, you can fine-tune a summarization model to consistently include key facts in a standardized template.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHandling edge cases: \u003c/strong\u003eReal-world data often contains irregularities and edge cases. Fine-tuning allows models to learn from a wider array of examples, including rare cases. You can fine-tune the model on new data samples so that it learns to handle edge cases when deployed to production.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn short, fine-tuning allows teams to efficiently adapt powerful models to new tasks and datasets, allowing ML teams to customize general models to their specific use cases and business needs through extensive training on curated data. High-quality fine-tuning datasets are crucial to improve performance by teaching models the nuances and complexity of the target domain more extensively than possible through prompts alone.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOpen AI’s recommended dataset guidelines\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo fine-tune an Open AI model, it is required to provide at least ten examples. Research has shown clear improvements from fine-tuning on 50 to 100 training examples with GPT-3.5-turbo. Data quality, over data quantity, is also critical to the success of the fine-tuned model. \u003c/p\u003e\u003cp\u003eYou can learn more about preparing a dataset in \u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?ref=labelbox-guides.ghost.io\"\u003eOpen AI’s documentation\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"how-to-use-labelbox-for-fine-tuning\"\u003eHow to use Quantumworks Lab for fine-tuning\u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform for building intelligent applications. With a suite of powerful data curation, labeling, and model evaluation tools, the platform is built to help continuously improve and iterate on model performance. For this example, we will use the Quantumworks Lab platform to create a high-quality fine-tuning dataset. \u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can prepare a dataset of prompts and responses to fine-tune large language models (LLMs). Quantumworks Lab supports dataset creation for a variety of fine-tuning tasks including summarization, classification, question-answering, and generation.\u003c/p\u003e\u003cp\u003eWhen you set up an \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003eLLM data generation project\u003c/a\u003e in Quantumworks Lab, you will be prompted to specify how you will be using the editor. You have three choices for specifying your LLM data generation workflow:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 1: Humans generate prompts and responses\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, the prompt and response fields will be required. This will indicate to your team that they should create a prompt and response from scratch.\u003c/p\u003e\u003cp\u003eYou can make a copy of this \u003ca href=\"https://colab.research.google.com/drive/1ElV4U68ZJCJ-wIMLFTQAyzXrSSz6aU2Y?ref=labelbox-guides.ghost.io#scrollTo=HUhjjPp0mnPq\"\u003eGoogle Colab Notebook\u003c/a\u003e to generate a prompt and response dataset in Quantumworks Lab and fine-tune GPT-3.5 Turbo in OpenAI. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 2: Humans generate prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, only the prompt field will be required. This will indicate to your team that they should create a prompt from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 3: Humans generate responses to uploaded prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, a previously uploaded prompt will appear. Your team will need to create responses for that prompt.\u003c/p\u003e\u003cp\u003eYou can make a copy of this \u003ca href=\"https://colab.research.google.com/drive/1J21Nb0fqbZCcnxyR4wOKLlADwcayOlSq?ref=labelbox-guides.ghost.io#scrollTo=vGHl-lKkZ9cK\"\u003eGoogle Colab Notebook\u003c/a\u003e to generate a dataset of responses to uploaded prompts in Quantumworks Lab and fine-tune GPT-3.5 Turbo in OpenAI. \u003c/p\u003e\u003cp\u003eIn the below example, we’ll be walking through a sample use case of summarizing and removing PII from customer support chats with Quantumworks Lab and OpenAI’s GPT-3.5 Turbo. Imagining we’re a company who wishes to summarize support logs without revealing personally identifiable information in the process, we’ll be fine-tuning an LLM to summarize and remove PII from customer support logs.\u003c/p\u003e\u003ch3 id=\"step-1-evaluate-how-gpt-35-performs-against-the-desired-task\"\u003eStep 1: Evaluate how GPT-3.5 performs against the desired task\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/e00fk9u59h\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBefore we begin the fine-tuning process, let’s first evaluate how ChatGPT (using GPT-3.5) performs against the desired task off-the-shelf. \u003c/p\u003e\u003cp\u003eWe uploaded the following sample chat log to ChatGPT:\u003c/p\u003e\u003cp\u003e“Summarize this chat log and remove any personally identifiable information in the summary:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTom: \u003c/strong\u003eI need to reset my account access.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUrsula: \u003c/strong\u003eI can help with that, Tom. What’s your account email?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTom: \u003c/strong\u003eIt’s \u003ca href=\"mailto:tom@example.com\"\u003etom@example.com\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUrsula: \u003c/strong\u003eGreat, Tom. I’ve sent you a link to update your credentials”\u003c/p\u003e\u003cp\u003eIn the above prompt, we’ve asked ChatGPT to summarize the chat log and remove any personally identifiable information. \u003c/p\u003e\u003cp\u003eUpon evaluation, the default GPT-3.5 model misses the mark for our desired use case. \u003c/p\u003e\u003cp\u003eThe summary includes both Tom and Ursula’s names and explicitly mentions Tom’s email address. In order to reliably use the model for our business use case, we need to fine-tune it so that it appropriately excludes elements of personally identifiable information. To do so, we will leverage Quantumworks Lab to generate our fine-tuning dataset and use it to fine-tune GPT-3.5 through OpenAI. \u003c/p\u003e\u003ch3 id=\"step-2-create-a-llm-data-generation-project-in-labelbox\"\u003eStep 2: Create a LLM data generation project in Quantumworks Lab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ldpv9nwh14\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to upload our support chat logs to \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Catalog \u003c/a\u003e– this will allow us to browse, curate, and send these data rows for labeling.\u003c/p\u003e\u003cp\u003eNext, we’ll need to create a LLM data generation labeling project in \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Annotate\u003c/a\u003e.\u003c/p\u003e\u003cul\u003e\u003cli\u003eSince we have an available dataset, this will be a ‘Humans generate response to uploaded prompts’ project.\u003c/li\u003e\u003cli\u003eWhen configuring the ontology, we will set the response type as ‘text’ and make the appropriate response to “summarize and remove personally identifiable information in the summary”.\u003c/li\u003e\u003cli\u003eDuring ontology creation, you can also define a character minimum or maximum and upload necessary instructions for the labeling team.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-label-data\"\u003eStep 3: Label data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/i8tzhezn70\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter successfully setting up an LLM data generation project, we can queue the uploaded chat logs in Catalog for labeling in Annotate. To label data, you have the option of leveraging \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost’s\u003c/a\u003e extensive workforce or use your own internal team to summarize and remove personally identifiable information in the summary.\u003c/p\u003e\u003cp\u003eFor larger or more complex fine-tuning tasks, you can scale up to hundreds or thousands of labeled data rows. Once all data has been labeled, you can review the corresponding summary to each prompt and export the data rows.\u003c/p\u003e\u003ch3 id=\"step-4-export-data-from-labelbox-and-fine-tune-it-in-openai\"\u003e\u003cbr\u003eStep 4: Export data from Quantumworks Lab and fine-tune it in OpenAI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bx1z0xy4db\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWith all necessary data labeled, we can export the dataset from Quantumworks Lab and upload it in a format that is readable by OpenAI. \u003c/p\u003e\u003cp\u003eOpenAI requires a dataset to be in the structure of their \u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create?ref=labelbox-guides.ghost.io\"\u003echat completions API\u003c/a\u003e, whereby each message has a role, content, and optional name. You can learn more about specific dataset requirements in OpenAI’s \u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. Using a script, we can convert the Quantumworks Lab export into OpenAI’s required conversational chat format.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e{\n \"messages\":\n \t{\"role\":\"system\",\n    \"content\":\"Given a chat log, summarize and remove personal \t\tidentifiable information in the summary.\"},\n    {\"role\":\"user\",\n    \"content\":\"Andy:Why has my order not shipped yet?! Bella: I \tapologize for the delay, Andy:May I have your order number? Andy: \t  It's ORDER5678. Please hurry! Bella: Thank you Andy. It's \t\t\texpedited and will ship today.\"},\n\t{\"role\":\"assistant\",\n    \"content\":\"Customer inquires about the delay in the shipment of his order. Support agent requests the order number and upon receiving \t  it, assures customer that the order has been expedited and will \t\tship that day.\"\n    }\n ]\n}\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003eAfter formatting our dataset, we can upload it and \u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model?ref=labelbox-guides.ghost.io\"\u003estart a fine-tuning job\u003c/a\u003e using the OpenAI SDK.\u003c/p\u003e\u003cp\u003eYou can use a copy of the following \u003ca href=\"https://colab.research.google.com/drive/1J21Nb0fqbZCcnxyR4wOKLlADwcayOlSq?ref=labelbox-guides.ghost.io#scrollTo=vGHl-lKkZ9cK\"\u003eGoogle Colab notebook\u003c/a\u003e to export data from Quantumworks Lab in a format compatible with fine-tuning GPT-3.5 Turbo and begin a fine-tuning job. \u003c/p\u003e\u003ch3 id=\"step-5-assess-the-fine-tuned-model%E2%80%99s-performance-in-openai\"\u003eStep 5: Assess the fine-tuned model’s performance in OpenAI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9kdb8m0xm0\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 5) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter the fine-tuning job has succeeded, you can navigate to the OpenAI playground and select the newly fine-tuned model for evaluation. Similarly to evaluating the initial GPT-3.5 model, we can enter a sample chat log and see how the newly fine-tuned model performs.\u003c/p\u003e\u003cp\u003eCompared to the off-the-shelf GPT-3.5 model, this model that has been fine-tuned on our training data is performing as expected. We can see that all names and relevant information that would be considered as personally identifiable information has been retracted. \u003c/p\u003e\u003cp\u003eWe can also compare the fine-tuned model to the initial GPT-3.5 model and see how it performs on the same prompt. Again, we can see that while GPT-3.5 excludes some aspects of personally identifiable information, it still includes the user’s first name, so it doesn’t quite meet the expectations for our business use case.\u003c/p\u003e\u003cp\u003eThe newly fine-tuned model has allowed us to adapt GPT-3.5 to our specific use case of concealing personally identifiable information. With Quantumworks Lab, teams can iteratively identify gaps and outdated samples in the fine-tuning data, then generate fresh high-quality data, allowing model accuracy to be maintained over time. Updating fine-tuning datasets through this circular feedback process is crucial for adapting to new concepts and keeping models performing at a high level within continuously changing environments.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eTo improve LLM performance, Quantumworks Lab simplifies the process for subject matter experts to generate high-quality datasets for fine-tuning with leading model providers and tools, like OpenAI. \u003c/p\u003e\u003cp\u003eUnlock the full potential of large language models with Quantumworks Lab’s end-to-end platform and a \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox.ghost.io\"\u003enew suite of LLM tools\u003c/a\u003e to generate high-quality training data and optimize LLMs for your most valuable AI use cases.\u003c/p\u003e","comment_id":"64ff69ca935e200001ee0180","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Group-3081.jpg","featured":false,"visibility":"public","created_at":"2023-09-11T19:26:02.000+00:00","updated_at":"2024-05-28T17:02:27.000+00:00","published_at":"2023-09-11T19:58:50.000+00:00","custom_excerpt":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/","excerpt":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","og_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Group-3081-1.jpg","twitter_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","twitter_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","meta_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","meta_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64fb3e9f935e200001ee0106","uuid":"9f549cdf-80e9-4441-8b4e-f1ce92572f56","title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","slug":"how-to-fine-tune-vertex-ai-models-with-labelbox","html":"\u003cp\u003eIn machine learning, fine-tuning pre-trained models is a powerful technique that adapts models to new tasks and datasets. Fine-tuning takes a model that has already learned representations on a large dataset, such as a large language model, and leverages prior knowledge to efficiently “teach” the model a new task. \u003c/p\u003e\u003cp\u003eThe key benefit of fine-tuning is that it allows you to take advantage of transfer learning. Rather than training a model from scratch, which requires massive datasets and compute resources, you can start with an existing model and specialize it to your use case with much less data and resources. Fine-tuning allows ML teams to efficiently adapt powerful models to new tasks with limited data and compute. It is essential for applying state-of-the-art models to real-world applications of AI.\u003c/p\u003e\u003cp\u003eVertex AI provides \u003ca href=\"https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models?ref=labelbox-guides.ghost.io#supported_models\"\u003eseveral base models\u003c/a\u003e that can be fine-tuned:\u003c/p\u003e\u003cul\u003e\u003cli\u003etext-bison@001\u003c/li\u003e\u003cli\u003ecode-bison@001\u003c/li\u003e\u003cli\u003ecodechat-bison@001\u003c/li\u003e\u003cli\u003echat-bison@001\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.\u003c/p\u003e\u003chr\u003e\u003cp\u003eThe goal of model fine-tuning is to improve the model’s performance against a specific task. Over other techniques to optimize model output, such as prompt design, fine-tuning can help achieve:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigher quality results: \u003c/strong\u003eFine-tuning allows the model to learn from a much larger and more diverse dataset than can fit into a prompt. The model can learn more granular patterns and semantics that are relevant to your use case through extensive fine-tuning training. Prompts are limited in how much task-specific context they can provide, while fine-tuning teaches the model your specific domain.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eToken savings:\u003c/strong\u003e Fine-tuned models require less prompting to produce quality outputs. With fine-tuning, you can leverage a shorter, more general prompt since the model has learned your domain – saving prompt engineering effort and tokens. Whereas highly-specific prompts can often hit token limits.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLower latency: \u003c/strong\u003eHeavily engineered prompts can increase latency as they require more processing. As fine-tuned models are optimized for your specific task, they allow faster inference and can quickly retrieve knowledge for your domain.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFine-tuning is especially beneficial for adapting models to your specific use case and business needs. There are several common scenarios where fine-tuning really can help models capture the nuances required for an application:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStyle, tone, or format customization:\u003c/strong\u003e Fine-tuning allows you to adapt models to match the specific style or tone required for a use case, whether it be a particular brand voice or difference in tone for speaking to various audiences.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDesired output structure: \u003c/strong\u003eFine-tuning can teach models to follow a required structure or schema in outputs. For example, you can fine-tune a summarization model to consistently include key facts in a standardized template. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHandling edge cases: \u003c/strong\u003eReal-world data often contains irregularities and edge cases. Fine-tuning allows models to learn from a wider array of examples, including rare cases. You can fine-tune the model on new data samples so that it learns to handle edge cases when deployed to production.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn short, fine-tuning allows teams to efficiently adapt powerful models to new tasks and datasets, allowing ML teams to customize general models to their specific use cases and business needs through extensive training on curated data. High-quality fine-tuning datasets are crucial to improve performance by teaching models the nuances and complexity of the target domain more extensively than possible through prompts alone.\u003c/p\u003e\u003ch2 id=\"how-to-use-labelbox-for-fine-tuning\"\u003eHow to use Quantumworks Lab for fine-tuning\u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform for building intelligent applications. With a suite of powerful data curation, labeling, and model evaluation tools, the platform is built to help continuously improve and iterate on model performance. We will use the Quantumworks Lab platform to create a high-quality fine-tuning dataset. \u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can prepare a dataset of prompts and responses to fine-tune large language models (LLMs). Quantumworks Lab supports dataset creation for a variety of fine-tuning tasks including summarization, classification, question-answering, and generation.\u003c/p\u003e\u003ch3 id=\"step-1-evaluate-how-a-model-performs-against-the-desired-task\"\u003eStep 1: Evaluate how a model performs against the desired task\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/n4ob00h4cj\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-2-create-an-llm-data-generation-dataset-in-labelbox\"\u003eStep 2: Create an LLM data generation dataset in Quantumworks Lab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1248\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 1248w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eWhen you set up an \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003eLLM data generation project\u003c/a\u003e in Quantumworks Lab, you will be prompted to specify how you will be using the editor. You have three choices for specifying your LLM data generation workflow:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 1: Humans generate prompts and responses\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1252\" height=\"578\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 1252w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn the editor, the prompt and response fields will be required. This will indicate to your team that they should create a prompt and response from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 2: Humans generate prompts\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1260\" height=\"576\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 1260w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn the editor, only the prompt field will be required. This will indicate to your team that they should create a prompt from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 3: Humans generate responses to uploaded prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, a previously uploaded prompt will appear. Your team will need to create responses for that prompt.\u003c/p\u003e\u003cp\u003eFor our project, we'll want to create a \"Humans generate responses to uploaded prompts\" project. Namely, we want humans to create responses in the form of a list of airlines. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/irnaoi0u64\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-3-upload-data-to-vertex-ai\"\u003eStep 3: Upload data to Vertex AI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1dwwdeggyj\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eExport the Quantumworks Lab fine-tuning dataset \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve constructed a fine-tuning dataset with Quantumworks Lab, you can export it using our \u003ca href=\"https://colab.research.google.com/drive/1imCvNhd1rZNEf_dCIsPT-wLfnHkv3o_A?ref=labelbox-guides.ghost.io\"\u003eLabelbox to Vertex AI conversion script\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStart a model tuning job using Vertex AI \u0026amp; deploy the model\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAfter exporting the fine-tuned dataset, start a \u003ca href=\"https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models?ref=labelbox-guides.ghost.io\"\u003emodel tuning job using Vertex\u003c/a\u003e. When a fine-tuning job is run, the model learns additional parameters that help it encode the necessary information to perform the desired behavior or learn the desired behavior. \u003c/p\u003e\u003cp\u003eThe output of the tuning job is a new model, which is effectively a combination of the newly learned parameters and the original model. Once the fine-tuning job is complete, you can deploy the model and return to Quantumworks Lab for model evaluation.\u003c/p\u003e\u003ch3 id=\"step-4-evaluate-and-iterate-on-fine-tuning-dataset-quality\"\u003eStep 4: Evaluate and iterate on fine-tuning dataset quality\u003c/h3\u003e\u003cp\u003eA well-performing fine-tuned model indicates the effective optimization of model architecture, training data, and hyperparameters. It signifies that the training dataset used for fine-tuning is high-quality and is representative of the real-world use case. This allows for the fine-tuned model to achieve better performance on tasks compared to the base model in less time than it would have to train a model from scratch. \u003c/p\u003e\u003cp\u003eReal-world conditions and data are often dynamic. As the use case evolves, it's crucial to maintain representativeness and relevance in the fine-tuning data. Continuous evaluation of the fine-tuned model’s performance can help detect edge cases or model errors.  You can evaluate model performance and debug errors leveraging \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e. Utilize interactive auto-populated model metrics, such as a confusion matrix, precision, recall, F1 score, and more to surface model errors. Detect and visualize corner-cases where the model is underperforming and generate high-impact data to drastically improve model performance. After running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1258\" height=\"632\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 1258w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eLabelbox Model\u003c/em\u003e\u003c/i\u003e\u003c/a\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e allows teams to debug models and iteratively improve model performance\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBy iteratively identifying gaps and outdated samples in the fine-tuning data, then generating fresh high-quality data, model accuracy can be maintained over time. Updating fine-tuning datasets through this circular feedback process is crucial for adapting to new concepts and keeping models performing at a high level within continuously changing environments.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eTo improve LLM performance, Quantumworks Lab simplifies the process for subject matter experts to generate high-quality datasets for fine-tuning with leading model providers and tools, like Google Vertex AI. \u003c/p\u003e\u003cp\u003eUnlock the full potential of large language models with Quantumworks Lab’s end-to-end platform and a \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox.ghost.io\"\u003enew suite of LLM tools\u003c/a\u003e to generate high-quality training data and optimize LLMs for your most valuable AI use cases. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..\u0026ref=labelbox-guides.ghost.io\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"64fb3e9f935e200001ee0106","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299.png","featured":false,"visibility":"public","created_at":"2023-09-08T15:32:47.000+00:00","updated_at":"2024-03-26T16:25:35.000+00:00","published_at":"2023-09-08T18:03:36.000+00:00","custom_excerpt":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-vertex-ai-models-with-labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-vertex-ai-models-with-labelbox/","excerpt":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","reading_time":6,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299-1.png","og_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","og_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299-2.png","twitter_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","twitter_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","meta_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","meta_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64e7a2c3b0c09f000179dbed","uuid":"2b64de59-c4f8-4bf5-aa4b-07d1aabcb933","title":"How to build a powerful product recommendation system for retail","slug":"how-to-build-a-powerful-product-recommendation-system-for-retail","html":"\u003cp\u003ePersonalized experiences are at the heart of customer satisfaction and are key to long-term brand loyalty and success. Amidst the abundance of choices available to the modern consumer, businesses must find innovative ways to stand out and forge meaningful relationships with their audience. \u003c/p\u003e\u003cp\u003eThe rise of AI has enabled companies to craft personalized experiences at an unprecedented scale. Organizations can now rely on algorithms taught to recognize customer preferences, behavior, and provide recommendations based on purchase history, and more. With a powerful product recommendation system, retailers can create individualized customer interactions and foster stronger connections to boost customer loyalty and increase key metrics such as conversion rate, average order value, and repeat purchase rates. \u003c/p\u003e\u003cp\u003eHowever, building a robust and effective AI-powered product recommendation system can be challenging for many teams. Some key challenges include: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eBuilding a strong recommendation system that makes accurate predictions requires a vast amount of high-quality data. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability: \u003c/strong\u003eAs a business grows and their product catalog expands, the recommendation system should be able to handle new and incoming data. Ensuring scalability and maintaining model performance with new data can be particularly challenging for teams relying on in-house solutions or disparate ML tools.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrivacy and Security: \u003c/strong\u003eWhen it comes to customer data and specific product information, ensuring user privacy and safeguarding against potential security violations is critical to maintain trust with customers and build a successful recommendation system. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform to help build the best personalized product recommendation engine. Rather than spending valuable time building in-house or relying on disparate systems and applications, teams can leverage Quantumworks Lab’s platform to seamlessly build an end-to-end workflow that integrates with your existing tech stack and helps teams build AI systems faster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1425\" height=\"635\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/image-3.png 1425w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build a powerful recommendation system, ensuring your customers embark on a seamless and delightful shopping journey that keeps them coming back for more.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"see-it-in-action-how-to-build-a-powerful-product-recommendation-system-in-labelbox\"\u003eSee it in action: How to build a powerful product recommendation system in Quantumworks Lab\u003c/h2\u003e\u003cp\u003e\u003cem\u003eThe  walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eCatalog\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eAnnotate\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eModel\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003ecreate a Quantumworks Lab account\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1: \u003c/strong\u003eExplore and enhance your data (\u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003eGoogle Colab Notebook\u003c/a\u003e)\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Prepare data and evaluate model performance: (\u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\"\u003eGoogle Colab Notebook\u003c/a\u003e)\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u003c/strong\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\" class=\"kg-btn kg-btn-accent\"\u003ePart 1: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"painlessly-consolidate-all-your-product-data\"\u003ePainlessly consolidate all your product data\u003c/h3\u003e\u003cp\u003eBuilding a recommendation engine requires consolidating data of different types from various sources. Such data can include product, business, and customer information that might be siloed or stored in different databases. To holistically browse and visualize your entire product catalog, leverage Quantumworks Lab Catalog to bring and view all of your data in a single place.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rdlcin4zh8\" title=\"[Personalized Experiences Demo] Data ingestion Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ve provided a sample retail dataset for you to view in your Quantumworks Lab app: \u003c/p\u003e\u003cp\u003e1) Input your Quantumworks Lab API key into the provided \u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003eGoogle Colab notebook\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e2) Specify an amount of data that you wish to ingest — we’ve provided up to 44,000 data rows for ingestion, but keep in mind that this will accrue \u003ca href=\"https://docs.labelbox.com/docs/billing?ref=labelbox-guides.ghost.io#labelbox-units-lbus\"\u003eLBUs\u003c/a\u003e in your account. \u003cem\u003eIf you are using Quantumworks Lab for free, we suggest that you ingest around 5,000 data rows.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e3) Select ‘Runtime’ in the navigation bar and hit ‘Run all’ to bring the selected amount of data rows into your \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Catalog \u003c/a\u003e— where you can browse, explore, and curate the data for insights and model development.\u003c/p\u003e\u003ch3 id=\"accelerate-product-discovery-across-your-entire-catalog\"\u003eAccelerate product discovery across your entire catalog\u003c/h3\u003e\u003cp\u003eAn effective product recommendation relies on training a model with a thorough understanding of your product data, encompassing product tags, categories, and more. However, retailers often have an ever-growing product list with hundreds or thousands of products. Dealing with this volume of data at scale and effectively searching, organizing, and managing data for machine learning tasks can be a challenge.\u003c/p\u003e\u003cp\u003eYou can leverage Quantumworks Lab Catalog to visualize, browse, and curate your product listings.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ehcuz2b4rk\" title=\"[Personalized Experiences Demo] Search and curate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSearch and curate data\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eYou’ll now be able to see the sample retail dataset in your Quantumworks Lab Catalog. Try searching across key product-specific metadata such as category, the year the item was released, season, gender, and more. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1226\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 2304w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003eSearch across datasets\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003efind similar data\u003c/a\u003e in seconds with off-the-shelf embeddings \u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003enatural language\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003elayer structured and unstructured filters\u003c/a\u003e for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"categorize-and-curate-product-listings-faster\"\u003eCategorize and curate product listings faster\u003c/h3\u003e\u003cp\u003eAdvanced ML teams often adopt partially automated labeling workflows to mitigate costs and accelerate model development. Product recommendation models require a vast amount of accurately labeled data with a wide array of features. Manually labeling this data can not only be time consuming, but can also get exponentially expensive. Scaling data curation and enrichment effectively is key to quickly creating a powerful ML solution. \u003c/p\u003e\u003cp\u003eOne simple way to achieve this is by leveraging bulk classification and using human-in-the-loop review for quality assurance. Some AI teams using this technique have cut labeling costs by nearly 90%.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/epl8hstb7c\" title=\"[Personalized Experiences Demo] Streamlined labeling automation through bulk classification Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eStreamline labeling automation through bulk classification\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCreate a new labeling project\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"905\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve narrowed in on a specific slice of data that you’d like to take action on, you can send them to a labeling project of interest in just a few clicks.\u003c/p\u003e\u003cp\u003e1) Create a new image project in \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e2) Configure the editor and create a new ontology. For the ontology, create a new classification called ‘Occasions’ with options such as ‘Casual’, ‘Sports’, ‘Formal’, etc. feel free to add any other classifications of interest.\u003c/p\u003e\u003cp\u003e3) Save your labeling project.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSend a subset of data to the labeling project\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"739\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eReturn to Catalog and surface a specific subset of data that you want to bulk classify. For example, you can surface all instances of ‘Sporty apparel’ clothing with a natural language search.\u003c/p\u003e\u003cp\u003e1) Highlight any data rows of interest and select ‘Manage selection’ \u0026gt; ‘Add classifications’.\u003c/p\u003e\u003cp\u003e2) Select the labeling project that you made in the previous step and determine a step of the project’s review workflow that you would like to send the classifications to. In the above demo, we are sending these to the ‘Done’ stage because we have verified that these images fall under the ‘Sports’ category and want to automatically create ground truth labels.\u003c/p\u003e\u003cp\u003e3) Pick ‘Sports’ under the Classification section and you can submit the classification batch. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSave high-impact searches\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1029\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can save any combination of searches as a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e in Catalog. For example, saving the natural language search of ‘Sporty apparel’ as a slice called ‘Sporty apparel’, creates a dynamic slice of data that can easily be revisited or edited as project needs evolve. Any future data that gets uploaded to Quantumworks Lab will automatically populate in any relevant slices based on its filters, creating an automatic data curation pipeline as your product catalog grows.\u003c/p\u003e\u003ch2 id=\"part-2-prepare-data-and-evaluate-model-performance\"\u003ePart 2: Prepare data and evaluate model performance\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eFollow along with the below with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u003c/strong\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\" class=\"kg-btn kg-btn-accent\"\u003ePart 2: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"prepare-a-training-dataset-for-model-diagnosis\"\u003ePrepare a training dataset for model diagnosis\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eModel diagnosis can play a pivotal role when training a model for personalized shopping experiences. The success of personalized recommendation systems hinge on the accuracy of a model’s understanding of a retailer’s product catalog or individual customer preferences. A properly curated and organized training dataset serves as the foundation for accurate model performance evaluation and fine-tuning.\u003c/p\u003e\u003cp\u003eSend the curated training dataset from Quantumworks Lab Annotate to Model in a few clicks to efficiently diagnose model performance of the training dataset.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate a training dataset for evaluation\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/zo945lphbi\" title=\"[Personalized Experiences Demo] Prepare a training dataset for model diagnosis Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eCreating a new model\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1866\" height=\"1408\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 1866w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Navigate to the Model tab, select ‘Experiments’, and create a new model.\u003c/p\u003e\u003cp\u003e2) Name your model (e.g ‘Occasions’) and select a model thumbnail.\u003c/p\u003e\u003cp\u003e3) Select the same ontology that was used to bulk classify the data rows in the previous step.\u003c/p\u003e\u003cp\u003e4) Select the project that the classifications were sent to in the previous step. After selecting both the correct ontology and project, you should see the number of data rows that were bulk classified and ready to be added to the new model.\u003c/p\u003e\u003cp\u003e5) Hit ‘Create model’.\u003c/p\u003e\u003cp\u003e\u003cem\u003eCreating a new model run\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"938\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 2306w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Once you’ve created your model, you can navigate back to the ‘Experiments’ tab and find the newly created model.\u003c/p\u003e\u003cp\u003e2) Create a ‘new model run.’ \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003eA model run\u003c/a\u003e is a model training experiment within a model, providing a versioned data snapshot of all data rows, annotations, and data splits for that model run.\u003c/p\u003e\u003cp\u003e3) To create a model run, you’ll need to give it a name (e.g ‘dataset version 1’) and can adjust the balance of the data split. For this demo, we will leave them in the default setting (80% train, 10% validate, 10% test).\u003c/p\u003e\u003cp\u003e4) After creating the model run, you’ll be able to see the populated training data classifications.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTrain a model on a provided training dataset\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ab1phltynu\" title=\"[Personalized Experiences] Train a model on the provided training dataset Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eWe’ll be using the \u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\"\u003eColab notebook\u003c/a\u003e to train a model on the training dataset and bring back inferences from the trained model for evaluation and diagnosis.\u003c/p\u003e\u003cp\u003eFor this step, you will need:\u003c/p\u003e\u003cul\u003e\u003cli\u003eYour Ontology ID — found in the Settings tab on the model run page\u003c/li\u003e\u003cli\u003eYour Model Run ID — found in the gear icon on the top-right of the model run page\u003c/li\u003e\u003cli\u003eYour API key\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e1) Enter your API key, Ontology ID, and Model Run ID in the Colab Notebook.\u003c/p\u003e\u003cp\u003e2)  Once those are inputted, you can select ‘Runtime’ in the navigation bar and hit ‘Run all’ – this will take the classifications from your model run and train a provided image classification model. After training, the notebook will also take the trained model and use it to run inference on the data.\u003c/p\u003e\u003cp\u003e3)  If you want to adjust your data splits, you can leverage search filters in Model to surface any data rows and move them to the train, test, and validation splits.\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness-for-retail\"\u003eEvaluate and diagnose model effectiveness for retail\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eA well-performing model can accurately predict consumer behavior or recommend products based on past preferences. Effective model diagnosis helps fine-tune recommendation algorithms, resulting in more accurate and appealing product suggestions. \u003c/p\u003e\u003cp\u003eModel diagnosis and evaluation are not one-time tasks. By leveraging effective diagnostic tools and an active learning workflow, retailers can continuously identify areas of improvement and adapt to changing customer behavior or an evolving product catalog. This iterative approach keeps personalized experiences relevant and effective for driving business outcomes over time.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qwqdx21ml2\" title=\"[Personalized Experiences Demo] Diagnose model performance and fix model errors Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eDiagnose model performance with model metrics\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"692\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 2330w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) After running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/p\u003e\u003cp\u003e2) Use the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/p\u003e\u003cp\u003e3) Model metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/p\u003e\u003cp\u003e4) Detect and visualize corner-cases where the model is underperforming. For example, in the demo above, we notice that the model is classifying this type of white shoe as a ‘Sports’ shoe when in fact it is a ‘Casual’ shoe.\u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1172\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 2324w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Select any corner-cases and select ‘Find similar in Catalog’ from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows to the selected example.\u003c/p\u003e\u003cp\u003e2) In addition to the similarity search, you can filter on ‘Annotation’ \u0026gt; ‘is none’ to surface only unlabeled data rows that you can retrain the model on to boost model performance.\u003c/p\u003e\u003cp\u003e3) Select any relevant examples and ‘Manage selection’ \u0026gt; ‘Add classifications’. In this case, we’d want to bulk classify these examples to reinforce to the model that these images are ‘casual’ shoes.\u003c/p\u003e\u003cp\u003e4) This step is similar to the bulk classification step in part 1. Select the labeling project that you made in the previous step and determine a step of the project’s review workflow that you would like to send the classifications to. We can send these to the ‘Done’ stage because we want to tell the model these white shoes fall under the ‘Casual’ category and want to automatically create ground truth labels.\u003c/p\u003e\u003cp\u003e5) Create a new model run (within the same model) and have the newly added classifications as a part of the training dataset.\u003c/p\u003e\u003cp\u003e6) Run the notebook again to train the model on this new training dataset and evaluate model performance with model metrics. You can compare results from the initial model run with the new model run to evaluate how the adjustment influenced model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAs consumer businesses strive to distinguish themselves in a competitive market, the power of AI-driven product recommendation systems cannot be underestimated. Companies can tap into their vast data stores and harness the capabilities of advanced algorithms to forge deeper connections with their customers.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..\u0026ref=labelbox-guides.ghost.io\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e. \u003c/p\u003e","comment_id":"64e7a2c3b0c09f000179dbed","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078--1-.png","featured":false,"visibility":"public","created_at":"2023-08-24T18:34:43.000+00:00","updated_at":"2023-11-20T19:39:23.000+00:00","published_at":"2023-08-24T19:15:09.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-build-a-powerful-product-recommendation-system-for-retail/","excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","reading_time":10,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078--1--2.png","og_title":"How to build a powerful product recommendation system for retail","og_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078--1--1.png","twitter_title":"How to build a powerful product recommendation system for retail","twitter_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","meta_title":"How to build a powerful product recommendation system for retail","meta_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":74,"tag":{"slug":"build-ai","id":"653aa45d375d13000123d7de","name":"Build AI","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","count":{"posts":74},"url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"slug":"build-ai","currentPage":"3"},"__N_SSG":true},"page":"/guides/tag/[id]/page/[pagenum]","query":{"id":"build-ai","pagenum":"3"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/tag/build-ai/page/3/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:49:45 GMT -->
</html>