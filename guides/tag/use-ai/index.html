<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/tag/use-ai/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:31:53 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../static/scripts/munchkin.js"></script><script src="../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../_next/static/chunks/pages/guides/tag/%5bid%5d-cc2bd40a983392d8.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../index.html"><img width="106" height="24" alt="logo" src="../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../build-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Build AI</a><a href="index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Use AI</a><a href="../explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="../label-data-for-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Label data for AI</a><a href="../train-fine-tune-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="mb-10"><h2 class="text-3xl md:text-4xl font-medium mb-4">Use AI</h2><p class="text-base max-w-2xl font-medium text-neutral-500"></p></div><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../evaluating-leading-text-to-speech-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index6ff8.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../evaluating-leading-text-to-speech-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Evaluating leading text-to-speech models</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../using-multimodal-chat-to-enhance-a-customers-online-support-experience/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexdc84.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../using-multimodal-chat-to-enhance-a-customers-online-support-experience/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using multimodal chat to enhance a customer’s online support experience</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../ai-foundations-understanding-embeddings/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexfbc8.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../ai-foundations-understanding-embeddings/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">AI foundations: Understanding embeddings</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-accelerate-labeling-projects/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--5-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--5-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--5-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--5-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--5-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--5-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--5-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--5-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index1968.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--5-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-accelerate-labeling-projects/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to accelerate labeling projects using GPT–4V in Foundry</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn 3 specific examples of how teams can use Quantumworks Lab to accelerate labeling projects by using multimodal models to create high-quality labels for various data types including images, HTML, and text. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../detecting-swimming-pools-with-gpt4-visual/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3060--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3060--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3060--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3060--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3060--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3060--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3060--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3060--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index5a0b.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-3060--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../detecting-swimming-pools-with-gpt4-visual/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Detecting swimming pools with GPT4 Visual</p><p class="text-base max-w-2xl undefined line-clamp-3">Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart LLM development, decreasing costs and accelerating time-to-value. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index0a79.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to analyze customer reviews and improve customer care with NLP</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage Quantumworks Lab&#x27;s data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-evaluate-object-detection-models-using-labelbox-model-foundry/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3084.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3084.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3084.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3084.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3084.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3084.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3084.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3084.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexf011.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3084.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-evaluate-object-detection-models-using-labelbox-model-foundry/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to evaluate object detection models with Quantumworks Lab Model Foundry</p><p class="text-base max-w-2xl undefined line-clamp-3">Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../zero-shot-learning-few-shot-learning-fine-tuning/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index109f.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FGroup-3083.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../zero-shot-learning-few-shot-learning-fine-tuning/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Zero-Shot Learning vs. Few-Shot Learning vs. Fine-Tuning: A technical walkthrough using OpenAI&#x27;s APIs &amp; models</p><p class="text-base max-w-2xl undefined line-clamp-3">With large language models (LLMs) gaining popularity, new techniques have emerged for applying them to NLP tasks. Three techniques in particular — zero-shot learning, few-shot learning, and fine-tuning — take different approaches to leveraging LLMs. In this guide, we’ll walk through the key difference between these techniques and how to implement them. 

We’ll walk through a case study of extracting airline names from tweets to compare the techniques. Using an entity extraction dataset, we’ll be</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../guide-to-using-model-foundry/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index0b48.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../guide-to-using-model-foundry/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to use the model Foundry for automated data labeling and enrichment</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexfcac.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.</p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8">Page 1 of 2<a class="ml-9 text-neutral-700 mb-1" href="page/2/index.html">&gt;</a></div></div></div></div></div></div>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"66bfded301c06500016e8b1c","uuid":"a2c40e88-6376-4824-b57f-e0e3673d8b1b","title":"Evaluating leading text-to-speech models","slug":"evaluating-leading-text-to-speech-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in speech generation evaluation, visit \u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe evolution of Text-to-Speech (TTS) models has been groundbreaking, with various companies offering high-quality solutions for a range of applications. Whether it's enhancing accessibility, automating customer service, or generating voiceovers, TTS models play a vital role in today's digital landscape. \u003c/p\u003e\u003cp\u003eWe recently evaluated the performance of six leading TTS models—Google TTS, Cartesia, AWS Polly, OpenAI TTS, Deepgram, and Eleven Labs—using key evaluation metrics such as Word Error Rate (WER) and human preference ratings.\u003c/p\u003e\u003cp\u003eTo provide an in-depth analysis, we evaluated each model on 500 prompts. Each prompt was reviewed by three expert labelers to ensure accuracy and provide diverse perspectives.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"evaluation-process\"\u003e\u003cstrong\u003eEvaluation Process\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate speech outputs across several key criteria:\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer\"\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWER measures the accuracy of the generated speech by calculating the number of insertions, deletions, and substitutions compared to a reference transcription. The lower the WER, the better the model performed in terms of generating accurate and coherent speech.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1120\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"word-errors\"\u003eWord errors\u003c/h3\u003e\u003cp\u003eRaters evaluated the presence of word errors in the generated TTS audio by counting the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Insertion Errors\u003c/strong\u003e: Additional words not present in the original text.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Deletion Errors\u003c/strong\u003e: Words from the original text that were omitted.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Substitution Errors\u003c/strong\u003e: Incorrect replacements for original words.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: No word errors or only minor errors (0-1 error).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some errors (2-3 errors).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Significant errors (4 or more errors).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"speech-naturalness\"\u003eSpeech naturalness\u003c/h3\u003e\u003cp\u003eRaters assessed how human-like the generated speech sounded.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Very human-like, natural flow with appropriate pauses and inflections.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some human qualities but with occasional robotic or awkward elements.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Clearly robotic or artificial, with choppy or monotone speech.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"pronunciation-accuracy\"\u003ePronunciation accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated how clearly and correctly words were pronounced in the TTS output.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: All words are pronounced clearly and correctly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: 1-2 words are mispronounced or unclear.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: 3 or more words are mispronounced or difficult to understand.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"noise\"\u003eNoise\u003c/h3\u003e\u003cp\u003eRaters evaluated the level of background noise or artifacts in the audio.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNone\u003c/strong\u003e: No detectable noise or artifacts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Minor noise, such as a faint hiss, but not distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Noticeable noise or artifacts that may be somewhat distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Significant noise or artifacts that interfere with comprehension.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"context-awareness\"\u003eContext awareness\u003c/h3\u003e\u003cp\u003eRaters assessed the TTS system's ability to adjust to context, including tone, emphasis, and punctuation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Excellent adaptation to context, with clear tonal shifts and pauses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Adequate adaptation but misses some nuances.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Little to no adaptation; reads text monotonously without context cues.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"prosody-accuracy\"\u003eProsody accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated the rhythm, stress, and intonation (prosody) of the generated speech.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Natural-sounding rhythm and intonation that closely mimics human speech.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Mostly natural but with occasional awkwardness in stress or rhythm.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Monotonous or robotic intonation with unnatural pauses and emphasis\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"prompts-curation-process\"\u003ePrompts curation process\u003c/h2\u003e\u003cp\u003eThe diverse set of prompts used in our TTS evaluation provides a comprehensive test of model capabilities across multiple dimensions. \u003c/p\u003e\u003cp\u003eConsider these examples:\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Wow! Did you see the fireworks last night? They were spectacular!\"\u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003e This short, exclamatory sentence tests the model's ability to convey excitement and emphasis. It challenges the TTS system to appropriately modulate tone and pitch to express enthusiasm.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"I wish you were on the right path...I-I-I truly do, but you're not,' he said sadly.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis prompt evaluates the model's handling of emotional nuance, hesitation (indicated by the stuttering \"I-I-I\"), and the use of ellipsis for pauses. It also tests the system's ability to convey sadness as specified in the dialogue tag.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Alright, listen up, team! The mission briefing is as follows: Operation Nighthawk commences at 0300 hours. Rendezvous at coordinates 51°30'26.5\"N 0°07'39.0\"W. Your login credentials for the secure server are: Username: NightOwl_Alpha, Password: 3ch3l0n_X9#. Remember, this is a stealth operation. Radio silence must be maintained except for emergency protocol Sierra-Tango-Seven. If compromised, use the failsafe code: Zulu-Yankee-4-7-2-Bravo. Any questions? No? Then synchronize your watches... now. Good luck, and godspeed.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis complex prompt tests multiple aspects:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical pronunciation of alphanumeric strings and military jargon\u003c/li\u003e\u003cli\u003eConveying urgency and authority\u003c/li\u003e\u003cli\u003eHandling of coordinates and precise numerical information\u003c/li\u003e\u003cli\u003eProper pacing for a lengthy, detailed instruction set\u003c/li\u003e\u003cli\u003eAppropriate pausing, indicated by punctuation and paragraph breaks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese prompts demonstrate how we cover a range of linguistic features, emotional tones, and formatting challenges. They span from simple exclamations to complex, technical instructions, ensuring a thorough assessment of prosody, emphasis, and versatility.\u003c/p\u003e\u003cp\u003eThis diversity in prompts matters because it enables a comprehensive evaluation that reflects real-world applicability. By covering such a broad spectrum of scenarios, we can identify specific strengths and weaknesses in each TTS model, allowing for fair comparisons and pinpointing areas for improvement. Whether it's expressing joy over fireworks, conveying emotional conflict, or delivering a detailed mission briefing, these prompts help us ensure that TTS models can handle the variety and complexity of human communication.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overall-ranking-best-to-worst-model-samples\"\u003eOverall Ranking best to worst model samples\u003c/h2\u003e\u003cp\u003eRaters ranked each TTS sample from best to worst based on performance across all categories above.\u003c/p\u003e\u003ch3 id=\"model-configurations-used-at-test-time\"\u003e\u003cstrong\u003eModel configurations used at test time\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: tts-1-hd\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: alloy\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle Text-to-Speech (TTS)\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel:\u003c/strong\u003e Default\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice: \u003c/strong\u003eDefault with neutral gender voice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eElevenLabs\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: eleven_monolingual_v1 (As of 30th September, eleven_monolingual_v1 is an outdated version and Quantumworks Lab will run a new experiment with\u0026nbsp;eleven_multilingual_v2 in the future)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: pMsXgVXv3BLzUgSXRplE\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: aura-asteria-en\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Default\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAmazon Polly\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Generative\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Ruth\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: sonic-english\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Teacher Lady\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe selected these models for our evaluation as they represent the cutting-edge offerings from leading companies in the TTS space, showcasing the latest advancements in voice synthesis technology.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"results\"\u003eResults\u003c/h1\u003e\u003cp\u003eOur evaluation of 500 diverse and complex prompts provided valuable insights into the capabilities of Google, Cartesia, AWS Polly, OpenAI, Deepgram, and Eleven Labs TTS models. Each prompt was meticulously assessed by three different raters to ensure accurate and diverse perspectives. This rigorous testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of these leading TTS models:\u003c/p\u003e\u003ch3 id=\"rank-distribution-percentages\"\u003eRank distribution percentages\u003cstrong\u003e\u0026nbsp; \u0026nbsp; \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"overall-ranking\"\u003eOverall ranking\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1:\u003c/strong\u003e\u0026nbsp; (Open AI TTS) - occurred 607 times, 42.93%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2:\u003c/strong\u003e\u0026nbsp; (Cartesia) - occurred 460 times, 32.53%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 410 times, 29.00%\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 4:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 338 times, 23.90%. Followed by Eleven labs as a close second with occurrence 335, 23.69%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 5:\u003c/strong\u003e\u0026nbsp; (Deepgram) - occurred 438 times, 30.98%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 6:\u003c/strong\u003e\u0026nbsp; (Google TTS) - occurred 868 times, 61.39%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote: Rank is determined by the most frequently selected model for each position, which is why some AWS Polly appears twice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1122\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003eFig: % of times each model was ranked first.\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer-results\"\u003eWord Error Rate (WER) Results\u003c/h3\u003e\u003cp\u003eThe WER scores offer an objective look at how accurately each model converts text to speech.\u003c/p\u003e\u003cul\u003e\u003cli\u003eDeepgram: 5.67%\u003c/li\u003e\u003cli\u003eOpenAI TTS: 4.19%\u003c/li\u003e\u003cli\u003eGoogle TTS: 3.36%\u003c/li\u003e\u003cli\u003eAWS Polly: 3.18%\u003c/li\u003e\u003cli\u003eCartesia: 3.87%\u003c/li\u003e\u003cli\u003eEleven Labs: 2.83%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs achieved the lowest WER at 2.83%, making it the most accurate model. Deepgram, however, had the highest WER at 5.67%, indicating room for improvement in transcription accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" title=\"Chart\" width=\"2000\" height=\"1126\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1123\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-specific-performance-results\"\u003eModel-Specific Performance Results\u003c/h2\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each TTS model based on key evaluation criteria.\u003c/p\u003e\u003ch3 id=\"openai-tts\"\u003eOpenAI TTS\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 4.1898%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 89.60% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 87.13% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 92.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 63.37% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 64.57% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOpenAI TTS excels at producing natural-sounding speech, making it a top choice for applications requiring lifelike vocal output. It demonstrates exceptional accuracy in pronunciation and performs excellently in producing clean audio with minimal background noise. The model shows strong capabilities in understanding and conveying contextual nuances in speech and is proficient in delivering appropriate intonation and rhythm. Despite ranking high in human preference, it shows moderate accuracy in word reproduction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cm24ur6wwf\" title=\"OpenAI: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gicqvkqc5e\" title=\"OpenAI: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kg9wqqftu4\" title=\"OpenAI: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"cartesia\"\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 58.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 53.11% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 93.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 80.98% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 77.02% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.8707%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCartesia demonstrates good accuracy in word reproduction. While not as consistent as OpenAI TTS in naturalness, it still performs very well in producing natural-sounding speech. It shows excellent accuracy in pronunciation and excels at producing clean audio with minimal background noise. The model displays good capabilities in understanding contextual nuances and performs well in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/3gukhi7z2j\" title=\"Cartesia: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y3lbes0sin\" title=\"Cartesia: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f0lmkto1xk\" title=\"Cartesia: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"aws-polly\"\u003e\u003cstrong\u003eAWS Polly\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 56.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 55.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.39% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 84.72% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Medium in 58.20% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.1770%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAWS Polly shows very good accuracy in word reproduction, ranking second among the models. It struggles more with producing natural-sounding speech compared to top performers but maintains excellent pronunciation accuracy. The model performs well in producing clean audio and shows good capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qawx1dwc9g\" title=\"Polly: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/klmnfcsno5\" title=\"AWS Polly: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ubzrqtjnc9\" title=\"AWS Polly: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"eleven-labs\"\u003e\u003cstrong\u003eEleven Labs\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 2.8302%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: Medium in 44.98% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 81.97% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 80.27% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 44.70% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 46.25% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs demonstrates the best accuracy in word reproduction among all models. It shows mixed results in producing natural-sounding speech but maintains excellent pronunciation accuracy. The model performs reasonably well in producing clean audio but has more noise issues than some competitors. It shows moderate capabilities in understanding contextual nuances and has room for improvement in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/x3qzhkt0ab\" title=\"Eleven Labs: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kl0lc7oqmz\" title=\"Eleven Labs: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bc19r3iqip\" title=\"Eleven Labs: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"deepgram\"\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 5.6743%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: High in 57.78% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 64.43% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 88.83% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 53.18% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 55.52% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDeepgram shows the highest error rate in word reproduction among all models. It performs moderately well in producing natural-sounding speech and demonstrates good accuracy in pronunciation, although lagging behind some competitors. The model performs well in producing clean audio with minimal background noise but shows moderate capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mngnvgtzx3\" title=\"Deepgram: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6h3bgmcg6d\" title=\"Deepgram: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uaj3x9bln8\" title=\"Deepgram: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"google-tts\"\u003e\u003cstrong\u003eGoogle TTS\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.3574%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Low in 78.01% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 77.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.46% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: Medium in 39.25% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: Low in 45.83% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z27ett9q8m\" title=\"Google TTS: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/06pqi5qc13\" title=\"Google TTS: Wow Did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kwotkto0u6\" title=\"Google TTS: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation reveals that while Eleven Labs leads in transcription accuracy with the lowest WER, OpenAI TTS emerges as the top choice when it comes to human preference, particularly for speech naturalness, pronunciation accuracy, and prosody.\u003c/p\u003e\u003cp\u003eConversely, Google TTS, despite a relatively low WER, ranks last in human preference due to poor performance in categories like speech naturalness and context awareness. This demonstrates the importance of a balanced evaluation that considers both quantitative metrics like WER and qualitative factors like user satisfaction.\u003c/p\u003e\u003cp\u003eAs the TTS field continues to evolve, the key to further advancements will lie in combining high transcription accuracy with natural, expressive, and context-aware speech. Models like OpenAI TTS and Eleven Labs are setting the standard, but there remains room for improvement across all models to fully meet the growing demands of diverse TTS applications.\u003c/p\u003e\u003cp\u003eIn summary, while WER is crucial, the broader context of user experience—focusing on factors like speech naturalness, context awareness, and prosody accuracy—is essential in evaluating the overall effectiveness of TTS models.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-speech models presented here represents a significant advancement in assessing AI-generated speech. By leveraging this methodology alongside Quantumworks Lab's cutting-edge platform, AI teams can dramatically accelerate the development and refinement of sophisticated, domain-specific speech generation models.\u003c/p\u003e\u003cp\u003eOur solution offers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eEfficient dataset curation\u003c/li\u003e\u003cli\u003eAutomated evaluation techniques\u003c/li\u003e\u003cli\u003eHuman-in-the-loop quality assurance\u003c/li\u003e\u003cli\u003eCustomizable workflows for TTS model assessment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-speech model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdohXa2KeUawTMTWS7MfKSUYC6fVPCHjtmcFCRt5ycmB7S4k4-JElTwnmRb6ufW2kR7F3Bqu1PwzbvL83LKcqgz8jbkoY02tefklFLp98wvqm2liOKBglEGaRITIk6Wx1tY3fi-43TaCJnpwEwRVCz7q101?key=Qy_gZn4bA_q73M6n7nwEIA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"1024\"\u003e\u003c/figure\u003e","comment_id":"66bfded301c06500016e8b1c","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-12.12.46-PM.png","featured":false,"visibility":"public","created_at":"2024-08-16T23:20:51.000+00:00","updated_at":"2024-11-26T00:11:38.000+00:00","published_at":"2024-08-17T19:13:25.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/evaluating-leading-text-to-speech-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"668c2f59878a56000112a4e0","uuid":"93014cdf-3555-4d2b-b480-2b9db686e2db","title":"Using multimodal chat to enhance a customer’s online support experience","slug":"using-multimodal-chat-to-enhance-a-customers-online-support-experience","html":"\u003cp\u003eCustomer service systems are prime targets for multimodal chat as this new technology leverages an intuitive interface for answering user questions\u0026nbsp; while enabling companies to unlock previously hidden insights about customer journeys and sentiment.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can streamline the process of labeling and managing data for various use cases, including intent classification, dialogue retrieval, state tracking, and model evaluation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. We’ll illustrate the various labeling tasks that Quantumworks Lab can accomplish to ensure the chatbot is intelligent and effective, providing customers with relevant and timely answers.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://files.readme.io/ce6e056-Labelbox_2024-06-04_10-37-061.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"678\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eLabelbox offers a \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/docs/live-multimodal-chat-evaluation?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003enumber of editor types\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, including the live multimodal chat editor.\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"labeling-tasks\"\u003e\u003cstrong\u003eLabeling Tasks\u003c/strong\u003e\u003c/h1\u003e\u003ch2 id=\"multimodal-intent-classification-dialogue-retrieval-and-state-tracking\"\u003e\u003cstrong\u003eMultimodal Intent Classification, Dialogue Retrieval and State Tracking\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xs53lmro56\" title=\"Customer Query Resolution Training - Part 1 - Training Data Collection + Using Classifications Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 1: Generate prompt-response pairs for supervised fine-tuning of agentic workflows.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eLabel the customer’s intent and sentiments in a given turn of the chat, so you can identify the appropriate next steps accordingly.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1427\" height=\"1333\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/first-one.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/first-one.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png 1427w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe customer appears frustrated, so the sentiment is labeled as “frustrated.”\u003c/p\u003e\u003cp\u003eFurthermore, the customer appears to be reporting their order was damaged and is requesting a new one, so both of those requests are labeled using checklist classifications.\u003c/p\u003e\u003cp\u003eWith these labels, you know the customer’s intent (get a new piece) and can also record the current state of the conversation (damage reported but no action taken yet). As the conversation proceeds, you can update the state based on what actions are taken and keep checking the intent and tone of the customer to ensure they are satisfied.\u003c/p\u003e\u003cp\u003eLabelbox allows you to label every aspect of every turn-by-turn conversation, enabling you to develop fine-grained data sets of annotated real-world conversations.\u003c/p\u003e\u003cp\u003eYou can use these labels to train your agent to retrieve similar past interactions and follow similar response patterns or simply be inspired by them.\u003c/p\u003e\u003ch2 id=\"model-evaluation-ranking-and-selection\"\u003e\u003cstrong\u003eModel evaluation, ranking and selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eYou trained a few models offline with the previously generated data using supervised fine-tuning or RLHF. How do you know which of the models is the best performing, and how often do they make mistakes?\u003c/p\u003e\u003cp\u003eLabelbox allows you to connect these models hosted behind your custom endpoints. Then, with the Live Multimodal Chat editor, labelers can interact with all of them simultaneously by feeding prompts, comparing and contrasting responses, and even rewriting the best ones to be even better.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1vpskixih4\" title=\"Customer Query Resolution Training - Part 2 - LLM Development \u0026amp; Evaluation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 2: Evaluate a single model, generate multiple responses and write your own gold standard response.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eWhen evaluating a single model, Quantumworks Lab allows you to generate multiple responses and also add your own response. This is very useful for use cases where all of the model’s responses are lacking in quality and a human-written response provides the gold standard for further downstream training. Furthermore, in the course of the labeling conversation, you can choose one of these responses to be passed in as prior context into later turns, allowing you to choose the path of the conversation in real-time for better training data generation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uzesocscxk\" title=\"Customer Query Resolution Training - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 3: Evaluate multiple models at once in “chat arena” style.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/secone-one-1-1-1-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"450\" height=\"640\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox also supports evaluating multiple models in a “chat arena” style setup.\u003c/p\u003e\u003cp\u003eSay you have trained candidate models and now want to compare them against each other and also foundation models like GPT 4o. Connect your own models with Quantumworks Lab as \u003cem\u003ecustom\u003c/em\u003e models, and easily choose any latest and greatest foundation model from Foundry within Labelbox. The chat arena excludes information about which response was generated from which model so as not to bias the labeling process.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWith Quantumworks Lab's multimodal chat functionalities, you can generate high-quality datasets to train, evaluate, and test your custom LLMs or pit off-the-shelf LLMs like GPT4o and Gemini 1.5 against each other in a Chatbot Arena-style competition to see which one suits your needs best.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/b3f423vxob\" title=\"Customer Query Resolution Training - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 4: Survey of all LLM capabilities in Quantumworks Lab\u003c/em\u003e\u003c/p\u003e\u003cp\u003eAs the landscape of LLMs evolves rapidly, Quantumworks Lab will continue to build functionality to ensure our customers can align models to their use cases quickly and reliably. With differentiated data, streamlined operations, and transparent orchestration, you can build and refine foundation models that meet your specific needs.\u003c/p\u003e\u003ch2 id=\"evaluate-multimodal-chat-data-today-for-free\"\u003eEvaluate multimodal chat data today for free\u003c/h2\u003e\u003cp\u003eReady to deliver on the next-generation of generative AI?\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eSign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more and we’d love to hear from you.\u003c/p\u003e","comment_id":"668c2f59878a56000112a4e0","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/mmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp","featured":false,"visibility":"public","created_at":"2024-07-08T18:26:33.000+00:00","updated_at":"2024-07-10T16:09:14.000+00:00","published_at":"2024-07-09T16:06:15.000+00:00","custom_excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-multimodal-chat-to-enhance-a-customers-online-support-experience/","excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6656135f9132db0001f20551","uuid":"f173905b-1c7a-4f87-8064-ac8af0ccdaa7","title":"AI foundations: Understanding embeddings","slug":"ai-foundations-understanding-embeddings","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eEmbeddings transform complex data into meaningful vector representations, enabling powerful applications across various domains.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis post covers the theoretical foundations, practical examples, and demonstrates how embeddings enhance key use cases at Labelbox.\u003c/p\u003e\u003cp\u003eWhether you’re new to the concept or looking to deepen your understanding, this guide will provide valuable insights into the world of embeddings and their practical uses.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"what-is-an-embedding\"\u003eWhat is an embedding?\u003c/h1\u003e\u003cp\u003eAt its core, an embedding is a vector representation of information. This information can be of any modality—video, audio, text, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe process of generating embeddings involves a deep learning model trained on specific data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach index in a vector represents a numerical value, often a floating-point value between 0 and 1.\u003c/p\u003e\u003ch2 id=\"the-relationship-between-dimensions-and-detail\"\u003eThe relationship between dimensions and detail\u003c/h2\u003e\u003cp\u003eThe dimensions of a vector describe the level of detail it can capture. Higher dimensionality means higher detail and accuracy in the similarity score.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, a vector with only two dimensions is unlikely to store all relevant information, leading to simpler but less accurate representations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1362\" height=\"626\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1362w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"vector-representations-of-images\"\u003eVector representations of images\u003c/h2\u003e\u003cp\u003eFor instance, consider an image. The underlying compression algorithm converts this image into a vector representation, preserving all relevant information. These embeddings can then be used to determine the similarity between different pieces of data. For example, images of a cat and a dog would have different embeddings, resulting in a low similarity score.\u003c/p\u003e\u003cp\u003eAnother example: Let's consider a model trained on images of helicopters, planes, and humans. If we input an image of a helicopter, the model generates a vector representation reflecting this. If the image contains both a helicopter and humans, the representation changes accordingly. This process helps in understanding how embeddings are generated and used.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1240\" height=\"618\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1240w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Helicopter or not helicopter?\" captured via embeddings.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"importance-and-applications-of-embeddings\"\u003eImportance and applications of embeddings\u003c/h1\u003e\u003ch2 id=\"why-embeddings-matter\"\u003eWhy embeddings matter\u0026nbsp;\u003c/h2\u003e\u003cp\u003eAs we’ve described in the prior sections, understanding what embeddings are and how they work is crucial for several reasons:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCapturing Semantic Relationships\u003c/strong\u003e: Embeddings help capture complex relationships within data, enabling more accurate predictions and recommendations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Search Capabilities\u003c/strong\u003e: Embeddings facilitate efficient and accurate similarity searches, essential in applications like search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"applications-of-embeddings\"\u003eApplications of embeddings\u003c/h2\u003e\u003cp\u003eEmbeddings are extremely useful in search problems, especially within recommender systems.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHere are a few examples:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImage Similarity\u003c/strong\u003e: Finding images similar to an input image, like searching for all images of dogs based on a sample image.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eText-to-Image Search\u003c/strong\u003e: Using a textual query, such as \"image of a dog,\" to find relevant images.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContent Recommendations\u003c/strong\u003e: Identifying articles or movies similar to ones you've enjoyed, enhancing search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1296\" height=\"874\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1296w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEmbeddings are the essential ingredient to a smooth and enjoyable family movie night via recommendations systems, which can suggest movies similar to ones you've enjoyed in the past.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"how-embeddings-are-generated\"\u003eHow embeddings are generated\u003c/h1\u003e\u003cp\u003eDeep learning models, trained on either generalized or specific datasets, learn patterns from data to generate embeddings. Models trained on specific datasets, like cat images, are good at identifying different breeds of cats but not dogs. Generalized models, trained on diverse data, can identify various entities.\u003c/p\u003e\u003cp\u003eModern models produce embeddings of over 1024 dimensions, increasing accuracy and detail. However, this also raises challenges related to the cost of embedding generation, storage, and computational resources.\u003c/p\u003e\u003cp\u003eSome of the earliest work with creating embedding models included word2vec for NLP and convolutional neural networks for computer vision.\u003c/p\u003e\u003ch2 id=\"the-significance-of-word2vec\"\u003eThe significance of word2vec\u003c/h2\u003e\u003cp\u003eThe introduction of word2vec by Mikolov et al. in 2013 marked a significant milestone in creating word embeddings. Word2vec enabled the capture of semantic relationships between words, such as the famous king-queen and man-woman analogy, demonstrating how vectors can represent complex relationships.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis breakthrough revolutionized natural language processing (NLP), enhancing tasks like machine translation, sentiment analysis, and information retrieval.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWord2vec laid the foundation for subsequent embedding techniques and deep learning models in NLP.\u003c/p\u003e\u003ch2 id=\"equivalent-of-word2vec-for-images\"\u003eEquivalent of word2vec for Images\u003c/h2\u003e\u003cp\u003eFor images, convolutional neural networks (CNNs) serve as the equivalent of word2vec. Models like AlexNet, VGG, and ResNet revolutionized image processing by creating effective image embeddings.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese models convert images into high-dimensional vectors, preserving spatial hierarchies and semantic information.\u0026nbsp;\u003c/p\u003e\u003cp\u003eJust as word2vec transformed text data into meaningful vectors, CNNs transform images into embeddings that capture essential features, enabling tasks like object detection, image classification, and visual similarity search.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k_icon.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"250\" height=\"250\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eImageNet is a large-scale visual database designed for use in visual object recognition research, containing millions of labeled images across thousands of categories. Models like AlexNet, VGG, and ResNet demonstrated the power of CNNs using datasets like ImageNet.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"uploading-custom-embeddings-to-labelbox\"\u003eUploading custom embeddings to Quantumworks Lab\u0026nbsp;\u0026nbsp;\u003c/h2\u003e\u003cp\u003eLabelbox now supports importing and exporting custom embeddings seamlessly, allowing for better integration into workflows. This new feature provides flexibility in how embeddings are utilized, making it easier to leverage embeddings for various applications.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eimport Quantumworks Lab as lb\nimport transformers\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nimport requests\nimport numpy as np\n\n# Add your API key\nAPI_KEY = \"your_api_key_here\"\nclient = lb.Client(API_KEY)\n\n# Get images from a Quantumworks Lab dataset\nDATASET_ID = \"your_dataset_id_here\"\ndataset = client.get_dataset(DATASET_ID)\nexport_task = dataset.export_v2()\nexport_task.wait_till_done()\n\ndata_row_urls = [dr_url['data_row']['row_data'] for dr_url in export_task.result]\n\n# Get ResNet-50 from HuggingFace\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = transformers.ResNetModel.from_pretrained(\"microsoft/resnet-50\")\n\nimg_emb = []\n\nfor url in data_row_urls:\n    response = requests.get(url, stream=True)\n    image = Image.open(response.raw).convert('RGB').resize((224, 224))\n    img_hf = image_processor(image, return_tensors=\"pt\")\n    with torch.no_grad():\n        last_layer = model(**img_hf, output_hidden_states=True).last_hidden_state\n        resnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\n        resnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\n        img_emb.append(resnet_embeddings.cpu().numpy())\n\ndata_rows = []\n\nfor url, embedding in zip(data_row_urls, img_emb):\n    data_rows.append({\n        \"row_data\": url,\n        \"embeddings\": [{\"embedding_id\": new_custom_embedding_id, \"vector\": embedding[0].tolist()}]\n    })\n\ndataset = client.create_dataset(name='image_custom_embedding_resnet', iam_integration=None)\ntask = dataset.create_data_rows(data_rows)\nprint(task.errors)\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eTo compute your ow embeddings and send them to Quantumworks Lab, you can use models from Hugging Face. Here’s a brief example using ResNet-50:\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003cscript src=\"https://gist.github.com/mmbazel-labelbox/edb8efbab1904106009e1ed630199e29.js\"\u003e\u003c/script\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eThis code snippet demonstrates how to create and upload custom embeddings, which can then be used for similarity searches within Labelbox.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1694\" height=\"1176\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1694w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"why-you-should-care-about-custom-embeddings\"\u003eWhy you should care about custom embeddings\u003c/h3\u003e\u003cp\u003eCustom embeddings provide several advantages:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTailored Representations\u003c/strong\u003e: Custom embeddings can be tailored to your specific dataset, improving accuracy in domain-specific tasks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Performance\u003c/strong\u003e: They allow for more precise similarity searches and recommendations by capturing nuances specific to your data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFlexibility\u003c/strong\u003e: Custom embeddings offer flexibility in handling various types of data and use cases, making them a versatile tool in machine learning workflows.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"using-embeddings-for-better-search\"\u003eUsing embeddings for better search\u003c/h1\u003e\u003cp\u003eEarlier we mentioned that search (a core activity of search engines, recommendation systems, data curation, etc) is a common and important application of embeddings. How? By comparing how similar two (or many more) items are to each other.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"measuring-similarity\"\u003eMeasuring similarity\u003c/h2\u003e\u003cp\u003eThe most popular algorithm for measuring similarity between two vectors is cosine similarity. It calculates the angle between two vectors: the smaller the angle, the more similar they are. Cosine similarity scores range from 0 to 1, or -1 to 1 for dissimilarity.\u003c/p\u003e\u003cp\u003eFor example, if vectors W and V are close, their cosine similarity might be 0.79. If they are opposite, the similarity is lower, indicating dissimilarity. This method helps in visually and computationally understanding the similarity between different assets.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J0ha8suIAyD2qrFVEWI9TqDRLb2c8ForgqTmyvKQ8ZKsXLeHf5H96V0tQSMV_bCbqBsLWcBb3ljxSP8vKYDKElnlTv1zHq36uQG7mwzyP9HELFlHgGf7FTZO8AWH_ndg5OAXm3yLmbbEpVPg1Cvp108\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"718\" height=\"521\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOriginal source: \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Similarity-between-vectors-can-be-estimated-by-calculating-the-cosine-of-the-angle-th_fig1_333734049?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Similarity between vectors can be estimated by calculating the cosine of the angle θ between them.\"\u003c/span\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBecause embeddings are essentially vectors, we can apply many of the same operations used to analyze vectors to embeddings.\u003c/p\u003e\u003ch3 id=\"strategies-for-similarity-computation\"\u003eStrategies for similarity computation\u003c/h3\u003e\u003cp\u003eAdditional strategies for performing similarity computation include:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eBrute Force Search\u003c/strong\u003e: Comparing one asset against every other asset. This provides the highest accuracy but requires significant computational resources and time. For instance, running brute force searches on a dataset with millions of entries can lead to significant delays and high computational costs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eApproximate Nearest Neighbors (ANN)\u003c/strong\u003e: Comparing one asset against a subset of assets. This method reduces computational resources and latency while maintaining high accuracy. ANN algorithms like locality-sensitive hashing (LSH) and KD-trees provide faster search times, making them suitable for real-time applications, although they may sacrifice some accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHashing Methods\u003c/strong\u003e: Techniques like MinHash and SimHash provide efficient similarity searches by hashing data into compact representations, allowing quick comparison of hash values to estimate similarity. These methods are particularly useful for text and document similarity.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTree-based methods (KD-trees, R-trees, and VP-trees) and graph-based methods (like Hierarchical Navigable Small World, also known as HNSW) are also options.\u003c/p\u003e\u003ch2 id=\"leveraging-embeddings-for-data-curation-management-and-analysis-in-labelbox\"\u003eLeveraging embeddings for data curation, management, and analysis in Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThrough search, embeddings improve the efficiency and accuracy of data curation, management, and analysis in several ways:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eAutomated Data Organization\u003c/strong\u003e: Embeddings help group similar data points together, making it easier to organize and manage large datasets.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Search and Retrieval\u003c/strong\u003e: By transforming data into embeddings, search and retrieval operations become faster and more accurate, enabling quick access to relevant information.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Data Analysis\u003c/strong\u003e: Embeddings capture underlying patterns and relationships within data, facilitating more effective analysis and insights extraction.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAnd these are exactly the ways we use embeddings in our products at Quantumworks Lab to \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimprove search for data\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, in \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e, users can find images similar to an input image, such as searching for basketball players on a court. Our natural language search allows users to input a textual query, like \"dog,\" and find relevant images.\u003c/p\u003e\u003cp\u003eLabelbox offers several features to streamline embedding workflows, including:\u003c/p\u003e\u003ch3 id=\"data-curation-and-management\"\u003eData curation and management\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/custom-embeddings?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCustom Embeddings\u003c/u\u003e\u003c/a\u003e in Catalog: Surfaces high-impact data, with automatic computation for various data types.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/manage-selections?ref=labelbox-guides.ghost.io#smart-select\"\u003e\u003cu\u003eSmart Select\u003c/u\u003e\u003c/a\u003e in Catalog: Curates data using random, ordered, and cluster-based methods.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/cluster-view?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCluster View\u003c/u\u003e\u003c/a\u003e: Discovers similar data rows and identifies labeling or model mistakes.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1620\" height=\"838\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1620w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Cluster view to explore relationships between data rows, identify edge cases, and select rows for pre-labeling or human review.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eNatural Language\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSimilarity Search\u003c/u\u003e\u003c/a\u003e: Supports video and audio assets.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-metrics?ref=labelbox-guides.ghost.io#automatic-metrics\"\u003e\u003cu\u003ePrediction-level Custom Metrics\u003c/u\u003e\u003c/a\u003e: Filters and sorts by custom metrics, enhancing model error analysis.\u003c/li\u003e\u003cli\u003ePre-label Generation from \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFoundry\u003c/u\u003e\u003c/a\u003e Models: Streamlines project workflows.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/video-annotations?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnhanced Video Player\u003c/u\u003e\u003c/a\u003e: Aids in curating, evaluating, and visualizing video data.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1630\" height=\"904\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1630w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWould a rose smell as sweet if it was merely one of a thousand easily discovered in a flower dataset using a natural language search? Trick question: you can't smell photos. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"embeddings-schema\"\u003eEmbeddings schema\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/working-with-features?ref=labelbox-guides.ghost.io#create-features-from-the-schema-tab\"\u003e\u003cu\u003eSchema Tab\u003c/u\u003e\u003c/a\u003e: Includes an Embeddings subtab for viewing and creating custom embeddings.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"api-and-inferencing\"\u003eAPI and inferencing\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/run-foundry-apps?ref=labelbox-guides.ghost.io#run-app-using-rest-api\"\u003e\u003cu\u003eInferencing Endpoints\u003c/u\u003e\u003c/a\u003e: Generate predictions via REST API without creating data rows or datasets.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"additional-workflow-enhancements\"\u003eAdditional workflow enhancements\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBoost Express: Request additional labelers for data projects.\u003c/li\u003e\u003cli\u003eExport v2 Workflows and Improved SDK: Streamlines data management.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese features ensure that embeddings are utilized effectively, making your workflows more efficient and your models more accurate.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eEmbeddings are a powerful tool in machine learning, enabling efficient and accurate similarity searches and recommendations. We hope this post has clarified what embeddings are and how they can be utilized.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor more on uploading custom embeddings, check out our \u003ca href=\"https://docs.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/page/tutorials?ref=labelbox-guides.ghost.io\"\u003edeveloper guides\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAdditional resources on embeddings can be found here:\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jbwzrxh814\" title=\"Embeddings Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWatch the video: \u003ca href=\"https://labelbox.wistia.com/medias/jbwzrxh814?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eUnderstanding embeddings in machine learning\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eLearn more in our community about:\u003cu\u003e \u003c/u\u003e\u003ca href=\"https://community.labelbox.com/t/how-to-upload-custom-embedding-s-and-why-you-should-care/1169?ref=labelbox-guides.ghost.io#why-would-i-need-embedding-2\"\u003e\u003cu\u003eHow to upload custom embeddings and why you should care\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThanks to our contributors Tomislav Peharda, Paul Tancre, and Mikiko Bazeley! \u003c/p\u003e","comment_id":"6656135f9132db0001f20551","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/05/YT-Thumbnails--23-.jpg","featured":false,"visibility":"public","created_at":"2024-05-28T17:24:47.000+00:00","updated_at":"2024-09-04T18:02:27.000+00:00","published_at":"2024-05-28T20:32:29.000+00:00","custom_excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},"url":"https://labelbox-guides.ghost.io/ai-foundations-understanding-embeddings/","excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65ab1fd23c5a9d00013a705f","uuid":"612cb839-1267-4900-89ed-9362aacf55a4","title":"How to accelerate labeling projects using GPT–4V in Foundry","slug":"how-to-accelerate-labeling-projects","html":"\u003cp\u003eWorking closely with hundreds of companies at the forefront of AI, we are seeing a growing interest from teams wanting to use foundation models to pre-label data before combining a human-in-the-loop workflow to inject their unique domain expertise and automate specific tasks that have been previously very time-consuming or manually intensive.\u003c/p\u003e\u003cp\u003eIn this post, we’ll walk through 3 specific examples of how teams can use Quantumworks Lab to accelerate labeling projects by using GPT-4V to create high-quality labels for various data types, including images, HTML, and text.\u0026nbsp;\u003c/p\u003e\u003cp\u003eGenerating high-quality datasets is often one of the most tedious parts of the development process for ML teams. By using Quantumworks Lab Foundry, ML teams can now quickly use LLMs to their advantage to pre-label or enrich data that span a wide range of use cases, such as identifying amenities for rental listings, classifying items in a product catalog, or categorizing support messages.\u003c/p\u003e\u003ch2 id=\"pre-labeling-use-case-1-classifying-amenities\"\u003e\u003cstrong\u003ePre-labeling Use Case #1: Classifying amenities \u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eOnline travel marketplaces such as Expedia, Booking.com, Airbnb, VRBO, etc.,  often show product listings that have a main image with additional images as supplements. These travel marketplaces can enhance their user experience and conversion rates by enriching objects and desired characteristics in product listings to give users more context about visual assets.\u003c/p\u003e\u003cp\u003eAs an example of this in action, we’ve seen ML teams upload primary and supporting images as a single entity, referred to as a data row. A data row can be considered as a task that a human or AI can do. Afterwards, the ML team will tap into\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFoundry\u003c/u\u003e\u003c/a\u003e, which helps automate labeling tasks.\u003c/p\u003e\u003cp\u003eThe example below focuses on identifying various amenities in a rental listing.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Attachment-preview-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"910\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Attachment-preview-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Attachment-preview-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Attachment-preview-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Attachment-preview-.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe main listing image is accompanied by supporting images.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003e\u003cstrong\u003eStep 1: Select images and choose a foundation model of interest\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/select-image-and-model-1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"960\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/select-image-and-model-1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/select-image-and-model-1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/select-image-and-model-1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/select-image-and-model-1.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWorkflow of selecting the images and model of choice.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data, users can use Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eCatalog\u003c/a\u003e filters, including media attributes, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eUsers can then click “Predict with Model Foundry” once the data of interest has appeared.\u003c/li\u003e\u003cli\u003eUsers will then be prompted to choose a model of interest for a model run.\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task - such as image classification, object detection, and image captioning.\u003c/li\u003e\u003cli\u003eTo locate a specific model, users can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine-learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003e\u003cstrong\u003eStep 2: Configure model settings and submit a model run\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/oX69xeF-beq-3wT3y_58xEdqAUefnmBzU55avrqbOc12PRWizA28ATCxCmmTh_KixA7CunMbpkYZCgeMizjFSkKh4IoKCR6bcEPTihnevrneo2HSZ3CW9dTZLQekxw5GjwEe3TO6lNlV-FCA3YFuoIeGzavyQNx3bCMX3vZ0H1Ys4p7yCl6Pt-SVYgqCpQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"800\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWorkflow of selecting the ontology, prompt and generating a preview of the prediction.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eOnce the model of interest is selected, users can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of settings, which can be found in the Advanced model setting. For this use case, the only Foundry model setting I changed was to select “use_image_attachments” to pass the supporting images to GPT-4V.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can then generate preview predictions on up to five data rows to understand how the current model settings affect the final predictions.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows users to confirm the configuration settings confidently:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf users are unhappy with the generated preview predictions, they can edit the model settings and continue to generate preview predictions until they're satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eAfter users are satisfied with the preview predictions, a model run can be submitted.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Image-airbnb-end-result-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"720\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Image-airbnb-end-result-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Image-airbnb-end-result-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Image-airbnb-end-result-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Image-airbnb-end-result-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eIn addition to the obvious amenities, GPT-4v identified the subtle amenities like heater, lakeview, mountain View, and stove. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003ePrompt used for Amenities classification: \u003c/p\u003e\u003cul\u003e\u003cli\u003eYou are a helpful assistant. What amenities are in the images? Respond with the following options. [Kitchen, TV, Heater, Stove, Hot tub, Skis, Lake view, Refrigerator, Microwave, Mountain view, Shower]. Return the result as a dictionary. {“Amenities” : [“\u0026lt;prediction\u0026gt;”]}\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-send-the-images-to-annotate\"\u003e\u003cstrong\u003eStep 3: Send the images to Annotate\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eUsers can transfer the results to a labeling project using the UI via the \"Send to Annotate\" feature. Labelers can then quickly review labels for accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/kZlndAKlXfBsSFIF68sr8Q34BTRIUFIuLnWQRm88a32mquEuqIwzVVFkgYPIyiyRDxdqK8dGAQAhvcjuxSuvlZSV2No5ZpL0-mNymUjeKqiEioiN2MTRlOK0q_dsFoLPb49Vh6m6GCvmaKPPtVXPg1vT4TmZcX45Wy2NQQAmFdRwOhjFPkdIsgUQez-mlg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"795\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUI for manually reviewing the labels created by LLMs. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"pre-labeling-use-case-2-recommendation-engine\"\u003e\u003cbr\u003e\u003cstrong\u003ePre-labeling Use Case #2: Recommendation engine \u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eE-commerce companies such as Amazon, Wayfair, Etsy, and eBay offer a \"similar products\" feature that improves the discoverability of products and provides an easy way to compare items, thereby increasing customer satisfaction and reducing return rates.\u003c/p\u003e\u003cp\u003eA recommendation engine often powers the product similarity feature that requires integrating text and images into a unified file. ML teams can use Quantumworks Lab to help automate this workflow as we support HTML files that state-of-the-art models can label or enrich. For the HTML product similarity task, the initial steps 1-2 remain the same, but the prompt and ontology will be adjusted to focus on classifying whether the products are identical, and GPT-4V will provide reasoning.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/side-by-side.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"725\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/side-by-side.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/side-by-side.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/side-by-side.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/side-by-side.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe see that GPT-4v predicted the products to not be the same and provided an accurate explanation for the answer. GPT-4v also classified it correctly to be a bottle type. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003ePrompt used for recommendation engine: \u003c/p\u003e\u003cul\u003e\u003cli\u003eYou are a helpful assistant. Based on the descriptions and images provided for the two products, determine whether the products being described are the same.\u003c/li\u003e\u003cli\u003eClassify Is the product the same? pick one of the options: [yes, no].\u0026nbsp;\u003c/li\u003e\u003cli\u003eClassify Item type as the following: [Food, Perishable, Liquid, Bottle, Nonperishable].\u003c/li\u003e\u003cli\u003eAnswer Explanation with why yes or no.\u0026nbsp;\u003c/li\u003e\u003cli\u003eReturn the result as a JSON. {\"Is the product the same?\" : \"\u0026lt;prediction\u0026gt;\", \"Item type\" : [\"\u0026lt;prediction\u0026gt;\"], \"Explanation\", \"\u0026lt;prediction\u0026gt;\"}\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"pre-labeling-use-case-3-support-chat-classification\"\u003e\u003cstrong\u003ePre-labeling Use Case #3: Support chat classification\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eEnterprises with a significant user base require a full-fledged customer support team to ensure smooth operations. Providing efficient customer support is typically achieved by efficiently categorizing and triaging high volumes of real-time support messages.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, support teams often have to manually classify tickets, which can be time-consuming and prone to human error. This process is necessary to sort the messages into the correct reporting categories and identify which engineering teams should handle which bugs.\u003c/p\u003e\u003cp\u003eBy using advanced large language models (LLMs) such as GPT-4V, customer chat intent classification can be automated, and then labelers can review and edit the labels if needed. \u003c/p\u003e\u003cp\u003eThe following prompt was used to classify customer messages, and default Foundry model settings were used. If GPT-4V fails to produce an expected answer, users can add an if statement to capture the edge cases, as shown below.\u003c/p\u003e\u003cul\u003e\u003cli\u003eYou are a helpful support assistant. Read the following text and classify them to the information below.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf the tier is Enterprise and urgency is Critical then Priority is Priority 0.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf the the ticket is related to feature request and Tier is Enterprise then Priority is P2. Otherwise all feature request tickets are P4.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf the tier is Free and urgency is low, then Priority is Priority 4.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf description is related to python or coding then engineering team is SDK.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf description is related to labeling editor then engineering team is Perception.\u003c/li\u003e\u003cli\u003eIf description is related to log in issues or app crashes\u0026nbsp; then engineering team is Platform. \u0026nbsp;\u003c/li\u003e\u003cli\u003eIf urgency is critical and tier is not enterprise then priority should be Priority 2.\u003c/li\u003e\u003cli\u003eIf description is stating to support a feature then that is a feature request.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eClassify Message intent, pick one of the options: [Accidental payment, Unauthorized payment, Irrelevant, Unable to login, Reset password, Cancel subscription, App bug, Feature Request]. Classify Priority, pick one of the options: [Priority 1, Priority 2, Priority 3, Priority 4, Priority 0]. Classify Engineering team, pick one of the options: [SDK, Perception, Platform].\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1994\" height=\"676\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 1994w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBased on the prompt, GPT4-V correctly classified this as P0, Platform team and App bug.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"678\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe prompt mentioned to classify feature support requests as \"Feature Request\" and set the priority to 2 due to enterprise tier.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"676\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eGPT-4V initially classified this as P4. To correct it, \"If urgency is critical and tier is not enterprise, then the priority should be Priority 2\" was added to the prompt.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"678\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eGPT-4V correctly used the prompt to classify the text based on the if statements. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"additional-considerations\"\u003e\u003cstrong\u003eAdditional Considerations\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAdditional considerations as users incorporate Foundry labels into their projects and workflows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIncorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman evaluators\u003c/u\u003e\u003c/a\u003e to find edge cases and updating the prompt would give the best results as users create more labels with Foundry. For example, if statements were added in the chat classification use case to account for edge cases.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can also \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA/B test different models \u003c/u\u003e\u003c/a\u003efrom Foundry to find the model that best fits the use case using Quantumworks Lab Model.\u003c/li\u003e\u003cli\u003eIn addition to GPT-4V, users can utilize various state-of-the-art models like Gemini 1.5 Pro, Claude 3 Opus, and more from Quantumworks Lab Foundry, as seen \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eIf we do not currently support your use case or any questions arise, feel free to contact our \u003ca href=\"https://docs.labelbox.com/docs/contacting-customer-support?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esupport team\u003c/u\u003e\u003c/a\u003e, as we would love to hear your feedback to improve Foundry.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003cbr\u003e\u003c/h2\u003e\u003cp\u003eLarge Language Models (LLMs) and generative AI are showing an enormous impact on the individual productivity of knowledge workers. As these large foundation models improve, we’ll continue to see impressive real-world use cases for automating pre-labeling more quickly and cheaply. Completing labeling projects with Quantumworks Lab Foundry combines both AI-assistance and human-in-the-loop workflows to automate one or more specific tasks. \u003c/p\u003e\u003cp\u003eCheck out our additional resources on how to utilize state-of-the-art AI models in Foundry, including using model distillation and fine-tuning to leverage the power of foundation models:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-build-generative-captioning-using-foundation-models-for-product-listings/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eHow to build generative captioning and enrich product listings faster with foundation models\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eHow to build a powerful product recommendation system for retail\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eIntroduction to model distillation\u0026nbsp;\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://event.on24.com/wcc/r/4460171/5D5357846BEAB1FD1E12A5ED146DE6E1?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAutomated Labeling With Any Foundation Model\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eFine tune LLMs\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"65ab1fd23c5a9d00013a705f","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/03/thumbnail--5-.png","featured":false,"visibility":"public","created_at":"2024-01-20T01:20:18.000+00:00","updated_at":"2024-03-27T15:19:55.000+00:00","published_at":"2024-03-26T06:03:00.000+00:00","custom_excerpt":"Learn 3 specific examples of how teams can use Quantumworks Lab to accelerate labeling projects by using multimodal models to create high-quality labels for various data types including images, HTML, and text. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-accelerate-labeling-projects/","excerpt":"Learn 3 specific examples of how teams can use Quantumworks Lab to accelerate labeling projects by using multimodal models to create high-quality labels for various data types including images, HTML, and text. ","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65523b444a21b900018ee9ba","uuid":"4e760591-7da7-43c2-af80-554fdb2ed601","title":"Detecting swimming pools with GPT4 Visual","slug":"detecting-swimming-pools-with-gpt4-visual","html":"\u003cp\u003eThe rise of off-the-shelf and foundation models has enabled AI teams to fine-tune existing models and pre-label data faster and more accurately — in short, significantly accelerating AI development. However, using these models at scale for building AI can quickly become expensive. One way to mitigate these costs and reduce waste is to add a model comparison process to your workflow, ensuring that any model you choose to integrate for AI development is the best choice for your requirements.\u003c/p\u003e\u003cp\u003eA comprehensive model comparison process evaluates models on various metrics, such as performance, robustness, and business fit. The results will enable teams to quickly kickstart model development, decrease time to value, and ensure the best results with less time and costs for their specific use case.\u003c/p\u003e\u003cp\u003eEmbedding model comparison into the AI development workflow, however, comes with its own unique challenges, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConfidently assessing the potential and limitations of pre-trained models\u003c/li\u003e\u003cli\u003eVisualizing models’ performance for comparison\u003c/li\u003e\u003cli\u003eEffectively sharing experiment results\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this blog post, we’ll explore how you can tackle these challenges for a computer vision use case with \u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003ethe Foundry add-on for Quantumworks Lab Model\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"why-is-selecting-the-right-model-critical\"\u003eWhy is selecting the right model critical?\u0026nbsp;\u003c/h3\u003e\u003cp\u003eSelecting the most suitable off-the-shelf model is pivotal for ensuring accurate and reliable predictions tailored to your specific business use case, often leading to accelerated AI development. As different models exhibit diverse performance characteristics, diligently comparing the models’ predictions on your data can help distinguish which model excels in metrics such as accuracy, precision, recall, and more. This systematic approach to model evaluation and comparison enables you to refine the model’s performance with a “store of record” for future reference to continuously improve model performance.\u003c/p\u003e\u003cp\u003eChoosing the best off-the-shelf model provides a quick and efficient pathway to production, ensuring that the model aligns well with the business objectives. This alignment is crucial for the model's immediate performance and sets the stage for future improvements and adaptability to evolving requirements. The most suitable model for your use case also enables you to reduce the time and money spent on labeling a project. For instance, when pre-labels generated by a high-performing model are sent for annotation, less editing is required, making the labeling project quicker and more cost-effective. This is due to better Intersection Over Union (IOU) for tasks like Bounding Box, resulting in higher quality pre-labels and, therefore, fewer corrections. Furthermore, utilizing the best model can make your trove of data more queryable by enriching your data, thereby enhancing its searchability.\u003c/p\u003e\u003ch2 id=\"comparing-vision-llms-models-using-labelbox-model-foundry\"\u003eComparing vision LLMs models using Quantumworks Lab Model Foundry\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/h2VyJaeRIswlqFJEn0IhiQumboC5OqF_mOj8KjCCcFT5nvztUALIucquzxtsYtX47PvvhHXtWP1_xDr2dllNebQf-FCFxu9YLQr184snn_y-SiKVUyB9ONmkttllzI1p6htDg_Nny5GDCCjFg7AF3RA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"239\"\u003e\u003c/figure\u003e\u003cp\u003eThis flow chart provides a high-level overview of the model comparison process when using Quantumworks Lab Model Foundry.\u003c/p\u003e\u003cp\u003eWith the Foundry add-on for Quantumworks Lab Model, you can evaluate a range of models for computer vision tasks to select the best model to perform pre-labeling or data enrichment on your data.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003e\u003cstrong\u003eStep 1: Select images and choose a foundation model of interest\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/L1IEgc57cHhpmPUlxyw7kqGv_RZoYOEII6Vr2iBfYUMe6iDFWViUlmHpXHWM5yfy8ZLj3WN4wZjrMUSTpPF5R5Ra17eeT7ggEoukjvqeh7rJ-vsVWiMWASsURgs-8_uxvuk2rkBN3arCrJB0RSZa9Cs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the images on which the predictions should be made on.\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “Predict with Model Foundry”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run.\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task - such as image classification, object detection, and image captioning.\u003c/li\u003e\u003cli\u003eTo locate a specific model, you can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-hyperparameters-and-submit-a-model-run\"\u003eStep 2: \u003cstrong\u003eConfigure model hyperparameters and submit a model run\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/NnyPrWdiuwQmxl8y1IMTUWEFGZ2pdyoS4RmKme8-S9hoxvVjZscwQwAN03TMizRD8M70z1CvySOFEShP_4KXZpUjC3Z7cg6peF-T-ceKxlSi5ys432EP_AMPe1NQi8ip0-VHV1FLMT-0n0rmJS5XKxc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve located a specific model of interest, you can click on the model to view and set the model and ontology settings or prompt. In this case, we will enter the following prompt “Is there an entire swimming pool clearly visible in this image? Reply with yes or no only, without period.” This prompt is designed to facilitate responses from the model in a simple 'Yes' or 'No' format.\"\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of hyperparameters, which you can find in the Advanced model setting.\u003c/li\u003e\u003cli\u003eTo get an idea of how your current model settings affect the final predictions, you can generate preview predictions on up to five data rows.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-predictions-will-appear-in-the-model-tab\"\u003eStep 3: Predictions will appear in the Model tab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/image.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/11/image.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/11/image.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eEach model run is submitted with a unique name, allowing you to distinguish between each subsequent model run\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWhen the model run completes, you can:\u003cul\u003e\u003cli\u003eView prediction results\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results across a variety of model runs different models\u003c/li\u003e\u003cli\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-repeat-steps-1-5-for-another-model-from-model-foundry\"\u003eStep 4: Repeat steps 1-5 for another model from Model Foundry\u003c/h3\u003e\u003cul\u003e\u003cli\u003eYou can repeat steps 1-5 with a different model, on the same data and for the same desired machine learning task, to evaluate and compare model performance\u003c/li\u003e\u003cli\u003eBy comparing the predictions and outputs from different models, you can assess and determine which one would be the most valuable in helping automate your data labeling tasks\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-5-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 5: Create a model run with predictions and ground truth\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"898\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo create a model run with model predictions and ground truth, users currently have to use a \u003ca href=\"https://colab.research.google.com/drive/1f0WkWtsDh2LYiy35pc3JF16srj6SxeTA?ref=labelbox-guides.ghost.io#scrollTo=W8tE-H7VmKea\"\u003escript\u003c/a\u003e to import the predictions from the Foundry add-on for Quantumworks Lab Model and ground truth from a project into a new model run. \u003c/p\u003e\u003cp\u003eIn the near future, this will be possible via the UI, and the script will be optional.\u003c/p\u003e\u003ch3 id=\"step-6-evaluate-predictions-from-different-models-from-model-foundry-in-labelbox-model\"\u003eStep 6: Evaluate predictions from different models from Model foundry in Quantumworks Lab Model\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003cp\u003eAfter running the notebook, you'll be able to visually compare model predictions between two models.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-7-send-model-predictions-as-pre-labels-to-a-labeling-project-in-annotate\"\u003eStep 7: Send model predictions as pre-labels to a labeling project in Annotate\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/MZHoMXGtxeXRYWIH41mENw-0Uz26ydqJBOkE-wtsY-1BYzGdyABhi5UXN6s5QUxrkx-ZHzwuhirnUDB4tNrDrSOEVyvlI7aHcCDMI7zekND5QRDEkBh48DfbxHrzgtpHGKxsRSVcxz6W8AikGbgZRdg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"349\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eSelect the best performing model and leverage the model predictions as pre-labels.\u003c/li\u003e\u003cli\u003eRather than manually labeling data rows, select and send a subset of data to your labeling project with pre-labels to automate the process.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"model-comparison-in-practice\"\u003eModel comparison in-practice:\u003c/h2\u003e\u003ch3 id=\"llava-vs-gpt-4v\"\u003eLLaVA vs GPT-4V\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"893\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eMetrics view \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn this example, LLaVA has 25 true positives, while GPT-4V has 30. Additionally, LLaVA has 10 more false positives. Therefore, GPT-4V is the superior model.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLet’s now take a look at how the predictions appear on the images below:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Jh89URRISL3Qy_D8JCfPEN2KiksoCYJGSaLEulXsh-ZzztLaKoiOEPaGMdatag8YGCCPFrGTP6E2GCY4380KliNWyPXkAlPq-GzP9l0wr44cT81MG4xq2_fUm72YsS1g8KiNnpAXYpv22pLp-RvDxe0\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/4xlsujbFfyNcvACvwcAXIYVeL5E13FYt0RZC0EtsGdkVbrU_542h-EjEr3vGGcnpDManAjF6YcbaWBqaxHoCqOd9Pe31vhHGkGWyK11HGjBHy74iJt3PeuaMoY4p68RbeStGWzGEY3_bPwVge257les\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/zewKkLUtDyBrb0nu6Ci_LgW_mkCPcg6s_TIMFUHwMzvudwilvTP19XWwevbdrCbXx2nKNIWwsNe1ZrH05WN0St36HcnkJ-2mPxBXxeiCEWaVlhVJvxlWF2ptxlPvyycNpg-nlLDe5b8lMI1xdQMQsDw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cp\u003eIn the examples above, GPT-4V and LLaVA both correctly predicted there to be a swimming pool.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/DLBp8vaTB5ixPdI7mvxOkPM5eWQKzwfyrWHZnTIRnUqO4uycb8wPSdwWrqLzgR6Yp2rTqgIXvc9uBLPLuOwmW8hlWKVdcnelWGVuF4ajkV_bfvp064NWuNMTsJ4Tj_Kbgup4qmgFtuRtMKUXWk3Okt4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"345\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Qxzm1UsZc6osUix_3cmT9AQVhWVll5FOVe2_Svd1ArO7pZj0pxQFbAk_dsUF57rnhtO_iMYpBNGYBWMRwvo4Mw9BxhyvE_wXvbbrbA5sEiXsOXZ6j7dRCryJWDZ4NL4IcaC3mD4rblY57A1P7umfuiU\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"345\"\u003e\u003c/figure\u003e\u003cp\u003eIn the two instances above, GPT-4V correctly identified the presence of a swimming pool, while LLaVA incorrectly predicted its absence. It's possible that LLaVA was misled by the shadows over the pool.\u003c/p\u003e\u003ch3 id=\"imagen-vs-gpt-4v\"\u003eImagen vs GPT-4V\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/11/image-1.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/11/image-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eMetrics view \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eImagen has 6 more true positives and 1 less false positive than GPT-4V, so Imagen is the best model in this test of 40 images based on the metrics.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/tCGt5rcYTHcjW0fkwL2ay4J-jkXni3TDU12eUjVOTwg7X4wzieQ3zodx3gTI02pE2YZsi6R9bcBhslM_69Ixb0T3zDJsY00ISbyNYL4aI0RmusF6pxVdxucBUDVaRWfl400cTYfQEwRlW6cqU7zDFlw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"335\"\u003e\u003c/figure\u003e\u003cp\u003eIn the above example, GPT-4V and Imagen both correctly predicted there to be a swimming pool.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Rl9GXiF4XURZlNr5QreUz7fYaVr9sKrAEv94XI7ciYRGjvCPAxajuBOsmmMb2EoOO2Vq3_aTGyC-_iPniCuCgL7w4YAjKJ8zp6zRm866nNjojMeigw4sMVrwSBb1-oxuXvyGPc3CfsLbyApnidR1S1w\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"335\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Kd4aif9Oa08qnFziD5dr8a3L96H0bEsMeuxsqeMqnjAPpyP9OxqNoemqpxiTYSeVt504k-SVMyTqGr5ZL83fT2ra2FwWsWC2pbC2i27nGhSclbgXFBoJ0wEGl0k_V7W7FZdtAgrouHsTx1Xd_kmlzv8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"345\"\u003e\u003c/figure\u003e\u003cp\u003eIn the two instances above, GPT-4V and Imagen both correctly predicted there to be no obvious swimming pool.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u0026nbsp;Send the predictions as pre-labels to Quantumworks Lab Annotate for labeling\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eSince we've evaluated that Imagen is the best model from our test, we can send model predictions as pre-labels to our labeling project by highlighting all data rows and selecting \"Send to annotate\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/MZHoMXGtxeXRYWIH41mENw-0Uz26ydqJBOkE-wtsY-1BYzGdyABhi5UXN6s5QUxrkx-ZHzwuhirnUDB4tNrDrSOEVyvlI7aHcCDMI7zekND5QRDEkBh48DfbxHrzgtpHGKxsRSVcxz6W8AikGbgZRdg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"349\"\u003e\u003c/figure\u003e\u003cp\u003eIn conclusion, you can leverage Model Foundry to not only select the most appropriate model to accelerate data labeling, but to automate data labeling workflows. Use quantitative and qualitative analysis, along with model metrics, to surface the strengths and limitations of each model and select the best performing model for your use case. Doing so can help reveal detailed insights, such as seen in the above comparison between Imagen and GPT-4V. In the above model comparison example, we can see that Imagen emerged as the superior model that will allow us to rapidly automate data tasks for our given use case.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003eModel Foundry\u003c/a\u003e streamlines the process of comparing model predictions, ensuring teams are leveraging the most optimal model for data enrichment and automation tasks. With the right model, teams can easily create pre-labels in Quantumworks Lab Annotate – rather than starting from scratch, teams can boost labeling productivity by correcting the existing pre-labels.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"65523b444a21b900018ee9ba","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-3060--1-.png","featured":false,"visibility":"public","created_at":"2023-11-13T15:05:40.000+00:00","updated_at":"2023-12-06T22:27:12.000+00:00","published_at":"2023-11-13T15:11:32.000+00:00","custom_excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart LLM development, decreasing costs and accelerating time-to-value. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},"url":"https://labelbox-guides.ghost.io/detecting-swimming-pools-with-gpt4-visual/","excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart LLM development, decreasing costs and accelerating time-to-value. ","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"654bbab4016f5100016579c3","uuid":"6c33e4d1-fcaa-44de-b994-0fe65fce3dcc","title":"How to analyze customer reviews and improve customer care with NLP","slug":"how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp","html":"\u003cp\u003eCustomer reviews have become a critical tool for businesses looking to improve their products, services, and customer satisfaction. In today’s digital world, review sites like Yelp and social media make it easier than ever for customers to share their experiences with the world. Customer care can range in the services and support that businesses provide to their customers before, during, and after purchase. Great customer care can create positive brand experiences that lead to greater loyalty and customer satisfaction. In the ever-evolving world of retail, it also helps keep your business competitive and at the forefront of your customer’s sentiment and desires.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile companies now have access to a wealth of customer feedback data, sifting through all of these reviews can be incredibly time-consuming and manual. By leveraging AI, teams can analyze \u003ca href=\"https://birdeye.com/blog/review-management/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ecustomer reviews and feedback\u003c/a\u003e at scale, to gain insights into common review topics or customer sentiment. This allows businesses to identify common themes and pinpoint areas of improvement to enhance\u0026nbsp; the customer experience.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for customer care. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eImproving customer care requires a vast amount of data in the form of customer reviews. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape: \u003c/strong\u003eThe changing nature and format of customer review data from multiple sources (e.g webpages, apps, social media, etc.) poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their customer care through advanced natural language processing. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s assisted data enrichment and flexible training frameworks to quickly build AI systems that uncover actionable insights from customer reviews. Tackle unique customer care challenges with AI-driven insights to create more thoughtful and strategic customer interactions. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1444\" height=\"784\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 1444w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build an NLP model to improve customer care. Specifically, this guide will walk through how you can explore and better understand review topics and classify review sentiment to make more data-driven business decisions around customer care initiatives.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-accelerate-and-train-an-nlp-model-to-improve-customer-care\"\u003eSee it in action: How to accelerate and train an NLP model to improve customer care\u0026nbsp;\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below in either: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e or \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs customer reviews and feedback across channels proliferate, brands want to learn from customer feedback to foster positive experiences. For this use case, we’ll be working with a dataset of customer hotel reviews – with the goal of analyzing the reviews to demonstrate how a hospitality company could gain insight into how their customers feel about the quality of service they receive.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/q4dqjyg9xf\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"498\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003eFor the purpose of this tutorial, we’ve provided a sample open-source \u003ca href=\"https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe?ref=labelbox-guides.ghost.io\"\u003eKaggle dataset\u003c/a\u003e that can be downloaded.  \u003c/p\u003e\u003cp\u003ePlease \u003ca href=\"https://drive.google.com/file/d/1hh2MYzol6-4PSsqX7mbX2YSLLGA4jAYZ/view?usp=drivesdk\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edownload the dataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your text data as a CSV. \u003c/li\u003e\u003cli\u003eIf your text snippets sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your text data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xn3sj0uc8j\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExplore topics of interest\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith your data in Quantumworks Lab, you can begin to leverage Catalog to uncover interesting topics to get a sense of what customers are talking about from hotel reviews.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize your data –\u0026nbsp; you can click through individual data rows to get a sense for what customers are writing reviews on\u0026nbsp;\u003c/li\u003e\u003cli\u003eDrill into specific topics of interest\u003cstrong\u003e \u003c/strong\u003e– leverage a natural language search, for example searching “interior design,” to bring up all related reviews related to interior design. You can adjust the confidence threshold of your searches accordingly (this can be helpful in gauging the volume of data related to the topic of interest)\u0026nbsp;\u003c/li\u003e\u003cli\u003eBegin to surface subtopics or trends within your initial search – for example is the interior design review related to the style of design, attention to detail, or the type of environment created from the interior design\u003c/li\u003e\u003cli\u003eEasily find all instances of similar examples to data of interest\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate and save data slices\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf you have a search query that you’re interested in saving or reusing in the future, you can save it as \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003ea slice\u003c/a\u003e. You can construct a slice by using one or more filters to curate a collection of data rows. Users often combine filters to surface high-impact data and then save the results as a slice.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this example, we’ve surfaced reviews on the topic of breakfast that all talk about the value and price of the hotel’s breakfast. We can save this as a slice for future reference (“Breakfast_value”) and as we ingest more data that matches the slice’s criteria, they will automatically get filed into the slice.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-an-ontology\"\u003eCreate an ontology\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can create our ontology. \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eOntologies\u003c/a\u003e can be reused across different projects and they are required for data labeling, model training, and evaluation.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gdczmynqjt\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a new ontology:\u003c/p\u003e\u003cp\u003e1) Navigate to the ‘Schema’ tab\u003c/p\u003e\u003cp\u003e2) Hit ‘Create new ontology’\u003c/p\u003e\u003cp\u003e3) Select the media type that you wish to work with – for this use case ‘Text’\u003c/p\u003e\u003cp\u003e4) Give your ontology a name\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Add objects and classifications based on you use case\u003c/p\u003e\u003cp\u003e6) Objects are named entities\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003ePerson’s name\u0026nbsp;\u003c/li\u003e\u003cli\u003eLocation\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003e7) Classifications\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eReview sentiment such as positive or negative (radio)\u0026nbsp;\u003c/li\u003e\u003cli\u003eReview topics such as breakfast, dinner, location, staff, interior design (checklist)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAdd sub-classifications as desired\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003e8) Save and create your ontology\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter creating an ontology, you can begin labeling your data to fine-tune or train a model.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"label-data-of-interest\"\u003eLabel data of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003eWith Quantumworks Lab, you can label your data in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e: leverage our global network of\u0026nbsp; specialized labelers for a variety of tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWorkforce Boost provides a collaborative platform for labeling services in a self-serve manner — this is great for teams that don’t have the technical expertise to build a machine learning system yet are looking for an easy-to-use technology to get a quick turnaround on quality training data. You can learn more about our Boost offerings \u003ca href=\"https://docs.labelbox.com/docs/using-data-labeling-service?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create pre-labels with foundation models\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn addition to creating pre-labels for classification projects, you have the ability to send model predictions as pre-labels to your labeling project. This can be done in one of two ways:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel-assisted labeling\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eImport computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel Foundry\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eAutomate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePre-label data with Model Foundry\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModel Foundry\u003c/a\u003e acts as the copilot to create your training data –\u0026nbsp; instead of going into unstructured text datasets blindly, you can use pre-existing LLMs to pre-label data or pre-tag parts of it, reducing manual labeling efforts and cost.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9zspjgoau7\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1) Select data you wish to label in Catalog\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Hit \"Predict with Model Foundry\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Choose a foundation model\u003c/p\u003e\u003cul\u003e\u003cli\u003eYou can select a foundation model based on your use case to have the model take a first pass at labeling your data\u003c/li\u003e\u003cli\u003eThese pre-labels can be verified with human-in-the-loop review in Quantumworks Lab Annotate\u003c/li\u003e\u003cli\u003eFor this use case, we’ve selected the GPT-4 model\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e4) Configure the model’s settings\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect the previously created ontology in the earlier part of the tutorial\u0026nbsp;\u003c/li\u003e\u003cli\u003eLabelbox will auto-generate a prompt based on your ontology and use case – in this case we wish to classify the sentiment (positive or negative) and classify a topic with one or more options (breakfast, dinner, location, staff, room, facilities, value for money, or interior design)\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e5) Generate preview predictions\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore submitting the model run, you can generate prediction previews to understand how the model will perform\u003c/li\u003e\u003cli\u003eIt is recommended that you preview some predictions to confirm the model parameters are configured as desired\u003c/li\u003e\u003cli\u003eBased on the preview, you can then make any adjustments to the settings or choose to submit the model run as-is\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e6) Name and submit the model run\u0026nbsp;\u003c/p\u003e\u003cp\u003e7) View the model run in the Model tab to explore results\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce your model run is complete, you navigate to the Model tab\u003c/li\u003e\u003cli\u003eExplore the model’s results and click into each data row to dig deeper into the model’s predictions\u003c/li\u003e\u003cli\u003eFor this example, we can see that there are instances where GPT-4 has correctly tagged named entities and identified sentiment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve evaluated and are satisfied with GPT-4’s predictions, you can send them to a labeling project in Quantumworks Lab Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAdd a batch to a labeling project as pre-labels\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eBefore you can send these model predictions to a labeling project as pre-labels, you need to create a labeling project. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7zgl76n76w\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eCreate a new labeling project\u003c/em\u003e\u003c/p\u003e\u003cp\u003e1) Navigate to the Annotate tab\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Create a ‘New project’\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Select the project type – in this case we want to create a ‘Text’ project\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Name your project\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Attach your model’s ontology (created in a previous step)\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce you’ve created your labeling project and configured the ontology, head back to the Model tab to send your batch of data with pre-labels to that labeling project.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Highlight all data rows of interest\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Select ‘Manage selection’ \u0026gt; ‘Add batch to project’\u003c/p\u003e\u003cp\u003e3) Select the appropriate project that you created in the above step\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) You can give the batch a priority (from 1-5)\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Select the appropriate model run of the predictions you wish to send\u0026nbsp;\u003c/p\u003e\u003cp\u003e6) You can explore and select the various tags that have been applied and uncheck those that aren’t of interest\u0026nbsp;\u003c/p\u003e\u003cp\u003e7) Submit the batch\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can now navigate back to your project in Annotate and hit ‘Start labeling’.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"verify-data-quality-with-custom-workflows\"\u003eVerify data quality with custom workflows\u003c/h3\u003e\u003cp\u003eRather than starting from scratch, your internal or external team of labelers can now see predictions from the Model Foundry run. From here, you can validate or edit predictions as necessary and submit data rows to create ground truth labels.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/56bratjais\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs you begin to progress through your data rows, you’ll notice data rows that are initially marked up and reviewed by labelers in the ‘Initial review’ task (for your reviewers to verify and approve), with all submitted data rows falling into ‘Done’.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can create customizable, multi-step review and rework pipelines to drive efficiency and automation for your review tasks. Set a review task based on specific parameters that are unique to your labeling team or desired outcome.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eInitial labeling task: reserved for all data rows that have been queued for labeling\u003c/li\u003e\u003cli\u003eInitial review task: first review task for data rows with submitted labels\u003c/li\u003e\u003cli\u003eRework task: reserved for data rows that have been rejected\u003c/li\u003e\u003cli\u003eDone task: reserved for data rows that have a) moved through their qualified tasks in the workflow or b) did not qualify for any of the tasks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce all data rows have been reviewed and moved to the ‘Done’ step, you can begin the model training process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn Part 1 of this tutorial, we have looked at how we can leverage Catalog to understand the topics that exist within your dataset and construct an appropriate ontology. To accelerate our initial labeling job, we leveraged Model Foundry as part of our model-assisted labeling pipeline to use pre-labels from GPT-4 to our labeling workforce for validation. Those initial annotations can be exported via a model run and can be used to train or fine-tune a model outside of Labelbox.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"part-2-train-or-fine-tune-a-model-and-evaluate-model-performance\"\u003ePart 2: Train or fine-tune a model and evaluate model performance\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e or \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"train-a-custom-model-on-a-subset-of-data-outside-of-labelbox\"\u003eTrain a custom model on a subset of data outside of Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/07yc0p652w\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn the previous step, we leveraged Model Foundry to create pre-labels that were passed through Annotate for review with human-in-the-loop validation. Now that we have our appropriate annotation data, we can train a series of initial models on sentiment, topic classification, and named entity recognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eYou can reference \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io#scrollTo=EDmBNjYP_0u7\"\u003ethis step\u003c/a\u003e (Databricks) or \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io#scrollTo=EDmBNjYP_0u7\" rel=\"noreferrer\"\u003ethis step\u003c/a\u003e (Google Colab) in either notebook.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBring the trained model’s predictions back into a model run\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce the model has been trained, you can create an inference pipeline that leverages each model to classify different attributes for review. We can then leverage this for two things:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun inference on the model run dataset and upload it to Quantumworks Lab for evaluation\u003c/li\u003e\u003cli\u003eRun inference on our remaining dataset and use the predictions for model-assisted labeling, to be refined in the platform and used to accelerate labeling efforts\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ePlease follow \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io#scrollTo=B9_dYR_D_0vE\"\u003ethis step\u003c/a\u003e (Databricks) or \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io#scrollTo=B9_dYR_D_0vE\" rel=\"noreferrer\"\u003ethis step\u003c/a\u003e (Google Colab) to create an inference pipeline and to upload predictions to the model run and evaluate it against ground truth.\u003c/p\u003e\u003cp\u003eAfter following the notebook, you’ll be able to compare ground truth (green) versus the model’s predictions (red) for sentiment and topic.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness\"\u003eEvaluate and diagnose model effectiveness\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dqzdzj1seb\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 8 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eDiagnose model performance with model metrics\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn addition to visualizing the difference between model predictions and ground truth, you can click into the ‘Metrics’ view to get a better sense of how your model is performing.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the \"Metrics view\" to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor example, we can click into false negatives or false positives to narrow down situations where there might be false positives – where ‘negative’ sentiment is predicted whereas ground truth sentiment is ‘positive’.\u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select \"Find similar in Catalog\" from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled reviews that you can send to your model for labeling, you can filter on the \"Annotation is\" filter and select \"none.\" This will only show unlabeled text reviews that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all reviews that apply and select \"Add batch to project\"\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eUse model predictions as model-assisted labeling pipeline\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/r4p2h6iklg\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 9 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve filtered for and have selected reviews that you wish to label you can \"Add batch to project\" to send them to your labeling project in Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Name your batch\u003c/p\u003e\u003cp\u003e2) Select your labeling project from the dropdown\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Include model predictions (from your model run) – this will perform better than the initial GPT-4 run with Model Foundry since it has been trained on your custom data\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Select or uncheck any predictions as desired\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Submit the batch\u003c/p\u003e\u003cp\u003eWhen you return to Quantumworks Lab Annotate, you will now see the original batch that we added at the start of the project, as well as the newly added batch ready for labeling.\u0026nbsp;\u003c/p\u003e\u003cp\u003eRather than starting from scratch, similar to the predictions created by GPT-4 in Model Foundry, your labelers will now see the custom model predictions and validate them with human-in-the-loop review in the same manner. This workflow helps accelerate model iterations, allowing your team to bring in the latest model prediction as pre-labels for your project to reduce the amount of human labeling effort required to create ground truth labels.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model and can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003eCustomer reviews and feedback data represent an invaluable yet untapped opportunity for businesses. Manually analyzing this growing mountain of data is no longer practical. Instead, forward-thinking companies are turning to AI to efficiently sift through and extract actionable insights from reviews.\u003c/p\u003e\u003cp\u003eNatural language processing can help identify customer sentiment, pain points, and unmet needs. By leveraging AI to tap into this feedback treasure trove, businesses can drive measurable improvements in customer satisfaction, retention, and advocacy. They can refine products, enhance user experiences, and preemptively address concerns.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"654bbab4016f5100016579c3","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-2457--1-.png","featured":false,"visibility":"public","created_at":"2023-11-08T16:43:32.000+00:00","updated_at":"2024-06-25T16:28:21.000+00:00","published_at":"2023-11-08T21:47:13.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/","excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","reading_time":12,"access":true,"comments":false,"og_image":null,"og_title":"How to analyze customer reviews and improve customer care with NLP","og_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-2457--1--1.png","twitter_title":"How to analyze customer reviews and improve customer care with NLP","twitter_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","meta_title":"How to analyze customer reviews and improve customer care with NLP","meta_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"653feb16375d13000123da16","uuid":"62c9c370-85df-4a74-a997-f0f25da4b1ee","title":"How to evaluate object detection models with Quantumworks Lab Model Foundry","slug":"how-to-evaluate-object-detection-models-using-labelbox-model-foundry","html":"\u003cp\u003eThe rise of off-the-shelf and foundation models has enabled AI teams to fine-tune existing models and pre-label data faster and more accurately — in short, significantly accelerating AI development. However, using these models at scale for building AI can quickly become expensive. One way to mitigate these costs and reduce waste is to add a model comparison process to your workflow, ensuring that any model you choose to integrate for AI development is the best choice for your requirements.\u003c/p\u003e\u003cp\u003eA comprehensive model comparison process evaluates models on various metrics, such as performance, robustness, and business fit. The results will enable teams to quickly kickstart model development, decrease time to value, and ensure the best results with less time and costs for their specific use case.\u003c/p\u003e\u003cp\u003eEmbedding model comparison into the AI development workflow, however, comes with its own unique challenges, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConfidently assessing the potential and limitations of pre-trained models\u003c/li\u003e\u003cli\u003eVisualizing models’ performance for comparison\u003c/li\u003e\u003cli\u003eEffectively sharing experiment results\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this blog post, we’ll explore how you can tackle these challenges for a computer vision use case with \u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003ethe Foundry add-on for Quantumworks Lab Model\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"why-is-selecting-the-right-model-critical\"\u003eWhy is selecting the right model critical?\u0026nbsp;\u003c/h3\u003e\u003cp\u003eSelecting the most suitable off-the-shelf model is pivotal for ensuring accurate and reliable predictions tailored to your specific business use case, often leading to accelerated AI development. As different models exhibit diverse performance characteristics, diligently comparing the models’ predictions on your data can help distinguish which model excels in metrics such as accuracy, precision, recall, and more. This systematic approach to model evaluation and comparison enables you to refine the model’s performance with a “store of record” for future reference to continuously improve model performance.\u003c/p\u003e\u003cp\u003eChoosing the best off-the-shelf model provides a quick and efficient pathway to production, ensuring that the model aligns well with the business objectives. This alignment is crucial for the model's immediate performance and sets the stage for future improvements and adaptability to evolving requirements. The most suitable model for your use case also enables you to reduce the time and money spent on labeling a project. For instance, when pre-labels generated by a high-performing model are sent for annotation, less editing is required, making the labeling project quicker and more cost-effective. This is due to better Intersection Over Union (IOU) for tasks like Bounding Box, resulting in higher quality pre-labels and, therefore, fewer corrections. Furthermore, utilizing the best model can make your trove of data more queryable by enriching your data, thereby enhancing its searchability.\u003c/p\u003e\u003ch2 id=\"comparing-computer-vision-models-with-labelbox-model-foundry\"\u003eComparing computer vision models with Quantumworks Lab Model Foundry\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"748\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThis flow chart provides a high-level overview of the model comparison process when using the Foundry add-on for Quantumworks Lab Model. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWith Quantumworks Lab Model Foundry, you can evaluate a range of models for computer vision tasks to select the best model to perform pre-labeling or data enrichment on your data.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select images and choose a foundation model of interest \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jrgrl20l0x\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data and refine the images on which the predictions should be made, leverage filters in Catalog, including media attribute, natural language search, and more\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “Predict with Model Foundry.” You will then be prompted to choose a foundation model that you wish to use in the model run\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task — such as image classification, object detection, and/or image captioning\u003c/li\u003e\u003cli\u003eTo locate a specific model, you can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the models available for this machine learning task\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-hyperparameters-and-submit-a-model-run\"\u003e\u003cstrong\u003eStep 2: Configure model hyperparameters and submit a model run\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/tqszjqj6l1\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve located a specific model of interest, you can click into the model to view and set the model and ontology settings.\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of hyperparameters, which you can find in the Advanced model setting. To get an idea of how your current model settings affect the final predictions, you can generate preview predictions on up to five data rows.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings. If you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;Once you’re satisfied with the predictions, you can submit your model run.\u003c/p\u003e\u003ch3 id=\"step-3-predictions-will-appear-in-the-model-tab\"\u003eStep 3: Predictions will appear in the Model tab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ro3vxhcagr\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eEach model run is submitted with a unique name, allowing you to distinguish between each subsequent model run. When the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eView prediction results\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results across a variety of model runs different models\u003c/li\u003e\u003cli\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate \u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-repeat-steps-1-5-for-another-model-from-labelbox-model-foundry\"\u003eStep 4: Repeat steps 1-5 for another model from Quantumworks Lab Model Foundry\u003c/h3\u003e\u003cp\u003eYou can repeat steps 1-5 with a different model, on the same data and for the same desired machine learning task, to evaluate and compare model performance. By comparing the predictions and outputs from different models, you can assess and determine which one would be the most valuable in helping automate your data labeling tasks.\u003c/p\u003e\u003ch3 id=\"step-5-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 5: Create a model run with predictions and ground truth\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rzqnbhasuz\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a model run with model predictions and ground truth, users currently have to use a \u003ca href=\"https://colab.research.google.com/drive/1f0WkWtsDh2LYiy35pc3JF16srj6SxeTA?ref=labelbox-guides.ghost.io#scrollTo=W8tE-H7VmKea\"\u003escript\u003c/a\u003e to import the predictions from the Foundry add-on for Quantumworks Lab Model and ground truth from a project into a new model run. \u003c/p\u003e\u003cp\u003eIn the near future, this will be possible via the UI, and the script will be optional.\u003c/p\u003e\u003ch3 id=\"step-6-evaluate-predictions-from-different-models-from-model-foundry-in-labelbox-model\"\u003eStep 6: Evaluate predictions from different models from Model foundry in Quantumworks Lab Model\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qbx1mvx878\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter running the notebook, you'll be able to visually compare model predictions between two models. Use the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors. \u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/p\u003e\u003ch3 id=\"step-7-send-model-predictions-as-pre-labels-to-a-labeling-project-in-annotate\"\u003eStep 7: Send model predictions as pre-labels to a labeling project in Annotate\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/d3rxjzg111\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"640\" height=\"360\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eSelect the best performing model and leverage the model predictions as pre-labels. Rather than manually labeling data rows, select and send a subset of data to your labeling project with pre-labels to automate the process.\u003c/p\u003e\u003ch2 id=\"model-comparison-in-practice\"\u003eModel comparison in-practice:\u003c/h2\u003e\u003ch3 id=\"google-cloud-vision-vs-microsoft-azure-ai\"\u003eGoogle Cloud Vision vs Microsoft Azure AI\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/hms66_aBLd_Kj7jsKaYhr30ChoP5kflDM6nDmlqseCR63P-8uwSriJ9FqVf-biUS-uIQelFbtSvxq7Dq-Us-tq8qy3vkyxvs_--3CSShlVLZkbF3uS2-eHEaEV0SVugIfAVxB_xmDA4kL1ZFV2Z77hA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"880\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the metrics overview, we see that Google Cloud Vision outperforms Microsoft Azure AI for recall, f1 score, intersection over union and false negatives.\u003c/p\u003e\u003cp\u003eMicrosoft Azure AI boasts a precision score of 0.8633, which outperforms the 0.7948 score of Google Cloud Vision.\u0026nbsp;Microsoft Azure AI has an intersection over union score of 0.4034, an F-1 score of 0.7805, and a recall of 0.3852. In contrast, Google Cloud Vision exhibits a superior intersection over union score of 0.4187, an F-1 score of 0.7832, and a recall of 0.4149.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe can also see that the Microsoft Azure AI model has 12,665 more false negatives than Google Cloud Vision, and for our use case, we want the model with the least false negatives.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/xWXS7lRpI1UxITFAcG50Yq3RJMdKsSqK26uRudhilYHn_LvlSdyd7WMnUv8gtaj3C-GTJq2_e_4v0ZcxjVduI-VVui0AF57ZcL-2WUQURwHPRzO7ER2rGPNOKpY8YIW0NYqgz9-SWuMfMYRjgU0Z_cc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"589\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eF1 scores for both Microsoft Azure AI and Google Cloud Vision Model are generally comparable, with a few instances showcasing superior performance by the Google Cloud Vision Model. Here are the specific results for each category:\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Microsoft Azure AI scored 0.857, while Google Cloud Vision Model scored higher with 0.932\u003c/li\u003e\u003cli\u003eFor the boat category, Microsoft Azure AI achieved a score of 0.619, compared to the slightly higher 0.656 of Google Cloud Vision Model\u003c/li\u003e\u003cli\u003eFor the person category, Microsoft Azure AI obtained a score of 0.773, whereas Google Cloud Vision Model marginally outperformed with a score of 0.785\u003c/li\u003e\u003cli\u003eFor the airplane, Microsoft Azure AI scored 0.868, with Google Cloud Vision Model again performing better with a score of 0.893\u003c/li\u003e\u003cli\u003eFor the bus category, Microsoft Azure AI had a score of 0.705, significantly lower than the 0.925 achieved by Google Cloud Vision Model\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/SC9uJlITaaDLqRvZqSjBoVGIgHdQiQHgrLiiBBovLzjHK-Yiwt-URHAGyy9Z7l5WETRaFlS2Gukx4_UbeyQrazgJiljGnP5qv-wzJgMfTfW5yCUhnvvgUPEKz870g1FEmh5pe2zpnJHxX_QMYMYXul0\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"573\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eGenerally, the Google Cloud Vision Model exhibits superior performance in terms of intersection over union for classes such as train, boat, person, airplane, and bus.\u0026nbsp; Intersection over union (IOU) is crucial as it dictates the accuracy of the bounding box prediction area.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Microsoft Azure AI scored 0.304, while Google Cloud Vision Model significantly outperformed with a score of 0.745\u003c/li\u003e\u003cli\u003eFor the boat category, Microsoft Azure AI achieved a score of 0.251, compared to a higher 0.394 by Google Cloud Vision Model\u003c/li\u003e\u003cli\u003eFor the person category, Microsoft Azure AI secured a score of 0.697, with Google Cloud Vision Model slightly ahead at 0.704\u003c/li\u003e\u003cli\u003eFor the airplane category, Microsoft Azure AI scored 0.603, whereas Google Cloud Vision Model again demonstrated superior performance with a score of 0.738\u003c/li\u003e\u003cli\u003eFor the bus category, Microsoft Azure AI recorded a low score of 0.05, markedly lower than the 0.637 achieved by Google Cloud Vision Model\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/65NRl_i7lXdcTqhIrn968_0t_p_d4nqYsEKkEDAT_j1GVt3snxpTXrNmEwB-pQmeXD4sbOPxN3_arHpEgA-iD50iEDSTXC-ZSR1nc6JzI4BIJMYBBrxrr0hUTuhIM2t94PKY6AenEBgwCYQYUnSb18M\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"573\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eIn summary, Google Cloud Vision Model exhibits superior recall values across the categories of train, boat, person, airplane, and bus.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Microsoft Azure AI exhibited a recall of 0.331, while Google Cloud Vision Model showcased a considerably higher recall of 0.769\u003c/li\u003e\u003cli\u003eFor the boat category, Microsoft Azure AI demonstrated a recall of 0.203, compared to a higher 0.308 by Google Cloud Vision Model\u003c/li\u003e\u003cli\u003eFor the person category, Microsoft Azure AI achieved a recall of 0.618, with Google Cloud Vision Model slightly ahead at 0.622\u003c/li\u003e\u003cli\u003eFor the airplane category, Microsoft Azure AI registered a recall of 0.719, whereas Google Cloud Vision Model marginally outperformed with a recall of 0.747\u003c/li\u003e\u003cli\u003eFor the bus category, Microsoft Azure AI posted a low recall of 0.04, significantly lower than the 0.652 achieved by Google Cloud Vision Model\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLet’s now take a look at how the predictions appear on the images below\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J9PexX_zah5-eoCAgFdV6g6PftInYqzHoupESkHtdLeYzz47V0bsp5upVVrNmd5R6EXuqoKU-PrWnRa_JgwqQy0W6-vwFS-1kjoqzO3E_80WWYxaHSEcPsmpEnDbh7MCWNobGK2R5nEVqcvVSCpF0ZE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"741\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eHere we can see that the model from Google Cloud Vision has a blue bounding box that properly captures the dimension of the airplane. Whereas Azure’s orange bounding box only covers ~3/4 of the airplane.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Uy4LGTk8uhGha-Rkw6qvZaUpLQ9Zo7-EIJvPTrS0fcygRWRIJvIPi04HQ9zjTpWC4y3-A2sSy9OirucS20JObzVgfWVCVXKYpfmCItE-M7PEajJX6XalsmNKRnAO5I8MHL30r8gJHs0EurUwtNBnUGw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"763\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAnother example where Google Cloud Vision in blue bounding box has better IOU than Azure model’s orange bounding box. Based on the qualitative and quantitative analysis above, Google Cloud Vision is the superior model compared to Microsoft Azure AI.\u003c/p\u003e\u003ch3 id=\"google-cloud-vision-vs-amazon-rekognition\"\u003eGoogle Cloud Vision vs Amazon Rekognition\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/5Qsk1VepZUnxPhjfAGB33fhejnnJn5rOwQ0rrqQjEScKS38ep9vAnlViHSV1MynCeEVWfkOjcnZbF19tqFeJFXnYZ2SpLuZp5CR8x3QJ4-w8Z-XDeW-NiMKMOogsnbdeBHxvCc1wmNEaCXMG05GaY0s\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"655\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the metrics overview, Amazon Rekognition generally demonstrates better performance in false negatives, true positives, recall, and IOU against Google Cloud Vision.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor false negatives, Amazon Rekognition reported 21,935, whereas Google Cloud Vision had a slightly higher count of 22,868\u003c/li\u003e\u003cli\u003eFor true positives, Amazon Rekognition significantly outperformed with 25,953, compared to 10,112 recorded by Google Cloud Vision\u003c/li\u003e\u003cli\u003eFor recall, Amazon Rekognition showcased a higher value of 0.5419, while Google Cloud Vision exhibited a lower recall of 0.3913\u003c/li\u003e\u003cli\u003eFor Intersection over Union (IOU), Amazon Rekognition achieved a score of 0.4596, surpassing the 0.4212 scored by Google Cloud Vision\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/B5_bkD0aYI0lRIoUJZpX7m-J5_BzgJbzrCKhWtbzhOcw4EjWt2NY858HNCFV1duFbBwTpuHUZnMl8rggUcMQwWc6-7QhlKIgikUMKFZocxCeNiD0auUQLHSEd8xEVuSLUpIVtXU2HgEfO9Nm0TxzmTM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"518\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAmazon Rekognition outperformed Google Cloud Vision in the train, airplane, and bus categories\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Amazon Rekognition achieved an F-1 score of 0.969, outperforming Google Cloud Vision, which scored 0.932\u003c/li\u003e\u003cli\u003eFor the boat category, Amazon Rekognition scored 0.747, while Google Cloud Vision significantly outshone with a score of 0.956\u003c/li\u003e\u003cli\u003eFor the person category, Amazon Rekognition registered an F-1 score of 0.566, with Google Cloud Vision achieving a higher score of 0.773\u003c/li\u003e\u003cli\u003eFor the airplane category, Amazon Rekognition had an F-1 score of 0.919, slightly higher than the 0.893 scored by Google Cloud Vision\u003c/li\u003e\u003cli\u003eFor the bus category, Amazon Rekognition secured an F-1 score of 0.929, marginally outperforming Google Cloud Vision, which scored 0.925\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/XbeoxMVEBEttqsPy4S3P9gRbofKkTYbuQT_UeFdMmkCMWAt_YmKRHdXU6ltUSe7c1B4ov2PhYdnpJC4LS8XTj07tVu6HSqfnhDXDjA0-oObUG76juz61tgutFGdKRU84LGc18j6yc5NI4Ip8nzrF03c\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"518\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAmazon Rekognition led in the bus, boat and person categories,\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Amazon Rekognition has an IOU score of 0.741, closely competing with Google Cloud Vision which scored 0.745\u003c/li\u003e\u003cli\u003eFor the boat category, Amazon Rekognition outperformed with an IOU score of 0.454, compared to Google Cloud Vision's 0.394\u003c/li\u003e\u003cli\u003eFor the person category, Amazon Rekognition significantly led with a score of 0.813, while Google Cloud Vision scored 0.704\u003c/li\u003e\u003cli\u003eFor the airplane category, Amazon Rekognition recorded an IOU of 0.677, slightly below Google Cloud Vision's 0.738\u003c/li\u003e\u003cli\u003eFor the bus category, Amazon Rekognition and Google Cloud Vision scored closely with IOU scores of 0.65 and 0.637 respectively\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/tCrtU-Xg3sv5kX3oNYLbqafdToRY9VJ78cWeF9UwQBOAe7fvBBIV_cJFQbaXDMiLxxOlxMk1bU3DSzvNbsjvS8mIzwn22L85p4xnqsr1zIYUfUAhP-8j2onpPqDJSVgPxfFlEi1eTomuh4uzaBBBt6o\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"518\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eRecall for Amazon Rekognition for boat, person, airplane, and bus, is better than Google Cloud Vision.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Amazon Rekognition demonstrated a recall score of 0.893, while Google Cloud Vision significantly outperformed with a score of 0.983\u003c/li\u003e\u003cli\u003eFor the boat category, Amazon Rekognition led with a recall score of 0.432, compared to Google Cloud Vision's lower score of 0.308\u003c/li\u003e\u003cli\u003eFor the person category, Amazon Rekognition achieved a higher recall score of 0.787, surpassing Google Cloud Vision's 0.622\u003c/li\u003e\u003cli\u003eFor the airplane category, Amazon Rekognition outperformed with a recall score of 0.851, as opposed to Google Cloud Vision's 0.743\u003c/li\u003e\u003cli\u003eFor the bus category, Amazon Rekognition showcased a recall score of 0.836, significantly higher than Google Cloud Vision's 0.652\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/rgrVd-plFthhvoZgeUDJgc8BXH4KwkKK-wQMBmrAY-tkhr5lh-PP3gyyGIRVIWWL9_sO1OBgCqhrGAdr9gmPy4mwXkGOnlOXItmSicvvPQ1H5hExjipwUVye2Ep970YX33rzEYTRlH4WIIVUXVrhybI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"810\"\u003e\u003c/figure\u003e\u003cp\u003eHere, Google Cloud Vision in the orange bounding box has detected only one boat, but Amazon Rekognition has detected 5 more boats, a person, and a car.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/JycIqkuvst751-UDITIYiwj-xH-3WDp_5fE3I8KKFvBOicjJ80dfPXt4wjDbfbsN1fqYwPA4JbHDBGyI6E8sjS1J-yd3e52S67093113NvjpmKDQ4Worn0_-_nLDR9tiVpfC6PjL-1hcEmrttzVApHs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"810\"\u003e\u003c/figure\u003e\u003cp\u003eAmazon Rekognition in the blue bounding box has detected more boats and a clock tower.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/oOtbtSHmfi3Ob6pf7hLRSk1JrSlO6zWUlM2m92ILaa20Oc0HL4GZYcqzYh6mi-kVbuU2l8oyObhbB36EgrvLJOkpPEyBke7hZ6BlMWwHb08bUhln0mFMAry9EBAczeRuLFCN9Dz24BlTA3lLGdZQypA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"860\"\u003e\u003c/figure\u003e\u003cp\u003eAmazon Rekognition in the blue bounding box has an IOU of the truck with full coverage, whereas the IOU for Google Cloud Vision is around 90%.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/l87bsRsULhAXMrcUMtDbW6k2p3xGA4E7TpxuuaiVzjZGlWO4WjHhtbAMwomLKXf2BD58DXi6BznC-2zSmKvPbuki11Y5F3deYJGYC9tsfpLgqShOwS2IuhI0DWb1NHJufQDnhx7BuaVqgs3JQMss4hg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"810\"\u003e\u003c/figure\u003e\u003cp\u003eThe IOU for person is also better in addition to being able to detect cars in the background for Amazon Rekognition in the blue bounding box compared to Google Cloud Vision. Based on the analysis above, Amazon Rekognition is the best model for our use case.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSend the predictions as pre-labels to Quantumworks Lab Annotate for labeling\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eSince we've evaluated that Amazon Rekognition is the best model for our use case, we can send model predictions as pre-labels to our labeling project by highlighting all data rows and selecting \"Add batch to project.\" \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Yxobsiulkb3gR3KREdjS2x0m_aSOTVV_Mx_XXnGrRk1Q-8YueaAw33Y_uoZxJb6rDhJsF2PqWWb2yc2H5P6vEvfKW6qDoGwHFiJQ1VqeD5COxfagUNORNuzco1n6CXnKqJAv67UAGH8Yw_dQsdi6fHc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"484\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eIn conclusion, you can leverage the Foundry add-on for Quantumworks Lab Model to not only select the most appropriate model to accelerate data labeling, but to automate data labeling workflows. Use quantitative and qualitative analysis, along with model metrics, to surface the strengths and limitations of each model and select the best performing model for your use case. Doing so can help reveal detailed insights, such as seen in the above comparison between Google Cloud Vision and Amazon Rekognition. In the above model comparison example, we can see that Amazon Rekognition emerged as particularly well-suited for our project’s requirements and allows us to rapidly automate data tasks for our given use case.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Model Foundry\u003c/a\u003e streamlines the process of comparing model predictions, ensuring teams are leveraging the most optimal model for data enrichment and automation tasks. With the right model, teams can easily create pre-labels in Quantumworks Lab Annotate – rather than starting from scratch, teams can boost labeling productivity by correcting the existing pre-labels.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"653feb16375d13000123da16","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3084.png","featured":false,"visibility":"public","created_at":"2023-10-30T17:42:46.000+00:00","updated_at":"2024-02-02T18:32:57.000+00:00","published_at":"2023-11-01T15:46:26.000+00:00","custom_excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-evaluate-object-detection-models-with-model-foundry","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},"url":"https://labelbox-guides.ghost.io/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/","excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","reading_time":12,"access":true,"comments":false,"og_image":null,"og_title":"How to evaluate object detection models with Model Foundry","og_description":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3084-1.png","twitter_title":"How to evaluate object detection models with Model Foundry","twitter_description":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","meta_title":"How to evaluate object detection models with Model Foundry","meta_description":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6537fb91375d13000123d799","uuid":"6609e3d3-fc0e-4183-91ef-a83c801c7c6d","title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine-Tuning: A technical walkthrough using OpenAI's APIs \u0026 models","slug":"zero-shot-learning-few-shot-learning-fine-tuning","html":"\u003cp\u003eWith large language models (LLMs) gaining popularity, new techniques have emerged for applying them to NLP tasks. Three techniques in particular — zero-shot learning, few-shot learning, and fine-tuning — take different approaches to leveraging LLMs. In this guide, we’ll walk through the key difference between these techniques and how to implement them.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll walk through a case study of extracting airline names from tweets to compare the techniques. Using an entity extraction dataset, we’ll benchmark performance starting with zero-shot prompts, then experiment with few-shot learning, and finally fine-tune a model. By analyzing the results, we can better understand when to use zero-shot, few-shot, or fine-tuning with LLMs. You’ll also pick up tips for constructing effective prompts and setting up LLM experiments.\u003c/p\u003e\u003cp\u003eThe goal of this guide is to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompare the quantitative results among zero-shot learning, few-shot learning, and fine-tuning on an NER use case\u003c/li\u003e\u003cli\u003eExplore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026amp; Quantumworks Lab Model\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"zero-shot-learning-few-shot-learning-and-fine-tuning-in-action\"\u003eZero-shot learning, few-shot learning, and fine-tuning in action\u003c/h2\u003e\u003cp\u003eFor the purposes of this case study, we will be walking through an example of entity extraction featured in \u003ca href=\"https://colab.research.google.com/drive/1OCD8ivCtPS84cEhtjXkIWNAQX9oyKfFe?usp=sharing\u0026ref=labelbox-guides.ghost.io\"\u003ethis Google Colab Notebook\u003c/a\u003e. Specifically, we have a dataset of tweets about major airlines, and the task is to use an API to extract all airlines names that appear in the tweets.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe full dataset can be found on Kaggle \u003ca href=\"https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eHere is an example data row:\u003c/p\u003e\u003cp\u003e\"@AmericanAir do you have the phone number of a supervisor i can speak to regarding my travels today,['American Airlines’]\u003c/p\u003e\u003cp\u003ewhere the:\u003c/p\u003e\u003cp\u003eTWEET: @AmericanAir do you have the phone number of a supervisor i can speak to regarding my travels today\u003c/p\u003e\u003cp\u003eLABEL: [‘American Airlines']\"\u003c/p\u003e\u003cp\u003eBefore delving into the details, a few key technical definitions to keep in mind:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eZero-shot learning\u003c/strong\u003e — a technique whereby we prompt an LLM without any examples, attempting to take advantage of the reasoning patterns it has gleaned (i.e. a generalist LLM)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFew-shot learning\u003c/strong\u003e — a technique whereby we prompt an LLM with several concrete examples of task performance\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-tuning\u003c/strong\u003e — a technique whereby we take an off-the-shelf open-source or proprietary model, re-train it on a variety of concrete examples, and save the updated weights as a new model checkpoint\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"establishing-a-benchmark-baseline-with-zero-shot-learning\"\u003eEstablishing a benchmark baseline with zero-shot learning\u003c/h3\u003e\u003cp\u003eAs with any scientific or machine learning experiment, it is important to establish a benchmark baseline. For this case study, we will use zero-shot learning as the baseline by experimenting with various zero-shot prompts and evaluating the performance (precision, recall, f1-score, accuracy) of these prompts against the test set.\u003c/p\u003e\u003cp\u003eThe demo video below shows how we can leverage the \u003cstrong\u003e‘Humans Generate Prompt’ \u003c/strong\u003eoption of the \u003ca href=\"https://www.google.com/url?q=https://docs.labelbox.com/docs/llm-data-generation\u0026sa=D\u0026source=docs\u0026ust=1732675975664445\u0026usg=AOvVaw1fM_zT8IUOw2SSFmqUnUcd\" rel=\"noreferrer\"\u003ePrompt and Response Editor\u003c/a\u003e within Quantumworks Lab Annotate to create various zero-shot prompts using in-house and/or external teams to scale out our annotation operations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/44tuzazslq\" title=\"[Guide] Create Annotate Project \u0026amp; Airline Prompts in LLM Editor Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"534\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce we have constructed our dataset of zero-shot prompts within Quantumworks Lab using a prompt engineering workforce, we can export them from the Quantumworks Lab UI  the Quantumworks Lab Python SDK and use them in our script, as shown in the video below.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kryaxyhdx3\" title=\"[Guide] Exporting Prompts from LB Annotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eUsing gpt-3.5-turbo, we see the results of the prompts, along with their performance metrics.\u003c/p\u003e\u003cp\u003eFrom these results, we observe that prompts that fared best provided a clear and structured instruction to the model. These prompts explicitly mention the expected format for identifying airline names. Furthermore, these prompt structures introduce a pattern that the model can recognize and follow. In the case of \"Detect airline references...\" the model is prompted to look for references in a specific format (e.g. hashtags), which may be a common pattern in tweets mentioning airlines.\u003c/p\u003e\u003cp\u003ePrompts that fared worst had a theme: ambiguity in the prompt response format. Examples like \"What are the airlines in this tweet?\" and \"Find all airline mentions in the tweet\" are open- ended and do not provide a specific structure, making it harder for the model to interpret the task.\u003c/p\u003e\u003cp\u003eWhat’s interesting to note is that the prompt “Identify airlines like this - [#AIRLINE_NAME_1]:'{tweet}’” did NOT perform well even though it provided an example response format. This underscores the profound impact that punctuation and grammatical structure can have for prompt engineering. Instead of interpreting “[#AIRLINE_NAME_1]\" as the LLM-output format, the LLM instead interpreted it as a pattern-matching task to identify all airlines within a tweet that contain this specific format [#AIRLINE_NAME_1], of which there are none (hence, 0% across the evaluation metrics).\u003c/p\u003e\u003cp\u003eIn addition to testing different prompts, we can run various experiments to evaluate the efficacy of the task and evaluate the impact of those experiments in Quantumworks Lab Model. One such experiment could be to compare different models (GPT-4 vs. GPT-3.5, etc.). The video below shows how we can compare GPT-4 vs. GPT-3.5 on how each model performs on extracting airlines from the prompts we created above.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/13yquhzp4o\" title=\"[Guide] Airline extraction model comparison Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"534\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eZero-shot learning netted us the following baseline benchmark on the test set: 19%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"few-shot-learning\"\u003eFew-shot learning\u0026nbsp;\u003c/h3\u003e\u003cp\u003eTo build upon this benchmark, we used one of the prompts that performed well in zero-shot learning in tandem with few-shot learning — a technique whereby along with the prompt, we also feed an LLM concrete examples of task performance. These concrete examples are chosen from our training dataset (found in \u003cstrong\u003eairline_train.csv\u003c/strong\u003e).\u003c/p\u003e\u003cp\u003eThis is an example few-shot prompt that we passed to the LLM:\u003c/p\u003e\u003cp\u003eGiven the following tweets and their corresponding airlines, separated by new lines:\u003c/p\u003e\u003cp\u003e1) SouthwestAir bags fly free..just not to where you're going.,['Southwest Airlines']\u003c/p\u003e\u003cp\u003e2) Jet Blue I don't know- no one would tell me where they were coming from,['JetBlue Airways']\u003c/p\u003e\u003cp\u003ePlease extract the airline(s) from the following tweet:\u003c/p\u003e\u003cp\u003e\"SouthwestAir Just got companion pass and trying to add companion flg. Help!\"\u003c/p\u003e\u003cp\u003eUsing the following format - ['#AIRLINE_NAME_1] for one airline or ['#AIRLINE_NAME_1, #AIRLINE_NAME_2...] for multiple airlines.\u003c/p\u003e\u003cp\u003eEvaluating our model (gpt-3.5-turbo) on the test set via few-shot learning, we achieved an accuracy of 96.66%! There were 7 total misclassifications. Further inspection into these misclassifications actually reveals that there may be issues within the ground-truth dataset.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFew-shot learning netted us the following accuracy benchmark on the test set: 97%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"fine-tuning-with-a-training-dataset\"\u003eFine-tuning with a training dataset\u003c/h3\u003e\u003cp\u003eLastly, we seek to determine whether fine-tuning would improve our results. To ensure parity across our experiments, we used the same 100 randomly generated examples from the training dataset for few-shot learning as our control variable for the fine-tuning task.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFine-tuning netted us the following accuracy benchmark on the test set: 97%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"comparing-results\"\u003e\u003cstrong\u003eComparing results\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThese were the final results:\u003c/p\u003e\u003cp\u003e1) Zero-shot learning netted us the following accuracy baseline benchmark on the test set: 19%.\u003c/p\u003e\u003cp\u003e2) Few-shot learning netted us the following accuracy benchmark on the test set: 97%.\u003c/p\u003e\u003cp\u003e3) Fine tuning netted us the following accuracy benchmark on the test set: 91%.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKey takeaways\u003c/strong\u003e\u003c/p\u003e\u003cp\u003ePrompt engineering in tandem with few-shot learning versus fine-tuning both yield similar results for the task of extracting airline references from tweets. The ultimate consideration between the two boils down to achieving economies of scale. If teams find themselves needing to execute a prompt 100,000 times, the effort invested in terms of both human hours and GPU usage (or, token costs, if utilizing an API) can be justified when fine-tuning, as the cumulative savings in prompt tokens and the potential for improved output quality can accumulate significantly. Conversely, if we’re only using the prompt ten times and it's already effective, there's no rationale for fine-tuning it.\u003c/p\u003e\u003cp\u003eRegardless of which option you choose, we’ve seen how \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Annotate\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e can help achieve both outcomes.\u003c/p\u003e\u003cp\u003eWe can also improve results by ensuring variability within the few-shot learning examples. Currently, we are using a naive approach by selecting 100 randomly generated examples from the training dataset, so that it fits within the context window of gpt-3.5-turbo. Despite leveraging ~10% of our training data (100 rows / 900 possible training data rows), it’s less about data quantity and more about data quality. If we can curate a few-shot learning dataset of only 100 rows that has enough variance to be representative of the larger 900 rows, then that would be most ideal, from a token-sizing and, subsequently, cost perspective in addition to an engineering perspective (achieving more with less).\u003c/p\u003e\u003cp\u003eExamples of variance include tweet structure as well as a healthy mix of tweets with multiple airlines referenced. For tweet structure, we can use regular expressions to create patterns to capture mentions (@username), hashtags (#hashtag), airline stock ticker symbols, or emojis. In our use case, hashtags and ticker symbols would be super beneficial, since we see them scattered throughout our training dataset (e.g. #LUVisbetter, #jetblue, #UnitedAirlines, AA, etc.).\u003c/p\u003e\u003cp\u003eUsing \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e, we can version our few-shot learning and fine-tuning experiments by tying each experiment to a corresponding model run. Comparing evaluation metrics across different model runs allows us to either:\u003c/p\u003e\u003cul\u003e\u003cli\u003eChoose the few-shot learning prompt with the best precision, accuracy, and/or recall metric\u003c/li\u003e\u003cli\u003eChoose the fine-tuned model with the best precision, accuracy, and/or recall metric\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"get-started-today\"\u003eGet started today \u003c/h2\u003e\u003cp\u003eThrough our case study extracting airline names from tweets, we've explored the key differences between zero-shot learning, few-shot learning, and fine-tuning for applying large language models to NLP tasks.\u003c/p\u003e\u003cp\u003eThe best technique depends on your use case and available resources. The key is understanding how different data inputs affect an LLM's behavior. With the right approach, you can take advantage of LLMs' reasoning skills for a wide range of NLP applications. You can leverage each of these learning techniques with Quantumworks Lab’s LLM data generation editor and Quantumworks Lab Model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox is a data factory that empowers teams to generate high-quality data and accelerate the development of differentiated genAI solutions. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026gclid=Cj0KCQiA6Ou5BhCrARIsAPoTxrBVxRVcyUPeK8y7Mn2rtcM2M257gsxyxejwhFES91vcP7xYdoij4OQaAjUwEALw_wcB\u0026attr=arcade\u0026landingPageAnonymousId=%22ce322bab-56ea-43b8-a113-000851a3ebaf%22\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"6537fb91375d13000123d799","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083.png","featured":false,"visibility":"public","created_at":"2023-10-24T17:14:57.000+00:00","updated_at":"2024-11-27T02:11:35.000+00:00","published_at":"2023-10-24T20:34:32.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/zero-shot-learning-few-shot-learning-fine-tuning","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},"url":"https://labelbox-guides.ghost.io/zero-shot-learning-few-shot-learning-fine-tuning/","excerpt":"With large language models (LLMs) gaining popularity, new techniques have emerged for applying them to NLP tasks. Three techniques in particular — zero-shot learning, few-shot learning, and fine-tuning — take different approaches to leveraging LLMs. In this guide, we’ll walk through the key difference between these techniques and how to implement them. \n\nWe’ll walk through a case study of extracting airline names from tweets to compare the techniques. Using an entity extraction dataset, we’ll be","reading_time":6,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083-2.png","og_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","og_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083-1.png","twitter_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","twitter_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","meta_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","meta_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65300bc0a6fedb0001417480","uuid":"59641c97-b4ef-4ade-8e83-52f3887883d0","title":"How to use the model Foundry for automated data labeling and enrichment","slug":"guide-to-using-model-foundry","html":"\u003cp\u003eFoundation models, such as GPT-4, are ushering in a new era of AI by outperforming humans on numerous tasks across modalities including images and language. With the introduction of\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e \u003c/a\u003e\u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ethe Foundry add-on for Quantumworks Lab Model\u003c/u\u003e\u003c/a\u003e, we’re bringing the power of foundation models into the Quantumworks Lab platform.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eMeet your new AI co-pilot: Foundry\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLabelbox Foundry enables AI teams to use world-class foundation models to enrich datasets and automate tasks. In just a few clicks, AI builders can explore, test and integrate powerful models to build vital workflows for pre-labeling, or other specific data tasks. Kickstart your AI efforts with this AI copilot to build intelligent applications faster than ever.\u003c/p\u003e\u003cp\u003eEarly tests with Fortune 500 companies show up to 88% reductions in human labeling time, with complex tasks going from days to hours. Foundry integrates these state-of-the-art models into every step of the workflow, unlocking the next evolution of AI development. Kickstart your AI and data labeling efforts with this co-pilot to build intelligent applications faster than ever.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re excited to announce that Foundry is available as an add-on for Quantumworks Lab Model!\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith Foundry, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAccess the world’s most cutting-edge models and combine them with human-in-the-loop systems to accelerate model development.\u003c/li\u003e\u003cli\u003ePre-label data in a few clicks to reduce labeling costs by up to 90%.\u003c/li\u003e\u003cli\u003eTailor intelligence to your needs using Quantumworks Lab’s comprehensive end-to-end platform and workflow in Foundry. From fine-tuning to model distillation, you can customize intelligence beyond pre-built AI.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFoundry works across Quantumworks Lab's products — Catalog, Annotate, and Model — to supercharge your labeling and model development.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"how-to-access-foundry\"\u003eHow to access Foundry:\u003c/h2\u003e\u003ch3 id=\"free-tier-organizations\"\u003eFree Tier organizations\u003c/h3\u003e\u003cp\u003eFoundry is only available to our Starter and Enterprise plans, to access Foundry you will need to:\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Upgrade your account to our \u003cstrong\u003eStarter plan\u003c/strong\u003e:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn the \u003ca href=\"https://app.labelbox.com/workspace-settings/billing?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBilling tab\u003c/u\u003e\u003c/a\u003e, locate “Starter” in the All Plans list and select “Switch to Plan.” \u003cem\u003eThe credit card on file will only be charged when you exceed your existing free 10,000 LBU.\u0026nbsp;\u003c/em\u003e\u003c/li\u003e\u003cli\u003eUpgrades take effect immediately so you'll have access to Foundry right away on the Starter plan. After upgrading, you’ll see the option to activate Foundry for your organization.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-04_16-22-54--1--min-1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1904\" height=\"932\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/2023-12-04_16-22-54--1--min-1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/2023-12-04_16-22-54--1--min-1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/2023-12-04_16-22-54--1--min-1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-04_16-22-54--1--min-1.gif 1904w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e2) After upgrading to our Starter plan, please follow the instructions under the Starter plan below to activate Foundry as an add-on for your organization.\u003c/p\u003e\u003ch3 id=\"self-serve-organizations-on-a-starter-or-standard-plan\"\u003eSelf-serve organizations on a Starter or Standard plan\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs part of the self-serve Starter or Standard plan, you automatically have the ability to opt-in to using Foundry.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eTo generate model predictions, just submit a model run and you’ll be guided to activate Foundry along the way:\u0026nbsp;\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Generate predictions:\u003c/strong\u003e Start by selecting data in Catalog and click “Predict with Foundry.” You’ll be able to select a foundation model and submit a model run to generate predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Activate Foundry:\u003c/strong\u003e The first time you submit a model run, you’ll see the option to activate Foundry for your organization’s \u003ca href=\"https://docs.labelbox.com/docs/members-group-management?ref=labelbox-guides.ghost.io#member-roles\"\u003e\u003cu\u003eAdmins\u003c/u\u003e\u003c/a\u003e. You’ll need to agree to Quantumworks Lab’s Model Foundry add-on service terms and confirm you understand the associated compute fees.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Payment method:\u003c/strong\u003e With Foundry, you will be charged for the Quantumworks Lab units (LBUs) your account consumes, plus any\u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003epass-through compute costs\u003c/u\u003e\u003c/a\u003e for the models you use.\u003c/p\u003e\u003cp\u003eFor self-serve Starter and Standard tier to enable billing for these model compute charges, you'll be asked to confirm the credit card on file for your Quantumworks Lab account. Alternatively, you may add another card if you prefer to keep the Foundry charges separate. By confirming your payment method, you agree to let Quantumworks Lab to bill your card as Foundry model compute fees accrue based on your usage.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/OLBO-X7WF_y9EXTcmQ5nM1-6W75tJyYWojkmQp_IH-bdx2R_i985qA00Tbf3nMqHLGzUIkKo-0MES9-jfKUlv86kFjlmASbzfCo_CZTAau-38nUtH22IYHxUJ4mHgYi_VgfTdf9CaBsT3PK-46FeAF8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"305\"\u003e\u003c/figure\u003e\u003ch3 id=\"enterprise-organizations\"\u003eEnterprise organizations\u003c/h3\u003e\u003cp\u003eAs an organization on our Enterprise plan, you automatically have the ability to opt-in to using Foundry.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eTo generate model predictions, just submit a model run and you’ll be guided to activate Foundry along the way:\u0026nbsp;\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Generate predictions:\u003c/strong\u003e Start by selecting data in Catalog and click “Predict with Foundry.” You’ll be able to select a foundation model and submit a model run to generate predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Activate Foundry:\u003c/strong\u003e The first time you submit a model run, you’ll see the option to activate Foundry for your organization’s \u003ca href=\"https://docs.labelbox.com/docs/members-group-management?ref=labelbox-guides.ghost.io#member-roles\"\u003e\u003cu\u003eAdmins\u003c/u\u003e\u003c/a\u003e. You’ll need to agree to Quantumworks Lab’s Model Foundry add-on service terms and confirm you understand the associated compute fees.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Payment method:\u003c/strong\u003e With Foundry, you will be charged for the Quantumworks Lab units (LBUs) your account consumes, plus any\u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003epass-through compute costs\u003c/u\u003e\u003c/a\u003e for the models you use.\u003c/p\u003e\u003cp\u003eIf you are on an annual contract plan, you may add a credit card on file to pay for the associated compute costs of a model or choose to receive a monthly invoice at the end of each month based on your organization’s Foundry usage.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-06_12-19-47.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1016\" height=\"738\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/2023-12-06_12-19-47.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/2023-12-06_12-19-47.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-06_12-19-47.png 1016w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"see-it-in-action-how-foundry-works\"\u003eSee it in action: How Foundry works\u003c/h2\u003e\u003cp\u003eIn the below interactive demos, you'll learn how to leverage foundation models to generate model predictions, send predictions as pre-labels to a labeling project in Annotate, and verify the predictions with human-in-the-loop review:\u003c/p\u003e\u003ch3 id=\"step-1-how-to-generate-model-predictions-with-foundry\"\u003eStep 1: How to generate model predictions with Foundry\u003c/h3\u003e\u003cp\u003eAccess a vast range of ready-to-use foundation models that embed advanced AI into your data tasks with ease. Quickly generate predictions to pre-label datasets or to enrich your existing data to extract better insights that boost productivity and increase time-savings.\u003c/p\u003e\u003ch3 id=\"step-2-how-to-send-foundry-predictions-as-pre-labels-to-a-labeling-project\"\u003e\u003cstrong\u003eStep 2: How to send Foundry predictions as pre-labels to a labeling project\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRather than labeling from scratch, combine the power of foundation models with human-in-the-loop review to accelerate your labeling operations.\u003c/p\u003e\u003ch3 id=\"step-3-how-to-verify-pre-labels-with-human-in-the-loop-review\"\u003eStep 3: How to verify pre-labels with human-in-the-loop review\u003c/h3\u003e\u003cp\u003eFocus human intelligence on critical quality assurance instead of on initial labeling efforts. Seamlessly validate model-generated pre-labels in Annotate – approving accurate predictions with a click and easily editing or sending incorrect labels to be corrected.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out the below tutorials to learn how Foundry can accelerate your pre-labeling and data enrichment workflows in Labelbox.\u0026nbsp;\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003cdiv style=\"position: relative; padding-bottom: calc(48.601036269430054% + 41px); height: 0;\"\u003e\u003ciframe src=\"https://demo.arcade.software/HnecUTysIiTR66fMWJ1C?embed\" title=\"Object Detection - Foundry Arcade \" frameborder=\"0\" loading=\"lazy\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;color-scheme: light;\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\u003c!--kg-card-end: html--\u003e\n\n\u003c!--kg-card-begin: html--\u003e\n\u003cdiv style=\"position: relative; padding-bottom: calc(48.57586742620404% + 41px); height: 0;\"\u003e\u003ciframe src=\"https://demo.arcade.software/PqkNFZRaNrGWBPjlotve?embed\" title=\"Named Entity Recognition - Foundry Arcade\" frameborder=\"0\" loading=\"lazy\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;color-scheme: light;\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003ch2 id=\"what-use-cases-is-foundry-available-for\"\u003eWhat use cases is Foundry available for?\u003c/h2\u003e\u003cp\u003eFoundry currently supports a variety of tasks for computer vision and natural language processing. This includes:\u003c/p\u003e\u003ch3 id=\"computer-vision\"\u003eComputer Vision\u003c/h3\u003e\u003cul\u003e\u003cli\u003eObject detection\u003c/li\u003e\u003cli\u003eImage classification\u003c/li\u003e\u003cli\u003eImage segmentation \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWatch the below demo to see how to use Foundry for an object detection use case with Amazon Rekognition and Grounding Dino:\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/78pt3397io\" title=\"Model Foundry - Image Demo with Amazon Rekognition and Grounding Dino Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThis demo uses Amazon Rekognition and Grounding Dino, but you can leverage any other \u003ca href=\"https://labelbox.com/model-foundry/models/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision model available in Foundry\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"natural-language-processing\"\u003eNatural Language Processing\u003c/h3\u003e\u003cul\u003e\u003cli\u003eText generation\u003c/li\u003e\u003cli\u003eTranslation\u003c/li\u003e\u003cli\u003eQuestion answering\u003c/li\u003e\u003cli\u003eZero-shot classification\u003c/li\u003e\u003cli\u003eSummarization\u003c/li\u003e\u003cli\u003eConversational\u003c/li\u003e\u003cli\u003eText classification\u003c/li\u003e\u003cli\u003eNamed entity recognition\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWatch the below demo to see how to use Foundry for a text classification use case with GPT-4:\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jf0g7ou9bk\" title=\"Model Foundry - Text Demo with GPT-4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThis demo uses GPT-4, but you can leverage any other \u003ca href=\"https://labelbox.com/model-foundry/models/?ref=labelbox-guides.ghost.io\"\u003elarge language model available in Foundry\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"how-is-foundry-priced\"\u003e\u003cstrong\u003eHow is Foundry priced?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFoundry pricing will be calculated and billed monthly based on the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInference cost\u003c/strong\u003e – Quantumworks Lab will charge customers for inference costs for all models hosted by Labelbox. Inference costs will be bespoke to each model available in Foundry. The inference price is determined based on vendors or our compute costs – these are published publicly on \u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour website\u003c/u\u003e\u003c/a\u003e as well as inside the product.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLabelbox's platform cost\u003c/strong\u003e – each asset with predictions generated by Foundry will accrue LBUs.\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eModel LBU\u003c/strong\u003e is consumed for every data row that goes through Foundry. \u003cstrong\u003eAnnotate LBU\u003c/strong\u003e is consumed for every data row that has a prediction (from Foundry) submitted as a label.\u003c/p\u003e\u003cp\u003eThe unit compute costs are listed on the model card for each model found in the Quantumworks Lab app.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Iqv7Lg9_XlQFIdXK_NVWx-Az4ig-zIlTZ2H2ZFiCsb-BQ-U94-VdccpvJ17SpayYK4VkS-GbAnGBWu8qpvfiARtORoPagIpr4wcqUSHRWq9LcmLBJ5eNOGCLMpQ-d-_c1_Xo9IrT1iZ9nAmigLk0A_A\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"220\"\u003e\u003c/figure\u003e\u003cp\u003eLearn more about the pricing of the Foundry add-on for Quantumworks Lab Model on our \u003ca href=\"https://labelbox.com/pricing/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003epricing page\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"where-can-i-provide-feedback-or-ask-questions\"\u003e\u003cstrong\u003eWhere can I provide feedback or ask questions?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf you have questions or feedback related to the Foundry workflow, please\u003ca href=\"https://labelbox.atlassian.net/servicedesk/customer/portal/2/group/3/create/214?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003esubmit a ticket here\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"additional-resources\"\u003eAdditional Resources\u0026nbsp;\u003c/h2\u003e\u003cp\u003eTo learn more about Foundry and familiarize yourself with the workflow, we recommend checking out the below resources:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox/docs/foundry?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eFoundry Documentation\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eDemo videos\u0026nbsp;\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.wistia.com/medias/78pt3397io?ref=labelbox-guides.ghost.io\"\u003eImage \u003c/a\u003e- Amazon Rekognition and Grounding DINO Demo\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.wistia.com/medias/jf0g7ou9bk?ref=labelbox-guides.ghost.io\"\u003eText \u003c/a\u003e- GPT-4 Demo\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModels currently available in Foundry\u003c/a\u003e\u003c/li\u003e\u003cli\u003eRecent blog posts exploring the power of foundation models:\u003cul\u003e\u003cli\u003eExplore six of the most powerful foundation models available to AI builders, the use cases and applications they are best suited for. \u003ca href=\"https://labelbox.com/blog/6-cutting-edge-foundation-models-for-computer-vision-and-how-to-use-them/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eGPT-4 offers a versatile toolkit to help enterprises kickstart labeling. Learn how to leverage GPT-4 for your machine learning or specific business use case. \u003ca href=\"https://labelbox.com/blog/how-to-unlock-the-full-power-of-gpt-4-for-enterprise-ai/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eMany businesses have more specialized and domain-specific use cases. Discover the benefits of incorporating foundation models into your specialized AI application development workflow. \u003ca href=\"https://labelbox.com/blog/why-you-should-leverage-foundation-models-for-specialized-ai-applications/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eRemoving personally identifiable information from datasets is crucial for teams building AI. Explore how you can extract PII from your datasets with higher accuracy and speed. \u003ca href=\"https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"65300bc0a6fedb0001417480","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/12/Welcome-to-the-Foundry-1--2-.png","featured":false,"visibility":"public","created_at":"2023-10-18T16:45:52.000+00:00","updated_at":"2023-12-12T15:05:47.000+00:00","published_at":"2023-10-18T17:21:36.000+00:00","custom_excerpt":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/guide-to-using-model-foundry","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/guide-to-using-model-foundry/","excerpt":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":"How to use the model Foundry for automated data labeling and enrichment","og_description":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to use the model Foundry for automated data labeling and enrichment","meta_description":"Introducing the model Foundry - Enrich data and automate tasks using foundation models","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64ff69ca935e200001ee0180","uuid":"89463a23-02d8-47e3-98db-49448eb94d96","title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","slug":"how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox","html":"\u003cp\u003eIn machine learning, fine-tuning pre-trained models is a powerful technique that adapts models to new tasks and datasets. Fine-tuning takes a model that has already learned representations on a large dataset, such as a large language model, and leverages prior knowledge to efficiently “teach” the model a new task. \u003c/p\u003e\u003cp\u003eThe key benefit of fine-tuning is that it allows you to take advantage of transfer learning. Rather than training a model from scratch, which requires massive datasets and compute resources, you can start with an existing model and specialize it to your use case with much less data and resources. Fine-tuning allows ML teams to efficiently adapt powerful models to new tasks with limited data and compute. It is essential for applying state-of-the-art models to real-world applications of AI. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning?ref=labelbox-guides.ghost.io\"\u003eOpen AI’s fine-tuning API\u003c/a\u003e allows teams to fine-tune the following models:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGPT-3.5 -turbo-0613 (recommended)\u003c/li\u003e\u003cli\u003eBabbage-002\u003c/li\u003e\u003cli\u003eDavinci-002\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we’ll cover how to leverage \u003ca href=\"https://www.ssw.com.au/rules/what-is-gpt/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eOpen AI’s GPT-3.5\u003c/a\u003e and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe goal of model fine-tuning is to improve the model’s performance against a specific task. Over other techniques to optimize model output, such as prompt design, fine-tuning can help achieve:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigher quality results: \u003c/strong\u003eFine-tuning allows the model to learn from a much larger and more diverse dataset than can fit into a prompt. The model can learn more granular patterns and semantics that are relevant to your use case through extensive fine-tuning training. Prompts are limited in how much task-specific context they can provide, while fine-tuning teaches the model your specific domain.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eToken savings: \u003c/strong\u003eFine-tuned models require less prompting to produce quality outputs. With fine-tuning, you can leverage a shorter, more general prompt since the model has learned your domain – saving prompt engineering effort and tokens. Whereas highly-specific prompts can often hit token limits.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLower latency: \u003c/strong\u003eHeavily engineered prompts can increase latency as they require more processing. As fine-tuned models are optimized for your specific task, they allow faster inference and can quickly retrieve knowledge for your domain.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFine-tuning is especially beneficial for adapting models to your specific use case and business needs. There are several common scenarios where fine-tuning really can help models capture the nuances required for an application:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStyle, tone, or format customization:\u003c/strong\u003e Fine-tuning allows you to adapt models to match the specific style or tone required for a use case, whether it be a particular brand voice or difference in tone for speaking to various audiences.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDesired output structure: \u003c/strong\u003eFine-tuning can teach models to follow a required structure or schema in outputs. For example, you can fine-tune a summarization model to consistently include key facts in a standardized template.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHandling edge cases: \u003c/strong\u003eReal-world data often contains irregularities and edge cases. Fine-tuning allows models to learn from a wider array of examples, including rare cases. You can fine-tune the model on new data samples so that it learns to handle edge cases when deployed to production.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn short, fine-tuning allows teams to efficiently adapt powerful models to new tasks and datasets, allowing ML teams to customize general models to their specific use cases and business needs through extensive training on curated data. High-quality fine-tuning datasets are crucial to improve performance by teaching models the nuances and complexity of the target domain more extensively than possible through prompts alone.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOpen AI’s recommended dataset guidelines\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo fine-tune an Open AI model, it is required to provide at least ten examples. Research has shown clear improvements from fine-tuning on 50 to 100 training examples with GPT-3.5-turbo. Data quality, over data quantity, is also critical to the success of the fine-tuned model. \u003c/p\u003e\u003cp\u003eYou can learn more about preparing a dataset in \u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?ref=labelbox-guides.ghost.io\"\u003eOpen AI’s documentation\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"how-to-use-labelbox-for-fine-tuning\"\u003eHow to use Quantumworks Lab for fine-tuning\u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform for building intelligent applications. With a suite of powerful data curation, labeling, and model evaluation tools, the platform is built to help continuously improve and iterate on model performance. For this example, we will use the Quantumworks Lab platform to create a high-quality fine-tuning dataset. \u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can prepare a dataset of prompts and responses to fine-tune large language models (LLMs). Quantumworks Lab supports dataset creation for a variety of fine-tuning tasks including summarization, classification, question-answering, and generation.\u003c/p\u003e\u003cp\u003eWhen you set up an \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003eLLM data generation project\u003c/a\u003e in Quantumworks Lab, you will be prompted to specify how you will be using the editor. You have three choices for specifying your LLM data generation workflow:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 1: Humans generate prompts and responses\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, the prompt and response fields will be required. This will indicate to your team that they should create a prompt and response from scratch.\u003c/p\u003e\u003cp\u003eYou can make a copy of this \u003ca href=\"https://colab.research.google.com/drive/1ElV4U68ZJCJ-wIMLFTQAyzXrSSz6aU2Y?ref=labelbox-guides.ghost.io#scrollTo=HUhjjPp0mnPq\"\u003eGoogle Colab Notebook\u003c/a\u003e to generate a prompt and response dataset in Quantumworks Lab and fine-tune GPT-3.5 Turbo in OpenAI. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 2: Humans generate prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, only the prompt field will be required. This will indicate to your team that they should create a prompt from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 3: Humans generate responses to uploaded prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, a previously uploaded prompt will appear. Your team will need to create responses for that prompt.\u003c/p\u003e\u003cp\u003eYou can make a copy of this \u003ca href=\"https://colab.research.google.com/drive/1J21Nb0fqbZCcnxyR4wOKLlADwcayOlSq?ref=labelbox-guides.ghost.io#scrollTo=vGHl-lKkZ9cK\"\u003eGoogle Colab Notebook\u003c/a\u003e to generate a dataset of responses to uploaded prompts in Quantumworks Lab and fine-tune GPT-3.5 Turbo in OpenAI. \u003c/p\u003e\u003cp\u003eIn the below example, we’ll be walking through a sample use case of summarizing and removing PII from customer support chats with Quantumworks Lab and OpenAI’s GPT-3.5 Turbo. Imagining we’re a company who wishes to summarize support logs without revealing personally identifiable information in the process, we’ll be fine-tuning an LLM to summarize and remove PII from customer support logs.\u003c/p\u003e\u003ch3 id=\"step-1-evaluate-how-gpt-35-performs-against-the-desired-task\"\u003eStep 1: Evaluate how GPT-3.5 performs against the desired task\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/e00fk9u59h\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBefore we begin the fine-tuning process, let’s first evaluate how ChatGPT (using GPT-3.5) performs against the desired task off-the-shelf. \u003c/p\u003e\u003cp\u003eWe uploaded the following sample chat log to ChatGPT:\u003c/p\u003e\u003cp\u003e“Summarize this chat log and remove any personally identifiable information in the summary:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTom: \u003c/strong\u003eI need to reset my account access.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUrsula: \u003c/strong\u003eI can help with that, Tom. What’s your account email?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTom: \u003c/strong\u003eIt’s \u003ca href=\"mailto:tom@example.com\"\u003etom@example.com\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUrsula: \u003c/strong\u003eGreat, Tom. I’ve sent you a link to update your credentials”\u003c/p\u003e\u003cp\u003eIn the above prompt, we’ve asked ChatGPT to summarize the chat log and remove any personally identifiable information. \u003c/p\u003e\u003cp\u003eUpon evaluation, the default GPT-3.5 model misses the mark for our desired use case. \u003c/p\u003e\u003cp\u003eThe summary includes both Tom and Ursula’s names and explicitly mentions Tom’s email address. In order to reliably use the model for our business use case, we need to fine-tune it so that it appropriately excludes elements of personally identifiable information. To do so, we will leverage Quantumworks Lab to generate our fine-tuning dataset and use it to fine-tune GPT-3.5 through OpenAI. \u003c/p\u003e\u003ch3 id=\"step-2-create-a-llm-data-generation-project-in-labelbox\"\u003eStep 2: Create a LLM data generation project in Quantumworks Lab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ldpv9nwh14\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to upload our support chat logs to \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Catalog \u003c/a\u003e– this will allow us to browse, curate, and send these data rows for labeling.\u003c/p\u003e\u003cp\u003eNext, we’ll need to create a LLM data generation labeling project in \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Annotate\u003c/a\u003e.\u003c/p\u003e\u003cul\u003e\u003cli\u003eSince we have an available dataset, this will be a ‘Humans generate response to uploaded prompts’ project.\u003c/li\u003e\u003cli\u003eWhen configuring the ontology, we will set the response type as ‘text’ and make the appropriate response to “summarize and remove personally identifiable information in the summary”.\u003c/li\u003e\u003cli\u003eDuring ontology creation, you can also define a character minimum or maximum and upload necessary instructions for the labeling team.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-label-data\"\u003eStep 3: Label data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/i8tzhezn70\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter successfully setting up an LLM data generation project, we can queue the uploaded chat logs in Catalog for labeling in Annotate. To label data, you have the option of leveraging \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost’s\u003c/a\u003e extensive workforce or use your own internal team to summarize and remove personally identifiable information in the summary.\u003c/p\u003e\u003cp\u003eFor larger or more complex fine-tuning tasks, you can scale up to hundreds or thousands of labeled data rows. Once all data has been labeled, you can review the corresponding summary to each prompt and export the data rows.\u003c/p\u003e\u003ch3 id=\"step-4-export-data-from-labelbox-and-fine-tune-it-in-openai\"\u003e\u003cbr\u003eStep 4: Export data from Quantumworks Lab and fine-tune it in OpenAI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bx1z0xy4db\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWith all necessary data labeled, we can export the dataset from Quantumworks Lab and upload it in a format that is readable by OpenAI. \u003c/p\u003e\u003cp\u003eOpenAI requires a dataset to be in the structure of their \u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create?ref=labelbox-guides.ghost.io\"\u003echat completions API\u003c/a\u003e, whereby each message has a role, content, and optional name. You can learn more about specific dataset requirements in OpenAI’s \u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. Using a script, we can convert the Quantumworks Lab export into OpenAI’s required conversational chat format.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e{\n \"messages\":\n \t{\"role\":\"system\",\n    \"content\":\"Given a chat log, summarize and remove personal \t\tidentifiable information in the summary.\"},\n    {\"role\":\"user\",\n    \"content\":\"Andy:Why has my order not shipped yet?! Bella: I \tapologize for the delay, Andy:May I have your order number? Andy: \t  It's ORDER5678. Please hurry! Bella: Thank you Andy. It's \t\t\texpedited and will ship today.\"},\n\t{\"role\":\"assistant\",\n    \"content\":\"Customer inquires about the delay in the shipment of his order. Support agent requests the order number and upon receiving \t  it, assures customer that the order has been expedited and will \t\tship that day.\"\n    }\n ]\n}\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003eAfter formatting our dataset, we can upload it and \u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model?ref=labelbox-guides.ghost.io\"\u003estart a fine-tuning job\u003c/a\u003e using the OpenAI SDK.\u003c/p\u003e\u003cp\u003eYou can use a copy of the following \u003ca href=\"https://colab.research.google.com/drive/1J21Nb0fqbZCcnxyR4wOKLlADwcayOlSq?ref=labelbox-guides.ghost.io#scrollTo=vGHl-lKkZ9cK\"\u003eGoogle Colab notebook\u003c/a\u003e to export data from Quantumworks Lab in a format compatible with fine-tuning GPT-3.5 Turbo and begin a fine-tuning job. \u003c/p\u003e\u003ch3 id=\"step-5-assess-the-fine-tuned-model%E2%80%99s-performance-in-openai\"\u003eStep 5: Assess the fine-tuned model’s performance in OpenAI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9kdb8m0xm0\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 5) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter the fine-tuning job has succeeded, you can navigate to the OpenAI playground and select the newly fine-tuned model for evaluation. Similarly to evaluating the initial GPT-3.5 model, we can enter a sample chat log and see how the newly fine-tuned model performs.\u003c/p\u003e\u003cp\u003eCompared to the off-the-shelf GPT-3.5 model, this model that has been fine-tuned on our training data is performing as expected. We can see that all names and relevant information that would be considered as personally identifiable information has been retracted. \u003c/p\u003e\u003cp\u003eWe can also compare the fine-tuned model to the initial GPT-3.5 model and see how it performs on the same prompt. Again, we can see that while GPT-3.5 excludes some aspects of personally identifiable information, it still includes the user’s first name, so it doesn’t quite meet the expectations for our business use case.\u003c/p\u003e\u003cp\u003eThe newly fine-tuned model has allowed us to adapt GPT-3.5 to our specific use case of concealing personally identifiable information. With Quantumworks Lab, teams can iteratively identify gaps and outdated samples in the fine-tuning data, then generate fresh high-quality data, allowing model accuracy to be maintained over time. Updating fine-tuning datasets through this circular feedback process is crucial for adapting to new concepts and keeping models performing at a high level within continuously changing environments.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eTo improve LLM performance, Quantumworks Lab simplifies the process for subject matter experts to generate high-quality datasets for fine-tuning with leading model providers and tools, like OpenAI. \u003c/p\u003e\u003cp\u003eUnlock the full potential of large language models with Quantumworks Lab’s end-to-end platform and a \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox.ghost.io\"\u003enew suite of LLM tools\u003c/a\u003e to generate high-quality training data and optimize LLMs for your most valuable AI use cases.\u003c/p\u003e","comment_id":"64ff69ca935e200001ee0180","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Group-3081.jpg","featured":false,"visibility":"public","created_at":"2023-09-11T19:26:02.000+00:00","updated_at":"2024-05-28T17:02:27.000+00:00","published_at":"2023-09-11T19:58:50.000+00:00","custom_excerpt":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/","excerpt":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","og_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Group-3081-1.jpg","twitter_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","twitter_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","meta_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","meta_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":17,"allPosts":[{"id":"66bfded301c06500016e8b1c","uuid":"a2c40e88-6376-4824-b57f-e0e3673d8b1b","title":"Evaluating leading text-to-speech models","slug":"evaluating-leading-text-to-speech-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in speech generation evaluation, visit \u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe evolution of Text-to-Speech (TTS) models has been groundbreaking, with various companies offering high-quality solutions for a range of applications. Whether it's enhancing accessibility, automating customer service, or generating voiceovers, TTS models play a vital role in today's digital landscape. \u003c/p\u003e\u003cp\u003eWe recently evaluated the performance of six leading TTS models—Google TTS, Cartesia, AWS Polly, OpenAI TTS, Deepgram, and Eleven Labs—using key evaluation metrics such as Word Error Rate (WER) and human preference ratings.\u003c/p\u003e\u003cp\u003eTo provide an in-depth analysis, we evaluated each model on 500 prompts. Each prompt was reviewed by three expert labelers to ensure accuracy and provide diverse perspectives.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"evaluation-process\"\u003e\u003cstrong\u003eEvaluation Process\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate speech outputs across several key criteria:\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer\"\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWER measures the accuracy of the generated speech by calculating the number of insertions, deletions, and substitutions compared to a reference transcription. The lower the WER, the better the model performed in terms of generating accurate and coherent speech.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1120\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"word-errors\"\u003eWord errors\u003c/h3\u003e\u003cp\u003eRaters evaluated the presence of word errors in the generated TTS audio by counting the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Insertion Errors\u003c/strong\u003e: Additional words not present in the original text.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Deletion Errors\u003c/strong\u003e: Words from the original text that were omitted.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Substitution Errors\u003c/strong\u003e: Incorrect replacements for original words.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: No word errors or only minor errors (0-1 error).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some errors (2-3 errors).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Significant errors (4 or more errors).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"speech-naturalness\"\u003eSpeech naturalness\u003c/h3\u003e\u003cp\u003eRaters assessed how human-like the generated speech sounded.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Very human-like, natural flow with appropriate pauses and inflections.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some human qualities but with occasional robotic or awkward elements.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Clearly robotic or artificial, with choppy or monotone speech.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"pronunciation-accuracy\"\u003ePronunciation accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated how clearly and correctly words were pronounced in the TTS output.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: All words are pronounced clearly and correctly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: 1-2 words are mispronounced or unclear.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: 3 or more words are mispronounced or difficult to understand.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"noise\"\u003eNoise\u003c/h3\u003e\u003cp\u003eRaters evaluated the level of background noise or artifacts in the audio.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNone\u003c/strong\u003e: No detectable noise or artifacts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Minor noise, such as a faint hiss, but not distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Noticeable noise or artifacts that may be somewhat distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Significant noise or artifacts that interfere with comprehension.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"context-awareness\"\u003eContext awareness\u003c/h3\u003e\u003cp\u003eRaters assessed the TTS system's ability to adjust to context, including tone, emphasis, and punctuation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Excellent adaptation to context, with clear tonal shifts and pauses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Adequate adaptation but misses some nuances.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Little to no adaptation; reads text monotonously without context cues.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"prosody-accuracy\"\u003eProsody accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated the rhythm, stress, and intonation (prosody) of the generated speech.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Natural-sounding rhythm and intonation that closely mimics human speech.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Mostly natural but with occasional awkwardness in stress or rhythm.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Monotonous or robotic intonation with unnatural pauses and emphasis\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"prompts-curation-process\"\u003ePrompts curation process\u003c/h2\u003e\u003cp\u003eThe diverse set of prompts used in our TTS evaluation provides a comprehensive test of model capabilities across multiple dimensions. \u003c/p\u003e\u003cp\u003eConsider these examples:\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Wow! Did you see the fireworks last night? They were spectacular!\"\u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003e This short, exclamatory sentence tests the model's ability to convey excitement and emphasis. It challenges the TTS system to appropriately modulate tone and pitch to express enthusiasm.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"I wish you were on the right path...I-I-I truly do, but you're not,' he said sadly.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis prompt evaluates the model's handling of emotional nuance, hesitation (indicated by the stuttering \"I-I-I\"), and the use of ellipsis for pauses. It also tests the system's ability to convey sadness as specified in the dialogue tag.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Alright, listen up, team! The mission briefing is as follows: Operation Nighthawk commences at 0300 hours. Rendezvous at coordinates 51°30'26.5\"N 0°07'39.0\"W. Your login credentials for the secure server are: Username: NightOwl_Alpha, Password: 3ch3l0n_X9#. Remember, this is a stealth operation. Radio silence must be maintained except for emergency protocol Sierra-Tango-Seven. If compromised, use the failsafe code: Zulu-Yankee-4-7-2-Bravo. Any questions? No? Then synchronize your watches... now. Good luck, and godspeed.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis complex prompt tests multiple aspects:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical pronunciation of alphanumeric strings and military jargon\u003c/li\u003e\u003cli\u003eConveying urgency and authority\u003c/li\u003e\u003cli\u003eHandling of coordinates and precise numerical information\u003c/li\u003e\u003cli\u003eProper pacing for a lengthy, detailed instruction set\u003c/li\u003e\u003cli\u003eAppropriate pausing, indicated by punctuation and paragraph breaks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese prompts demonstrate how we cover a range of linguistic features, emotional tones, and formatting challenges. They span from simple exclamations to complex, technical instructions, ensuring a thorough assessment of prosody, emphasis, and versatility.\u003c/p\u003e\u003cp\u003eThis diversity in prompts matters because it enables a comprehensive evaluation that reflects real-world applicability. By covering such a broad spectrum of scenarios, we can identify specific strengths and weaknesses in each TTS model, allowing for fair comparisons and pinpointing areas for improvement. Whether it's expressing joy over fireworks, conveying emotional conflict, or delivering a detailed mission briefing, these prompts help us ensure that TTS models can handle the variety and complexity of human communication.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overall-ranking-best-to-worst-model-samples\"\u003eOverall Ranking best to worst model samples\u003c/h2\u003e\u003cp\u003eRaters ranked each TTS sample from best to worst based on performance across all categories above.\u003c/p\u003e\u003ch3 id=\"model-configurations-used-at-test-time\"\u003e\u003cstrong\u003eModel configurations used at test time\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: tts-1-hd\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: alloy\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle Text-to-Speech (TTS)\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel:\u003c/strong\u003e Default\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice: \u003c/strong\u003eDefault with neutral gender voice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eElevenLabs\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: eleven_monolingual_v1 (As of 30th September, eleven_monolingual_v1 is an outdated version and Quantumworks Lab will run a new experiment with\u0026nbsp;eleven_multilingual_v2 in the future)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: pMsXgVXv3BLzUgSXRplE\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: aura-asteria-en\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Default\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAmazon Polly\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Generative\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Ruth\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: sonic-english\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Teacher Lady\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe selected these models for our evaluation as they represent the cutting-edge offerings from leading companies in the TTS space, showcasing the latest advancements in voice synthesis technology.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"results\"\u003eResults\u003c/h1\u003e\u003cp\u003eOur evaluation of 500 diverse and complex prompts provided valuable insights into the capabilities of Google, Cartesia, AWS Polly, OpenAI, Deepgram, and Eleven Labs TTS models. Each prompt was meticulously assessed by three different raters to ensure accurate and diverse perspectives. This rigorous testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of these leading TTS models:\u003c/p\u003e\u003ch3 id=\"rank-distribution-percentages\"\u003eRank distribution percentages\u003cstrong\u003e\u0026nbsp; \u0026nbsp; \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"overall-ranking\"\u003eOverall ranking\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1:\u003c/strong\u003e\u0026nbsp; (Open AI TTS) - occurred 607 times, 42.93%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2:\u003c/strong\u003e\u0026nbsp; (Cartesia) - occurred 460 times, 32.53%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 410 times, 29.00%\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 4:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 338 times, 23.90%. Followed by Eleven labs as a close second with occurrence 335, 23.69%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 5:\u003c/strong\u003e\u0026nbsp; (Deepgram) - occurred 438 times, 30.98%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 6:\u003c/strong\u003e\u0026nbsp; (Google TTS) - occurred 868 times, 61.39%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote: Rank is determined by the most frequently selected model for each position, which is why some AWS Polly appears twice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1122\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003eFig: % of times each model was ranked first.\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer-results\"\u003eWord Error Rate (WER) Results\u003c/h3\u003e\u003cp\u003eThe WER scores offer an objective look at how accurately each model converts text to speech.\u003c/p\u003e\u003cul\u003e\u003cli\u003eDeepgram: 5.67%\u003c/li\u003e\u003cli\u003eOpenAI TTS: 4.19%\u003c/li\u003e\u003cli\u003eGoogle TTS: 3.36%\u003c/li\u003e\u003cli\u003eAWS Polly: 3.18%\u003c/li\u003e\u003cli\u003eCartesia: 3.87%\u003c/li\u003e\u003cli\u003eEleven Labs: 2.83%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs achieved the lowest WER at 2.83%, making it the most accurate model. Deepgram, however, had the highest WER at 5.67%, indicating room for improvement in transcription accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" title=\"Chart\" width=\"2000\" height=\"1126\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1123\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-specific-performance-results\"\u003eModel-Specific Performance Results\u003c/h2\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each TTS model based on key evaluation criteria.\u003c/p\u003e\u003ch3 id=\"openai-tts\"\u003eOpenAI TTS\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 4.1898%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 89.60% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 87.13% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 92.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 63.37% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 64.57% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOpenAI TTS excels at producing natural-sounding speech, making it a top choice for applications requiring lifelike vocal output. It demonstrates exceptional accuracy in pronunciation and performs excellently in producing clean audio with minimal background noise. The model shows strong capabilities in understanding and conveying contextual nuances in speech and is proficient in delivering appropriate intonation and rhythm. Despite ranking high in human preference, it shows moderate accuracy in word reproduction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cm24ur6wwf\" title=\"OpenAI: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gicqvkqc5e\" title=\"OpenAI: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kg9wqqftu4\" title=\"OpenAI: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"cartesia\"\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 58.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 53.11% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 93.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 80.98% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 77.02% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.8707%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCartesia demonstrates good accuracy in word reproduction. While not as consistent as OpenAI TTS in naturalness, it still performs very well in producing natural-sounding speech. It shows excellent accuracy in pronunciation and excels at producing clean audio with minimal background noise. The model displays good capabilities in understanding contextual nuances and performs well in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/3gukhi7z2j\" title=\"Cartesia: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y3lbes0sin\" title=\"Cartesia: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f0lmkto1xk\" title=\"Cartesia: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"aws-polly\"\u003e\u003cstrong\u003eAWS Polly\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 56.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 55.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.39% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 84.72% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Medium in 58.20% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.1770%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAWS Polly shows very good accuracy in word reproduction, ranking second among the models. It struggles more with producing natural-sounding speech compared to top performers but maintains excellent pronunciation accuracy. The model performs well in producing clean audio and shows good capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qawx1dwc9g\" title=\"Polly: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/klmnfcsno5\" title=\"AWS Polly: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ubzrqtjnc9\" title=\"AWS Polly: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"eleven-labs\"\u003e\u003cstrong\u003eEleven Labs\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 2.8302%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: Medium in 44.98% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 81.97% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 80.27% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 44.70% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 46.25% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs demonstrates the best accuracy in word reproduction among all models. It shows mixed results in producing natural-sounding speech but maintains excellent pronunciation accuracy. The model performs reasonably well in producing clean audio but has more noise issues than some competitors. It shows moderate capabilities in understanding contextual nuances and has room for improvement in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/x3qzhkt0ab\" title=\"Eleven Labs: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kl0lc7oqmz\" title=\"Eleven Labs: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bc19r3iqip\" title=\"Eleven Labs: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"deepgram\"\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 5.6743%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: High in 57.78% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 64.43% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 88.83% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 53.18% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 55.52% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDeepgram shows the highest error rate in word reproduction among all models. It performs moderately well in producing natural-sounding speech and demonstrates good accuracy in pronunciation, although lagging behind some competitors. The model performs well in producing clean audio with minimal background noise but shows moderate capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mngnvgtzx3\" title=\"Deepgram: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6h3bgmcg6d\" title=\"Deepgram: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uaj3x9bln8\" title=\"Deepgram: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"google-tts\"\u003e\u003cstrong\u003eGoogle TTS\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.3574%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Low in 78.01% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 77.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.46% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: Medium in 39.25% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: Low in 45.83% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z27ett9q8m\" title=\"Google TTS: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/06pqi5qc13\" title=\"Google TTS: Wow Did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kwotkto0u6\" title=\"Google TTS: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation reveals that while Eleven Labs leads in transcription accuracy with the lowest WER, OpenAI TTS emerges as the top choice when it comes to human preference, particularly for speech naturalness, pronunciation accuracy, and prosody.\u003c/p\u003e\u003cp\u003eConversely, Google TTS, despite a relatively low WER, ranks last in human preference due to poor performance in categories like speech naturalness and context awareness. This demonstrates the importance of a balanced evaluation that considers both quantitative metrics like WER and qualitative factors like user satisfaction.\u003c/p\u003e\u003cp\u003eAs the TTS field continues to evolve, the key to further advancements will lie in combining high transcription accuracy with natural, expressive, and context-aware speech. Models like OpenAI TTS and Eleven Labs are setting the standard, but there remains room for improvement across all models to fully meet the growing demands of diverse TTS applications.\u003c/p\u003e\u003cp\u003eIn summary, while WER is crucial, the broader context of user experience—focusing on factors like speech naturalness, context awareness, and prosody accuracy—is essential in evaluating the overall effectiveness of TTS models.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-speech models presented here represents a significant advancement in assessing AI-generated speech. By leveraging this methodology alongside Quantumworks Lab's cutting-edge platform, AI teams can dramatically accelerate the development and refinement of sophisticated, domain-specific speech generation models.\u003c/p\u003e\u003cp\u003eOur solution offers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eEfficient dataset curation\u003c/li\u003e\u003cli\u003eAutomated evaluation techniques\u003c/li\u003e\u003cli\u003eHuman-in-the-loop quality assurance\u003c/li\u003e\u003cli\u003eCustomizable workflows for TTS model assessment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-speech model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdohXa2KeUawTMTWS7MfKSUYC6fVPCHjtmcFCRt5ycmB7S4k4-JElTwnmRb6ufW2kR7F3Bqu1PwzbvL83LKcqgz8jbkoY02tefklFLp98wvqm2liOKBglEGaRITIk6Wx1tY3fi-43TaCJnpwEwRVCz7q101?key=Qy_gZn4bA_q73M6n7nwEIA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"1024\"\u003e\u003c/figure\u003e","comment_id":"66bfded301c06500016e8b1c","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-12.12.46-PM.png","featured":false,"visibility":"public","created_at":"2024-08-16T23:20:51.000+00:00","updated_at":"2024-11-26T00:11:38.000+00:00","published_at":"2024-08-17T19:13:25.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/evaluating-leading-text-to-speech-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"668c2f59878a56000112a4e0","uuid":"93014cdf-3555-4d2b-b480-2b9db686e2db","title":"Using multimodal chat to enhance a customer’s online support experience","slug":"using-multimodal-chat-to-enhance-a-customers-online-support-experience","html":"\u003cp\u003eCustomer service systems are prime targets for multimodal chat as this new technology leverages an intuitive interface for answering user questions\u0026nbsp; while enabling companies to unlock previously hidden insights about customer journeys and sentiment.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can streamline the process of labeling and managing data for various use cases, including intent classification, dialogue retrieval, state tracking, and model evaluation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. We’ll illustrate the various labeling tasks that Quantumworks Lab can accomplish to ensure the chatbot is intelligent and effective, providing customers with relevant and timely answers.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://files.readme.io/ce6e056-Labelbox_2024-06-04_10-37-061.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"678\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eLabelbox offers a \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/docs/live-multimodal-chat-evaluation?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003enumber of editor types\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, including the live multimodal chat editor.\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"labeling-tasks\"\u003e\u003cstrong\u003eLabeling Tasks\u003c/strong\u003e\u003c/h1\u003e\u003ch2 id=\"multimodal-intent-classification-dialogue-retrieval-and-state-tracking\"\u003e\u003cstrong\u003eMultimodal Intent Classification, Dialogue Retrieval and State Tracking\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xs53lmro56\" title=\"Customer Query Resolution Training - Part 1 - Training Data Collection + Using Classifications Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 1: Generate prompt-response pairs for supervised fine-tuning of agentic workflows.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eLabel the customer’s intent and sentiments in a given turn of the chat, so you can identify the appropriate next steps accordingly.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1427\" height=\"1333\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/first-one.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/first-one.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png 1427w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe customer appears frustrated, so the sentiment is labeled as “frustrated.”\u003c/p\u003e\u003cp\u003eFurthermore, the customer appears to be reporting their order was damaged and is requesting a new one, so both of those requests are labeled using checklist classifications.\u003c/p\u003e\u003cp\u003eWith these labels, you know the customer’s intent (get a new piece) and can also record the current state of the conversation (damage reported but no action taken yet). As the conversation proceeds, you can update the state based on what actions are taken and keep checking the intent and tone of the customer to ensure they are satisfied.\u003c/p\u003e\u003cp\u003eLabelbox allows you to label every aspect of every turn-by-turn conversation, enabling you to develop fine-grained data sets of annotated real-world conversations.\u003c/p\u003e\u003cp\u003eYou can use these labels to train your agent to retrieve similar past interactions and follow similar response patterns or simply be inspired by them.\u003c/p\u003e\u003ch2 id=\"model-evaluation-ranking-and-selection\"\u003e\u003cstrong\u003eModel evaluation, ranking and selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eYou trained a few models offline with the previously generated data using supervised fine-tuning or RLHF. How do you know which of the models is the best performing, and how often do they make mistakes?\u003c/p\u003e\u003cp\u003eLabelbox allows you to connect these models hosted behind your custom endpoints. Then, with the Live Multimodal Chat editor, labelers can interact with all of them simultaneously by feeding prompts, comparing and contrasting responses, and even rewriting the best ones to be even better.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1vpskixih4\" title=\"Customer Query Resolution Training - Part 2 - LLM Development \u0026amp; Evaluation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 2: Evaluate a single model, generate multiple responses and write your own gold standard response.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eWhen evaluating a single model, Quantumworks Lab allows you to generate multiple responses and also add your own response. This is very useful for use cases where all of the model’s responses are lacking in quality and a human-written response provides the gold standard for further downstream training. Furthermore, in the course of the labeling conversation, you can choose one of these responses to be passed in as prior context into later turns, allowing you to choose the path of the conversation in real-time for better training data generation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uzesocscxk\" title=\"Customer Query Resolution Training - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 3: Evaluate multiple models at once in “chat arena” style.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/secone-one-1-1-1-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"450\" height=\"640\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox also supports evaluating multiple models in a “chat arena” style setup.\u003c/p\u003e\u003cp\u003eSay you have trained candidate models and now want to compare them against each other and also foundation models like GPT 4o. Connect your own models with Quantumworks Lab as \u003cem\u003ecustom\u003c/em\u003e models, and easily choose any latest and greatest foundation model from Foundry within Labelbox. The chat arena excludes information about which response was generated from which model so as not to bias the labeling process.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWith Quantumworks Lab's multimodal chat functionalities, you can generate high-quality datasets to train, evaluate, and test your custom LLMs or pit off-the-shelf LLMs like GPT4o and Gemini 1.5 against each other in a Chatbot Arena-style competition to see which one suits your needs best.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/b3f423vxob\" title=\"Customer Query Resolution Training - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 4: Survey of all LLM capabilities in Quantumworks Lab\u003c/em\u003e\u003c/p\u003e\u003cp\u003eAs the landscape of LLMs evolves rapidly, Quantumworks Lab will continue to build functionality to ensure our customers can align models to their use cases quickly and reliably. With differentiated data, streamlined operations, and transparent orchestration, you can build and refine foundation models that meet your specific needs.\u003c/p\u003e\u003ch2 id=\"evaluate-multimodal-chat-data-today-for-free\"\u003eEvaluate multimodal chat data today for free\u003c/h2\u003e\u003cp\u003eReady to deliver on the next-generation of generative AI?\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eSign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more and we’d love to hear from you.\u003c/p\u003e","comment_id":"668c2f59878a56000112a4e0","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/mmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp","featured":false,"visibility":"public","created_at":"2024-07-08T18:26:33.000+00:00","updated_at":"2024-07-10T16:09:14.000+00:00","published_at":"2024-07-09T16:06:15.000+00:00","custom_excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-multimodal-chat-to-enhance-a-customers-online-support-experience/","excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6656135f9132db0001f20551","uuid":"f173905b-1c7a-4f87-8064-ac8af0ccdaa7","title":"AI foundations: Understanding embeddings","slug":"ai-foundations-understanding-embeddings","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eEmbeddings transform complex data into meaningful vector representations, enabling powerful applications across various domains.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis post covers the theoretical foundations, practical examples, and demonstrates how embeddings enhance key use cases at Labelbox.\u003c/p\u003e\u003cp\u003eWhether you’re new to the concept or looking to deepen your understanding, this guide will provide valuable insights into the world of embeddings and their practical uses.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"what-is-an-embedding\"\u003eWhat is an embedding?\u003c/h1\u003e\u003cp\u003eAt its core, an embedding is a vector representation of information. This information can be of any modality—video, audio, text, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe process of generating embeddings involves a deep learning model trained on specific data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach index in a vector represents a numerical value, often a floating-point value between 0 and 1.\u003c/p\u003e\u003ch2 id=\"the-relationship-between-dimensions-and-detail\"\u003eThe relationship between dimensions and detail\u003c/h2\u003e\u003cp\u003eThe dimensions of a vector describe the level of detail it can capture. Higher dimensionality means higher detail and accuracy in the similarity score.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, a vector with only two dimensions is unlikely to store all relevant information, leading to simpler but less accurate representations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1362\" height=\"626\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1362w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"vector-representations-of-images\"\u003eVector representations of images\u003c/h2\u003e\u003cp\u003eFor instance, consider an image. The underlying compression algorithm converts this image into a vector representation, preserving all relevant information. These embeddings can then be used to determine the similarity between different pieces of data. For example, images of a cat and a dog would have different embeddings, resulting in a low similarity score.\u003c/p\u003e\u003cp\u003eAnother example: Let's consider a model trained on images of helicopters, planes, and humans. If we input an image of a helicopter, the model generates a vector representation reflecting this. If the image contains both a helicopter and humans, the representation changes accordingly. This process helps in understanding how embeddings are generated and used.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1240\" height=\"618\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1240w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Helicopter or not helicopter?\" captured via embeddings.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"importance-and-applications-of-embeddings\"\u003eImportance and applications of embeddings\u003c/h1\u003e\u003ch2 id=\"why-embeddings-matter\"\u003eWhy embeddings matter\u0026nbsp;\u003c/h2\u003e\u003cp\u003eAs we’ve described in the prior sections, understanding what embeddings are and how they work is crucial for several reasons:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCapturing Semantic Relationships\u003c/strong\u003e: Embeddings help capture complex relationships within data, enabling more accurate predictions and recommendations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Search Capabilities\u003c/strong\u003e: Embeddings facilitate efficient and accurate similarity searches, essential in applications like search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"applications-of-embeddings\"\u003eApplications of embeddings\u003c/h2\u003e\u003cp\u003eEmbeddings are extremely useful in search problems, especially within recommender systems.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHere are a few examples:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImage Similarity\u003c/strong\u003e: Finding images similar to an input image, like searching for all images of dogs based on a sample image.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eText-to-Image Search\u003c/strong\u003e: Using a textual query, such as \"image of a dog,\" to find relevant images.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContent Recommendations\u003c/strong\u003e: Identifying articles or movies similar to ones you've enjoyed, enhancing search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1296\" height=\"874\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1296w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEmbeddings are the essential ingredient to a smooth and enjoyable family movie night via recommendations systems, which can suggest movies similar to ones you've enjoyed in the past.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"how-embeddings-are-generated\"\u003eHow embeddings are generated\u003c/h1\u003e\u003cp\u003eDeep learning models, trained on either generalized or specific datasets, learn patterns from data to generate embeddings. Models trained on specific datasets, like cat images, are good at identifying different breeds of cats but not dogs. Generalized models, trained on diverse data, can identify various entities.\u003c/p\u003e\u003cp\u003eModern models produce embeddings of over 1024 dimensions, increasing accuracy and detail. However, this also raises challenges related to the cost of embedding generation, storage, and computational resources.\u003c/p\u003e\u003cp\u003eSome of the earliest work with creating embedding models included word2vec for NLP and convolutional neural networks for computer vision.\u003c/p\u003e\u003ch2 id=\"the-significance-of-word2vec\"\u003eThe significance of word2vec\u003c/h2\u003e\u003cp\u003eThe introduction of word2vec by Mikolov et al. in 2013 marked a significant milestone in creating word embeddings. Word2vec enabled the capture of semantic relationships between words, such as the famous king-queen and man-woman analogy, demonstrating how vectors can represent complex relationships.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis breakthrough revolutionized natural language processing (NLP), enhancing tasks like machine translation, sentiment analysis, and information retrieval.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWord2vec laid the foundation for subsequent embedding techniques and deep learning models in NLP.\u003c/p\u003e\u003ch2 id=\"equivalent-of-word2vec-for-images\"\u003eEquivalent of word2vec for Images\u003c/h2\u003e\u003cp\u003eFor images, convolutional neural networks (CNNs) serve as the equivalent of word2vec. Models like AlexNet, VGG, and ResNet revolutionized image processing by creating effective image embeddings.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese models convert images into high-dimensional vectors, preserving spatial hierarchies and semantic information.\u0026nbsp;\u003c/p\u003e\u003cp\u003eJust as word2vec transformed text data into meaningful vectors, CNNs transform images into embeddings that capture essential features, enabling tasks like object detection, image classification, and visual similarity search.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k_icon.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"250\" height=\"250\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eImageNet is a large-scale visual database designed for use in visual object recognition research, containing millions of labeled images across thousands of categories. Models like AlexNet, VGG, and ResNet demonstrated the power of CNNs using datasets like ImageNet.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"uploading-custom-embeddings-to-labelbox\"\u003eUploading custom embeddings to Quantumworks Lab\u0026nbsp;\u0026nbsp;\u003c/h2\u003e\u003cp\u003eLabelbox now supports importing and exporting custom embeddings seamlessly, allowing for better integration into workflows. This new feature provides flexibility in how embeddings are utilized, making it easier to leverage embeddings for various applications.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eimport Quantumworks Lab as lb\nimport transformers\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nimport requests\nimport numpy as np\n\n# Add your API key\nAPI_KEY = \"your_api_key_here\"\nclient = lb.Client(API_KEY)\n\n# Get images from a Quantumworks Lab dataset\nDATASET_ID = \"your_dataset_id_here\"\ndataset = client.get_dataset(DATASET_ID)\nexport_task = dataset.export_v2()\nexport_task.wait_till_done()\n\ndata_row_urls = [dr_url['data_row']['row_data'] for dr_url in export_task.result]\n\n# Get ResNet-50 from HuggingFace\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = transformers.ResNetModel.from_pretrained(\"microsoft/resnet-50\")\n\nimg_emb = []\n\nfor url in data_row_urls:\n    response = requests.get(url, stream=True)\n    image = Image.open(response.raw).convert('RGB').resize((224, 224))\n    img_hf = image_processor(image, return_tensors=\"pt\")\n    with torch.no_grad():\n        last_layer = model(**img_hf, output_hidden_states=True).last_hidden_state\n        resnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\n        resnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\n        img_emb.append(resnet_embeddings.cpu().numpy())\n\ndata_rows = []\n\nfor url, embedding in zip(data_row_urls, img_emb):\n    data_rows.append({\n        \"row_data\": url,\n        \"embeddings\": [{\"embedding_id\": new_custom_embedding_id, \"vector\": embedding[0].tolist()}]\n    })\n\ndataset = client.create_dataset(name='image_custom_embedding_resnet', iam_integration=None)\ntask = dataset.create_data_rows(data_rows)\nprint(task.errors)\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eTo compute your ow embeddings and send them to Quantumworks Lab, you can use models from Hugging Face. Here’s a brief example using ResNet-50:\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003cscript src=\"https://gist.github.com/mmbazel-labelbox/edb8efbab1904106009e1ed630199e29.js\"\u003e\u003c/script\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eThis code snippet demonstrates how to create and upload custom embeddings, which can then be used for similarity searches within Labelbox.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1694\" height=\"1176\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1694w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"why-you-should-care-about-custom-embeddings\"\u003eWhy you should care about custom embeddings\u003c/h3\u003e\u003cp\u003eCustom embeddings provide several advantages:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTailored Representations\u003c/strong\u003e: Custom embeddings can be tailored to your specific dataset, improving accuracy in domain-specific tasks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Performance\u003c/strong\u003e: They allow for more precise similarity searches and recommendations by capturing nuances specific to your data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFlexibility\u003c/strong\u003e: Custom embeddings offer flexibility in handling various types of data and use cases, making them a versatile tool in machine learning workflows.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"using-embeddings-for-better-search\"\u003eUsing embeddings for better search\u003c/h1\u003e\u003cp\u003eEarlier we mentioned that search (a core activity of search engines, recommendation systems, data curation, etc) is a common and important application of embeddings. How? By comparing how similar two (or many more) items are to each other.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"measuring-similarity\"\u003eMeasuring similarity\u003c/h2\u003e\u003cp\u003eThe most popular algorithm for measuring similarity between two vectors is cosine similarity. It calculates the angle between two vectors: the smaller the angle, the more similar they are. Cosine similarity scores range from 0 to 1, or -1 to 1 for dissimilarity.\u003c/p\u003e\u003cp\u003eFor example, if vectors W and V are close, their cosine similarity might be 0.79. If they are opposite, the similarity is lower, indicating dissimilarity. This method helps in visually and computationally understanding the similarity between different assets.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J0ha8suIAyD2qrFVEWI9TqDRLb2c8ForgqTmyvKQ8ZKsXLeHf5H96V0tQSMV_bCbqBsLWcBb3ljxSP8vKYDKElnlTv1zHq36uQG7mwzyP9HELFlHgGf7FTZO8AWH_ndg5OAXm3yLmbbEpVPg1Cvp108\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"718\" height=\"521\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOriginal source: \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Similarity-between-vectors-can-be-estimated-by-calculating-the-cosine-of-the-angle-th_fig1_333734049?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Similarity between vectors can be estimated by calculating the cosine of the angle θ between them.\"\u003c/span\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBecause embeddings are essentially vectors, we can apply many of the same operations used to analyze vectors to embeddings.\u003c/p\u003e\u003ch3 id=\"strategies-for-similarity-computation\"\u003eStrategies for similarity computation\u003c/h3\u003e\u003cp\u003eAdditional strategies for performing similarity computation include:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eBrute Force Search\u003c/strong\u003e: Comparing one asset against every other asset. This provides the highest accuracy but requires significant computational resources and time. For instance, running brute force searches on a dataset with millions of entries can lead to significant delays and high computational costs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eApproximate Nearest Neighbors (ANN)\u003c/strong\u003e: Comparing one asset against a subset of assets. This method reduces computational resources and latency while maintaining high accuracy. ANN algorithms like locality-sensitive hashing (LSH) and KD-trees provide faster search times, making them suitable for real-time applications, although they may sacrifice some accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHashing Methods\u003c/strong\u003e: Techniques like MinHash and SimHash provide efficient similarity searches by hashing data into compact representations, allowing quick comparison of hash values to estimate similarity. These methods are particularly useful for text and document similarity.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTree-based methods (KD-trees, R-trees, and VP-trees) and graph-based methods (like Hierarchical Navigable Small World, also known as HNSW) are also options.\u003c/p\u003e\u003ch2 id=\"leveraging-embeddings-for-data-curation-management-and-analysis-in-labelbox\"\u003eLeveraging embeddings for data curation, management, and analysis in Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThrough search, embeddings improve the efficiency and accuracy of data curation, management, and analysis in several ways:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eAutomated Data Organization\u003c/strong\u003e: Embeddings help group similar data points together, making it easier to organize and manage large datasets.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Search and Retrieval\u003c/strong\u003e: By transforming data into embeddings, search and retrieval operations become faster and more accurate, enabling quick access to relevant information.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Data Analysis\u003c/strong\u003e: Embeddings capture underlying patterns and relationships within data, facilitating more effective analysis and insights extraction.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAnd these are exactly the ways we use embeddings in our products at Quantumworks Lab to \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimprove search for data\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, in \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e, users can find images similar to an input image, such as searching for basketball players on a court. Our natural language search allows users to input a textual query, like \"dog,\" and find relevant images.\u003c/p\u003e\u003cp\u003eLabelbox offers several features to streamline embedding workflows, including:\u003c/p\u003e\u003ch3 id=\"data-curation-and-management\"\u003eData curation and management\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/custom-embeddings?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCustom Embeddings\u003c/u\u003e\u003c/a\u003e in Catalog: Surfaces high-impact data, with automatic computation for various data types.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/manage-selections?ref=labelbox-guides.ghost.io#smart-select\"\u003e\u003cu\u003eSmart Select\u003c/u\u003e\u003c/a\u003e in Catalog: Curates data using random, ordered, and cluster-based methods.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/cluster-view?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCluster View\u003c/u\u003e\u003c/a\u003e: Discovers similar data rows and identifies labeling or model mistakes.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1620\" height=\"838\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1620w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Cluster view to explore relationships between data rows, identify edge cases, and select rows for pre-labeling or human review.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eNatural Language\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSimilarity Search\u003c/u\u003e\u003c/a\u003e: Supports video and audio assets.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-metrics?ref=labelbox-guides.ghost.io#automatic-metrics\"\u003e\u003cu\u003ePrediction-level Custom Metrics\u003c/u\u003e\u003c/a\u003e: Filters and sorts by custom metrics, enhancing model error analysis.\u003c/li\u003e\u003cli\u003ePre-label Generation from \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFoundry\u003c/u\u003e\u003c/a\u003e Models: Streamlines project workflows.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/video-annotations?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnhanced Video Player\u003c/u\u003e\u003c/a\u003e: Aids in curating, evaluating, and visualizing video data.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1630\" height=\"904\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1630w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWould a rose smell as sweet if it was merely one of a thousand easily discovered in a flower dataset using a natural language search? Trick question: you can't smell photos. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"embeddings-schema\"\u003eEmbeddings schema\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/working-with-features?ref=labelbox-guides.ghost.io#create-features-from-the-schema-tab\"\u003e\u003cu\u003eSchema Tab\u003c/u\u003e\u003c/a\u003e: Includes an Embeddings subtab for viewing and creating custom embeddings.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"api-and-inferencing\"\u003eAPI and inferencing\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/run-foundry-apps?ref=labelbox-guides.ghost.io#run-app-using-rest-api\"\u003e\u003cu\u003eInferencing Endpoints\u003c/u\u003e\u003c/a\u003e: Generate predictions via REST API without creating data rows or datasets.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"additional-workflow-enhancements\"\u003eAdditional workflow enhancements\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBoost Express: Request additional labelers for data projects.\u003c/li\u003e\u003cli\u003eExport v2 Workflows and Improved SDK: Streamlines data management.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese features ensure that embeddings are utilized effectively, making your workflows more efficient and your models more accurate.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eEmbeddings are a powerful tool in machine learning, enabling efficient and accurate similarity searches and recommendations. We hope this post has clarified what embeddings are and how they can be utilized.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor more on uploading custom embeddings, check out our \u003ca href=\"https://docs.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/page/tutorials?ref=labelbox-guides.ghost.io\"\u003edeveloper guides\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAdditional resources on embeddings can be found here:\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jbwzrxh814\" title=\"Embeddings Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWatch the video: \u003ca href=\"https://labelbox.wistia.com/medias/jbwzrxh814?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eUnderstanding embeddings in machine learning\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eLearn more in our community about:\u003cu\u003e \u003c/u\u003e\u003ca href=\"https://community.labelbox.com/t/how-to-upload-custom-embedding-s-and-why-you-should-care/1169?ref=labelbox-guides.ghost.io#why-would-i-need-embedding-2\"\u003e\u003cu\u003eHow to upload custom embeddings and why you should care\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThanks to our contributors Tomislav Peharda, Paul Tancre, and Mikiko Bazeley! \u003c/p\u003e","comment_id":"6656135f9132db0001f20551","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/05/YT-Thumbnails--23-.jpg","featured":false,"visibility":"public","created_at":"2024-05-28T17:24:47.000+00:00","updated_at":"2024-09-04T18:02:27.000+00:00","published_at":"2024-05-28T20:32:29.000+00:00","custom_excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},"url":"https://labelbox-guides.ghost.io/ai-foundations-understanding-embeddings/","excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65ab1fd23c5a9d00013a705f","uuid":"612cb839-1267-4900-89ed-9362aacf55a4","title":"How to accelerate labeling projects using GPT–4V in Foundry","slug":"how-to-accelerate-labeling-projects","html":"\u003cp\u003eWorking closely with hundreds of companies at the forefront of AI, we are seeing a growing interest from teams wanting to use foundation models to pre-label data before combining a human-in-the-loop workflow to inject their unique domain expertise and automate specific tasks that have been previously very time-consuming or manually intensive.\u003c/p\u003e\u003cp\u003eIn this post, we’ll walk through 3 specific examples of how teams can use Quantumworks Lab to accelerate labeling projects by using GPT-4V to create high-quality labels for various data types, including images, HTML, and text.\u0026nbsp;\u003c/p\u003e\u003cp\u003eGenerating high-quality datasets is often one of the most tedious parts of the development process for ML teams. By using Quantumworks Lab Foundry, ML teams can now quickly use LLMs to their advantage to pre-label or enrich data that span a wide range of use cases, such as identifying amenities for rental listings, classifying items in a product catalog, or categorizing support messages.\u003c/p\u003e\u003ch2 id=\"pre-labeling-use-case-1-classifying-amenities\"\u003e\u003cstrong\u003ePre-labeling Use Case #1: Classifying amenities \u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eOnline travel marketplaces such as Expedia, Booking.com, Airbnb, VRBO, etc.,  often show product listings that have a main image with additional images as supplements. These travel marketplaces can enhance their user experience and conversion rates by enriching objects and desired characteristics in product listings to give users more context about visual assets.\u003c/p\u003e\u003cp\u003eAs an example of this in action, we’ve seen ML teams upload primary and supporting images as a single entity, referred to as a data row. A data row can be considered as a task that a human or AI can do. Afterwards, the ML team will tap into\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFoundry\u003c/u\u003e\u003c/a\u003e, which helps automate labeling tasks.\u003c/p\u003e\u003cp\u003eThe example below focuses on identifying various amenities in a rental listing.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Attachment-preview-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"910\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Attachment-preview-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Attachment-preview-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Attachment-preview-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Attachment-preview-.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe main listing image is accompanied by supporting images.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003e\u003cstrong\u003eStep 1: Select images and choose a foundation model of interest\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/select-image-and-model-1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"960\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/select-image-and-model-1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/select-image-and-model-1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/select-image-and-model-1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/select-image-and-model-1.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWorkflow of selecting the images and model of choice.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data, users can use Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eCatalog\u003c/a\u003e filters, including media attributes, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eUsers can then click “Predict with Model Foundry” once the data of interest has appeared.\u003c/li\u003e\u003cli\u003eUsers will then be prompted to choose a model of interest for a model run.\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task - such as image classification, object detection, and image captioning.\u003c/li\u003e\u003cli\u003eTo locate a specific model, users can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine-learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003e\u003cstrong\u003eStep 2: Configure model settings and submit a model run\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/oX69xeF-beq-3wT3y_58xEdqAUefnmBzU55avrqbOc12PRWizA28ATCxCmmTh_KixA7CunMbpkYZCgeMizjFSkKh4IoKCR6bcEPTihnevrneo2HSZ3CW9dTZLQekxw5GjwEe3TO6lNlV-FCA3YFuoIeGzavyQNx3bCMX3vZ0H1Ys4p7yCl6Pt-SVYgqCpQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"800\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWorkflow of selecting the ontology, prompt and generating a preview of the prediction.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eOnce the model of interest is selected, users can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of settings, which can be found in the Advanced model setting. For this use case, the only Foundry model setting I changed was to select “use_image_attachments” to pass the supporting images to GPT-4V.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can then generate preview predictions on up to five data rows to understand how the current model settings affect the final predictions.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows users to confirm the configuration settings confidently:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf users are unhappy with the generated preview predictions, they can edit the model settings and continue to generate preview predictions until they're satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eAfter users are satisfied with the preview predictions, a model run can be submitted.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Image-airbnb-end-result-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"720\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Image-airbnb-end-result-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Image-airbnb-end-result-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Image-airbnb-end-result-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Image-airbnb-end-result-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eIn addition to the obvious amenities, GPT-4v identified the subtle amenities like heater, lakeview, mountain View, and stove. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003ePrompt used for Amenities classification: \u003c/p\u003e\u003cul\u003e\u003cli\u003eYou are a helpful assistant. What amenities are in the images? Respond with the following options. [Kitchen, TV, Heater, Stove, Hot tub, Skis, Lake view, Refrigerator, Microwave, Mountain view, Shower]. Return the result as a dictionary. {“Amenities” : [“\u0026lt;prediction\u0026gt;”]}\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-send-the-images-to-annotate\"\u003e\u003cstrong\u003eStep 3: Send the images to Annotate\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eUsers can transfer the results to a labeling project using the UI via the \"Send to Annotate\" feature. Labelers can then quickly review labels for accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/kZlndAKlXfBsSFIF68sr8Q34BTRIUFIuLnWQRm88a32mquEuqIwzVVFkgYPIyiyRDxdqK8dGAQAhvcjuxSuvlZSV2No5ZpL0-mNymUjeKqiEioiN2MTRlOK0q_dsFoLPb49Vh6m6GCvmaKPPtVXPg1vT4TmZcX45Wy2NQQAmFdRwOhjFPkdIsgUQez-mlg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"795\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUI for manually reviewing the labels created by LLMs. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"pre-labeling-use-case-2-recommendation-engine\"\u003e\u003cbr\u003e\u003cstrong\u003ePre-labeling Use Case #2: Recommendation engine \u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eE-commerce companies such as Amazon, Wayfair, Etsy, and eBay offer a \"similar products\" feature that improves the discoverability of products and provides an easy way to compare items, thereby increasing customer satisfaction and reducing return rates.\u003c/p\u003e\u003cp\u003eA recommendation engine often powers the product similarity feature that requires integrating text and images into a unified file. ML teams can use Quantumworks Lab to help automate this workflow as we support HTML files that state-of-the-art models can label or enrich. For the HTML product similarity task, the initial steps 1-2 remain the same, but the prompt and ontology will be adjusted to focus on classifying whether the products are identical, and GPT-4V will provide reasoning.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/side-by-side.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"725\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/side-by-side.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/side-by-side.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/side-by-side.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/side-by-side.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe see that GPT-4v predicted the products to not be the same and provided an accurate explanation for the answer. GPT-4v also classified it correctly to be a bottle type. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003ePrompt used for recommendation engine: \u003c/p\u003e\u003cul\u003e\u003cli\u003eYou are a helpful assistant. Based on the descriptions and images provided for the two products, determine whether the products being described are the same.\u003c/li\u003e\u003cli\u003eClassify Is the product the same? pick one of the options: [yes, no].\u0026nbsp;\u003c/li\u003e\u003cli\u003eClassify Item type as the following: [Food, Perishable, Liquid, Bottle, Nonperishable].\u003c/li\u003e\u003cli\u003eAnswer Explanation with why yes or no.\u0026nbsp;\u003c/li\u003e\u003cli\u003eReturn the result as a JSON. {\"Is the product the same?\" : \"\u0026lt;prediction\u0026gt;\", \"Item type\" : [\"\u0026lt;prediction\u0026gt;\"], \"Explanation\", \"\u0026lt;prediction\u0026gt;\"}\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"pre-labeling-use-case-3-support-chat-classification\"\u003e\u003cstrong\u003ePre-labeling Use Case #3: Support chat classification\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eEnterprises with a significant user base require a full-fledged customer support team to ensure smooth operations. Providing efficient customer support is typically achieved by efficiently categorizing and triaging high volumes of real-time support messages.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, support teams often have to manually classify tickets, which can be time-consuming and prone to human error. This process is necessary to sort the messages into the correct reporting categories and identify which engineering teams should handle which bugs.\u003c/p\u003e\u003cp\u003eBy using advanced large language models (LLMs) such as GPT-4V, customer chat intent classification can be automated, and then labelers can review and edit the labels if needed. \u003c/p\u003e\u003cp\u003eThe following prompt was used to classify customer messages, and default Foundry model settings were used. If GPT-4V fails to produce an expected answer, users can add an if statement to capture the edge cases, as shown below.\u003c/p\u003e\u003cul\u003e\u003cli\u003eYou are a helpful support assistant. Read the following text and classify them to the information below.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf the tier is Enterprise and urgency is Critical then Priority is Priority 0.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf the the ticket is related to feature request and Tier is Enterprise then Priority is P2. Otherwise all feature request tickets are P4.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf the tier is Free and urgency is low, then Priority is Priority 4.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf description is related to python or coding then engineering team is SDK.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf description is related to labeling editor then engineering team is Perception.\u003c/li\u003e\u003cli\u003eIf description is related to log in issues or app crashes\u0026nbsp; then engineering team is Platform. \u0026nbsp;\u003c/li\u003e\u003cli\u003eIf urgency is critical and tier is not enterprise then priority should be Priority 2.\u003c/li\u003e\u003cli\u003eIf description is stating to support a feature then that is a feature request.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eClassify Message intent, pick one of the options: [Accidental payment, Unauthorized payment, Irrelevant, Unable to login, Reset password, Cancel subscription, App bug, Feature Request]. Classify Priority, pick one of the options: [Priority 1, Priority 2, Priority 3, Priority 4, Priority 0]. Classify Engineering team, pick one of the options: [SDK, Perception, Platform].\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1994\" height=\"676\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 1994w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBased on the prompt, GPT4-V correctly classified this as P0, Platform team and App bug.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"678\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe prompt mentioned to classify feature support requests as \"Feature Request\" and set the priority to 2 due to enterprise tier.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"676\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eGPT-4V initially classified this as P4. To correct it, \"If urgency is critical and tier is not enterprise, then the priority should be Priority 2\" was added to the prompt.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"678\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eGPT-4V correctly used the prompt to classify the text based on the if statements. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"additional-considerations\"\u003e\u003cstrong\u003eAdditional Considerations\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAdditional considerations as users incorporate Foundry labels into their projects and workflows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIncorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman evaluators\u003c/u\u003e\u003c/a\u003e to find edge cases and updating the prompt would give the best results as users create more labels with Foundry. For example, if statements were added in the chat classification use case to account for edge cases.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can also \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA/B test different models \u003c/u\u003e\u003c/a\u003efrom Foundry to find the model that best fits the use case using Quantumworks Lab Model.\u003c/li\u003e\u003cli\u003eIn addition to GPT-4V, users can utilize various state-of-the-art models like Gemini 1.5 Pro, Claude 3 Opus, and more from Quantumworks Lab Foundry, as seen \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eIf we do not currently support your use case or any questions arise, feel free to contact our \u003ca href=\"https://docs.labelbox.com/docs/contacting-customer-support?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esupport team\u003c/u\u003e\u003c/a\u003e, as we would love to hear your feedback to improve Foundry.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003cbr\u003e\u003c/h2\u003e\u003cp\u003eLarge Language Models (LLMs) and generative AI are showing an enormous impact on the individual productivity of knowledge workers. As these large foundation models improve, we’ll continue to see impressive real-world use cases for automating pre-labeling more quickly and cheaply. Completing labeling projects with Quantumworks Lab Foundry combines both AI-assistance and human-in-the-loop workflows to automate one or more specific tasks. \u003c/p\u003e\u003cp\u003eCheck out our additional resources on how to utilize state-of-the-art AI models in Foundry, including using model distillation and fine-tuning to leverage the power of foundation models:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-build-generative-captioning-using-foundation-models-for-product-listings/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eHow to build generative captioning and enrich product listings faster with foundation models\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eHow to build a powerful product recommendation system for retail\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eIntroduction to model distillation\u0026nbsp;\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://event.on24.com/wcc/r/4460171/5D5357846BEAB1FD1E12A5ED146DE6E1?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAutomated Labeling With Any Foundation Model\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eFine tune LLMs\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"65ab1fd23c5a9d00013a705f","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/03/thumbnail--5-.png","featured":false,"visibility":"public","created_at":"2024-01-20T01:20:18.000+00:00","updated_at":"2024-03-27T15:19:55.000+00:00","published_at":"2024-03-26T06:03:00.000+00:00","custom_excerpt":"Learn 3 specific examples of how teams can use Quantumworks Lab to accelerate labeling projects by using multimodal models to create high-quality labels for various data types including images, HTML, and text. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-accelerate-labeling-projects/","excerpt":"Learn 3 specific examples of how teams can use Quantumworks Lab to accelerate labeling projects by using multimodal models to create high-quality labels for various data types including images, HTML, and text. ","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65523b444a21b900018ee9ba","uuid":"4e760591-7da7-43c2-af80-554fdb2ed601","title":"Detecting swimming pools with GPT4 Visual","slug":"detecting-swimming-pools-with-gpt4-visual","html":"\u003cp\u003eThe rise of off-the-shelf and foundation models has enabled AI teams to fine-tune existing models and pre-label data faster and more accurately — in short, significantly accelerating AI development. However, using these models at scale for building AI can quickly become expensive. One way to mitigate these costs and reduce waste is to add a model comparison process to your workflow, ensuring that any model you choose to integrate for AI development is the best choice for your requirements.\u003c/p\u003e\u003cp\u003eA comprehensive model comparison process evaluates models on various metrics, such as performance, robustness, and business fit. The results will enable teams to quickly kickstart model development, decrease time to value, and ensure the best results with less time and costs for their specific use case.\u003c/p\u003e\u003cp\u003eEmbedding model comparison into the AI development workflow, however, comes with its own unique challenges, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConfidently assessing the potential and limitations of pre-trained models\u003c/li\u003e\u003cli\u003eVisualizing models’ performance for comparison\u003c/li\u003e\u003cli\u003eEffectively sharing experiment results\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this blog post, we’ll explore how you can tackle these challenges for a computer vision use case with \u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003ethe Foundry add-on for Quantumworks Lab Model\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"why-is-selecting-the-right-model-critical\"\u003eWhy is selecting the right model critical?\u0026nbsp;\u003c/h3\u003e\u003cp\u003eSelecting the most suitable off-the-shelf model is pivotal for ensuring accurate and reliable predictions tailored to your specific business use case, often leading to accelerated AI development. As different models exhibit diverse performance characteristics, diligently comparing the models’ predictions on your data can help distinguish which model excels in metrics such as accuracy, precision, recall, and more. This systematic approach to model evaluation and comparison enables you to refine the model’s performance with a “store of record” for future reference to continuously improve model performance.\u003c/p\u003e\u003cp\u003eChoosing the best off-the-shelf model provides a quick and efficient pathway to production, ensuring that the model aligns well with the business objectives. This alignment is crucial for the model's immediate performance and sets the stage for future improvements and adaptability to evolving requirements. The most suitable model for your use case also enables you to reduce the time and money spent on labeling a project. For instance, when pre-labels generated by a high-performing model are sent for annotation, less editing is required, making the labeling project quicker and more cost-effective. This is due to better Intersection Over Union (IOU) for tasks like Bounding Box, resulting in higher quality pre-labels and, therefore, fewer corrections. Furthermore, utilizing the best model can make your trove of data more queryable by enriching your data, thereby enhancing its searchability.\u003c/p\u003e\u003ch2 id=\"comparing-vision-llms-models-using-labelbox-model-foundry\"\u003eComparing vision LLMs models using Quantumworks Lab Model Foundry\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/h2VyJaeRIswlqFJEn0IhiQumboC5OqF_mOj8KjCCcFT5nvztUALIucquzxtsYtX47PvvhHXtWP1_xDr2dllNebQf-FCFxu9YLQr184snn_y-SiKVUyB9ONmkttllzI1p6htDg_Nny5GDCCjFg7AF3RA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"239\"\u003e\u003c/figure\u003e\u003cp\u003eThis flow chart provides a high-level overview of the model comparison process when using Quantumworks Lab Model Foundry.\u003c/p\u003e\u003cp\u003eWith the Foundry add-on for Quantumworks Lab Model, you can evaluate a range of models for computer vision tasks to select the best model to perform pre-labeling or data enrichment on your data.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003e\u003cstrong\u003eStep 1: Select images and choose a foundation model of interest\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/L1IEgc57cHhpmPUlxyw7kqGv_RZoYOEII6Vr2iBfYUMe6iDFWViUlmHpXHWM5yfy8ZLj3WN4wZjrMUSTpPF5R5Ra17eeT7ggEoukjvqeh7rJ-vsVWiMWASsURgs-8_uxvuk2rkBN3arCrJB0RSZa9Cs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the images on which the predictions should be made on.\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “Predict with Model Foundry”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run.\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task - such as image classification, object detection, and image captioning.\u003c/li\u003e\u003cli\u003eTo locate a specific model, you can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-hyperparameters-and-submit-a-model-run\"\u003eStep 2: \u003cstrong\u003eConfigure model hyperparameters and submit a model run\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/NnyPrWdiuwQmxl8y1IMTUWEFGZ2pdyoS4RmKme8-S9hoxvVjZscwQwAN03TMizRD8M70z1CvySOFEShP_4KXZpUjC3Z7cg6peF-T-ceKxlSi5ys432EP_AMPe1NQi8ip0-VHV1FLMT-0n0rmJS5XKxc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve located a specific model of interest, you can click on the model to view and set the model and ontology settings or prompt. In this case, we will enter the following prompt “Is there an entire swimming pool clearly visible in this image? Reply with yes or no only, without period.” This prompt is designed to facilitate responses from the model in a simple 'Yes' or 'No' format.\"\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of hyperparameters, which you can find in the Advanced model setting.\u003c/li\u003e\u003cli\u003eTo get an idea of how your current model settings affect the final predictions, you can generate preview predictions on up to five data rows.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-predictions-will-appear-in-the-model-tab\"\u003eStep 3: Predictions will appear in the Model tab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/image.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/11/image.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/11/image.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eEach model run is submitted with a unique name, allowing you to distinguish between each subsequent model run\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWhen the model run completes, you can:\u003cul\u003e\u003cli\u003eView prediction results\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results across a variety of model runs different models\u003c/li\u003e\u003cli\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-repeat-steps-1-5-for-another-model-from-model-foundry\"\u003eStep 4: Repeat steps 1-5 for another model from Model Foundry\u003c/h3\u003e\u003cul\u003e\u003cli\u003eYou can repeat steps 1-5 with a different model, on the same data and for the same desired machine learning task, to evaluate and compare model performance\u003c/li\u003e\u003cli\u003eBy comparing the predictions and outputs from different models, you can assess and determine which one would be the most valuable in helping automate your data labeling tasks\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-5-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 5: Create a model run with predictions and ground truth\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"898\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo create a model run with model predictions and ground truth, users currently have to use a \u003ca href=\"https://colab.research.google.com/drive/1f0WkWtsDh2LYiy35pc3JF16srj6SxeTA?ref=labelbox-guides.ghost.io#scrollTo=W8tE-H7VmKea\"\u003escript\u003c/a\u003e to import the predictions from the Foundry add-on for Quantumworks Lab Model and ground truth from a project into a new model run. \u003c/p\u003e\u003cp\u003eIn the near future, this will be possible via the UI, and the script will be optional.\u003c/p\u003e\u003ch3 id=\"step-6-evaluate-predictions-from-different-models-from-model-foundry-in-labelbox-model\"\u003eStep 6: Evaluate predictions from different models from Model foundry in Quantumworks Lab Model\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003cp\u003eAfter running the notebook, you'll be able to visually compare model predictions between two models.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-7-send-model-predictions-as-pre-labels-to-a-labeling-project-in-annotate\"\u003eStep 7: Send model predictions as pre-labels to a labeling project in Annotate\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/MZHoMXGtxeXRYWIH41mENw-0Uz26ydqJBOkE-wtsY-1BYzGdyABhi5UXN6s5QUxrkx-ZHzwuhirnUDB4tNrDrSOEVyvlI7aHcCDMI7zekND5QRDEkBh48DfbxHrzgtpHGKxsRSVcxz6W8AikGbgZRdg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"349\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eSelect the best performing model and leverage the model predictions as pre-labels.\u003c/li\u003e\u003cli\u003eRather than manually labeling data rows, select and send a subset of data to your labeling project with pre-labels to automate the process.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"model-comparison-in-practice\"\u003eModel comparison in-practice:\u003c/h2\u003e\u003ch3 id=\"llava-vs-gpt-4v\"\u003eLLaVA vs GPT-4V\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"893\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eMetrics view \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn this example, LLaVA has 25 true positives, while GPT-4V has 30. Additionally, LLaVA has 10 more false positives. Therefore, GPT-4V is the superior model.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLet’s now take a look at how the predictions appear on the images below:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Jh89URRISL3Qy_D8JCfPEN2KiksoCYJGSaLEulXsh-ZzztLaKoiOEPaGMdatag8YGCCPFrGTP6E2GCY4380KliNWyPXkAlPq-GzP9l0wr44cT81MG4xq2_fUm72YsS1g8KiNnpAXYpv22pLp-RvDxe0\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/4xlsujbFfyNcvACvwcAXIYVeL5E13FYt0RZC0EtsGdkVbrU_542h-EjEr3vGGcnpDManAjF6YcbaWBqaxHoCqOd9Pe31vhHGkGWyK11HGjBHy74iJt3PeuaMoY4p68RbeStGWzGEY3_bPwVge257les\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/zewKkLUtDyBrb0nu6Ci_LgW_mkCPcg6s_TIMFUHwMzvudwilvTP19XWwevbdrCbXx2nKNIWwsNe1ZrH05WN0St36HcnkJ-2mPxBXxeiCEWaVlhVJvxlWF2ptxlPvyycNpg-nlLDe5b8lMI1xdQMQsDw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cp\u003eIn the examples above, GPT-4V and LLaVA both correctly predicted there to be a swimming pool.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/DLBp8vaTB5ixPdI7mvxOkPM5eWQKzwfyrWHZnTIRnUqO4uycb8wPSdwWrqLzgR6Yp2rTqgIXvc9uBLPLuOwmW8hlWKVdcnelWGVuF4ajkV_bfvp064NWuNMTsJ4Tj_Kbgup4qmgFtuRtMKUXWk3Okt4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"345\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Qxzm1UsZc6osUix_3cmT9AQVhWVll5FOVe2_Svd1ArO7pZj0pxQFbAk_dsUF57rnhtO_iMYpBNGYBWMRwvo4Mw9BxhyvE_wXvbbrbA5sEiXsOXZ6j7dRCryJWDZ4NL4IcaC3mD4rblY57A1P7umfuiU\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"345\"\u003e\u003c/figure\u003e\u003cp\u003eIn the two instances above, GPT-4V correctly identified the presence of a swimming pool, while LLaVA incorrectly predicted its absence. It's possible that LLaVA was misled by the shadows over the pool.\u003c/p\u003e\u003ch3 id=\"imagen-vs-gpt-4v\"\u003eImagen vs GPT-4V\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/11/image-1.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/11/image-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eMetrics view \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eImagen has 6 more true positives and 1 less false positive than GPT-4V, so Imagen is the best model in this test of 40 images based on the metrics.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/tCGt5rcYTHcjW0fkwL2ay4J-jkXni3TDU12eUjVOTwg7X4wzieQ3zodx3gTI02pE2YZsi6R9bcBhslM_69Ixb0T3zDJsY00ISbyNYL4aI0RmusF6pxVdxucBUDVaRWfl400cTYfQEwRlW6cqU7zDFlw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"335\"\u003e\u003c/figure\u003e\u003cp\u003eIn the above example, GPT-4V and Imagen both correctly predicted there to be a swimming pool.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Rl9GXiF4XURZlNr5QreUz7fYaVr9sKrAEv94XI7ciYRGjvCPAxajuBOsmmMb2EoOO2Vq3_aTGyC-_iPniCuCgL7w4YAjKJ8zp6zRm866nNjojMeigw4sMVrwSBb1-oxuXvyGPc3CfsLbyApnidR1S1w\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"335\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Kd4aif9Oa08qnFziD5dr8a3L96H0bEsMeuxsqeMqnjAPpyP9OxqNoemqpxiTYSeVt504k-SVMyTqGr5ZL83fT2ra2FwWsWC2pbC2i27nGhSclbgXFBoJ0wEGl0k_V7W7FZdtAgrouHsTx1Xd_kmlzv8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"345\"\u003e\u003c/figure\u003e\u003cp\u003eIn the two instances above, GPT-4V and Imagen both correctly predicted there to be no obvious swimming pool.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u0026nbsp;Send the predictions as pre-labels to Quantumworks Lab Annotate for labeling\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eSince we've evaluated that Imagen is the best model from our test, we can send model predictions as pre-labels to our labeling project by highlighting all data rows and selecting \"Send to annotate\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/MZHoMXGtxeXRYWIH41mENw-0Uz26ydqJBOkE-wtsY-1BYzGdyABhi5UXN6s5QUxrkx-ZHzwuhirnUDB4tNrDrSOEVyvlI7aHcCDMI7zekND5QRDEkBh48DfbxHrzgtpHGKxsRSVcxz6W8AikGbgZRdg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"349\"\u003e\u003c/figure\u003e\u003cp\u003eIn conclusion, you can leverage Model Foundry to not only select the most appropriate model to accelerate data labeling, but to automate data labeling workflows. Use quantitative and qualitative analysis, along with model metrics, to surface the strengths and limitations of each model and select the best performing model for your use case. Doing so can help reveal detailed insights, such as seen in the above comparison between Imagen and GPT-4V. In the above model comparison example, we can see that Imagen emerged as the superior model that will allow us to rapidly automate data tasks for our given use case.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003eModel Foundry\u003c/a\u003e streamlines the process of comparing model predictions, ensuring teams are leveraging the most optimal model for data enrichment and automation tasks. With the right model, teams can easily create pre-labels in Quantumworks Lab Annotate – rather than starting from scratch, teams can boost labeling productivity by correcting the existing pre-labels.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"65523b444a21b900018ee9ba","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-3060--1-.png","featured":false,"visibility":"public","created_at":"2023-11-13T15:05:40.000+00:00","updated_at":"2023-12-06T22:27:12.000+00:00","published_at":"2023-11-13T15:11:32.000+00:00","custom_excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart LLM development, decreasing costs and accelerating time-to-value. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},"url":"https://labelbox-guides.ghost.io/detecting-swimming-pools-with-gpt4-visual/","excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart LLM development, decreasing costs and accelerating time-to-value. ","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"654bbab4016f5100016579c3","uuid":"6c33e4d1-fcaa-44de-b994-0fe65fce3dcc","title":"How to analyze customer reviews and improve customer care with NLP","slug":"how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp","html":"\u003cp\u003eCustomer reviews have become a critical tool for businesses looking to improve their products, services, and customer satisfaction. In today’s digital world, review sites like Yelp and social media make it easier than ever for customers to share their experiences with the world. Customer care can range in the services and support that businesses provide to their customers before, during, and after purchase. Great customer care can create positive brand experiences that lead to greater loyalty and customer satisfaction. In the ever-evolving world of retail, it also helps keep your business competitive and at the forefront of your customer’s sentiment and desires.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile companies now have access to a wealth of customer feedback data, sifting through all of these reviews can be incredibly time-consuming and manual. By leveraging AI, teams can analyze \u003ca href=\"https://birdeye.com/blog/review-management/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ecustomer reviews and feedback\u003c/a\u003e at scale, to gain insights into common review topics or customer sentiment. This allows businesses to identify common themes and pinpoint areas of improvement to enhance\u0026nbsp; the customer experience.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for customer care. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eImproving customer care requires a vast amount of data in the form of customer reviews. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape: \u003c/strong\u003eThe changing nature and format of customer review data from multiple sources (e.g webpages, apps, social media, etc.) poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their customer care through advanced natural language processing. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s assisted data enrichment and flexible training frameworks to quickly build AI systems that uncover actionable insights from customer reviews. Tackle unique customer care challenges with AI-driven insights to create more thoughtful and strategic customer interactions. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1444\" height=\"784\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 1444w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build an NLP model to improve customer care. Specifically, this guide will walk through how you can explore and better understand review topics and classify review sentiment to make more data-driven business decisions around customer care initiatives.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-accelerate-and-train-an-nlp-model-to-improve-customer-care\"\u003eSee it in action: How to accelerate and train an NLP model to improve customer care\u0026nbsp;\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below in either: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e or \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs customer reviews and feedback across channels proliferate, brands want to learn from customer feedback to foster positive experiences. For this use case, we’ll be working with a dataset of customer hotel reviews – with the goal of analyzing the reviews to demonstrate how a hospitality company could gain insight into how their customers feel about the quality of service they receive.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/q4dqjyg9xf\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"498\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003eFor the purpose of this tutorial, we’ve provided a sample open-source \u003ca href=\"https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe?ref=labelbox-guides.ghost.io\"\u003eKaggle dataset\u003c/a\u003e that can be downloaded.  \u003c/p\u003e\u003cp\u003ePlease \u003ca href=\"https://drive.google.com/file/d/1hh2MYzol6-4PSsqX7mbX2YSLLGA4jAYZ/view?usp=drivesdk\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edownload the dataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your text data as a CSV. \u003c/li\u003e\u003cli\u003eIf your text snippets sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your text data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xn3sj0uc8j\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExplore topics of interest\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith your data in Quantumworks Lab, you can begin to leverage Catalog to uncover interesting topics to get a sense of what customers are talking about from hotel reviews.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize your data –\u0026nbsp; you can click through individual data rows to get a sense for what customers are writing reviews on\u0026nbsp;\u003c/li\u003e\u003cli\u003eDrill into specific topics of interest\u003cstrong\u003e \u003c/strong\u003e– leverage a natural language search, for example searching “interior design,” to bring up all related reviews related to interior design. You can adjust the confidence threshold of your searches accordingly (this can be helpful in gauging the volume of data related to the topic of interest)\u0026nbsp;\u003c/li\u003e\u003cli\u003eBegin to surface subtopics or trends within your initial search – for example is the interior design review related to the style of design, attention to detail, or the type of environment created from the interior design\u003c/li\u003e\u003cli\u003eEasily find all instances of similar examples to data of interest\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate and save data slices\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf you have a search query that you’re interested in saving or reusing in the future, you can save it as \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003ea slice\u003c/a\u003e. You can construct a slice by using one or more filters to curate a collection of data rows. Users often combine filters to surface high-impact data and then save the results as a slice.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this example, we’ve surfaced reviews on the topic of breakfast that all talk about the value and price of the hotel’s breakfast. We can save this as a slice for future reference (“Breakfast_value”) and as we ingest more data that matches the slice’s criteria, they will automatically get filed into the slice.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-an-ontology\"\u003eCreate an ontology\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can create our ontology. \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eOntologies\u003c/a\u003e can be reused across different projects and they are required for data labeling, model training, and evaluation.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gdczmynqjt\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a new ontology:\u003c/p\u003e\u003cp\u003e1) Navigate to the ‘Schema’ tab\u003c/p\u003e\u003cp\u003e2) Hit ‘Create new ontology’\u003c/p\u003e\u003cp\u003e3) Select the media type that you wish to work with – for this use case ‘Text’\u003c/p\u003e\u003cp\u003e4) Give your ontology a name\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Add objects and classifications based on you use case\u003c/p\u003e\u003cp\u003e6) Objects are named entities\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003ePerson’s name\u0026nbsp;\u003c/li\u003e\u003cli\u003eLocation\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003e7) Classifications\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eReview sentiment such as positive or negative (radio)\u0026nbsp;\u003c/li\u003e\u003cli\u003eReview topics such as breakfast, dinner, location, staff, interior design (checklist)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAdd sub-classifications as desired\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003e8) Save and create your ontology\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter creating an ontology, you can begin labeling your data to fine-tune or train a model.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"label-data-of-interest\"\u003eLabel data of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003eWith Quantumworks Lab, you can label your data in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e: leverage our global network of\u0026nbsp; specialized labelers for a variety of tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWorkforce Boost provides a collaborative platform for labeling services in a self-serve manner — this is great for teams that don’t have the technical expertise to build a machine learning system yet are looking for an easy-to-use technology to get a quick turnaround on quality training data. You can learn more about our Boost offerings \u003ca href=\"https://docs.labelbox.com/docs/using-data-labeling-service?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create pre-labels with foundation models\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn addition to creating pre-labels for classification projects, you have the ability to send model predictions as pre-labels to your labeling project. This can be done in one of two ways:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel-assisted labeling\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eImport computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel Foundry\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eAutomate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePre-label data with Model Foundry\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModel Foundry\u003c/a\u003e acts as the copilot to create your training data –\u0026nbsp; instead of going into unstructured text datasets blindly, you can use pre-existing LLMs to pre-label data or pre-tag parts of it, reducing manual labeling efforts and cost.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9zspjgoau7\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1) Select data you wish to label in Catalog\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Hit \"Predict with Model Foundry\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Choose a foundation model\u003c/p\u003e\u003cul\u003e\u003cli\u003eYou can select a foundation model based on your use case to have the model take a first pass at labeling your data\u003c/li\u003e\u003cli\u003eThese pre-labels can be verified with human-in-the-loop review in Quantumworks Lab Annotate\u003c/li\u003e\u003cli\u003eFor this use case, we’ve selected the GPT-4 model\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e4) Configure the model’s settings\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect the previously created ontology in the earlier part of the tutorial\u0026nbsp;\u003c/li\u003e\u003cli\u003eLabelbox will auto-generate a prompt based on your ontology and use case – in this case we wish to classify the sentiment (positive or negative) and classify a topic with one or more options (breakfast, dinner, location, staff, room, facilities, value for money, or interior design)\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e5) Generate preview predictions\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore submitting the model run, you can generate prediction previews to understand how the model will perform\u003c/li\u003e\u003cli\u003eIt is recommended that you preview some predictions to confirm the model parameters are configured as desired\u003c/li\u003e\u003cli\u003eBased on the preview, you can then make any adjustments to the settings or choose to submit the model run as-is\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e6) Name and submit the model run\u0026nbsp;\u003c/p\u003e\u003cp\u003e7) View the model run in the Model tab to explore results\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce your model run is complete, you navigate to the Model tab\u003c/li\u003e\u003cli\u003eExplore the model’s results and click into each data row to dig deeper into the model’s predictions\u003c/li\u003e\u003cli\u003eFor this example, we can see that there are instances where GPT-4 has correctly tagged named entities and identified sentiment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve evaluated and are satisfied with GPT-4’s predictions, you can send them to a labeling project in Quantumworks Lab Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAdd a batch to a labeling project as pre-labels\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eBefore you can send these model predictions to a labeling project as pre-labels, you need to create a labeling project. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7zgl76n76w\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eCreate a new labeling project\u003c/em\u003e\u003c/p\u003e\u003cp\u003e1) Navigate to the Annotate tab\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Create a ‘New project’\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Select the project type – in this case we want to create a ‘Text’ project\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Name your project\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Attach your model’s ontology (created in a previous step)\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce you’ve created your labeling project and configured the ontology, head back to the Model tab to send your batch of data with pre-labels to that labeling project.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Highlight all data rows of interest\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Select ‘Manage selection’ \u0026gt; ‘Add batch to project’\u003c/p\u003e\u003cp\u003e3) Select the appropriate project that you created in the above step\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) You can give the batch a priority (from 1-5)\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Select the appropriate model run of the predictions you wish to send\u0026nbsp;\u003c/p\u003e\u003cp\u003e6) You can explore and select the various tags that have been applied and uncheck those that aren’t of interest\u0026nbsp;\u003c/p\u003e\u003cp\u003e7) Submit the batch\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can now navigate back to your project in Annotate and hit ‘Start labeling’.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"verify-data-quality-with-custom-workflows\"\u003eVerify data quality with custom workflows\u003c/h3\u003e\u003cp\u003eRather than starting from scratch, your internal or external team of labelers can now see predictions from the Model Foundry run. From here, you can validate or edit predictions as necessary and submit data rows to create ground truth labels.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/56bratjais\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs you begin to progress through your data rows, you’ll notice data rows that are initially marked up and reviewed by labelers in the ‘Initial review’ task (for your reviewers to verify and approve), with all submitted data rows falling into ‘Done’.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can create customizable, multi-step review and rework pipelines to drive efficiency and automation for your review tasks. Set a review task based on specific parameters that are unique to your labeling team or desired outcome.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eInitial labeling task: reserved for all data rows that have been queued for labeling\u003c/li\u003e\u003cli\u003eInitial review task: first review task for data rows with submitted labels\u003c/li\u003e\u003cli\u003eRework task: reserved for data rows that have been rejected\u003c/li\u003e\u003cli\u003eDone task: reserved for data rows that have a) moved through their qualified tasks in the workflow or b) did not qualify for any of the tasks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce all data rows have been reviewed and moved to the ‘Done’ step, you can begin the model training process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn Part 1 of this tutorial, we have looked at how we can leverage Catalog to understand the topics that exist within your dataset and construct an appropriate ontology. To accelerate our initial labeling job, we leveraged Model Foundry as part of our model-assisted labeling pipeline to use pre-labels from GPT-4 to our labeling workforce for validation. Those initial annotations can be exported via a model run and can be used to train or fine-tune a model outside of Labelbox.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"part-2-train-or-fine-tune-a-model-and-evaluate-model-performance\"\u003ePart 2: Train or fine-tune a model and evaluate model performance\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e or \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"train-a-custom-model-on-a-subset-of-data-outside-of-labelbox\"\u003eTrain a custom model on a subset of data outside of Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/07yc0p652w\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn the previous step, we leveraged Model Foundry to create pre-labels that were passed through Annotate for review with human-in-the-loop validation. Now that we have our appropriate annotation data, we can train a series of initial models on sentiment, topic classification, and named entity recognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eYou can reference \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io#scrollTo=EDmBNjYP_0u7\"\u003ethis step\u003c/a\u003e (Databricks) or \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io#scrollTo=EDmBNjYP_0u7\" rel=\"noreferrer\"\u003ethis step\u003c/a\u003e (Google Colab) in either notebook.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBring the trained model’s predictions back into a model run\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce the model has been trained, you can create an inference pipeline that leverages each model to classify different attributes for review. We can then leverage this for two things:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun inference on the model run dataset and upload it to Quantumworks Lab for evaluation\u003c/li\u003e\u003cli\u003eRun inference on our remaining dataset and use the predictions for model-assisted labeling, to be refined in the platform and used to accelerate labeling efforts\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ePlease follow \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io#scrollTo=B9_dYR_D_0vE\"\u003ethis step\u003c/a\u003e (Databricks) or \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io#scrollTo=B9_dYR_D_0vE\" rel=\"noreferrer\"\u003ethis step\u003c/a\u003e (Google Colab) to create an inference pipeline and to upload predictions to the model run and evaluate it against ground truth.\u003c/p\u003e\u003cp\u003eAfter following the notebook, you’ll be able to compare ground truth (green) versus the model’s predictions (red) for sentiment and topic.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness\"\u003eEvaluate and diagnose model effectiveness\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dqzdzj1seb\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 8 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eDiagnose model performance with model metrics\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn addition to visualizing the difference between model predictions and ground truth, you can click into the ‘Metrics’ view to get a better sense of how your model is performing.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the \"Metrics view\" to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor example, we can click into false negatives or false positives to narrow down situations where there might be false positives – where ‘negative’ sentiment is predicted whereas ground truth sentiment is ‘positive’.\u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select \"Find similar in Catalog\" from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled reviews that you can send to your model for labeling, you can filter on the \"Annotation is\" filter and select \"none.\" This will only show unlabeled text reviews that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all reviews that apply and select \"Add batch to project\"\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eUse model predictions as model-assisted labeling pipeline\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/r4p2h6iklg\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 9 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve filtered for and have selected reviews that you wish to label you can \"Add batch to project\" to send them to your labeling project in Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Name your batch\u003c/p\u003e\u003cp\u003e2) Select your labeling project from the dropdown\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Include model predictions (from your model run) – this will perform better than the initial GPT-4 run with Model Foundry since it has been trained on your custom data\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Select or uncheck any predictions as desired\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Submit the batch\u003c/p\u003e\u003cp\u003eWhen you return to Quantumworks Lab Annotate, you will now see the original batch that we added at the start of the project, as well as the newly added batch ready for labeling.\u0026nbsp;\u003c/p\u003e\u003cp\u003eRather than starting from scratch, similar to the predictions created by GPT-4 in Model Foundry, your labelers will now see the custom model predictions and validate them with human-in-the-loop review in the same manner. This workflow helps accelerate model iterations, allowing your team to bring in the latest model prediction as pre-labels for your project to reduce the amount of human labeling effort required to create ground truth labels.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model and can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003eCustomer reviews and feedback data represent an invaluable yet untapped opportunity for businesses. Manually analyzing this growing mountain of data is no longer practical. Instead, forward-thinking companies are turning to AI to efficiently sift through and extract actionable insights from reviews.\u003c/p\u003e\u003cp\u003eNatural language processing can help identify customer sentiment, pain points, and unmet needs. By leveraging AI to tap into this feedback treasure trove, businesses can drive measurable improvements in customer satisfaction, retention, and advocacy. They can refine products, enhance user experiences, and preemptively address concerns.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"654bbab4016f5100016579c3","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-2457--1-.png","featured":false,"visibility":"public","created_at":"2023-11-08T16:43:32.000+00:00","updated_at":"2024-06-25T16:28:21.000+00:00","published_at":"2023-11-08T21:47:13.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/","excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","reading_time":12,"access":true,"comments":false,"og_image":null,"og_title":"How to analyze customer reviews and improve customer care with NLP","og_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-2457--1--1.png","twitter_title":"How to analyze customer reviews and improve customer care with NLP","twitter_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","meta_title":"How to analyze customer reviews and improve customer care with NLP","meta_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"653feb16375d13000123da16","uuid":"62c9c370-85df-4a74-a997-f0f25da4b1ee","title":"How to evaluate object detection models with Quantumworks Lab Model Foundry","slug":"how-to-evaluate-object-detection-models-using-labelbox-model-foundry","html":"\u003cp\u003eThe rise of off-the-shelf and foundation models has enabled AI teams to fine-tune existing models and pre-label data faster and more accurately — in short, significantly accelerating AI development. However, using these models at scale for building AI can quickly become expensive. One way to mitigate these costs and reduce waste is to add a model comparison process to your workflow, ensuring that any model you choose to integrate for AI development is the best choice for your requirements.\u003c/p\u003e\u003cp\u003eA comprehensive model comparison process evaluates models on various metrics, such as performance, robustness, and business fit. The results will enable teams to quickly kickstart model development, decrease time to value, and ensure the best results with less time and costs for their specific use case.\u003c/p\u003e\u003cp\u003eEmbedding model comparison into the AI development workflow, however, comes with its own unique challenges, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConfidently assessing the potential and limitations of pre-trained models\u003c/li\u003e\u003cli\u003eVisualizing models’ performance for comparison\u003c/li\u003e\u003cli\u003eEffectively sharing experiment results\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this blog post, we’ll explore how you can tackle these challenges for a computer vision use case with \u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003ethe Foundry add-on for Quantumworks Lab Model\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"why-is-selecting-the-right-model-critical\"\u003eWhy is selecting the right model critical?\u0026nbsp;\u003c/h3\u003e\u003cp\u003eSelecting the most suitable off-the-shelf model is pivotal for ensuring accurate and reliable predictions tailored to your specific business use case, often leading to accelerated AI development. As different models exhibit diverse performance characteristics, diligently comparing the models’ predictions on your data can help distinguish which model excels in metrics such as accuracy, precision, recall, and more. This systematic approach to model evaluation and comparison enables you to refine the model’s performance with a “store of record” for future reference to continuously improve model performance.\u003c/p\u003e\u003cp\u003eChoosing the best off-the-shelf model provides a quick and efficient pathway to production, ensuring that the model aligns well with the business objectives. This alignment is crucial for the model's immediate performance and sets the stage for future improvements and adaptability to evolving requirements. The most suitable model for your use case also enables you to reduce the time and money spent on labeling a project. For instance, when pre-labels generated by a high-performing model are sent for annotation, less editing is required, making the labeling project quicker and more cost-effective. This is due to better Intersection Over Union (IOU) for tasks like Bounding Box, resulting in higher quality pre-labels and, therefore, fewer corrections. Furthermore, utilizing the best model can make your trove of data more queryable by enriching your data, thereby enhancing its searchability.\u003c/p\u003e\u003ch2 id=\"comparing-computer-vision-models-with-labelbox-model-foundry\"\u003eComparing computer vision models with Quantumworks Lab Model Foundry\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"748\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThis flow chart provides a high-level overview of the model comparison process when using the Foundry add-on for Quantumworks Lab Model. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWith Quantumworks Lab Model Foundry, you can evaluate a range of models for computer vision tasks to select the best model to perform pre-labeling or data enrichment on your data.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select images and choose a foundation model of interest \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jrgrl20l0x\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data and refine the images on which the predictions should be made, leverage filters in Catalog, including media attribute, natural language search, and more\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “Predict with Model Foundry.” You will then be prompted to choose a foundation model that you wish to use in the model run\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task — such as image classification, object detection, and/or image captioning\u003c/li\u003e\u003cli\u003eTo locate a specific model, you can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the models available for this machine learning task\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-hyperparameters-and-submit-a-model-run\"\u003e\u003cstrong\u003eStep 2: Configure model hyperparameters and submit a model run\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/tqszjqj6l1\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve located a specific model of interest, you can click into the model to view and set the model and ontology settings.\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of hyperparameters, which you can find in the Advanced model setting. To get an idea of how your current model settings affect the final predictions, you can generate preview predictions on up to five data rows.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings. If you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;Once you’re satisfied with the predictions, you can submit your model run.\u003c/p\u003e\u003ch3 id=\"step-3-predictions-will-appear-in-the-model-tab\"\u003eStep 3: Predictions will appear in the Model tab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ro3vxhcagr\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eEach model run is submitted with a unique name, allowing you to distinguish between each subsequent model run. When the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eView prediction results\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results across a variety of model runs different models\u003c/li\u003e\u003cli\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate \u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-repeat-steps-1-5-for-another-model-from-labelbox-model-foundry\"\u003eStep 4: Repeat steps 1-5 for another model from Quantumworks Lab Model Foundry\u003c/h3\u003e\u003cp\u003eYou can repeat steps 1-5 with a different model, on the same data and for the same desired machine learning task, to evaluate and compare model performance. By comparing the predictions and outputs from different models, you can assess and determine which one would be the most valuable in helping automate your data labeling tasks.\u003c/p\u003e\u003ch3 id=\"step-5-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 5: Create a model run with predictions and ground truth\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rzqnbhasuz\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a model run with model predictions and ground truth, users currently have to use a \u003ca href=\"https://colab.research.google.com/drive/1f0WkWtsDh2LYiy35pc3JF16srj6SxeTA?ref=labelbox-guides.ghost.io#scrollTo=W8tE-H7VmKea\"\u003escript\u003c/a\u003e to import the predictions from the Foundry add-on for Quantumworks Lab Model and ground truth from a project into a new model run. \u003c/p\u003e\u003cp\u003eIn the near future, this will be possible via the UI, and the script will be optional.\u003c/p\u003e\u003ch3 id=\"step-6-evaluate-predictions-from-different-models-from-model-foundry-in-labelbox-model\"\u003eStep 6: Evaluate predictions from different models from Model foundry in Quantumworks Lab Model\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qbx1mvx878\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter running the notebook, you'll be able to visually compare model predictions between two models. Use the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors. \u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/p\u003e\u003ch3 id=\"step-7-send-model-predictions-as-pre-labels-to-a-labeling-project-in-annotate\"\u003eStep 7: Send model predictions as pre-labels to a labeling project in Annotate\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/d3rxjzg111\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"640\" height=\"360\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eSelect the best performing model and leverage the model predictions as pre-labels. Rather than manually labeling data rows, select and send a subset of data to your labeling project with pre-labels to automate the process.\u003c/p\u003e\u003ch2 id=\"model-comparison-in-practice\"\u003eModel comparison in-practice:\u003c/h2\u003e\u003ch3 id=\"google-cloud-vision-vs-microsoft-azure-ai\"\u003eGoogle Cloud Vision vs Microsoft Azure AI\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/hms66_aBLd_Kj7jsKaYhr30ChoP5kflDM6nDmlqseCR63P-8uwSriJ9FqVf-biUS-uIQelFbtSvxq7Dq-Us-tq8qy3vkyxvs_--3CSShlVLZkbF3uS2-eHEaEV0SVugIfAVxB_xmDA4kL1ZFV2Z77hA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"880\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the metrics overview, we see that Google Cloud Vision outperforms Microsoft Azure AI for recall, f1 score, intersection over union and false negatives.\u003c/p\u003e\u003cp\u003eMicrosoft Azure AI boasts a precision score of 0.8633, which outperforms the 0.7948 score of Google Cloud Vision.\u0026nbsp;Microsoft Azure AI has an intersection over union score of 0.4034, an F-1 score of 0.7805, and a recall of 0.3852. In contrast, Google Cloud Vision exhibits a superior intersection over union score of 0.4187, an F-1 score of 0.7832, and a recall of 0.4149.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe can also see that the Microsoft Azure AI model has 12,665 more false negatives than Google Cloud Vision, and for our use case, we want the model with the least false negatives.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/xWXS7lRpI1UxITFAcG50Yq3RJMdKsSqK26uRudhilYHn_LvlSdyd7WMnUv8gtaj3C-GTJq2_e_4v0ZcxjVduI-VVui0AF57ZcL-2WUQURwHPRzO7ER2rGPNOKpY8YIW0NYqgz9-SWuMfMYRjgU0Z_cc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"589\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eF1 scores for both Microsoft Azure AI and Google Cloud Vision Model are generally comparable, with a few instances showcasing superior performance by the Google Cloud Vision Model. Here are the specific results for each category:\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Microsoft Azure AI scored 0.857, while Google Cloud Vision Model scored higher with 0.932\u003c/li\u003e\u003cli\u003eFor the boat category, Microsoft Azure AI achieved a score of 0.619, compared to the slightly higher 0.656 of Google Cloud Vision Model\u003c/li\u003e\u003cli\u003eFor the person category, Microsoft Azure AI obtained a score of 0.773, whereas Google Cloud Vision Model marginally outperformed with a score of 0.785\u003c/li\u003e\u003cli\u003eFor the airplane, Microsoft Azure AI scored 0.868, with Google Cloud Vision Model again performing better with a score of 0.893\u003c/li\u003e\u003cli\u003eFor the bus category, Microsoft Azure AI had a score of 0.705, significantly lower than the 0.925 achieved by Google Cloud Vision Model\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/SC9uJlITaaDLqRvZqSjBoVGIgHdQiQHgrLiiBBovLzjHK-Yiwt-URHAGyy9Z7l5WETRaFlS2Gukx4_UbeyQrazgJiljGnP5qv-wzJgMfTfW5yCUhnvvgUPEKz870g1FEmh5pe2zpnJHxX_QMYMYXul0\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"573\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eGenerally, the Google Cloud Vision Model exhibits superior performance in terms of intersection over union for classes such as train, boat, person, airplane, and bus.\u0026nbsp; Intersection over union (IOU) is crucial as it dictates the accuracy of the bounding box prediction area.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Microsoft Azure AI scored 0.304, while Google Cloud Vision Model significantly outperformed with a score of 0.745\u003c/li\u003e\u003cli\u003eFor the boat category, Microsoft Azure AI achieved a score of 0.251, compared to a higher 0.394 by Google Cloud Vision Model\u003c/li\u003e\u003cli\u003eFor the person category, Microsoft Azure AI secured a score of 0.697, with Google Cloud Vision Model slightly ahead at 0.704\u003c/li\u003e\u003cli\u003eFor the airplane category, Microsoft Azure AI scored 0.603, whereas Google Cloud Vision Model again demonstrated superior performance with a score of 0.738\u003c/li\u003e\u003cli\u003eFor the bus category, Microsoft Azure AI recorded a low score of 0.05, markedly lower than the 0.637 achieved by Google Cloud Vision Model\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/65NRl_i7lXdcTqhIrn968_0t_p_d4nqYsEKkEDAT_j1GVt3snxpTXrNmEwB-pQmeXD4sbOPxN3_arHpEgA-iD50iEDSTXC-ZSR1nc6JzI4BIJMYBBrxrr0hUTuhIM2t94PKY6AenEBgwCYQYUnSb18M\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"573\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eIn summary, Google Cloud Vision Model exhibits superior recall values across the categories of train, boat, person, airplane, and bus.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Microsoft Azure AI exhibited a recall of 0.331, while Google Cloud Vision Model showcased a considerably higher recall of 0.769\u003c/li\u003e\u003cli\u003eFor the boat category, Microsoft Azure AI demonstrated a recall of 0.203, compared to a higher 0.308 by Google Cloud Vision Model\u003c/li\u003e\u003cli\u003eFor the person category, Microsoft Azure AI achieved a recall of 0.618, with Google Cloud Vision Model slightly ahead at 0.622\u003c/li\u003e\u003cli\u003eFor the airplane category, Microsoft Azure AI registered a recall of 0.719, whereas Google Cloud Vision Model marginally outperformed with a recall of 0.747\u003c/li\u003e\u003cli\u003eFor the bus category, Microsoft Azure AI posted a low recall of 0.04, significantly lower than the 0.652 achieved by Google Cloud Vision Model\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLet’s now take a look at how the predictions appear on the images below\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J9PexX_zah5-eoCAgFdV6g6PftInYqzHoupESkHtdLeYzz47V0bsp5upVVrNmd5R6EXuqoKU-PrWnRa_JgwqQy0W6-vwFS-1kjoqzO3E_80WWYxaHSEcPsmpEnDbh7MCWNobGK2R5nEVqcvVSCpF0ZE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"741\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eHere we can see that the model from Google Cloud Vision has a blue bounding box that properly captures the dimension of the airplane. Whereas Azure’s orange bounding box only covers ~3/4 of the airplane.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Uy4LGTk8uhGha-Rkw6qvZaUpLQ9Zo7-EIJvPTrS0fcygRWRIJvIPi04HQ9zjTpWC4y3-A2sSy9OirucS20JObzVgfWVCVXKYpfmCItE-M7PEajJX6XalsmNKRnAO5I8MHL30r8gJHs0EurUwtNBnUGw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"763\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAnother example where Google Cloud Vision in blue bounding box has better IOU than Azure model’s orange bounding box. Based on the qualitative and quantitative analysis above, Google Cloud Vision is the superior model compared to Microsoft Azure AI.\u003c/p\u003e\u003ch3 id=\"google-cloud-vision-vs-amazon-rekognition\"\u003eGoogle Cloud Vision vs Amazon Rekognition\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/5Qsk1VepZUnxPhjfAGB33fhejnnJn5rOwQ0rrqQjEScKS38ep9vAnlViHSV1MynCeEVWfkOjcnZbF19tqFeJFXnYZ2SpLuZp5CR8x3QJ4-w8Z-XDeW-NiMKMOogsnbdeBHxvCc1wmNEaCXMG05GaY0s\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"655\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the metrics overview, Amazon Rekognition generally demonstrates better performance in false negatives, true positives, recall, and IOU against Google Cloud Vision.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor false negatives, Amazon Rekognition reported 21,935, whereas Google Cloud Vision had a slightly higher count of 22,868\u003c/li\u003e\u003cli\u003eFor true positives, Amazon Rekognition significantly outperformed with 25,953, compared to 10,112 recorded by Google Cloud Vision\u003c/li\u003e\u003cli\u003eFor recall, Amazon Rekognition showcased a higher value of 0.5419, while Google Cloud Vision exhibited a lower recall of 0.3913\u003c/li\u003e\u003cli\u003eFor Intersection over Union (IOU), Amazon Rekognition achieved a score of 0.4596, surpassing the 0.4212 scored by Google Cloud Vision\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/B5_bkD0aYI0lRIoUJZpX7m-J5_BzgJbzrCKhWtbzhOcw4EjWt2NY858HNCFV1duFbBwTpuHUZnMl8rggUcMQwWc6-7QhlKIgikUMKFZocxCeNiD0auUQLHSEd8xEVuSLUpIVtXU2HgEfO9Nm0TxzmTM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"518\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAmazon Rekognition outperformed Google Cloud Vision in the train, airplane, and bus categories\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Amazon Rekognition achieved an F-1 score of 0.969, outperforming Google Cloud Vision, which scored 0.932\u003c/li\u003e\u003cli\u003eFor the boat category, Amazon Rekognition scored 0.747, while Google Cloud Vision significantly outshone with a score of 0.956\u003c/li\u003e\u003cli\u003eFor the person category, Amazon Rekognition registered an F-1 score of 0.566, with Google Cloud Vision achieving a higher score of 0.773\u003c/li\u003e\u003cli\u003eFor the airplane category, Amazon Rekognition had an F-1 score of 0.919, slightly higher than the 0.893 scored by Google Cloud Vision\u003c/li\u003e\u003cli\u003eFor the bus category, Amazon Rekognition secured an F-1 score of 0.929, marginally outperforming Google Cloud Vision, which scored 0.925\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/XbeoxMVEBEttqsPy4S3P9gRbofKkTYbuQT_UeFdMmkCMWAt_YmKRHdXU6ltUSe7c1B4ov2PhYdnpJC4LS8XTj07tVu6HSqfnhDXDjA0-oObUG76juz61tgutFGdKRU84LGc18j6yc5NI4Ip8nzrF03c\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"518\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAmazon Rekognition led in the bus, boat and person categories,\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Amazon Rekognition has an IOU score of 0.741, closely competing with Google Cloud Vision which scored 0.745\u003c/li\u003e\u003cli\u003eFor the boat category, Amazon Rekognition outperformed with an IOU score of 0.454, compared to Google Cloud Vision's 0.394\u003c/li\u003e\u003cli\u003eFor the person category, Amazon Rekognition significantly led with a score of 0.813, while Google Cloud Vision scored 0.704\u003c/li\u003e\u003cli\u003eFor the airplane category, Amazon Rekognition recorded an IOU of 0.677, slightly below Google Cloud Vision's 0.738\u003c/li\u003e\u003cli\u003eFor the bus category, Amazon Rekognition and Google Cloud Vision scored closely with IOU scores of 0.65 and 0.637 respectively\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/tCrtU-Xg3sv5kX3oNYLbqafdToRY9VJ78cWeF9UwQBOAe7fvBBIV_cJFQbaXDMiLxxOlxMk1bU3DSzvNbsjvS8mIzwn22L85p4xnqsr1zIYUfUAhP-8j2onpPqDJSVgPxfFlEi1eTomuh4uzaBBBt6o\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"518\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eRecall for Amazon Rekognition for boat, person, airplane, and bus, is better than Google Cloud Vision.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Amazon Rekognition demonstrated a recall score of 0.893, while Google Cloud Vision significantly outperformed with a score of 0.983\u003c/li\u003e\u003cli\u003eFor the boat category, Amazon Rekognition led with a recall score of 0.432, compared to Google Cloud Vision's lower score of 0.308\u003c/li\u003e\u003cli\u003eFor the person category, Amazon Rekognition achieved a higher recall score of 0.787, surpassing Google Cloud Vision's 0.622\u003c/li\u003e\u003cli\u003eFor the airplane category, Amazon Rekognition outperformed with a recall score of 0.851, as opposed to Google Cloud Vision's 0.743\u003c/li\u003e\u003cli\u003eFor the bus category, Amazon Rekognition showcased a recall score of 0.836, significantly higher than Google Cloud Vision's 0.652\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/rgrVd-plFthhvoZgeUDJgc8BXH4KwkKK-wQMBmrAY-tkhr5lh-PP3gyyGIRVIWWL9_sO1OBgCqhrGAdr9gmPy4mwXkGOnlOXItmSicvvPQ1H5hExjipwUVye2Ep970YX33rzEYTRlH4WIIVUXVrhybI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"810\"\u003e\u003c/figure\u003e\u003cp\u003eHere, Google Cloud Vision in the orange bounding box has detected only one boat, but Amazon Rekognition has detected 5 more boats, a person, and a car.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/JycIqkuvst751-UDITIYiwj-xH-3WDp_5fE3I8KKFvBOicjJ80dfPXt4wjDbfbsN1fqYwPA4JbHDBGyI6E8sjS1J-yd3e52S67093113NvjpmKDQ4Worn0_-_nLDR9tiVpfC6PjL-1hcEmrttzVApHs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"810\"\u003e\u003c/figure\u003e\u003cp\u003eAmazon Rekognition in the blue bounding box has detected more boats and a clock tower.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/oOtbtSHmfi3Ob6pf7hLRSk1JrSlO6zWUlM2m92ILaa20Oc0HL4GZYcqzYh6mi-kVbuU2l8oyObhbB36EgrvLJOkpPEyBke7hZ6BlMWwHb08bUhln0mFMAry9EBAczeRuLFCN9Dz24BlTA3lLGdZQypA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"860\"\u003e\u003c/figure\u003e\u003cp\u003eAmazon Rekognition in the blue bounding box has an IOU of the truck with full coverage, whereas the IOU for Google Cloud Vision is around 90%.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/l87bsRsULhAXMrcUMtDbW6k2p3xGA4E7TpxuuaiVzjZGlWO4WjHhtbAMwomLKXf2BD58DXi6BznC-2zSmKvPbuki11Y5F3deYJGYC9tsfpLgqShOwS2IuhI0DWb1NHJufQDnhx7BuaVqgs3JQMss4hg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"810\"\u003e\u003c/figure\u003e\u003cp\u003eThe IOU for person is also better in addition to being able to detect cars in the background for Amazon Rekognition in the blue bounding box compared to Google Cloud Vision. Based on the analysis above, Amazon Rekognition is the best model for our use case.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSend the predictions as pre-labels to Quantumworks Lab Annotate for labeling\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eSince we've evaluated that Amazon Rekognition is the best model for our use case, we can send model predictions as pre-labels to our labeling project by highlighting all data rows and selecting \"Add batch to project.\" \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Yxobsiulkb3gR3KREdjS2x0m_aSOTVV_Mx_XXnGrRk1Q-8YueaAw33Y_uoZxJb6rDhJsF2PqWWb2yc2H5P6vEvfKW6qDoGwHFiJQ1VqeD5COxfagUNORNuzco1n6CXnKqJAv67UAGH8Yw_dQsdi6fHc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"484\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eIn conclusion, you can leverage the Foundry add-on for Quantumworks Lab Model to not only select the most appropriate model to accelerate data labeling, but to automate data labeling workflows. Use quantitative and qualitative analysis, along with model metrics, to surface the strengths and limitations of each model and select the best performing model for your use case. Doing so can help reveal detailed insights, such as seen in the above comparison between Google Cloud Vision and Amazon Rekognition. In the above model comparison example, we can see that Amazon Rekognition emerged as particularly well-suited for our project’s requirements and allows us to rapidly automate data tasks for our given use case.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Model Foundry\u003c/a\u003e streamlines the process of comparing model predictions, ensuring teams are leveraging the most optimal model for data enrichment and automation tasks. With the right model, teams can easily create pre-labels in Quantumworks Lab Annotate – rather than starting from scratch, teams can boost labeling productivity by correcting the existing pre-labels.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"653feb16375d13000123da16","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3084.png","featured":false,"visibility":"public","created_at":"2023-10-30T17:42:46.000+00:00","updated_at":"2024-02-02T18:32:57.000+00:00","published_at":"2023-11-01T15:46:26.000+00:00","custom_excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-evaluate-object-detection-models-with-model-foundry","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},"url":"https://labelbox-guides.ghost.io/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/","excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","reading_time":12,"access":true,"comments":false,"og_image":null,"og_title":"How to evaluate object detection models with Model Foundry","og_description":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3084-1.png","twitter_title":"How to evaluate object detection models with Model Foundry","twitter_description":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","meta_title":"How to evaluate object detection models with Model Foundry","meta_description":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6537fb91375d13000123d799","uuid":"6609e3d3-fc0e-4183-91ef-a83c801c7c6d","title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine-Tuning: A technical walkthrough using OpenAI's APIs \u0026 models","slug":"zero-shot-learning-few-shot-learning-fine-tuning","html":"\u003cp\u003eWith large language models (LLMs) gaining popularity, new techniques have emerged for applying them to NLP tasks. Three techniques in particular — zero-shot learning, few-shot learning, and fine-tuning — take different approaches to leveraging LLMs. In this guide, we’ll walk through the key difference between these techniques and how to implement them.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll walk through a case study of extracting airline names from tweets to compare the techniques. Using an entity extraction dataset, we’ll benchmark performance starting with zero-shot prompts, then experiment with few-shot learning, and finally fine-tune a model. By analyzing the results, we can better understand when to use zero-shot, few-shot, or fine-tuning with LLMs. You’ll also pick up tips for constructing effective prompts and setting up LLM experiments.\u003c/p\u003e\u003cp\u003eThe goal of this guide is to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompare the quantitative results among zero-shot learning, few-shot learning, and fine-tuning on an NER use case\u003c/li\u003e\u003cli\u003eExplore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026amp; Quantumworks Lab Model\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"zero-shot-learning-few-shot-learning-and-fine-tuning-in-action\"\u003eZero-shot learning, few-shot learning, and fine-tuning in action\u003c/h2\u003e\u003cp\u003eFor the purposes of this case study, we will be walking through an example of entity extraction featured in \u003ca href=\"https://colab.research.google.com/drive/1OCD8ivCtPS84cEhtjXkIWNAQX9oyKfFe?usp=sharing\u0026ref=labelbox-guides.ghost.io\"\u003ethis Google Colab Notebook\u003c/a\u003e. Specifically, we have a dataset of tweets about major airlines, and the task is to use an API to extract all airlines names that appear in the tweets.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe full dataset can be found on Kaggle \u003ca href=\"https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eHere is an example data row:\u003c/p\u003e\u003cp\u003e\"@AmericanAir do you have the phone number of a supervisor i can speak to regarding my travels today,['American Airlines’]\u003c/p\u003e\u003cp\u003ewhere the:\u003c/p\u003e\u003cp\u003eTWEET: @AmericanAir do you have the phone number of a supervisor i can speak to regarding my travels today\u003c/p\u003e\u003cp\u003eLABEL: [‘American Airlines']\"\u003c/p\u003e\u003cp\u003eBefore delving into the details, a few key technical definitions to keep in mind:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eZero-shot learning\u003c/strong\u003e — a technique whereby we prompt an LLM without any examples, attempting to take advantage of the reasoning patterns it has gleaned (i.e. a generalist LLM)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFew-shot learning\u003c/strong\u003e — a technique whereby we prompt an LLM with several concrete examples of task performance\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-tuning\u003c/strong\u003e — a technique whereby we take an off-the-shelf open-source or proprietary model, re-train it on a variety of concrete examples, and save the updated weights as a new model checkpoint\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"establishing-a-benchmark-baseline-with-zero-shot-learning\"\u003eEstablishing a benchmark baseline with zero-shot learning\u003c/h3\u003e\u003cp\u003eAs with any scientific or machine learning experiment, it is important to establish a benchmark baseline. For this case study, we will use zero-shot learning as the baseline by experimenting with various zero-shot prompts and evaluating the performance (precision, recall, f1-score, accuracy) of these prompts against the test set.\u003c/p\u003e\u003cp\u003eThe demo video below shows how we can leverage the \u003cstrong\u003e‘Humans Generate Prompt’ \u003c/strong\u003eoption of the \u003ca href=\"https://www.google.com/url?q=https://docs.labelbox.com/docs/llm-data-generation\u0026sa=D\u0026source=docs\u0026ust=1732675975664445\u0026usg=AOvVaw1fM_zT8IUOw2SSFmqUnUcd\" rel=\"noreferrer\"\u003ePrompt and Response Editor\u003c/a\u003e within Quantumworks Lab Annotate to create various zero-shot prompts using in-house and/or external teams to scale out our annotation operations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/44tuzazslq\" title=\"[Guide] Create Annotate Project \u0026amp; Airline Prompts in LLM Editor Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"534\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce we have constructed our dataset of zero-shot prompts within Quantumworks Lab using a prompt engineering workforce, we can export them from the Quantumworks Lab UI  the Quantumworks Lab Python SDK and use them in our script, as shown in the video below.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kryaxyhdx3\" title=\"[Guide] Exporting Prompts from LB Annotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eUsing gpt-3.5-turbo, we see the results of the prompts, along with their performance metrics.\u003c/p\u003e\u003cp\u003eFrom these results, we observe that prompts that fared best provided a clear and structured instruction to the model. These prompts explicitly mention the expected format for identifying airline names. Furthermore, these prompt structures introduce a pattern that the model can recognize and follow. In the case of \"Detect airline references...\" the model is prompted to look for references in a specific format (e.g. hashtags), which may be a common pattern in tweets mentioning airlines.\u003c/p\u003e\u003cp\u003ePrompts that fared worst had a theme: ambiguity in the prompt response format. Examples like \"What are the airlines in this tweet?\" and \"Find all airline mentions in the tweet\" are open- ended and do not provide a specific structure, making it harder for the model to interpret the task.\u003c/p\u003e\u003cp\u003eWhat’s interesting to note is that the prompt “Identify airlines like this - [#AIRLINE_NAME_1]:'{tweet}’” did NOT perform well even though it provided an example response format. This underscores the profound impact that punctuation and grammatical structure can have for prompt engineering. Instead of interpreting “[#AIRLINE_NAME_1]\" as the LLM-output format, the LLM instead interpreted it as a pattern-matching task to identify all airlines within a tweet that contain this specific format [#AIRLINE_NAME_1], of which there are none (hence, 0% across the evaluation metrics).\u003c/p\u003e\u003cp\u003eIn addition to testing different prompts, we can run various experiments to evaluate the efficacy of the task and evaluate the impact of those experiments in Quantumworks Lab Model. One such experiment could be to compare different models (GPT-4 vs. GPT-3.5, etc.). The video below shows how we can compare GPT-4 vs. GPT-3.5 on how each model performs on extracting airlines from the prompts we created above.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/13yquhzp4o\" title=\"[Guide] Airline extraction model comparison Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"534\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eZero-shot learning netted us the following baseline benchmark on the test set: 19%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"few-shot-learning\"\u003eFew-shot learning\u0026nbsp;\u003c/h3\u003e\u003cp\u003eTo build upon this benchmark, we used one of the prompts that performed well in zero-shot learning in tandem with few-shot learning — a technique whereby along with the prompt, we also feed an LLM concrete examples of task performance. These concrete examples are chosen from our training dataset (found in \u003cstrong\u003eairline_train.csv\u003c/strong\u003e).\u003c/p\u003e\u003cp\u003eThis is an example few-shot prompt that we passed to the LLM:\u003c/p\u003e\u003cp\u003eGiven the following tweets and their corresponding airlines, separated by new lines:\u003c/p\u003e\u003cp\u003e1) SouthwestAir bags fly free..just not to where you're going.,['Southwest Airlines']\u003c/p\u003e\u003cp\u003e2) Jet Blue I don't know- no one would tell me where they were coming from,['JetBlue Airways']\u003c/p\u003e\u003cp\u003ePlease extract the airline(s) from the following tweet:\u003c/p\u003e\u003cp\u003e\"SouthwestAir Just got companion pass and trying to add companion flg. Help!\"\u003c/p\u003e\u003cp\u003eUsing the following format - ['#AIRLINE_NAME_1] for one airline or ['#AIRLINE_NAME_1, #AIRLINE_NAME_2...] for multiple airlines.\u003c/p\u003e\u003cp\u003eEvaluating our model (gpt-3.5-turbo) on the test set via few-shot learning, we achieved an accuracy of 96.66%! There were 7 total misclassifications. Further inspection into these misclassifications actually reveals that there may be issues within the ground-truth dataset.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFew-shot learning netted us the following accuracy benchmark on the test set: 97%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"fine-tuning-with-a-training-dataset\"\u003eFine-tuning with a training dataset\u003c/h3\u003e\u003cp\u003eLastly, we seek to determine whether fine-tuning would improve our results. To ensure parity across our experiments, we used the same 100 randomly generated examples from the training dataset for few-shot learning as our control variable for the fine-tuning task.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFine-tuning netted us the following accuracy benchmark on the test set: 97%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"comparing-results\"\u003e\u003cstrong\u003eComparing results\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThese were the final results:\u003c/p\u003e\u003cp\u003e1) Zero-shot learning netted us the following accuracy baseline benchmark on the test set: 19%.\u003c/p\u003e\u003cp\u003e2) Few-shot learning netted us the following accuracy benchmark on the test set: 97%.\u003c/p\u003e\u003cp\u003e3) Fine tuning netted us the following accuracy benchmark on the test set: 91%.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKey takeaways\u003c/strong\u003e\u003c/p\u003e\u003cp\u003ePrompt engineering in tandem with few-shot learning versus fine-tuning both yield similar results for the task of extracting airline references from tweets. The ultimate consideration between the two boils down to achieving economies of scale. If teams find themselves needing to execute a prompt 100,000 times, the effort invested in terms of both human hours and GPU usage (or, token costs, if utilizing an API) can be justified when fine-tuning, as the cumulative savings in prompt tokens and the potential for improved output quality can accumulate significantly. Conversely, if we’re only using the prompt ten times and it's already effective, there's no rationale for fine-tuning it.\u003c/p\u003e\u003cp\u003eRegardless of which option you choose, we’ve seen how \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Annotate\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e can help achieve both outcomes.\u003c/p\u003e\u003cp\u003eWe can also improve results by ensuring variability within the few-shot learning examples. Currently, we are using a naive approach by selecting 100 randomly generated examples from the training dataset, so that it fits within the context window of gpt-3.5-turbo. Despite leveraging ~10% of our training data (100 rows / 900 possible training data rows), it’s less about data quantity and more about data quality. If we can curate a few-shot learning dataset of only 100 rows that has enough variance to be representative of the larger 900 rows, then that would be most ideal, from a token-sizing and, subsequently, cost perspective in addition to an engineering perspective (achieving more with less).\u003c/p\u003e\u003cp\u003eExamples of variance include tweet structure as well as a healthy mix of tweets with multiple airlines referenced. For tweet structure, we can use regular expressions to create patterns to capture mentions (@username), hashtags (#hashtag), airline stock ticker symbols, or emojis. In our use case, hashtags and ticker symbols would be super beneficial, since we see them scattered throughout our training dataset (e.g. #LUVisbetter, #jetblue, #UnitedAirlines, AA, etc.).\u003c/p\u003e\u003cp\u003eUsing \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e, we can version our few-shot learning and fine-tuning experiments by tying each experiment to a corresponding model run. Comparing evaluation metrics across different model runs allows us to either:\u003c/p\u003e\u003cul\u003e\u003cli\u003eChoose the few-shot learning prompt with the best precision, accuracy, and/or recall metric\u003c/li\u003e\u003cli\u003eChoose the fine-tuned model with the best precision, accuracy, and/or recall metric\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"get-started-today\"\u003eGet started today \u003c/h2\u003e\u003cp\u003eThrough our case study extracting airline names from tweets, we've explored the key differences between zero-shot learning, few-shot learning, and fine-tuning for applying large language models to NLP tasks.\u003c/p\u003e\u003cp\u003eThe best technique depends on your use case and available resources. The key is understanding how different data inputs affect an LLM's behavior. With the right approach, you can take advantage of LLMs' reasoning skills for a wide range of NLP applications. You can leverage each of these learning techniques with Quantumworks Lab’s LLM data generation editor and Quantumworks Lab Model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox is a data factory that empowers teams to generate high-quality data and accelerate the development of differentiated genAI solutions. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026gclid=Cj0KCQiA6Ou5BhCrARIsAPoTxrBVxRVcyUPeK8y7Mn2rtcM2M257gsxyxejwhFES91vcP7xYdoij4OQaAjUwEALw_wcB\u0026attr=arcade\u0026landingPageAnonymousId=%22ce322bab-56ea-43b8-a113-000851a3ebaf%22\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"6537fb91375d13000123d799","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083.png","featured":false,"visibility":"public","created_at":"2023-10-24T17:14:57.000+00:00","updated_at":"2024-11-27T02:11:35.000+00:00","published_at":"2023-10-24T20:34:32.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/zero-shot-learning-few-shot-learning-fine-tuning","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},"url":"https://labelbox-guides.ghost.io/zero-shot-learning-few-shot-learning-fine-tuning/","excerpt":"With large language models (LLMs) gaining popularity, new techniques have emerged for applying them to NLP tasks. Three techniques in particular — zero-shot learning, few-shot learning, and fine-tuning — take different approaches to leveraging LLMs. In this guide, we’ll walk through the key difference between these techniques and how to implement them. \n\nWe’ll walk through a case study of extracting airline names from tweets to compare the techniques. Using an entity extraction dataset, we’ll be","reading_time":6,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083-2.png","og_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","og_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083-1.png","twitter_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","twitter_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","meta_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","meta_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65300bc0a6fedb0001417480","uuid":"59641c97-b4ef-4ade-8e83-52f3887883d0","title":"How to use the model Foundry for automated data labeling and enrichment","slug":"guide-to-using-model-foundry","html":"\u003cp\u003eFoundation models, such as GPT-4, are ushering in a new era of AI by outperforming humans on numerous tasks across modalities including images and language. With the introduction of\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e \u003c/a\u003e\u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ethe Foundry add-on for Quantumworks Lab Model\u003c/u\u003e\u003c/a\u003e, we’re bringing the power of foundation models into the Quantumworks Lab platform.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eMeet your new AI co-pilot: Foundry\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLabelbox Foundry enables AI teams to use world-class foundation models to enrich datasets and automate tasks. In just a few clicks, AI builders can explore, test and integrate powerful models to build vital workflows for pre-labeling, or other specific data tasks. Kickstart your AI efforts with this AI copilot to build intelligent applications faster than ever.\u003c/p\u003e\u003cp\u003eEarly tests with Fortune 500 companies show up to 88% reductions in human labeling time, with complex tasks going from days to hours. Foundry integrates these state-of-the-art models into every step of the workflow, unlocking the next evolution of AI development. Kickstart your AI and data labeling efforts with this co-pilot to build intelligent applications faster than ever.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re excited to announce that Foundry is available as an add-on for Quantumworks Lab Model!\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith Foundry, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAccess the world’s most cutting-edge models and combine them with human-in-the-loop systems to accelerate model development.\u003c/li\u003e\u003cli\u003ePre-label data in a few clicks to reduce labeling costs by up to 90%.\u003c/li\u003e\u003cli\u003eTailor intelligence to your needs using Quantumworks Lab’s comprehensive end-to-end platform and workflow in Foundry. From fine-tuning to model distillation, you can customize intelligence beyond pre-built AI.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFoundry works across Quantumworks Lab's products — Catalog, Annotate, and Model — to supercharge your labeling and model development.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"how-to-access-foundry\"\u003eHow to access Foundry:\u003c/h2\u003e\u003ch3 id=\"free-tier-organizations\"\u003eFree Tier organizations\u003c/h3\u003e\u003cp\u003eFoundry is only available to our Starter and Enterprise plans, to access Foundry you will need to:\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Upgrade your account to our \u003cstrong\u003eStarter plan\u003c/strong\u003e:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn the \u003ca href=\"https://app.labelbox.com/workspace-settings/billing?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBilling tab\u003c/u\u003e\u003c/a\u003e, locate “Starter” in the All Plans list and select “Switch to Plan.” \u003cem\u003eThe credit card on file will only be charged when you exceed your existing free 10,000 LBU.\u0026nbsp;\u003c/em\u003e\u003c/li\u003e\u003cli\u003eUpgrades take effect immediately so you'll have access to Foundry right away on the Starter plan. After upgrading, you’ll see the option to activate Foundry for your organization.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-04_16-22-54--1--min-1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1904\" height=\"932\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/2023-12-04_16-22-54--1--min-1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/2023-12-04_16-22-54--1--min-1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/2023-12-04_16-22-54--1--min-1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-04_16-22-54--1--min-1.gif 1904w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e2) After upgrading to our Starter plan, please follow the instructions under the Starter plan below to activate Foundry as an add-on for your organization.\u003c/p\u003e\u003ch3 id=\"self-serve-organizations-on-a-starter-or-standard-plan\"\u003eSelf-serve organizations on a Starter or Standard plan\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs part of the self-serve Starter or Standard plan, you automatically have the ability to opt-in to using Foundry.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eTo generate model predictions, just submit a model run and you’ll be guided to activate Foundry along the way:\u0026nbsp;\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Generate predictions:\u003c/strong\u003e Start by selecting data in Catalog and click “Predict with Foundry.” You’ll be able to select a foundation model and submit a model run to generate predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Activate Foundry:\u003c/strong\u003e The first time you submit a model run, you’ll see the option to activate Foundry for your organization’s \u003ca href=\"https://docs.labelbox.com/docs/members-group-management?ref=labelbox-guides.ghost.io#member-roles\"\u003e\u003cu\u003eAdmins\u003c/u\u003e\u003c/a\u003e. You’ll need to agree to Quantumworks Lab’s Model Foundry add-on service terms and confirm you understand the associated compute fees.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Payment method:\u003c/strong\u003e With Foundry, you will be charged for the Quantumworks Lab units (LBUs) your account consumes, plus any\u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003epass-through compute costs\u003c/u\u003e\u003c/a\u003e for the models you use.\u003c/p\u003e\u003cp\u003eFor self-serve Starter and Standard tier to enable billing for these model compute charges, you'll be asked to confirm the credit card on file for your Quantumworks Lab account. Alternatively, you may add another card if you prefer to keep the Foundry charges separate. By confirming your payment method, you agree to let Quantumworks Lab to bill your card as Foundry model compute fees accrue based on your usage.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/OLBO-X7WF_y9EXTcmQ5nM1-6W75tJyYWojkmQp_IH-bdx2R_i985qA00Tbf3nMqHLGzUIkKo-0MES9-jfKUlv86kFjlmASbzfCo_CZTAau-38nUtH22IYHxUJ4mHgYi_VgfTdf9CaBsT3PK-46FeAF8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"305\"\u003e\u003c/figure\u003e\u003ch3 id=\"enterprise-organizations\"\u003eEnterprise organizations\u003c/h3\u003e\u003cp\u003eAs an organization on our Enterprise plan, you automatically have the ability to opt-in to using Foundry.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eTo generate model predictions, just submit a model run and you’ll be guided to activate Foundry along the way:\u0026nbsp;\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Generate predictions:\u003c/strong\u003e Start by selecting data in Catalog and click “Predict with Foundry.” You’ll be able to select a foundation model and submit a model run to generate predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Activate Foundry:\u003c/strong\u003e The first time you submit a model run, you’ll see the option to activate Foundry for your organization’s \u003ca href=\"https://docs.labelbox.com/docs/members-group-management?ref=labelbox-guides.ghost.io#member-roles\"\u003e\u003cu\u003eAdmins\u003c/u\u003e\u003c/a\u003e. You’ll need to agree to Quantumworks Lab’s Model Foundry add-on service terms and confirm you understand the associated compute fees.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Payment method:\u003c/strong\u003e With Foundry, you will be charged for the Quantumworks Lab units (LBUs) your account consumes, plus any\u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003epass-through compute costs\u003c/u\u003e\u003c/a\u003e for the models you use.\u003c/p\u003e\u003cp\u003eIf you are on an annual contract plan, you may add a credit card on file to pay for the associated compute costs of a model or choose to receive a monthly invoice at the end of each month based on your organization’s Foundry usage.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-06_12-19-47.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1016\" height=\"738\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/2023-12-06_12-19-47.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/2023-12-06_12-19-47.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-06_12-19-47.png 1016w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"see-it-in-action-how-foundry-works\"\u003eSee it in action: How Foundry works\u003c/h2\u003e\u003cp\u003eIn the below interactive demos, you'll learn how to leverage foundation models to generate model predictions, send predictions as pre-labels to a labeling project in Annotate, and verify the predictions with human-in-the-loop review:\u003c/p\u003e\u003ch3 id=\"step-1-how-to-generate-model-predictions-with-foundry\"\u003eStep 1: How to generate model predictions with Foundry\u003c/h3\u003e\u003cp\u003eAccess a vast range of ready-to-use foundation models that embed advanced AI into your data tasks with ease. Quickly generate predictions to pre-label datasets or to enrich your existing data to extract better insights that boost productivity and increase time-savings.\u003c/p\u003e\u003ch3 id=\"step-2-how-to-send-foundry-predictions-as-pre-labels-to-a-labeling-project\"\u003e\u003cstrong\u003eStep 2: How to send Foundry predictions as pre-labels to a labeling project\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRather than labeling from scratch, combine the power of foundation models with human-in-the-loop review to accelerate your labeling operations.\u003c/p\u003e\u003ch3 id=\"step-3-how-to-verify-pre-labels-with-human-in-the-loop-review\"\u003eStep 3: How to verify pre-labels with human-in-the-loop review\u003c/h3\u003e\u003cp\u003eFocus human intelligence on critical quality assurance instead of on initial labeling efforts. Seamlessly validate model-generated pre-labels in Annotate – approving accurate predictions with a click and easily editing or sending incorrect labels to be corrected.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out the below tutorials to learn how Foundry can accelerate your pre-labeling and data enrichment workflows in Labelbox.\u0026nbsp;\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003cdiv style=\"position: relative; padding-bottom: calc(48.601036269430054% + 41px); height: 0;\"\u003e\u003ciframe src=\"https://demo.arcade.software/HnecUTysIiTR66fMWJ1C?embed\" title=\"Object Detection - Foundry Arcade \" frameborder=\"0\" loading=\"lazy\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;color-scheme: light;\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\u003c!--kg-card-end: html--\u003e\n\n\u003c!--kg-card-begin: html--\u003e\n\u003cdiv style=\"position: relative; padding-bottom: calc(48.57586742620404% + 41px); height: 0;\"\u003e\u003ciframe src=\"https://demo.arcade.software/PqkNFZRaNrGWBPjlotve?embed\" title=\"Named Entity Recognition - Foundry Arcade\" frameborder=\"0\" loading=\"lazy\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;color-scheme: light;\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003ch2 id=\"what-use-cases-is-foundry-available-for\"\u003eWhat use cases is Foundry available for?\u003c/h2\u003e\u003cp\u003eFoundry currently supports a variety of tasks for computer vision and natural language processing. This includes:\u003c/p\u003e\u003ch3 id=\"computer-vision\"\u003eComputer Vision\u003c/h3\u003e\u003cul\u003e\u003cli\u003eObject detection\u003c/li\u003e\u003cli\u003eImage classification\u003c/li\u003e\u003cli\u003eImage segmentation \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWatch the below demo to see how to use Foundry for an object detection use case with Amazon Rekognition and Grounding Dino:\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/78pt3397io\" title=\"Model Foundry - Image Demo with Amazon Rekognition and Grounding Dino Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThis demo uses Amazon Rekognition and Grounding Dino, but you can leverage any other \u003ca href=\"https://labelbox.com/model-foundry/models/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision model available in Foundry\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"natural-language-processing\"\u003eNatural Language Processing\u003c/h3\u003e\u003cul\u003e\u003cli\u003eText generation\u003c/li\u003e\u003cli\u003eTranslation\u003c/li\u003e\u003cli\u003eQuestion answering\u003c/li\u003e\u003cli\u003eZero-shot classification\u003c/li\u003e\u003cli\u003eSummarization\u003c/li\u003e\u003cli\u003eConversational\u003c/li\u003e\u003cli\u003eText classification\u003c/li\u003e\u003cli\u003eNamed entity recognition\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWatch the below demo to see how to use Foundry for a text classification use case with GPT-4:\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jf0g7ou9bk\" title=\"Model Foundry - Text Demo with GPT-4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThis demo uses GPT-4, but you can leverage any other \u003ca href=\"https://labelbox.com/model-foundry/models/?ref=labelbox-guides.ghost.io\"\u003elarge language model available in Foundry\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"how-is-foundry-priced\"\u003e\u003cstrong\u003eHow is Foundry priced?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFoundry pricing will be calculated and billed monthly based on the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInference cost\u003c/strong\u003e – Quantumworks Lab will charge customers for inference costs for all models hosted by Labelbox. Inference costs will be bespoke to each model available in Foundry. The inference price is determined based on vendors or our compute costs – these are published publicly on \u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour website\u003c/u\u003e\u003c/a\u003e as well as inside the product.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLabelbox's platform cost\u003c/strong\u003e – each asset with predictions generated by Foundry will accrue LBUs.\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eModel LBU\u003c/strong\u003e is consumed for every data row that goes through Foundry. \u003cstrong\u003eAnnotate LBU\u003c/strong\u003e is consumed for every data row that has a prediction (from Foundry) submitted as a label.\u003c/p\u003e\u003cp\u003eThe unit compute costs are listed on the model card for each model found in the Quantumworks Lab app.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Iqv7Lg9_XlQFIdXK_NVWx-Az4ig-zIlTZ2H2ZFiCsb-BQ-U94-VdccpvJ17SpayYK4VkS-GbAnGBWu8qpvfiARtORoPagIpr4wcqUSHRWq9LcmLBJ5eNOGCLMpQ-d-_c1_Xo9IrT1iZ9nAmigLk0A_A\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"220\"\u003e\u003c/figure\u003e\u003cp\u003eLearn more about the pricing of the Foundry add-on for Quantumworks Lab Model on our \u003ca href=\"https://labelbox.com/pricing/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003epricing page\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"where-can-i-provide-feedback-or-ask-questions\"\u003e\u003cstrong\u003eWhere can I provide feedback or ask questions?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf you have questions or feedback related to the Foundry workflow, please\u003ca href=\"https://labelbox.atlassian.net/servicedesk/customer/portal/2/group/3/create/214?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003esubmit a ticket here\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"additional-resources\"\u003eAdditional Resources\u0026nbsp;\u003c/h2\u003e\u003cp\u003eTo learn more about Foundry and familiarize yourself with the workflow, we recommend checking out the below resources:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox/docs/foundry?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eFoundry Documentation\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eDemo videos\u0026nbsp;\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.wistia.com/medias/78pt3397io?ref=labelbox-guides.ghost.io\"\u003eImage \u003c/a\u003e- Amazon Rekognition and Grounding DINO Demo\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.wistia.com/medias/jf0g7ou9bk?ref=labelbox-guides.ghost.io\"\u003eText \u003c/a\u003e- GPT-4 Demo\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModels currently available in Foundry\u003c/a\u003e\u003c/li\u003e\u003cli\u003eRecent blog posts exploring the power of foundation models:\u003cul\u003e\u003cli\u003eExplore six of the most powerful foundation models available to AI builders, the use cases and applications they are best suited for. \u003ca href=\"https://labelbox.com/blog/6-cutting-edge-foundation-models-for-computer-vision-and-how-to-use-them/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eGPT-4 offers a versatile toolkit to help enterprises kickstart labeling. Learn how to leverage GPT-4 for your machine learning or specific business use case. \u003ca href=\"https://labelbox.com/blog/how-to-unlock-the-full-power-of-gpt-4-for-enterprise-ai/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eMany businesses have more specialized and domain-specific use cases. Discover the benefits of incorporating foundation models into your specialized AI application development workflow. \u003ca href=\"https://labelbox.com/blog/why-you-should-leverage-foundation-models-for-specialized-ai-applications/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eRemoving personally identifiable information from datasets is crucial for teams building AI. Explore how you can extract PII from your datasets with higher accuracy and speed. \u003ca href=\"https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"65300bc0a6fedb0001417480","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/12/Welcome-to-the-Foundry-1--2-.png","featured":false,"visibility":"public","created_at":"2023-10-18T16:45:52.000+00:00","updated_at":"2023-12-12T15:05:47.000+00:00","published_at":"2023-10-18T17:21:36.000+00:00","custom_excerpt":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/guide-to-using-model-foundry","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/guide-to-using-model-foundry/","excerpt":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":"How to use the model Foundry for automated data labeling and enrichment","og_description":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to use the model Foundry for automated data labeling and enrichment","meta_description":"Introducing the model Foundry - Enrich data and automate tasks using foundation models","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64ff69ca935e200001ee0180","uuid":"89463a23-02d8-47e3-98db-49448eb94d96","title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","slug":"how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox","html":"\u003cp\u003eIn machine learning, fine-tuning pre-trained models is a powerful technique that adapts models to new tasks and datasets. Fine-tuning takes a model that has already learned representations on a large dataset, such as a large language model, and leverages prior knowledge to efficiently “teach” the model a new task. \u003c/p\u003e\u003cp\u003eThe key benefit of fine-tuning is that it allows you to take advantage of transfer learning. Rather than training a model from scratch, which requires massive datasets and compute resources, you can start with an existing model and specialize it to your use case with much less data and resources. Fine-tuning allows ML teams to efficiently adapt powerful models to new tasks with limited data and compute. It is essential for applying state-of-the-art models to real-world applications of AI. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning?ref=labelbox-guides.ghost.io\"\u003eOpen AI’s fine-tuning API\u003c/a\u003e allows teams to fine-tune the following models:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGPT-3.5 -turbo-0613 (recommended)\u003c/li\u003e\u003cli\u003eBabbage-002\u003c/li\u003e\u003cli\u003eDavinci-002\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we’ll cover how to leverage \u003ca href=\"https://www.ssw.com.au/rules/what-is-gpt/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eOpen AI’s GPT-3.5\u003c/a\u003e and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe goal of model fine-tuning is to improve the model’s performance against a specific task. Over other techniques to optimize model output, such as prompt design, fine-tuning can help achieve:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigher quality results: \u003c/strong\u003eFine-tuning allows the model to learn from a much larger and more diverse dataset than can fit into a prompt. The model can learn more granular patterns and semantics that are relevant to your use case through extensive fine-tuning training. Prompts are limited in how much task-specific context they can provide, while fine-tuning teaches the model your specific domain.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eToken savings: \u003c/strong\u003eFine-tuned models require less prompting to produce quality outputs. With fine-tuning, you can leverage a shorter, more general prompt since the model has learned your domain – saving prompt engineering effort and tokens. Whereas highly-specific prompts can often hit token limits.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLower latency: \u003c/strong\u003eHeavily engineered prompts can increase latency as they require more processing. As fine-tuned models are optimized for your specific task, they allow faster inference and can quickly retrieve knowledge for your domain.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFine-tuning is especially beneficial for adapting models to your specific use case and business needs. There are several common scenarios where fine-tuning really can help models capture the nuances required for an application:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStyle, tone, or format customization:\u003c/strong\u003e Fine-tuning allows you to adapt models to match the specific style or tone required for a use case, whether it be a particular brand voice or difference in tone for speaking to various audiences.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDesired output structure: \u003c/strong\u003eFine-tuning can teach models to follow a required structure or schema in outputs. For example, you can fine-tune a summarization model to consistently include key facts in a standardized template.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHandling edge cases: \u003c/strong\u003eReal-world data often contains irregularities and edge cases. Fine-tuning allows models to learn from a wider array of examples, including rare cases. You can fine-tune the model on new data samples so that it learns to handle edge cases when deployed to production.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn short, fine-tuning allows teams to efficiently adapt powerful models to new tasks and datasets, allowing ML teams to customize general models to their specific use cases and business needs through extensive training on curated data. High-quality fine-tuning datasets are crucial to improve performance by teaching models the nuances and complexity of the target domain more extensively than possible through prompts alone.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOpen AI’s recommended dataset guidelines\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo fine-tune an Open AI model, it is required to provide at least ten examples. Research has shown clear improvements from fine-tuning on 50 to 100 training examples with GPT-3.5-turbo. Data quality, over data quantity, is also critical to the success of the fine-tuned model. \u003c/p\u003e\u003cp\u003eYou can learn more about preparing a dataset in \u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?ref=labelbox-guides.ghost.io\"\u003eOpen AI’s documentation\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"how-to-use-labelbox-for-fine-tuning\"\u003eHow to use Quantumworks Lab for fine-tuning\u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform for building intelligent applications. With a suite of powerful data curation, labeling, and model evaluation tools, the platform is built to help continuously improve and iterate on model performance. For this example, we will use the Quantumworks Lab platform to create a high-quality fine-tuning dataset. \u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can prepare a dataset of prompts and responses to fine-tune large language models (LLMs). Quantumworks Lab supports dataset creation for a variety of fine-tuning tasks including summarization, classification, question-answering, and generation.\u003c/p\u003e\u003cp\u003eWhen you set up an \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003eLLM data generation project\u003c/a\u003e in Quantumworks Lab, you will be prompted to specify how you will be using the editor. You have three choices for specifying your LLM data generation workflow:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 1: Humans generate prompts and responses\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, the prompt and response fields will be required. This will indicate to your team that they should create a prompt and response from scratch.\u003c/p\u003e\u003cp\u003eYou can make a copy of this \u003ca href=\"https://colab.research.google.com/drive/1ElV4U68ZJCJ-wIMLFTQAyzXrSSz6aU2Y?ref=labelbox-guides.ghost.io#scrollTo=HUhjjPp0mnPq\"\u003eGoogle Colab Notebook\u003c/a\u003e to generate a prompt and response dataset in Quantumworks Lab and fine-tune GPT-3.5 Turbo in OpenAI. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 2: Humans generate prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, only the prompt field will be required. This will indicate to your team that they should create a prompt from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 3: Humans generate responses to uploaded prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, a previously uploaded prompt will appear. Your team will need to create responses for that prompt.\u003c/p\u003e\u003cp\u003eYou can make a copy of this \u003ca href=\"https://colab.research.google.com/drive/1J21Nb0fqbZCcnxyR4wOKLlADwcayOlSq?ref=labelbox-guides.ghost.io#scrollTo=vGHl-lKkZ9cK\"\u003eGoogle Colab Notebook\u003c/a\u003e to generate a dataset of responses to uploaded prompts in Quantumworks Lab and fine-tune GPT-3.5 Turbo in OpenAI. \u003c/p\u003e\u003cp\u003eIn the below example, we’ll be walking through a sample use case of summarizing and removing PII from customer support chats with Quantumworks Lab and OpenAI’s GPT-3.5 Turbo. Imagining we’re a company who wishes to summarize support logs without revealing personally identifiable information in the process, we’ll be fine-tuning an LLM to summarize and remove PII from customer support logs.\u003c/p\u003e\u003ch3 id=\"step-1-evaluate-how-gpt-35-performs-against-the-desired-task\"\u003eStep 1: Evaluate how GPT-3.5 performs against the desired task\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/e00fk9u59h\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBefore we begin the fine-tuning process, let’s first evaluate how ChatGPT (using GPT-3.5) performs against the desired task off-the-shelf. \u003c/p\u003e\u003cp\u003eWe uploaded the following sample chat log to ChatGPT:\u003c/p\u003e\u003cp\u003e“Summarize this chat log and remove any personally identifiable information in the summary:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTom: \u003c/strong\u003eI need to reset my account access.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUrsula: \u003c/strong\u003eI can help with that, Tom. What’s your account email?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTom: \u003c/strong\u003eIt’s \u003ca href=\"mailto:tom@example.com\"\u003etom@example.com\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUrsula: \u003c/strong\u003eGreat, Tom. I’ve sent you a link to update your credentials”\u003c/p\u003e\u003cp\u003eIn the above prompt, we’ve asked ChatGPT to summarize the chat log and remove any personally identifiable information. \u003c/p\u003e\u003cp\u003eUpon evaluation, the default GPT-3.5 model misses the mark for our desired use case. \u003c/p\u003e\u003cp\u003eThe summary includes both Tom and Ursula’s names and explicitly mentions Tom’s email address. In order to reliably use the model for our business use case, we need to fine-tune it so that it appropriately excludes elements of personally identifiable information. To do so, we will leverage Quantumworks Lab to generate our fine-tuning dataset and use it to fine-tune GPT-3.5 through OpenAI. \u003c/p\u003e\u003ch3 id=\"step-2-create-a-llm-data-generation-project-in-labelbox\"\u003eStep 2: Create a LLM data generation project in Quantumworks Lab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ldpv9nwh14\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to upload our support chat logs to \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Catalog \u003c/a\u003e– this will allow us to browse, curate, and send these data rows for labeling.\u003c/p\u003e\u003cp\u003eNext, we’ll need to create a LLM data generation labeling project in \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Annotate\u003c/a\u003e.\u003c/p\u003e\u003cul\u003e\u003cli\u003eSince we have an available dataset, this will be a ‘Humans generate response to uploaded prompts’ project.\u003c/li\u003e\u003cli\u003eWhen configuring the ontology, we will set the response type as ‘text’ and make the appropriate response to “summarize and remove personally identifiable information in the summary”.\u003c/li\u003e\u003cli\u003eDuring ontology creation, you can also define a character minimum or maximum and upload necessary instructions for the labeling team.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-label-data\"\u003eStep 3: Label data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/i8tzhezn70\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter successfully setting up an LLM data generation project, we can queue the uploaded chat logs in Catalog for labeling in Annotate. To label data, you have the option of leveraging \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost’s\u003c/a\u003e extensive workforce or use your own internal team to summarize and remove personally identifiable information in the summary.\u003c/p\u003e\u003cp\u003eFor larger or more complex fine-tuning tasks, you can scale up to hundreds or thousands of labeled data rows. Once all data has been labeled, you can review the corresponding summary to each prompt and export the data rows.\u003c/p\u003e\u003ch3 id=\"step-4-export-data-from-labelbox-and-fine-tune-it-in-openai\"\u003e\u003cbr\u003eStep 4: Export data from Quantumworks Lab and fine-tune it in OpenAI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bx1z0xy4db\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWith all necessary data labeled, we can export the dataset from Quantumworks Lab and upload it in a format that is readable by OpenAI. \u003c/p\u003e\u003cp\u003eOpenAI requires a dataset to be in the structure of their \u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create?ref=labelbox-guides.ghost.io\"\u003echat completions API\u003c/a\u003e, whereby each message has a role, content, and optional name. You can learn more about specific dataset requirements in OpenAI’s \u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. Using a script, we can convert the Quantumworks Lab export into OpenAI’s required conversational chat format.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e{\n \"messages\":\n \t{\"role\":\"system\",\n    \"content\":\"Given a chat log, summarize and remove personal \t\tidentifiable information in the summary.\"},\n    {\"role\":\"user\",\n    \"content\":\"Andy:Why has my order not shipped yet?! Bella: I \tapologize for the delay, Andy:May I have your order number? Andy: \t  It's ORDER5678. Please hurry! Bella: Thank you Andy. It's \t\t\texpedited and will ship today.\"},\n\t{\"role\":\"assistant\",\n    \"content\":\"Customer inquires about the delay in the shipment of his order. Support agent requests the order number and upon receiving \t  it, assures customer that the order has been expedited and will \t\tship that day.\"\n    }\n ]\n}\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003eAfter formatting our dataset, we can upload it and \u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model?ref=labelbox-guides.ghost.io\"\u003estart a fine-tuning job\u003c/a\u003e using the OpenAI SDK.\u003c/p\u003e\u003cp\u003eYou can use a copy of the following \u003ca href=\"https://colab.research.google.com/drive/1J21Nb0fqbZCcnxyR4wOKLlADwcayOlSq?ref=labelbox-guides.ghost.io#scrollTo=vGHl-lKkZ9cK\"\u003eGoogle Colab notebook\u003c/a\u003e to export data from Quantumworks Lab in a format compatible with fine-tuning GPT-3.5 Turbo and begin a fine-tuning job. \u003c/p\u003e\u003ch3 id=\"step-5-assess-the-fine-tuned-model%E2%80%99s-performance-in-openai\"\u003eStep 5: Assess the fine-tuned model’s performance in OpenAI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9kdb8m0xm0\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 5) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter the fine-tuning job has succeeded, you can navigate to the OpenAI playground and select the newly fine-tuned model for evaluation. Similarly to evaluating the initial GPT-3.5 model, we can enter a sample chat log and see how the newly fine-tuned model performs.\u003c/p\u003e\u003cp\u003eCompared to the off-the-shelf GPT-3.5 model, this model that has been fine-tuned on our training data is performing as expected. We can see that all names and relevant information that would be considered as personally identifiable information has been retracted. \u003c/p\u003e\u003cp\u003eWe can also compare the fine-tuned model to the initial GPT-3.5 model and see how it performs on the same prompt. Again, we can see that while GPT-3.5 excludes some aspects of personally identifiable information, it still includes the user’s first name, so it doesn’t quite meet the expectations for our business use case.\u003c/p\u003e\u003cp\u003eThe newly fine-tuned model has allowed us to adapt GPT-3.5 to our specific use case of concealing personally identifiable information. With Quantumworks Lab, teams can iteratively identify gaps and outdated samples in the fine-tuning data, then generate fresh high-quality data, allowing model accuracy to be maintained over time. Updating fine-tuning datasets through this circular feedback process is crucial for adapting to new concepts and keeping models performing at a high level within continuously changing environments.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eTo improve LLM performance, Quantumworks Lab simplifies the process for subject matter experts to generate high-quality datasets for fine-tuning with leading model providers and tools, like OpenAI. \u003c/p\u003e\u003cp\u003eUnlock the full potential of large language models with Quantumworks Lab’s end-to-end platform and a \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox.ghost.io\"\u003enew suite of LLM tools\u003c/a\u003e to generate high-quality training data and optimize LLMs for your most valuable AI use cases.\u003c/p\u003e","comment_id":"64ff69ca935e200001ee0180","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Group-3081.jpg","featured":false,"visibility":"public","created_at":"2023-09-11T19:26:02.000+00:00","updated_at":"2024-05-28T17:02:27.000+00:00","published_at":"2023-09-11T19:58:50.000+00:00","custom_excerpt":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/","excerpt":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","og_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Group-3081-1.jpg","twitter_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","twitter_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","meta_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","meta_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"tag":{"slug":"use-ai","id":"653aa4fb375d13000123d7e6","name":"Use AI","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","count":{"posts":17},"url":"https://labelbox-guides.ghost.io/tag/use-ai/"},"slug":"use-ai","currentPage":1},"__N_SSG":true},"page":"/guides/tag/[id]","query":{"id":"use-ai"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script>
    

    

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>

                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="/static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer>
</body>
<!-- Mirrored from labelbox.com/guides/tag/use-ai/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:32:03 GMT -->
</html>