<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/tag/use-ai/page/2/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:18:41 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../../../static/scripts/munchkin.js"></script><script src="../../../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/guides/tag/%5bid%5d/page/%5bpagenum%5d-da4e9ee1c105845a.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../../../index.html"><img width="106" height="24" alt="logo" src="../../../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../../../build-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Build AI</a><a href="../../index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Use AI</a><a href="../../../explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="../../../label-data-for-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Label data for AI</a><a href="../../../train-fine-tune-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../../../mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-fine-tune-vertex-ai-models-with-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index32b3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-fine-tune-vertex-ai-models-with-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to fine-tune Vertex AI LLMs with Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index38a9.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Meta&#x27;s Segment Anything (SAM) model on video with Quantumworks Lab&#x27;s model-assisted labeling</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-accelerate-image-text-pair-generation-with-blip-2/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexf5d3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-accelerate-image-text-pair-generation-with-blip-2/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to accelerate image-text pair generation with BLIP-2</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../automatically-label-text-with-96-accuracy-using-foundation-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexf3e3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../automatically-label-text-with-96-accuracy-using-foundation-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Automatically label text with 96%+ accuracy using foundation models</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab&#x27;s search capabilities, bulk classification, and foundation models.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../automatically-label-images-with-99-accuracy-using-foundation-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index200d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3058--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../automatically-label-images-with-99-accuracy-using-foundation-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Automatically label images with 99% accuracy using foundation models</p><p class="text-base max-w-2xl undefined line-clamp-3">Automatically label images with 99% accuracy leveraging Quantumworks Lab&#x27;s search capabilities, bulk classification, and foundation models. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-fine-tune-large-language-models-with-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index528e.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-fine-tune-large-language-models-with-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to fine-tune large language models (LLMs) with Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index4b8a.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we&#x27;ll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.</p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8"><a class="mr-9 text-neutral-700 mb-1" href="../1/index.html">&lt;</a>Page 2 of 2</div></div></div></div></div></div>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><span style="color: inherit; cursor: default;">Docs</span></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <span style="color: inherit; cursor: default;">Terms of Service</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Privacy Notice</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Copyright Dispute Policy</span>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"64fb3e9f935e200001ee0106","uuid":"9f549cdf-80e9-4441-8b4e-f1ce92572f56","title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","slug":"how-to-fine-tune-vertex-ai-models-with-labelbox","html":"\u003cp\u003eIn machine learning, fine-tuning pre-trained models is a powerful technique that adapts models to new tasks and datasets. Fine-tuning takes a model that has already learned representations on a large dataset, such as a large language model, and leverages prior knowledge to efficiently “teach” the model a new task. \u003c/p\u003e\u003cp\u003eThe key benefit of fine-tuning is that it allows you to take advantage of transfer learning. Rather than training a model from scratch, which requires massive datasets and compute resources, you can start with an existing model and specialize it to your use case with much less data and resources. Fine-tuning allows ML teams to efficiently adapt powerful models to new tasks with limited data and compute. It is essential for applying state-of-the-art models to real-world applications of AI.\u003c/p\u003e\u003cp\u003eVertex AI provides \u003ca href=\"https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models?ref=labelbox-guides.ghost.io#supported_models\"\u003eseveral base models\u003c/a\u003e that can be fine-tuned:\u003c/p\u003e\u003cul\u003e\u003cli\u003etext-bison@001\u003c/li\u003e\u003cli\u003ecode-bison@001\u003c/li\u003e\u003cli\u003ecodechat-bison@001\u003c/li\u003e\u003cli\u003echat-bison@001\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.\u003c/p\u003e\u003chr\u003e\u003cp\u003eThe goal of model fine-tuning is to improve the model’s performance against a specific task. Over other techniques to optimize model output, such as prompt design, fine-tuning can help achieve:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigher quality results: \u003c/strong\u003eFine-tuning allows the model to learn from a much larger and more diverse dataset than can fit into a prompt. The model can learn more granular patterns and semantics that are relevant to your use case through extensive fine-tuning training. Prompts are limited in how much task-specific context they can provide, while fine-tuning teaches the model your specific domain.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eToken savings:\u003c/strong\u003e Fine-tuned models require less prompting to produce quality outputs. With fine-tuning, you can leverage a shorter, more general prompt since the model has learned your domain – saving prompt engineering effort and tokens. Whereas highly-specific prompts can often hit token limits.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLower latency: \u003c/strong\u003eHeavily engineered prompts can increase latency as they require more processing. As fine-tuned models are optimized for your specific task, they allow faster inference and can quickly retrieve knowledge for your domain.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFine-tuning is especially beneficial for adapting models to your specific use case and business needs. There are several common scenarios where fine-tuning really can help models capture the nuances required for an application:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStyle, tone, or format customization:\u003c/strong\u003e Fine-tuning allows you to adapt models to match the specific style or tone required for a use case, whether it be a particular brand voice or difference in tone for speaking to various audiences.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDesired output structure: \u003c/strong\u003eFine-tuning can teach models to follow a required structure or schema in outputs. For example, you can fine-tune a summarization model to consistently include key facts in a standardized template. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHandling edge cases: \u003c/strong\u003eReal-world data often contains irregularities and edge cases. Fine-tuning allows models to learn from a wider array of examples, including rare cases. You can fine-tune the model on new data samples so that it learns to handle edge cases when deployed to production.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn short, fine-tuning allows teams to efficiently adapt powerful models to new tasks and datasets, allowing ML teams to customize general models to their specific use cases and business needs through extensive training on curated data. High-quality fine-tuning datasets are crucial to improve performance by teaching models the nuances and complexity of the target domain more extensively than possible through prompts alone.\u003c/p\u003e\u003ch2 id=\"how-to-use-labelbox-for-fine-tuning\"\u003eHow to use Quantumworks Lab for fine-tuning\u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform for building intelligent applications. With a suite of powerful data curation, labeling, and model evaluation tools, the platform is built to help continuously improve and iterate on model performance. We will use the Quantumworks Lab platform to create a high-quality fine-tuning dataset. \u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can prepare a dataset of prompts and responses to fine-tune large language models (LLMs). Quantumworks Lab supports dataset creation for a variety of fine-tuning tasks including summarization, classification, question-answering, and generation.\u003c/p\u003e\u003ch3 id=\"step-1-evaluate-how-a-model-performs-against-the-desired-task\"\u003eStep 1: Evaluate how a model performs against the desired task\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/n4ob00h4cj\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-2-create-an-llm-data-generation-dataset-in-labelbox\"\u003eStep 2: Create an LLM data generation dataset in Quantumworks Lab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1248\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 1248w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eWhen you set up an \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003eLLM data generation project\u003c/a\u003e in Quantumworks Lab, you will be prompted to specify how you will be using the editor. You have three choices for specifying your LLM data generation workflow:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 1: Humans generate prompts and responses\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1252\" height=\"578\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 1252w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn the editor, the prompt and response fields will be required. This will indicate to your team that they should create a prompt and response from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 2: Humans generate prompts\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1260\" height=\"576\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 1260w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn the editor, only the prompt field will be required. This will indicate to your team that they should create a prompt from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 3: Humans generate responses to uploaded prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, a previously uploaded prompt will appear. Your team will need to create responses for that prompt.\u003c/p\u003e\u003cp\u003eFor our project, we'll want to create a \"Humans generate responses to uploaded prompts\" project. Namely, we want humans to create responses in the form of a list of airlines. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/irnaoi0u64\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-3-upload-data-to-vertex-ai\"\u003eStep 3: Upload data to Vertex AI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1dwwdeggyj\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eExport the Quantumworks Lab fine-tuning dataset \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve constructed a fine-tuning dataset with Quantumworks Lab, you can export it using our \u003ca href=\"https://colab.research.google.com/drive/1imCvNhd1rZNEf_dCIsPT-wLfnHkv3o_A?ref=labelbox-guides.ghost.io\"\u003eLabelbox to Vertex AI conversion script\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStart a model tuning job using Vertex AI \u0026amp; deploy the model\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAfter exporting the fine-tuned dataset, start a \u003ca href=\"https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models?ref=labelbox-guides.ghost.io\"\u003emodel tuning job using Vertex\u003c/a\u003e. When a fine-tuning job is run, the model learns additional parameters that help it encode the necessary information to perform the desired behavior or learn the desired behavior. \u003c/p\u003e\u003cp\u003eThe output of the tuning job is a new model, which is effectively a combination of the newly learned parameters and the original model. Once the fine-tuning job is complete, you can deploy the model and return to Quantumworks Lab for model evaluation.\u003c/p\u003e\u003ch3 id=\"step-4-evaluate-and-iterate-on-fine-tuning-dataset-quality\"\u003eStep 4: Evaluate and iterate on fine-tuning dataset quality\u003c/h3\u003e\u003cp\u003eA well-performing fine-tuned model indicates the effective optimization of model architecture, training data, and hyperparameters. It signifies that the training dataset used for fine-tuning is high-quality and is representative of the real-world use case. This allows for the fine-tuned model to achieve better performance on tasks compared to the base model in less time than it would have to train a model from scratch. \u003c/p\u003e\u003cp\u003eReal-world conditions and data are often dynamic. As the use case evolves, it's crucial to maintain representativeness and relevance in the fine-tuning data. Continuous evaluation of the fine-tuned model’s performance can help detect edge cases or model errors.  You can evaluate model performance and debug errors leveraging \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e. Utilize interactive auto-populated model metrics, such as a confusion matrix, precision, recall, F1 score, and more to surface model errors. Detect and visualize corner-cases where the model is underperforming and generate high-impact data to drastically improve model performance. After running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1258\" height=\"632\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 1258w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eLabelbox Model\u003c/em\u003e\u003c/i\u003e\u003c/a\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e allows teams to debug models and iteratively improve model performance\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBy iteratively identifying gaps and outdated samples in the fine-tuning data, then generating fresh high-quality data, model accuracy can be maintained over time. Updating fine-tuning datasets through this circular feedback process is crucial for adapting to new concepts and keeping models performing at a high level within continuously changing environments.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eTo improve LLM performance, Quantumworks Lab simplifies the process for subject matter experts to generate high-quality datasets for fine-tuning with leading model providers and tools, like Google Vertex AI. \u003c/p\u003e\u003cp\u003eUnlock the full potential of large language models with Quantumworks Lab’s end-to-end platform and a \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox.ghost.io\"\u003enew suite of LLM tools\u003c/a\u003e to generate high-quality training data and optimize LLMs for your most valuable AI use cases. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..\u0026ref=labelbox-guides.ghost.io\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"64fb3e9f935e200001ee0106","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299.png","featured":false,"visibility":"public","created_at":"2023-09-08T15:32:47.000+00:00","updated_at":"2024-03-26T16:25:35.000+00:00","published_at":"2023-09-08T18:03:36.000+00:00","custom_excerpt":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-vertex-ai-models-with-labelbox","tags":[{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-vertex-ai-models-with-labelbox/","excerpt":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","reading_time":6,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299-1.png","og_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","og_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299-2.png","twitter_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","twitter_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","meta_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","meta_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6478fbf1f06ecd0001d70bc5","uuid":"506ee668-bb83-4bc8-80b1-8c9b4c76e759","title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling","slug":"using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWhile Yolov8 is no longer supported on Quantumworks Lab, this blog remains relevant if you are working with other object detection models. Alternatives such as OWL-ViT, Rekognition, GroundingDINO, and GroundingDINO + SAM can still be found and used on Quantumworks Lab’s platform.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this guide, we will demonstrate the application of foundation models, such as Meta’s Segment Anything and YOLOv8, to automatically detect, classify and draw masks on objects of interest in a video. This is a follow-up to earlier guide: \u003ca href=\"https://labelbox.com/guides/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/?ref=labelbox-guides.ghost.io\"\u003eUsing Meta’s Segment Anything with YOLOv8 to Automatically Classify Masks\u003c/a\u003e. In this guide, we’ll automatically detect and segment objects in a video.\u003c/p\u003e\u003cp\u003eVideos have many frames and are tedious to label. Segmentation masks are even more time consuming to label as they vary ever so slightly frame-by-frame, requiring manual fine-tuning each time. With foundation models, you can automate and significantly speed up the labeling process to label more video data, in less time. This allows you to focus valuable time on review, simply correcting the AI models’ output.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"498\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we will be walking through a simple semantic segmentation task: drawing masks around a person as they skateboard.\u003c/p\u003e\u003cp\u003eHere’s a high-level summary of the process that we will be walking through step-by-step below, with code:\u003c/p\u003e\u003cp\u003e1) Load YOLOv8, SAM and Quantumworks Lab Python SDK\u003c/p\u003e\u003cp\u003e2) For each frame of the video:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun an object detector to generate bounding boxes with classifications for specified classes\u003c/li\u003e\u003cli\u003eFeed the bounding boxes as inputs to Meta’s Segment Anything model which will produce segmentation masks\u003c/li\u003e\u003cli\u003ePrepare mask predictions in a format that Quantumworks Lab Python SDK expects\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e3) Upload all frames at once to Quantumworks Lab via prediction import\u003c/p\u003e\u003cp\u003e4) Open up video editor and review or modify the pre-labels as you usually do\u003c/p\u003e\u003cp\u003eYou can run all of the above out-of-the-box on your video(s) using our \u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/sam/meta_sam_labelbox_video.ipynb?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e. Simply load in your video and get automatically segmented masks, with classes in Quantumworks Lab, in minutes!\u003c/p\u003e\u003cp\u003eFor this guide, we will use the following video:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-orig.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-orig.gif 600w\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-load-yolov8\"\u003eStep 1: Load YOLOv8\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.ultralytics.com/?ref=labelbox-guides.ghost.io\"\u003eYOLOv8\u003c/a\u003e is a state-of-the-art object detector that produces bounding boxes and classes around common objects. It's the latest iteration of the YOLO (You Only Look Once) family of models, and it boasts some impressive features. YOLOv8 is known for its speed and accuracy, making it an invaluable tool for a wide range of applications. Here, we use YOLOv8 to automatically detect and localize the person skateboarding in the video.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport ultralytics\nultralytics.checks()\nfrom ultralytics import YOLO\nmodel = YOLO(f'{HOME}/yolov8n.pt')\n\n# each class id is assigned a different color\ncolors = np.random.randint(0, 256, size=(len(model.names), 3))\nprint(model.names)\n\n# Specify which classes you care about. The rest of classes will be filtered out.\nchosen_class_ids = [0] # 0 refers to person, as per model.names\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-2-load-sam\"\u003eStep 2: Load SAM\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eMeta's SAM model\u003c/a\u003e is a state-of-the-art computer vision model that is designed to accurately segment images and videos into distinct objects. Using advanced deep learning techniques, Segment Anything is able to identify and segment objects in images, making it a powerful tool for a wide range of applications. The SAM model is able to generate segmentation masks based on prompts, including bounding box prompts, which we will use in the code below.\u003c/p\u003e\u003cp\u003e\u003cbr\u003eFor an in-editor experience of SAM, please read our other blog post \u003ca href=\"https://labelbox.com/blog/coming-soon-auto-segment-powered-by-sam/?ref=labelbox-guides.ghost.io\"\u003eAuto-Segment 2.0 powered by Meta’s Segment Anything Model\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport torch\nimport matplotlib.pyplot as plt\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nsam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\nmask_predictor = SamPredictor(sam)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-3-load-labelboxs-python-sdk\"\u003eStep 3: Load Quantumworks Lab's Python SDK\u003c/h2\u003e\u003cp\u003eLabelbox’s Python SDK gives you easy methods to create ontologies, projects and datasets, and upload masks to a video.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab as lb\nimport labelbox.types as lb_types\n\n# Create a Quantumworks Lab API key for your account by following the instructions here:\n# https://docs.labelbox.com/reference/create-api-key\n# Then, fill it in here\nAPI_KEY = \"\"\nclient = lb.Client(API_KEY)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-4-run-yolov8-and-sam-per-frame\"\u003eStep 4: Run YOLOv8 and SAM per-frame\u003c/h2\u003e\u003cp\u003eHere we run the models on each frame and generate masks automatically.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003ecap = cv2.VideoCapture(VIDEO_PATH)\n\n# This will contain the resulting mask predictions for upload to Quantumworks Lab\nmask_frames = []\n\nframe_num = 1\nwhile cap.isOpened():\n  ret, frame = cap.read()\n  if not ret:\n    break\n\n  # Run frame through YOLOv8 to get detections\n  detections = model.predict(frame, conf=0.7)\n \n  # Run frame and detections through SAM to get masks\n  transformed_boxes = mask_predictor.transform.apply_boxes_torch(detections[0].boxes.xyxy, list(get_video_dimensions(cap)))\n  mask_predictor.set_image(frame)\n  masks, scores, logits = mask_predictor.predict_torch(\n    boxes = transformed_boxes,\n    multimask_output=False,\n    point_coords=None,\n    point_labels=None\n  )\n\n  # Combine mask predictions into a single mask, each with a different color\n  class_ids = detections[0].boxes.cpu().cls\n  merged_with_colors = add_color_to_mask(masks[0][0], colors[int(class_ids[0])]).astype(np.uint8)\n  for i in range(1, len(masks)):\n    curr_mask_with_colors = add_color_to_mask(masks[i][0], colors[int(class_ids[i])])\n    merged_with_colors = np.bitwise_or(merged_with_colors, curr_mask_with_colors)\n\n  # Upload multi-colored combined mask to temp location\n  # to get temp instance uri\n  instance_uri = get_instance_uri(client, global_key, merged_colored_mask)\n\n  # Create MaskFrame object to be uploaded to Quantumworks Lab\n  mask_frame = lb_types.MaskFrame(index=frame_num, instance_uri=instance_uri)\n  mask_frames.append(mask_frame)\n\n  frame_num += 1\n\ncap.release()\n\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-bboxes.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-bboxes.gif 600w\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-masks.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-masks.gif 600w\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-5-upload-the-predicted-masks-as-pre-labels-onto-labelbox\"\u003eStep 5: Upload the predicted masks as pre-labels onto Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThe predicted masks can be easily and seamlessly integrated into Quantumworks Lab via our SDK.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Create MaskInstance per unique class predicted / chosen\ninstances = []\n for cid in chosen_class_ids:\n   color = get_color(colors[int(cid)])\n   name = model.names[int(cid)]\n   instances.append(lb_types.MaskInstance(color_rgb=color, name=name))\n\n# Create list of VideoMaskAnnotation objects, one for each unique class\nannotations = []\nfor instance in instances:\n  video_mask_annotation = lb_types.VideoMaskAnnotation(\n       frames=mask_frames,\n       instances=[instance]\n   )\n  annotations.append(video_mask_annotation)\n\n# Create Label object\nlabels = [\nlb_types.Label(data=lb_types.VideoData(global_key=global_key),\n                  annotations=annotations))\n]\n\n# Run import job\nupload_job = lb.MALPredictionImport.create_from_objects(\n   client=client,\n   project_id=project.uid,\n   name=\"mal_import_job\" + str(uuid.uuid4()),\n   predictions=labels\n)\nupload_job.wait_until_done()\n\nprint(f\"Errors: {upload_job.errors}\", )\nprint(f\"Status of uploads: {upload_job.statuses}\")\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-LB.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"374\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-LB.gif 600w\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eCreating segmentation masks on video data can be tedious and time-consuming. Using the power of foundation models in Quantumworks Lab, you can easily generate masks with classifications in a matter of minutes. Rather than spending hours labeling video data, you now have a way to accelerate video labeling and not only reduce time to market, but also the cost of developing your models.\u003c/p\u003e","comment_id":"6478fbf1f06ecd0001d70bc5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074.png","featured":false,"visibility":"public","created_at":"2023-06-01T20:13:37.000+00:00","updated_at":"2024-11-25T21:18:27.000+00:00","published_at":"2023-06-01T22:14:48.000+00:00","custom_excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/","excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074-1.png","og_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","og_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074-2.png","twitter_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","twitter_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","meta_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","meta_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"645a7494225a9a00012fcfae","uuid":"e07975bd-27c6-4c00-bf8c-bc96c772ce56","title":"How to accelerate image-text pair generation with BLIP-2","slug":"how-to-accelerate-image-text-pair-generation-with-blip-2","html":"\u003cp\u003e\u003ca href=\"https://labelbox.com/solutions/generative-ai/?ref=labelbox-guides.ghost.io\"\u003eGenerative AI\u003c/a\u003e has taken the world by storm, opening doors to a plethora of applications, from creating realistic images and videos to generating novel text and music. The success of these applications often hinges on the \u003ca href=\"https://labelbox.com/blog/data-quality-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003equality and quantity of data used to train the underlying machine learning models\u003c/a\u003e, the production of which is often time consuming and costly. As a result, leading AI teams have been innovating on ways to streamline the caption creation process and empower human annotators to work more efficiently without sacrificing quality.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2301.12597?ref=labelbox-guides.ghost.io\"\u003eBLIP-2\u003c/a\u003e (Bootstrapping Language-Image Pre-training) is an AI model that can perform various multi-modal tasks like visual question answering, image-text retrieval (image-text matching) and image captioning. It can analyze an image, understand its content, and generate a relevant and concise caption. BLIP-2 helps language models understand images without changing their original structure. It does this by using querying transformer (q-former) that acts as a bridge between the image and the language model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"527\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSee the original flowchart as published in the \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2301.12597.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBLIP-2 research paper\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBLIP-2 achieves state-of-the-art performance on various vision-language tasks while being more compute efficient than existing methods. Powered by \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox-guides.ghost.io\"\u003eLarge Language Models (LLMs)\u003c/a\u003e, it can perform \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot image-to-text generation\u003c/a\u003e based on natural language instructions, enabling capabilities like visual knowledge reasoning and visual conversation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"898\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExamples of images and their BLIP 2 captions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, we'll explore how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions. Additionally, you can use any model to make pre-labels in Quantumworks Lab as shown \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e. Quantumworks Lab customers using \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003emodel-assisted labeling\u003c/a\u003e have seen 50-70% reductions in labeling costs driven by dramatic reductions in labeling time and complexity. Therefore, using a model like BLIP-2 will further reduce labeling time.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/BLIP-2-lucidchart--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1340\" height=\"420\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/BLIP-2-lucidchart--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/BLIP-2-lucidchart--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/BLIP-2-lucidchart--1-.png 1340w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHow to use BLIP-2 with Quantumworks Lab\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"step-1-create-a-project-and-attach-an-ontology\"\u003eStep 1: Create a project and attach an ontology.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eproject = client.create_project(name = \"BLIP project\", media_type=labelbox.MediaType.Image)\nproject.setup_editor(ontology)\nontology_from_project = labelbox.OntologyBuilder.from_project(project)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-initialize-and-load-a-pre-trained-blip-2-model\"\u003eStep 2: Initialize and load a pre-trained BLIP-2 model.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003edevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n   \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n)\nmodel.to(device)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-3-collect-inferences-to-be-used-as-pre-labels\"\u003eStep 3: Collect inferences to be used as pre-labels.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003equeued_data_rows = project.export_queued_data_rows()\nground_truth_list = list()\n\n\nfor data_row in queued_data_rows:\n url = data_row[\"rowData\"]\n image = Image.open(requests.get(url, stream=True).raw)\n inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n generated_ids = model.generate(**inputs, max_new_tokens=30)\n generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n print(generated_text) \n  text_annotation = labelbox.data.annotation_types.ClassificationAnnotation(\n     name=\"BLIP model prediction\",\n     value=labelbox.data.annotation_types.Text(answer = generated_text)\n   )\n  ground_truth_list.append(Label(\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-4-upload-pre-labels-to-your-project\"\u003eStep 4: Upload pre-labels to your project.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eupload_task = labelbox.MALPredictionImport.create_from_objects(client, project.uid, str(uuid.uuid4()), ground_truth_list)\nupload_task.wait_until_done()\nprint(upload_task.errors)\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1132\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of asset with a pre-label and no human-generated caption\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/unknown.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"893\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/unknown.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/unknown.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/unknown.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of an asset with a pre-label and a human-generated caption\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"review-labels\"\u003eReview labels\u003c/h2\u003e\u003cp\u003eAfter the labels have been annotated, you can use \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e to create highly customizable, multi-step review pipelines, making your review process more efficient and automated. Workflows offer granular control over how your data rows get reviewed, saving you both time and resources. You can create tasks that enable you to filter based on who created the label, what annotations exist and when the label was created.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/unknown--1--2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"804\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/unknown--1--2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/unknown--1--2.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/unknown--1--2.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLabelbox lets you customize labeling and review workflows to your exact requirements.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"export-labels\"\u003eExport labels\u003c/h2\u003e\u003cp\u003eAfter you are done reviewing the labels, you can easily export the annotations as show \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-JSON\"\u003e         \"annotations\": {\n             \"objects\": [],\n             \"classifications\": [\n               {\n                 \"feature_id\": \"clhdn79ae0ent076c4h579rxu\",\n                 \"name\": \"BLIP model prediction\",\n                 \"text_answer\": {\n                   \"content\": \"a yellow flower with a green background\"\n                 }\n               }\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eBy using captions generated by BLIP-2 or inferences by AI models as pre-labels, AI teams can significantly reduce labeling time and costs. To learn more, explore this \u003ca href=\"https://colab.research.google.com/drive/1vnD4gVBu8uAE3dn44h8qIXx3R_DYv5E2?ref=labelbox-guides.ghost.io#scrollTo=PcfsMaUu1GrV\"\u003efull script\u003c/a\u003e for using the BLIP-2 model to generate pre-labels. These captions can then be amended or approved in Quantumworks Lab by labelers. You can also learn more about the BLIP-2 model \u003ca href=\"https://huggingface.co/docs/transformers/main/model_doc/blip-2?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e and model-assisted labeling \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"645a7494225a9a00012fcfae","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Guide_BLIP-2.png","featured":false,"visibility":"public","created_at":"2023-05-09T16:28:04.000+00:00","updated_at":"2023-10-27T17:11:22.000+00:00","published_at":"2023-05-09T17:09:47.000+00:00","custom_excerpt":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-accelerate-image-text-pair-generation-with-blip-2/","tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-accelerate-image-text-pair-generation-with-blip-2/","excerpt":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"How to accelerate image-text pair generation with BLIP-2","og_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","twitter_image":null,"twitter_title":"How to accelerate image-text pair generation with BLIP-2","twitter_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","meta_title":"How to accelerate image-text pair generation with BLIP-2 | Quantumworks Lab","meta_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so a specialized workforce can further improve the image captions.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64590b32638f810001454877","uuid":"25f18d3c-fa4c-4206-a4c2-db2f9c6ba390","title":"Automatically label text with 96%+ accuracy using foundation models","slug":"automatically-label-text-with-96-accuracy-using-foundation-models","html":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, you’ll learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab and foundation models. We will be walking through a \u003ca href=\"https://labelbox.com/product/annotate/text/?ref=labelbox-guides.ghost.io\"\u003etext classification\u003c/a\u003e task: identifying news articles that talk about sports. \u003c/p\u003e\u003cp\u003eHere's a high-level summary of the process:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConnect your data to Quantumworks Lab with just a few lines of code.\u003c/li\u003e\u003cli\u003eLabelbox leverages foundation models to automatically enrich your data.\u003c/li\u003e\u003cli\u003eUse the powerful search capabilities of Quantumworks Lab to quickly find articles with similar traits and classify them in one click. With the help of foundation models, you can instantaneously label large amounts of data. \u003cem\u003ePro tip: Combine a variety of search techniques, such as a similarity search, natural language search, keyword search, and investigate clusters of similar data, to boost your results.\u003c/em\u003e\u003c/li\u003e\u003cli\u003eWhile foundation models are a helpful starting point, they might not always correctly classify data, especially on challenging or rare data points. In this case, utilize human-in-the-loop labeling and QA by pre-labeling data using foundation models and sending it for your internal or external labeling team to review.\u003c/li\u003e\u003cli\u003eAutomatically apply these rules to all new incoming data by creating a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e in Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, let’s take a look at how we can do the above in Labelbox. As a sneak peek into the process, by leveraging foundation models, we managed to \u003cstrong\u003eclassify 88% of our news articles in minutes with a 96.5% accuracy rate\u003c/strong\u003e. An additional 15% of our news articles were successfully pre-labeled using foundation models, with 85% accuracy, and sent for human review. This left us with only 493 data points that were missed by foundation models – a massive efficiency gain for any labeling team.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"step-1-connect-your-data-to-labelbox-with-a-few-lines-of-code\"\u003eStep 1: Connect your data to Quantumworks Lab with a few lines of code\u003c/h2\u003e\u003cp\u003eSince this is a classification task, our goal is to correctly have the model identify news articles about sports. We will be using the following Hugging Face dataset: \u003ca href=\"https://huggingface.co/datasets/ag_news?ref=labelbox-guides.ghost.io\"\u003eag_news\u003c/a\u003e which contains 120,000 articles, including 30,000 about sports, for our analysis.\u003c/p\u003e\u003cp\u003eTo begin, let's connect this data to Labelbox. Simply retrieve the dataset from Hugging Face and integrate it with Quantumworks Lab in just a few lines of code.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom datasets import load_dataset\ndataset = load_dataset(\"ag_news\",split='train')\n\n# iterate over the data\npayload_imgs = []\ncounter = 0\n\n\n# iterate over the data\npayloads = []\nglobal_keys = []\ncounter = 0\n\nfor data in dataset:\n\n  text = data['text']\n  label = data['label']\n  global_key = \"ag_news_\" + str(counter)\n  global_keys.append(global_key)\n\n  # create payload for texts\n  payloads.append({\n    \"row_data\": text, \n    \"global_key\": global_key,\n  })\n\n  counter += 1\n\n\n# create dataset in Quantumworks Lab\nlb_dataset = client.create_dataset(name=\"ag_news\") \n\n# add data in Quantumworks Lab\ntask = lb_dataset.create_data_rows(payloads)  \ntask.wait_till_done() # async\nprint(task.errors) # check errors \u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-leverage-foundation-models-to-instantly-enhance-your-data\"\u003eStep 2: Leverage foundation models to instantly enhance your data\u003c/h2\u003e\u003cp\u003eLabelbox will automatically compute and store MPNet embeddings for your data. We are using \u003ca href=\"https://huggingface.co/sentence-transformers/all-mpnet-base-v2?ref=labelbox-guides.ghost.io\"\u003ethis implementation\u003c/a\u003e available through Hugging Face. \u003c/p\u003e\u003cp\u003eOnce your data has been uploaded, watch as Quantumworks Lab enriches your data with foundation model embeddings. These embeddings are powerful in that they can be harnessed to automatically label, or pre-label, your data.  \u003c/p\u003e\u003cp\u003eIf you don’t want to use the default embeddings by Quantumworks Lab, you can also \u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003eupload custom embeddings\u003c/a\u003e from any other foundation model, with up to 100 custom embeddings for each data point. \u003c/p\u003e\u003cp\u003eWhether you’re using the default or custom embeddings, embeddings are helpful in curating and finding subsets of data that share similar characteristics. For instance, embeddings power \u003ca href=\"https://labelbox.com/blog/how-vector-similarity-search-works/?ref=labelbox-guides.ghost.io\"\u003eLabelbox’s similarity search\u003c/a\u003e, natural language search, and 2D projector view. You can search and explore all of your data with tools that help you powerfully surface specific subsets of similar texts.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"927\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-3-use-powerful-search-capabilities-to-quickly-find-data\"\u003eStep 3: Use powerful search capabilities to quickly find data\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities in Quantumworks Lab, you can easily find and classify data that share similar characteristics. This is a special case of \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot and few-shot learning\u003c/a\u003e: the challenge is to find all examples of sports articles, based on zero (or a few) examples. With the help of foundation models, and minimal human signal, you can quickly label a lot of data in just a few clicks. The following are tools in Quantumworks Lab that help provide labeling signal to make it easy to automatically classify your data:\u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-projector-view-for-classification\"\u003eZero-shot Labeling: Projector View for Classification\u003c/h3\u003e\u003cp\u003eLabelbox allows you to visualize data clusters in 2D. For this example, we can see distinct clusters. By inspecting a few examples, we discover that some of the data clusters correspond to sports news articles. We manually select each cluster and tag it with \"UMAP: sports. We intentionally leave out data points situated between clusters, as these represent challenging data points. This is expected since each labeling function won’t be perfect in isolation, and some data points are difficult and challenging.\u003c/p\u003e\u003cp\u003eWe then repeat the process with t-SNE instead of UMAP and tag each sports cluster with \"t-SNE: sports\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"569\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUMAP: a cluster of news articles about sports\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"705\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003et-SNE: a subcluster of news articles about basketball\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"zero-shot-labeling-keyword-search\"\u003eZero-shot Labeling: Keyword search\u003c/h3\u003e\u003cp\u003eLabelbox enables you to search all data points that contain some keywords. We filter all data points that contain the following keywords\u003cem\u003e: sport, sports, basketball, baseball, soccer, football, tennis, hockey\u003c/em\u003e. And tag these 5,990 texts as “Keyword search: sports”. \u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-natural-language-search-for-classification\"\u003eZero-shot Labeling: Natural language search for classification\u003c/h3\u003e\u003cp\u003eLabelbox enables you to conduct natural language searches on text. For example you can type in “news articles about sports” to surface all pieces of text about sports. Adjusting the similarity threshold will narrow the search to only relevant articles. For this use case, we filter for a similarity score higher than 0.85 and tag all of the 6,468 texts as “Natural language search: sports”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"922\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA natural language search will surface thousands of news articles about sports. By adjusting the similarity score, we can keep the most confident zero-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"few-shot-labeling-similarity-search-for-classification\"\u003eFew-shot Labeling: Similarity search for classification\u003c/h3\u003e\u003cp\u003eLabelbox also streamlines few-shot labeling. Quickly browse all your data in Catalog to surface 5 news articles about sports. For each of them, run a similarity search and tag the top results (e.g with a similarity score higher than 0.85) as “Similarity search: sports”. This provides us with 5 new labeling functions that surface sports news articles.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"967\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA similarity search example with an anchor article about college basketball. We can filter to keep the most confident few-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"combining-different-sources-of-signal-weak-labeling\"\u003eCombining different sources of signal: weak labeling\u003c/h3\u003e\u003cp\u003eWhile each of these labeling signals is powerful on its own, you can combine multiple sources in Labelbox. This allows you to apply simple rules in a weak supervision fashion to further enhance your results. Integrate different labeling signals, such as similarity searches, natural language searches, and data clusters, to boost your outcomes. You can \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003ecombine various filters\u003c/a\u003e by using the AND and OR functions.\u003c/p\u003e\u003ch2 id=\"step-4-automatically-classify-data-with-foundation-models-and-use-human-in-the-loop-qa-for-challenging-cases\"\u003eStep 4: Automatically classify data with foundation models and use human-in-the-loop QA for challenging cases\u003c/h2\u003e\u003ch3 id=\"high-confidence-data-points-direct-classification\"\u003eHigh confidence data points: direct classification\u003c/h3\u003e\u003cp\u003eFoundation models are highly confident about most data points. So much so that we can directly classify data points leveraging Quantumworks Lab’s \u003ca href=\"https://labelbox.com/blog/pre-label-and-enrich-your-data-with-bulk-classifications/?ref=labelbox-guides.ghost.io\"\u003ebulk classification feature\u003c/a\u003e. With this new feature, you can specify and send your texts to a specific step of the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox.ghost.io#how-it-works\"\u003elabeling and review workflow\u003c/a\u003e. We can directly move these high-confidence data points straight to the “Done” task.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe classify thousands of texts in bulk, and send them to the “Done” task of our labeling project, in just a click, since foundation models are confident on those.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these high-confident data points are those that belong to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe sports clusters, both with UMAP and t-SNE. But just how accurate are these predictions? To answer this question, we looked at the Hugging Face ground truths. 704 out of the 21,560 sports predictions are incorrect.\u003c/li\u003e\u003cli\u003eOr, the similarity search score to two or more anchors is higher than 0.85. This results in 3,548 sports classifications, all of which are accurate except 185.\u003c/li\u003e\u003cli\u003eThe sports cluster in UMAP or t-SNE and a natural language search higher than 0.85. This results in 1,219 sports classifications, all of which are accurate except 58. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis method of surfacing high-confident data points enables us to directly classify 26,427 pieces of text - with only 947 errors - achieving an \u003cstrong\u003eaccuracy of 96.5%.\u003c/strong\u003e Since 26,427 out of 30,000 sports articles have been classified directly by foundation models, the \u003cstrong\u003ecoverage is 88%.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhat about the 947 errors? Upon closer inspection of these errors, it turns out that they are all related to sports, but in the context of News, World, or Science, and hence have been labeled on Hugging Face according to those categories instead of Sports.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"952\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFoundation models failed on 947 news articles. It turns out that these articles are all related to sports, but are classified on HuggingFace as World news, or Business news, or Science \u0026amp; Tech news.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eNow, let’s move on to classify the remaining 12% of data rows, on which foundation models are less confident.\u003c/p\u003e\u003ch3 id=\"low-confidence-data-points-human-in-the-loop-labeling\"\u003eLow confidence data points: Human-in-the-Loop labeling\u003c/h3\u003e\u003cp\u003eFor some pieces of text, foundation models exhibit low confidence. We can bulk classify these data points in Quantumworks Lab, but move them to the “To Review” task. This will ensure a human is looped in and will review the classifications coming from foundation models.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe pre-label thousands of data points in bulk, and send them to “Review” in our labeling project, in just a click, since foundation models are moderately confident on these pieces of text.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these data points are those that belong to the sports cluster, with UMAP or t-SNE, and that haven’t been classified yet.\u003c/p\u003e\u003cp\u003eUsing this approach, we managed to classify 4,723 additional data rows, with an \u003cstrong\u003eaccuracy of 85%\u003c/strong\u003e (696 errors). We can send these low-confident data rows for Human-in-the-Loop review. \u003c/p\u003e\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eDirect classification with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eHuman in the loop with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eSports articles missed by foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of data rows classified\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e26,427\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e4,723\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e493\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of errors\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e947\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e696\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e-\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAccuracy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e96.5%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e85%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e-\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eFraction of sports articles\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e88%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e15%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e1.8%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 26,427 pieces of text (88%) in minutes, with \u003cstrong\u003e96.5% accuracy\u003c/strong\u003e thanks to foundation models. An additional 4,723 data points (13.5%) have been pre-labeled with foundation models, with \u003cstrong\u003e85% accuracy\u003c/strong\u003e, and sent for human review. This leaves only 493 data points talking about sports, missed by foundation models.\u003c/p\u003e\u003ch2 id=\"step-5-set-it-and-forget-it-%E2%80%93-automatically-apply-these-rules-to-fresh-incoming-data\"\u003eStep 5: Set it and forget it – automatically apply these rules to fresh, incoming data\u003c/h2\u003e\u003cp\u003eWith Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e, we automatically classify fresh, incoming news articles about sports. \u003c/p\u003e\u003cp\u003eFor example, we can set up a slice that automatically surfaces all new pieces of text, that have been connected to Quantumworks Lab in the past week, that haven’t been classified yet. We can set the slice's criteria to include only text data rows where the natural language search for the prompt is \"news articles about sports\" and is higher than 0.85 (since we know that these data rows are very likely to be on sports). \u003c/p\u003e\u003cp\u003eWith slices, you can easily surface and inspect any new, high-impact data that gets added to your data lake.\u003c/p\u003e\u003cp\u003eFrom there, it only takes one click to classify all of these text data rows as sports articles. \u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 26,427 pieces of text (88%) in minutes, with \u003cstrong\u003e96.5% accuracy\u003c/strong\u003e thanks to foundation models. An additional 4,723 data points (15%) have been pre-labeled with foundation models, with \u003cstrong\u003e85% accuracy\u003c/strong\u003e, and sent for human review. This leaves only 493 data points talking about sports, missed by foundation models.\u003c/p\u003e\u003chr\u003e\u003cp\u003eIf you’re a current Quantumworks Lab user who wants to leverage any foundation model to supercharge your data labeling process in just a few clicks,\u003ca href=\"https://app.labelbox.com/catalog?ref=labelbox-guides.ghost.io\"\u003e try our bulk classification feature today\u003c/a\u003e or \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started with a free Quantumworks Lab account\u003c/a\u003e.\u003cbr\u003eIf you’re interested in seeing how quickly you can label images leveraging foundation models, check out our guide on \u003ca href=\"https://labelbox.com/guides/automatically-label-images-with-99-accuracy-using-foundation-models/?ref=labelbox-guides.ghost.io\"\u003ehow to automatically label images with 99% accuracy.\u003c/a\u003e\u003c/p\u003e","comment_id":"64590b32638f810001454877","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1-.png","featured":false,"visibility":"public","created_at":"2023-05-08T14:46:10.000+00:00","updated_at":"2023-10-27T17:11:48.000+00:00","published_at":"2023-05-08T20:43:24.000+00:00","custom_excerpt":"Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/automatically-label-text-with-96-accuracy-using-foundation-models","tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/automatically-label-text-with-96-accuracy-using-foundation-models/","excerpt":"Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","reading_time":9,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1--2.png","og_title":"Automatically label text with 96%+ accuracy using foundation models","og_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1--1.png","twitter_title":"Automatically label text with 96%+ accuracy using foundation models","twitter_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","meta_title":"Automatically label text with 96%+ accuracy using foundation models","meta_description":"Automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","email_subject":null,"frontmatter":null,"feature_image_alt":"Automatically label text with 96%+ accuracy using foundation models","feature_image_caption":null},{"id":"6452689090e75e0001b3546c","uuid":"5a55750c-5b23-4ce7-b075-117a24a4f392","title":"Automatically label images with 99% accuracy using foundation models","slug":"automatically-label-images-with-99-accuracy-using-foundation-models","html":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, you’ll learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab and foundation models. We will be walking through a sample \u003ca href=\"https://labelbox.com/guides/image-annotation/?ref=labelbox-guides.ghost.io\"\u003eimage classification task\u003c/a\u003e: figuring out if images contain cats or dogs.\u003c/p\u003e\u003cp\u003eHere's a high-level summary of the process that we'll be walking through step-by-step below:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConnect your data to Quantumworks Lab with just a few lines of code.\u003c/li\u003e\u003cli\u003eLabelbox leverages foundation models to magically enrich your data.\u003c/li\u003e\u003cli\u003eUse the powerful search capabilities of Quantumworks Lab to quickly find data with similar traits and classify them in one click. With the help of foundation models, you can instantaneously label large amounts of data. \u003cem\u003ePro tip: Combine a variety of search techniques, such as a similarity search, natural language search, and investigate clusters of similar data, to boost your results.\u003c/em\u003e\u003c/li\u003e\u003cli\u003eWhile foundation models are a helpful starting point, they might not always correctly classify data, especially on challenging or rare data points. In this case, utilize human-in-the-loop labeling and QA by pre-labeling data using foundation models and sending it for your internal or external labeling team to review.\u003c/li\u003e\u003cli\u003eAutomatically apply these rules to all new incoming data by creating a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e in Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, let’s take a look at how we can do the above in Labelbox. As a sneak peek into the process, by leveraging foundation models, we managed to \u003cstrong\u003eclassify 86% of our images in minutes with a 99.9% accuracy rate\u003c/strong\u003e. An additional 13.5% of our images were successfully pre-labeled using foundation models, with 98% accuracy, and were sent for human review. This left us with less than 0.5% of images to manually label – a massive efficiency gain for any labeling team.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"step-1-connect-your-data-to-labelbox-with-a-few-lines-of-code\"\u003eStep 1: Connect your data to Quantumworks Lab with a few lines of code\u003c/h2\u003e\u003cp\u003eSince this is a classification task, our goal is to correctly have the model identify cats and dogs in images. We will be using the following Hugging Face dataset - \u003ca href=\"https://huggingface.co/datasets/cats_vs_dogs?ref=labelbox-guides.ghost.io\"\u003ecats_vs_dogs\u003c/a\u003e, containing 18,699 images for our analysis.\u003c/p\u003e\u003cp\u003eTo begin, let's connect this data to Labelbox. Simply retrieve the dataset from Hugging Face and integrate it with Quantumworks Lab in just a few lines of code.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom datasets import load_dataset\ndataset = load_dataset(\"cats_vs_dogs\",split='train')\n\n# iterate over the data\npayload_imgs = []\ncounter = 0\n\nfor data in dataset:\n  image = data['image']\n  label = data['labels'] # 0 is cat, 1 is dog\n  global_key = \"cat_vs_dog_\" + str(counter)\n\n  # save image locally\n  path = \"/content/images/\"+global_key+\".jpg\"\n  image.save(path) \n\n  # create payload for images\n  payload_imgs.append({\"row_data\": path, \"global_key\": global_key})\n  counter += 1\n\n# create dataset in Quantumworks Lab\nlb_dataset = client.create_dataset(name=\"Cat_vs_dog\") \n\n\n# add data in Quantumworks Lab\ntask = lb_dataset.create_data_rows(payload_imgs[i:i+1000])  task.wait_till_done() # async\nprint(task.errors) # check errors \u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-leverage-foundation-models-to-instantly-enhance-your-data\"\u003eStep 2: Leverage foundation models to instantly enhance your data\u003c/h2\u003e\u003cp\u003eLabelbox will automatically compute and store CLIP embeddings for your data. Our CLIP model leverages \u003ca href=\"https://openai.com/research/clip?ref=labelbox-guides.ghost.io\"\u003eOpenAI\u003c/a\u003e and we are using \u003ca href=\"https://huggingface.co/sentence-transformers/clip-ViT-B-32?ref=labelbox-guides.ghost.io\"\u003ethis implementation\u003c/a\u003e available through Hugging Face. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"922\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce your data has been uploaded, you can enrich your data with foundation model embeddings. These embeddings are powerful in that they can be harnessed to automatically label, or pre-label, your data.  \u003c/p\u003e\u003cp\u003eIf you don’t want to use the default embeddings by Quantumworks Lab, you can also \u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003eupload custom embeddings\u003c/a\u003e from any other foundation model, with up to 100 custom embeddings for each data point. \u003c/p\u003e\u003cp\u003eWhether you’re using the default or custom embeddings, embeddings are helpful in curating and finding subsets of data that share similar characteristics. For instance, embeddings power Quantumworks Lab’s \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003esimilarity search\u003c/a\u003e, natural language search, and 2D projector view. You can search and explore all of your data with tools that help you powerfully surface specific subsets of data.\u003c/p\u003e\u003ch2 id=\"step-3-use-powerful-search-capabilities-to-quickly-find-data\"\u003eStep 3: Use powerful search capabilities to quickly find data\u003c/h2\u003e\u003cp\u003eWith \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003epowerful search capabilities\u003c/a\u003e in Quantumworks Lab, you can easily find and classify data that share similar characteristics. This is a special case of \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot and few-shot learning\u003c/a\u003e: the challenge is to find all examples of cats and dogs, based on zero (or a few) examples from each class. \u003c/p\u003e\u003cp\u003eWith the help of foundation models, and minimal human signal, you can quickly label a lot of data in just a few clicks. The following are tools in Quantumworks Lab that help provide labeling signal to make it easy to automatically classify your data:\u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-projector-view-for-classification\"\u003eZero-shot Labeling: Projector View for classification\u003c/h3\u003e\u003cp\u003eLabelbox allows you to visualize data clusters in 2D. For this example, we can see two distinct clusters: one for cats and another for dogs. By inspecting a few examples, we can ensure the data clustering is accurate. We then manually select each cluster and tag it with \"UMAP: cats: high confidence\" and \"UMAP: dogs: high confidence\". We intentionally left out data points situated between clusters, as these represent challenging data points. This is expected since each labeling function won’t be perfect in isolation and some data points are difficult and challenging.\u003c/p\u003e\u003cp\u003eWe then repeat the process with t-SNE, instead of UMAP, and tag each cluster with \"t-SNE: cats: high confidence\" and \"t-SNE: dogs: high confidence\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"924\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUMAP: a cluster of cats with high confidence\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"923\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUMAP: a sub-cluster of images with two cats\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003et-SNE: the same sub-cluster of images with two cats\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"zero-shot-labeling-natural-language-search-for-classification\"\u003eZero-shot Labeling: Natural language search for classification\u003c/h3\u003e\u003cp\u003eLabelbox enables you to conduct \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003enatural language searches\u003c/a\u003e, for example you can type in “photos of cats” to surface all cat images. Adjusting the similarity threshold will narrow the search parameters to show only the images that contain cats. For this use case, we can filter for a similarity score higher than 0.61 and tag all of the 7,125 images as “Natural language search: Cats (high confidence)”. If we adjust the similarity score to be between 0.6 and 0.61, we can tag the 1,299 images as “Natural language search: Cats (low confidence)”. \u003c/p\u003e\u003cp\u003eWe take the same approach for images containing dogs. Using the same technique above, we tag 7,738 images of dogs as “Natural language search: Dogs (high confidence)” and surface and tag 2,750 images as “Natural language search: Dogs (low confidence)”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA natural language search will surface thousands of images of cats. By adjusting the similarity score, we can keep the most confident zero-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"few-shot-labeling-similarity-search-for-classification\"\u003eFew-shot Labeling: Similarity search for classification\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eLabelbox also streamlines few-shot labeling. Quickly browse all your data in Catalog to surface 5 images of cats and 5 images of dogs. Perform a similarity search in one click using these 10 images as anchors. For each anchor image, run a similarity search and tag the top results (e.g  with a similarity score of higher than 0.895) as “Labeling function: similarity search (cats: high confidence)”. This provides us with 10 new labeling functions that surface images similar to the anchor images.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"925\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA similarity search example with anchor images of dogs. We can filter to keep the most confident few-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"weak-labeling-combining-different-sources-of-signal\"\u003eWeak Labeling: Combining different sources of signal\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eWhile each of these labeling signals is powerful on its own, you can combine multiple sources in Labelbox. This allows you to apply simple rules in a weak supervision fashion to further enhance your results. Integrate different labeling signals, such as similarity searches, natural language searches, and data clusters, to boost your outcomes. You can \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003ecombine various filters\u003c/a\u003e by using the AND and OR functions.\u003c/p\u003e\u003ch2 id=\"step-4-automatically-classify-data-with-foundation-models-and-use-human-in-the-loop-qa-for-challenging-cases\"\u003eStep 4: Automatically classify data with foundation models and use human-in-the-loop QA for challenging cases\u003c/h2\u003e\u003ch3 id=\"high-confidence-data-points-direct-classification\"\u003eHigh confidence data points: direct classification\u003c/h3\u003e\u003cp\u003eFoundation models are highly confident about most data points. So much so that we can directly classify data points leveraging Quantumworks Lab’s \u003ca href=\"https://labelbox.com/blog/pre-label-and-enrich-your-data-with-bulk-classifications/?ref=labelbox-guides.ghost.io\"\u003ebulk classification feature\u003c/a\u003e. With this new feature, you can specify and send your data rows to a specific step of the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox.ghost.io#how-it-works\"\u003elabeling and review workflow\u003c/a\u003e. We can directly move these high-confidence data points straight to the “Done” task.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"927\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe can classify thousands of data points in bulk, and send them to the “Done” task of our labeling project, in just a click, since foundation models are confident on those.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these high-confident data points are those that belong to the cat or dog cluster, both with UMAP and t-SNE, and where the natural language score is higher than 0.61. This results in 7,627 dog classifications. But just how accurate are these classification predictions? \u003c/p\u003e\u003cp\u003eTo answer this question, we looked at the Hugging Face ground truths. On the surface, 10 out of the 7,627 dog predictions are incorrect (0.13%). However, upon closer inspection, it turns out that the Hugging Face dataset contains a few labeling mistakes and only 6 out of the 7,627 predictions (0.078%) of the foundation model’s predictions are actually incorrect. Similarly, there were 6,587 cat classifications. Only 6 out of the 6,587 cat predictions (0.09%) of the foundation model’s predictions are incorrect. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"761\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOut of 7,627 images, foundation models failed on these 10 by predicting dogs instead of cats.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eHigh-confident data points are also those that are found with a similarity search proximity to 2 or more anchors with a 0.895 or higher score. There were 907 dog classifications that fit this criteria, all of which were accurate except 1, and 1,022 cat classifications, all of which were accurate except 6. \u003c/p\u003e\u003cp\u003eBy leveraging the above methods, we were able to classify 16,143 data rows - with only 19 errors - achieving an \u003cstrong\u003eaccuracy of 99.9%. \u003c/strong\u003eSince 16,143 out of 18,699 data rows have been classified directly by foundation models, the \u003cstrong\u003ecoverage is 86%.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eNow, let’s move on to classify the remaining 14% of data rows, on which the foundation model appears to be less confident.\u003c/p\u003e\u003ch3 id=\"medium-confidence-data-points-human-in-the-loop-labeling\"\u003eMedium confidence data points: Human-in-the-Loop labeling\u003c/h3\u003e\u003cp\u003eFor some data points, foundation models exhibit moderate confidence. We can bulk classify these data points in Quantumworks Lab, but move them to the “To Review” task. This will ensure a human is looped in and will review the classifications coming from foundation models.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"922\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe can pre-label thousands of data points in bulk and send them to “Review” in our labeling project, since foundation models are moderately confident on these images.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these data points are those that belong to the cat or dog cluster, with UMAP or t-SNE, and that we hadn’t classified before:\u003c/p\u003e\u003cul\u003e\u003cli\u003e1,622 dogs classifications, which turn out to be all accurate except 10.\u003c/li\u003e\u003cli\u003e907 cat classifications, which turn out to be all accurate except 38. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eUsing this approach, we manage to classify 2,529 data rows, with an \u003cstrong\u003eaccuracy of 98%\u003c/strong\u003e (48 errors). Good that we send them to humans for review! So far, we’ve classified all data rows except 27, so the coverage is \u003cstrong\u003e99.85%\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eNow, let’s move on to classify the remaining 27 images.\u003c/p\u003e\u003ch3 id=\"low-confidence-data-points-manual-labeling\"\u003eLow confidence data points: Manual labeling\u003c/h3\u003e\u003cp\u003eAfter applying these rules, 18,672 data rows out of 18,699 (99.85%) have been labeled, leaving only 27 data rows unclassified. Foundation models lack the confidence to label these remaining data points.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1135\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSome data rows, 27 in our case, were not classified through foundation models. This includes images that are blurry, where animals are turning their backs or are barely visible behind a cage.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThese 27 data points will require manual labeling by humans, which represents only 0.14% of data rows - a massive efficiency gain in labeling effort and speed!\u003c/p\u003e\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eDirect classification with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eHuman in the loop with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eManual classification\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of data rows classified\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e16,143\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e2,529\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e27\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of errors\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e19\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e48\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e0\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAccuracy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e99.9%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e98%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e100%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eCoverage\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e86%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e13.5%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e0.15%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eCumulative coverage\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e86%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e99.8%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e100%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 16,143 images (86%) in minutes, with \u003cstrong\u003e99.9% accuracy\u003c/strong\u003e thanks to foundation models. An additional 2,529 data points (13.5%) have been pre-labeled with foundation models, with \u003cstrong\u003e98% accuracy\u003c/strong\u003e, and sent for human review. This leaves with only 27 very challenging images to label manually!\u003c/p\u003e\u003ch2 id=\"step-5-set-it-and-forget-it-%E2%80%93-automatically-apply-these-rules-to-fresh-incoming-data\"\u003eStep 5: Set it and forget it – automatically apply these rules to fresh, incoming data\u003c/h2\u003e\u003cp\u003eWith Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e, we can automatically classify fresh, incoming data as cats or dogs. \u003c/p\u003e\u003cp\u003eFor example, we can set up a slice that automatically surfaces all new images that have been connected to Quantumworks Lab in the past week, that haven’t been classified yet. We can set the slice’s criteria to include only images where the natural language search for the prompt “photo of a cat” is higher than 0.61 (since we know that these images are very likely to contain cats). \u003c/p\u003e\u003cp\u003eWith slices, you can easily surface and inspect any new and high-impact data that gets added to your data lake. \u003c/p\u003e\u003cp\u003eFrom there, it only takes one click to classify all of these images as cats.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1216\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWith slices and the ability to automatically surface high-impact data, we surfaced 7,000+ new images of cats that were connected to Quantumworks Lab in the past week. We can easily add a cat classification to all these images in one click.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can learn more about how to bulk classify data in our \u003ca href=\"https://docs.labelbox.com/docs/bulk-classification?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e or in our \u003ca href=\"https://labelbox.com/blog/pre-label-and-enrich-your-data-with-bulk-classifications/?ref=labelbox-guides.ghost.io\"\u003erecent blog post\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities, the bulk classification feature, and foundation models, we managed to classify 16,143 images (86%) in minutes, with \u003cstrong\u003e99.9% accuracy\u003c/strong\u003e. An additional 2,529 data points (13.5%) have been pre-labeled with \u003cstrong\u003e98% accuracy\u003c/strong\u003e and sent for human review. This only left us with 27 very challenging images that we needed to label manually. \u003c/p\u003e\u003chr\u003e\u003cp\u003eIf you’re a current Quantumworks Lab user who wants to leverage any foundation model to supercharge your data labeling process in just a few clicks,\u003ca href=\"https://app.labelbox.com/catalog?ref=labelbox-guides.ghost.io\"\u003e try our bulk classification feature today\u003c/a\u003e or \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started with a free Quantumworks Lab account\u003c/a\u003e.\u003c/p\u003e","comment_id":"6452689090e75e0001b3546c","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3058--2-.png","featured":false,"visibility":"public","created_at":"2023-05-03T13:58:40.000+00:00","updated_at":"2023-10-27T17:01:07.000+00:00","published_at":"2023-05-03T17:28:24.000+00:00","custom_excerpt":"Automatically label images with 99% accuracy leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/automatically-label-images-with-99-accuracy-using-foundation-models","tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/automatically-label-images-with-99-accuracy-using-foundation-models/","excerpt":"Automatically label images with 99% accuracy leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","reading_time":10,"access":true,"comments":false,"og_image":null,"og_title":"Automatically label images with 99% accuracy using foundation models","og_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","twitter_image":null,"twitter_title":"Automatically label images with 99% accuracy using foundation models","twitter_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","meta_title":"Automatically label images with 99% accuracy using foundation models","meta_description":"Automatically label images with 99% accuracy leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64134c4235aad5003db6f2b2","uuid":"72160526-3ea8-42ee-ab34-75b7b0fa3878","title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","slug":"how-to-fine-tune-large-language-models-with-labelbox","html":"\u003ch2 id=\"what-are-large-language-models-llms\"\u003eWhat are large language models (LLMs)?\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/usecases/large-language-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLarge language models\u003c/a\u003e leverage deep learning techniques to recognize, classify, analyze, generate and even predict text. Critical in \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003enatural language processing (NLP)\u003c/a\u003e applications like AI voice assistants, chatbots, translation, and sentiment analysis — large language models rely upon large volumes of data to consistently comprehend, capture and convey the nuances of human language.\u003c/p\u003e\u003ch2 id=\"why-should-you-consider-using-one\"\u003eWhy should you consider using one?\u003c/h2\u003e\u003cp\u003eRecent advancements and accessibility of large language models can serve as a powerful starting point for your machine learning team. Although you’ll still need to retrain these base models on data that is contextually-relevant to your use case, leveraging a foundational model saves significant times and costs. Large language models significantly improve with Reinforcement Learning from Human Preferences (RLHP). This guide provides a framework for using Quantumworks Lab to fine-tune OpenAI 's popular GPT-3 large language model for your use case. \u003c/p\u003e\u003ch2 id=\"getting-started\"\u003e\u003cstrong\u003eGetting Started\u003c/strong\u003e \u003c/h2\u003e\u003cp\u003eThis guide and Colab notebook will walk you through how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect your own relevant classification ontology to use with foundational large language models like GPT-3.\u003c/li\u003e\u003cli\u003eCreate a Project in Quantumworks Lab Annotate to generate labeled training data.\u003c/li\u003e\u003cli\u003eLeverage iterative model runs to rapidly tune OpenAI large language models.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model to diagnose performance, find high impact data, get the data labeled, and create another model run for the next iteration of fine tuning.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/deya57pbun\" title=\"How to fine-tune large language models (LLMs) with Quantumworks Lab Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"1-import-colab-notebook-packages\"\u003e1. Import Colab notebook packages\u003c/h3\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1p3nwxBHUpnUMP4B3mWAACwL-g4uOpjzw?ref=labelbox-guides.ghost.io\" class=\"kg-btn kg-btn-accent\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003eTo help you get started, for this workflow, we’ve created a \u003ca href=\"https://colab.research.google.com/drive/1p3nwxBHUpnUMP4B3mWAACwL-g4uOpjzw?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e which has installed Open AI and Quantumworks Lab packages within the same python notebook. Import all of the packages and use the corresponding Open AI and Quantumworks Lab API keys to connect to your instances. \u003c/p\u003e\u003cp\u003e\u003ccode\u003e!pip install Quantumworks Lab[data] --upgrade -q !pip install openai -q \u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"2-create-a-project-based-on-your-desired-data-ontology\"\u003e\u003cstrong\u003e2\u003c/strong\u003e.\u003cstrong\u003e Create a Project based on your desired data ontology\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNext, \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003ecreate a Project\u003c/a\u003e in the platform that matches the defined ontology for the data you want to classify using Open AI’s GPT-3 model. Add this ontology to a Project. Our Colab notebook example focuses on classifying e-commerce assets into the following four categories that comprise 80% of all e-commerce assets sold:\u003c/p\u003e\u003cul\u003e\u003cli\u003e“Electronics”\u003c/li\u003e\u003cli\u003e“Household”\u003c/li\u003e\u003cli\u003e“Books”\u003c/li\u003e\u003cli\u003e“Clothing \u0026amp; Accessories” up  common e-commerce use case.  \u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e``import os \nimport openai\nfrom labelbox.schema.ontology import OntologyBuilder, Tool, Classification, Option\nfrom Quantumworks Lab import Client, LabelingFrontend, LabelImport, MALPredictionImport``\n\n``from labelbox.data.annotation_types import (\n    Label, ImageData, ObjectAnnotation, MaskData,\n    Rectangle, Point, Line, Mask, Polygon,\n    Radio, Checklist, Text,\n    ClassificationAnnotation, ClassificationAnswer\n    )``\n\n``from labelbox.data.serialization import NDJsonConverter\nimport pandas as pd\nimport shutil\nimport labelbox.data\nimport json\nimport uuid ``\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"3-generate-training-data\"\u003e3. Generate training data\u003c/h3\u003e\u003cp\u003eOnce you’ve determined the ontology and added it to your Quantumworks Lab Project, you can begin curating the training data that supports your requirements. This will ultimately play a critical role towards adapting GPT-3 to support your use case. Once you add your ontology to a project, you have multiple options for creating training data, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab Annotate to label the data yourself\u003c/li\u003e\u003cli\u003eImporting existing annotations you have in your data lake\u003c/li\u003e\u003cli\u003eLeveraging our experts from \u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edata labeling services\u003c/a\u003e for support rapidly curating training-quality data \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"4-push-the-batch-of-training-data-labels-to-model-runs\"\u003e4. Push the batch of training data labels to model runs\u003c/h3\u003e\u003cp\u003eOnce you've created the training data, select the appropriate, corresponding data rows and export them to \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Model\u003c/a\u003e for model training. Note that you'll need to implement model training settings that connect your model training environment and cloud service provider (CSP) to Labelbox. Our Colab Notebook demo uses Google Cloud Platform (GCP) but this same workflow and Quantumworks Lab’s cloud-agnostic platform works well with any model training environment. \u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1148\" height=\"586\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png 1148w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eYou will iteratively run the last section of this notebook as you build momentum fine-tuning your model and improving performance.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"5-iteratively-improve-and-fine-tune-the-model\"\u003e5. Iteratively improve and fine-tune the model\u003c/h3\u003e\u003cp\u003eNext the Colab notebook features a list of prompts to iteratively train the Open AI GPT-3 model based on the annotations that fit your unique use case. As you review the model predictions in Quantumworks Lab Model, the platform will help you easily identify mis-predictions and target areas where the model consistently performs poorly. \u003c/p\u003e\u003cp\u003eIn Quantumworks Lab Catalog, you can leverage embeddings and similarity search features to find training data samples that exhibit similar characteristics to data where your model is consistently performing poorly – then queue that data for labeling so it can be incorporated during your next retraining iteration. You'll do this by iteratively \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003esubmitting a batch\u003c/a\u003e – featuring these newly-curated Data Rows – to the same Project you created earlier (in step two) for fine-tuning your LLM. Ultimately, this iterative loop of exposing the model to new prompts will allow you to continuously fine-tune the GPT-3 model to perform based on your own data priorities.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1656\" height=\"834\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 1656w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFind and prioritize data from Catalog most likely to have the highest impact on your next training iteration.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe notebook feeds the output results from Open AI back into the Quantumworks Lab Model tab. This iterative back and forth workflow empowers you to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLeverage Quantumworks Lab Model to measure model performance \u003c/li\u003e\u003cli\u003eEvaluate model output predictions and identify areas where the model performs poorly\u003c/li\u003e\u003cli\u003eLeverage Catalog to prioritize data from your Catalog that will have the most maximal impact towards addressing your edge cases\u003c/li\u003e\u003cli\u003eFine-tune the Open AI large language model by iteratively feeding it relevant data addressing your ontology, finding erroneous model predictions and fixing areas where the model performs poorly.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eGet started today by following the prompts in this \u003ca href=\"https://colab.research.google.com/drive/1p3nwxBHUpnUMP4B3mWAACwL-g4uOpjzw?ref=labelbox-guides.ghost.io#scrollTo=KSsXvqbaeLXs\"\u003eColab notebook\u003c/a\u003e. To learn more about Quantumworks Lab Model, check out the following guides below:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-train-evaluate-and-improve-your-ml-models/?ref=labelbox-guides.ghost.io\"\u003eHow to get started in Quantumworks Lab Model: Train, evaluate, and improve your ML models\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-run-model-assisted-labeling-with-active-learning-on-ner-data-with-a-hugging-face-model/?ref=labelbox-guides.ghost.io\"\u003eHow to run model-assisted labeling and active learning on NER data with a Hugging Face model\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"64134c4235aad5003db6f2b2","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Frame-2299--2-.png","featured":false,"visibility":"public","created_at":"2023-03-16T17:05:06.000+00:00","updated_at":"2024-10-02T22:30:29.000+00:00","published_at":"2023-03-22T23:00:29.000+00:00","custom_excerpt":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-large-language-models-with-Labelbox","tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-large-language-models-with-labelbox/","excerpt":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","og_description":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","twitter_image":null,"twitter_title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","twitter_description":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","meta_title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","meta_description":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6406883577b361003d3619d5","uuid":"4ab62fd8-b9da-474e-a833-f24758c230db","title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","slug":"using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data","html":"\u003cp\u003eOne of the biggest challenges that ML teams face is how difficult it is to select the right data to improve their ML models. From working with hundreds of teams, we’ve seen that ML teams possess a vast amount of unlabeled data, but lack a structured process for effectively finding and prioritizing \u003cem\u003especific data\u003c/em\u003e that can dramatically improve model performance. \u003c/p\u003e\u003cp\u003eThis manifests itself in the form of trying to find specific examples of an edge case where your model is struggling, or in the case of wanting to surface all occurrences of a rare data point that needs to be labeled in priority. In these cases, what is the best way for your team to efficiently surface this high-impact data?\u003c/p\u003e\u003ch2 id=\"what-will-you-learn-in-this-guide\"\u003eWhat will you learn in this guide? \u003c/h2\u003e\u003cp\u003eIn this guide, we'll show you how you can use a foundation model, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data. This technique will help your team quickly enrich your data with the latest advances in off-the-shelf models and embeddings.\u003c/p\u003e\u003cp\u003eBy the end of this guide, you’ll know how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGenerate custom embeddings with Hugging Face using a single line of code and upload your data to Quantumworks Lab in order to better explore and visualize your data.\u003c/li\u003e\u003cli\u003eBetter understand the distribution of your data and quickly find similar high-impact data.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab as a native similarity search engine, where you can leverage both off-the-shelf embeddings computed by Quantumworks Lab (for image, text, and documents) and upload your own custom embeddings to quickly find all instances of similar data.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"what-are-embeddings\"\u003eWhat are embeddings? \u003c/h2\u003e\u003cp\u003eIn machine learning, an embedding, or feature vector, is an array of numbers assigned to an asset by a neural net. Assets that have similar content will also have similar embeddings. \u003c/p\u003e\u003cp\u003eFor example, in a dataset comprising images of apples and oranges, an appropriate embedding used for image similarity will show that all the vectors corresponding to apples have similar values. The vectors for all images of oranges will also be grouped together. \u003c/p\u003e\u003cp\u003eIn other words, the neural network acts as a feature extractor: it extracts an embedding vector that contains rich information about the data.\u003c/p\u003e\u003ch3 id=\"off-the-shelf-embeddings-vs-custom-embeddings\"\u003eOff-the-shelf embeddings vs custom embeddings\u003c/h3\u003e\u003cp\u003eWhen you connect your data to Quantumworks Lab, we automatically compute \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io#supported-embeddings\"\u003eoff-the-shelf\u003c/a\u003e\u003cstrong\u003e \u003c/strong\u003eembeddings on your data – this includes CLIP embeddings for images and PDFs and All-mpnet-base-v2 embeddings for text. These off-the-shelf embeddings are a useful starting point for you to explore your data and conduct similarity searches. \u003c/p\u003e\u003cp\u003eHowever, in some cases where your data has unique attributes, you may want to use your own \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io#how-to-upload-custom-embeddings\"\u003ecustom embeddings\u003c/a\u003e to power your data selection. Quantumworks Lab allows you to upload up to 100 custom embeddings in addition to the off-the-shelf embeddings that are automatically computed. \u003c/p\u003e\u003cp\u003eYou can easily compare the results of these custom and provided off-the-shelf embeddings in Quantumworks Lab to discover the best embeddings to use for data selection.\u003c/p\u003e\u003ch2 id=\"how-to-upload-custom-embeddings\"\u003eHow to upload custom embeddings\u003c/h2\u003e\u003cp\u003eFirst, connect your data with Labelbox. You can integrate your cloud storage bucket with Quantumworks Lab via IAM delegated access:\u003c/p\u003e\u003cp\u003e\u003cem\u003eHow to set up a delegated access integration with Quantumworks Lab\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-amazons3-and-labelbox/?ref=labelbox-guides.ghost.io\"\u003eAmazon S3\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-gcp-storage-labelbox/?ref=labelbox-guides.ghost.io\"\u003eGCP Storage\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-azure-and-labelbox/?ref=labelbox-guides.ghost.io\"\u003eMicrosoft Azure Blob Storage\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve successfully uploaded your data, Quantumworks Lab will automatically compute off-the-shelf embeddings on your data.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1013\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can then compute and upload custom embeddings from Hugging Face on your data:\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/huggingface/huggingface.ipynb?ref=labelbox-guides.ghost.io\" class=\"kg-btn kg-btn-accent\"\u003eGoogle Colab notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003e\u003cem\u003eFollow along in this \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/huggingface/huggingface.ipynb?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eColab notebook \u003c/em\u003e\u003c/a\u003e\u003cem\u003ewith examples shown using ResNet-50 embeddings from Hugging Face.\u003c/em\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003eImport Quantumworks Lab into your notebook\u003c/li\u003e\u003c/ol\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# for Quantumworks Lab\n!pip3 install -q Quantumworks Lab[data]\nimport Quantumworks Lab as lb\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e2. Import the \u003ca href=\"https://github.com/Quantumworks Lab/advlib/tree/main/pylib/advlib?ref=labelbox-guides.ghost.io\"\u003eADVLib\u003c/a\u003e. This is a library built by Quantumworks Lab for you to upload custom embeddings.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# for custom embeddings in Quantumworks Lab\n!pip3 install -q 'git+https://github.com/Quantumworks Lab/advlib.git'\n#ndjson\n!pip3 install -q ndjson\nimport ndjson\nimport time\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e3. Select the data rows (images or text) in Quantumworks Lab on which you want to add custom embeddings.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# get images from a Quantumworks Lab dataset\ndataset = client.get_dataset(\"clemr01l42uil07y36qkq7ygn\")\ndrs = list(dataset.export_data_rows(timeout_seconds=9999))\ndata_row_ids = [dr.uid for dr in drs]\ndata_row_urls = [dr.row_data for dr in drs]\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e4. Use Hugging Face to generate your custom embeddings by loading a specific neural network (e.g. Resnet50).\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# import HuggingFace\n!pip3 install -q transformers\n!pip3 install -q timm\n\n# load a neural network from HuggingFace \nimport transformers\ntransformers.logging.set_verbosity(50)\nimport torch\nimport torch.nn.functional as F\nimport PIL, requests\nfrom tqdm import tqdm\n\n# get ResNet-50\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = transformers.ResNetModel.from_pretrained(\"microsoft/resnet-50\")\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e5. Generate custom embeddings by iterating over your image or text data. \u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: \u003c/em\u003eThis should take approximately ~2 minutes for 512 images. For the similarity search function to work in Quantumworks Lab, you must upload at least 1,000 embeddings. \u003c/p\u003e\u003cul\u003e\u003cli\u003eRetrieve your images/text and run model inference \u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# process images\nimg_hf = image_processor(imgs, return_tensors=\"pt\")\n\n# generate resnet embeddings, thanks to inference\nwith torch.no_grad():\n\tlast_layer = model(**img_hf, output_hidden_states=True).last_hidden_state\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\u003cli\u003eRemember to do global pooling on the last layer of your embedding to reduce dimensionality\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eresnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\nresnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e6. Create the payload to upload custom embeddings to Quantumworks Lab in the form of an NDJSON file.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# create the payload\npayload = []\nfor (dr_id,resnet_embedding) in zip(dr_ids, resnet_embeddings):\n\tpayload.append({\"id\": dr_id, \"vector\": resnet_embedding})\n\n# write to NDJson file\nwith open('payload.ndjson', 'w') as f:\n\tndjson.dump(payload, f)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e7. Pick an existing custom embedding or create a custom embedding.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# max pool to reduce dimensionality\nresnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\nresnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e8. Upload your payload of custom embeddings into Labelbox.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e!advtool embeddings import \u0026lt;EMB ID\u0026gt; ./payload.ndjson\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e9. Use Quantumworks Lab Catalog UI to start conducting similarity searches.\u003c/p\u003e\u003ch2 id=\"how-to-quickly-find-instances-of-similar-data\"\u003eHow to quickly find instances of similar data\u003c/h2\u003e\u003cp\u003eOnce you have uploaded your custom embeddings to Quantumworks Lab, you can focus on curating data in Catalog that will dramatically improve your model’s performance.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eIdentify an edge case or rare example image/text you want to use to find similar data.\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis can include examples of data on which your model might be struggling. For example, let’s say the model is incorrectly classifying images with sparse patches of grass as having been affected by a wildfire. \u003c/p\u003e\u003cp\u003eIn the example below, the model appears to struggle on recognizing images with ‘no wildfire'\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1530\" height=\"736\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png 1530w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e2. Surface all instances of similar data.\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-16-47--1--1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1775\" height=\"959\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/2023-03-06_17-16-47--1--1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/2023-03-06_17-16-47--1--1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/2023-03-06_17-16-47--1--1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-16-47--1--1.gif 1775w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can run \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003esimilarity searches\u003c/a\u003e to find all instances of similar data. A similarity search will automatically surface all similar data rows – you can select multiple data rows as anchors to continue to refine your similarity search. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Combine a similarity search with other search filters.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo filter the dataset even further, you can combine a similarity search with other \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003esearch filters\u003c/a\u003e. This includes filtering on metadata, media attribute, annotation, and more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1516\" height=\"824\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png 1516w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e4. Compare similarity search results.\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-23-53--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1772\" height=\"964\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/2023-03-06_17-23-53--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/2023-03-06_17-23-53--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/2023-03-06_17-23-53--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-23-53--1-.gif 1772w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can compare the results of the similarity search on different embeddings (across off-the-shelf and custom embeddings). This gives you an understanding of which embeddings are most effective towards providing your desired results. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5. Add all instances of similar data to a labeling project or save it as a slice.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve found additional examples of similar data rows on which your model is struggling, you can queue them to your labeling project in priority or save the filters as a slice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-29-38--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"954\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/2023-03-06_17-29-38--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/2023-03-06_17-29-38--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/2023-03-06_17-29-38--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-29-38--1-.gif 1774w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003eBy saving your similarity search as a slice, any new incoming data that matches the search criteria will automatically show up in the slice. This enables automatic data curation.\u003c/p\u003e\u003cp\u003eLearn more about other key ML workflows that you can perform using similarity search \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003ein this guide\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eLeveraging embeddings as a powerful similarity search technique can help you find specific data points within an ocean of data. With a similarity search, you can easily query and curate specific data that will dramatically improve your model performance. If you’re interested in learning more, please check out the additional resources below.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAdditional resources:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-kickstart-and-scale-your-data-labeling-efforts/?ref=labelbox-guides.ghost.io\"\u003eHow to kickstart and scale your data labeling efforts\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003eHow to filter and sort your data\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003eHow to find similar data in one click\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"6406883577b361003d3619d5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-3012--4-.png","featured":false,"visibility":"public","created_at":"2023-03-07T00:41:25.000+00:00","updated_at":"2024-02-02T18:28:44.000+00:00","published_at":"2023-03-08T19:31:53.000+00:00","custom_excerpt":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-imapctful-data","tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/","excerpt":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","og_description":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","twitter_image":null,"twitter_title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","twitter_description":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","meta_title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","meta_description":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":17,"tag":{"slug":"use-ai","id":"653aa4fb375d13000123d7e6","name":"Use AI","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","count":{"posts":17},"url":"https://labelbox-guides.ghost.io/tag/use-ai/"},"slug":"use-ai","currentPage":"2"},"__N_SSG":true},"page":"/guides/tag/[id]/page/[pagenum]","query":{"id":"use-ai","pagenum":"2"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script>
    

    

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>

                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><span style="color: inherit; cursor: default;">Docs</span></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="/static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <span style="color: inherit; cursor: default;">Terms of Service</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Privacy Notice</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Copyright Dispute Policy</span>
                </div>
            </div>
        </div>
    </footer>
</body>
<!-- Mirrored from labelbox.com/guides/tag/use-ai/page/2/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:18:45 GMT -->
</html>