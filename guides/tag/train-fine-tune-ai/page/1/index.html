<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/tag/train-fine-tune-ai/page/1/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:53:14 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../../../static/scripts/munchkin.js"></script><script src="../../../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/guides/tag/%5bid%5d/page/%5bpagenum%5d-da4e9ee1c105845a.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../../../index.html"><img width="106" height="24" alt="logo" src="../../../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../../../build-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Build AI</a><a href="../../../use-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Use AI</a><a href="../../../explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="../../../label-data-for-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Label data for AI</a><a href="../../index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../../../mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexa17d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Quantumworks Lab Leaderboards: Redefining AI evaluations with human-centric assessments</p><p class="text-base max-w-2xl undefined line-clamp-3">Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../evaluating-leading-text-to-speech-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index6ff8.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../evaluating-leading-text-to-speech-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Evaluating leading text-to-speech models</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../metrics-based-rag-development-with-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexd10c.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../metrics-based-rag-development-with-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Metrics-based RAG Development with Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../ai-foundations-understanding-embeddings/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexfbc8.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FYT-Thumbnails--23-.jpg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../ai-foundations-understanding-embeddings/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">AI foundations: Understanding embeddings</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../end-to-end-workflow-for-knowledge-distillation-with-nlp/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexd0a1.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2Fdistilling.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../end-to-end-workflow-for-knowledge-distillation-with-nlp/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Distilling a faster and smaller custom LLM using Google Gemini</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../end-to-end-workflow-with-model-distillation-for-computer-vision/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index98f9.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F02%2FSocial-Cards-Example--1-.jpg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../end-to-end-workflow-with-model-distillation-for-computer-vision/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">End-to-end workflow with model distillation for computer vision</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index0a79.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FGroup-2457--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to analyze customer reviews and improve customer care with NLP</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage Quantumworks Lab&#x27;s data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-build-a-content-moderation-model-to-detect-disinformation/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index0db2.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2FFrame-2299--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-build-a-content-moderation-model-to-detect-disinformation/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to build a content moderation model to detect disinformation</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust &amp; safety applications. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexfcac.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FGroup-3081.jpg&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-fine-tune-vertex-ai-models-with-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index32b3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FFrame-2299.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-fine-tune-vertex-ai-models-with-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to fine-tune Vertex AI LLMs with Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.</p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8">Page 1 of 3<a class="ml-9 text-neutral-700 mb-1" href="../2/index.html">&gt;</a></div></div></div></div></div></div>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><span style="color: inherit; cursor: default;">Docs</span></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <span style="color: inherit; cursor: default;">Terms of Service</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Privacy Notice</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Copyright Dispute Policy</span>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"684755a1e82e4e00013fe307","uuid":"fab31394-c337-43cc-bc44-725a7b69cc61","title":"Quantumworks Lab Leaderboards: Redefining AI evaluations with human-centric assessments","slug":"labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments","html":"\u003cp\u003eIn the rapidly evolving landscape of artificial intelligence, traditional benchmarks are no longer sufficient to capture the true capabilities of AI models. At Quantumworks Lab, we're excited to introduce our groundbreaking \u003ca href=\"https://labelbox.com/leaderboards/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e—an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks. \u003c/p\u003e\u003ch2 id=\"the-limitations-of-current-benchmarks-and-leaderboards\"\u003e\u003cstrong\u003eThe limitations of current benchmarks and leaderboards\u003c/strong\u003e\u003c/h2\u003e\u003ch3 id=\"benchmark-contamination\"\u003e\u003cstrong\u003eBenchmark contamination\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOne of the most pressing issues in AI evaluation today is benchmark contamination. As large language models are trained on vast amounts of internet data, they often inadvertently include the very datasets used to evaluate them. This leads to inflated performance metrics that don't accurately reflect real-world capabilities. For example:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe LAMBADA dataset, designed to test language understanding, has been found in the training data of several popular language models, with an \u003ca href=\"https://hitz-zentroa.github.io/lm-contamination/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLM Contamination Index\u003c/u\u003e\u003c/a\u003e of 29.3%.\u003c/li\u003e\u003cli\u003ePortions of the SQuAD question-answering dataset have been discovered in the pretraining corpora of multiple large language models.\u003c/li\u003e\u003cli\u003eEven coding benchmarks like HumanEval have seen their \u003ca href=\"https://arxiv.org/pdf/2407.07565?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esolutions leaked online\u003c/u\u003e\u003c/a\u003e, potentially contaminating future model training.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis contamination makes it increasingly difficult to trust traditional benchmark results, as models may be “cheating” by memorizing test data rather than demonstrating true understanding or capability.\u003c/p\u003e\u003ch3 id=\"existing-leaderboards-a-step-forward-but-not-enough\"\u003e\u003cstrong\u003eExisting leaderboards: A step forward, but not enough\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWhile several leaderboards have emerged to address the limitations of traditional benchmarks, they each come with their own set of challenges.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eLMSYS chatbot arena\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLMSYS Chatbot Arena, despite its broad accessibility, faces notable challenges in providing objective AI evaluations. Its reliance on non-expert assessments and emphasis on chat-based evaluations may introduce personal biases, potentially favoring engaging responses over true intelligence. Researchers worry that this approach could lead companies to prioritize optimizing for superficial metrics rather than genuine real-world performance. Furthermore, LMSYS's commercial ties raise concerns about impartiality and the potential for an uneven evaluation playing field, as usage data may be selectively shared with certain partners.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScale AI's SEAL\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eScale’s Safety, Evaluations, and Alignment Lab (SEAL), released few months ago, offers detailed insights/evaluations for topics such as reasoning, coding, and agentic tool use. However, the infrequent updates and primary focus on language models, while useful, may not capture the full spectrum of rapidly advancing multimodal AI capabilities.\u003c/p\u003e\u003ch2 id=\"challenges-in-ai-evaluation\"\u003e\u003cstrong\u003eChallenges in AI evaluation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThese and other existing leaderboards all run into core challenges with AI evaluations:\u003c/p\u003e\u003cp\u003e1) Data contamination and overfitting to public benchmarks\u003c/p\u003e\u003cp\u003e2) Scalability issues as models improve and more are added\u003c/p\u003e\u003cp\u003e3) Lack of standards for evaluation instructions and criteria\u003c/p\u003e\u003cp\u003e4) Difficulty in linking evaluation results to real-world outcomes\u003c/p\u003e\u003cp\u003e5) Potential bias in human evaluations\u003c/p\u003e\u003ch2 id=\"introducing-the-labelbox-leaderboards-a-comprehensive-approach-to-ai-evaluation\"\u003e\u003cstrong\u003eIntroducing the Quantumworks Lab Leaderboards: A comprehensive approach to AI evaluation\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Quantumworks Lab Leaderboards are the first to tackle these challenges by conducting structured evaluations on subjective AI model outputs using human experts and a scientific process that provides detailed feature-level metrics and multiple ratings. Leaderboards are available for \u003ca href=\"https://labelbox.com/leaderboards/complex-reasoning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eComplex Reasoning\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/multimodal-reasoning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eMultimodal Reasoning\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eImage Generation\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/speech-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSpeech Generation\u003c/u\u003e\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/leaderboards/video-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eVideo Generation\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eOur goal is to go beyond traditional leaderboards and benchmarks by incorporating the following elements:\u003c/p\u003e\u003ch3 id=\"1-multimodal-and-niche-focus\"\u003e\u003cstrong\u003e1. Multimodal and niche focus\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eUnlike leaderboards that primarily focus on text-based large language models, we evaluate a diverse range of AI modalities and specialized applications, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImage generation and analysis\u003c/li\u003e\u003cli\u003eAudio processing and synthesis\u003c/li\u003e\u003cli\u003eVideo creation and manipulation\u003c/li\u003e\u003cli\u003eComplex and multimodal reasoning\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"2-expert-human-evaluation\"\u003e\u003cstrong\u003e2. Expert human evaluation\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFor every evaluation, public or private, it’s critical for the raters to reflect your target audience. We place expert human judgment, using our \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAlignerr\u003c/u\u003e\u003c/a\u003e workforce, at the core of the evaluation process to ensure:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSubjective quality assessment:\u003c/strong\u003e Humans assess aspects like aesthetic appeal, realism, and expressiveness.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContextual understanding:\u003c/strong\u003e Evaluators consider the broader context and intended use.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAlignment with human preferences:\u003c/strong\u003e Raters ensure evaluations reflect criteria that matter to end-users.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResistance to contamination:\u003c/strong\u003e Human evaluations on novel tasks are less prone to data contamination.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"3-reliable-and-transparent-methodology\"\u003e\u003cstrong\u003e3. Reliable and transparent methodology\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWe are committed to performing trustworthy evaluations using a variety of sophisticated metrics. Quantumworks Lab balances privacy with openness by providing detailed feature-level metrics (e.g. prompt alignment, visual appeal, and numerical count for text-image models) and multiple ratings.\u003c/p\u003e\u003cp\u003eIn addition to critical human experts performing the evaluations, our methodology utilizes the \u003ca href=\"https://labelbox.com/product/platform/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLablebox Platform\u003c/a\u003e to generate advanced metrics on both the rater and model performance. We provide the following metrics across our three leaderboards:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eElo rating system:\u003c/strong\u003e Adapted from competitive chess, our Elo system provides a dynamic rating that adjusts based on head-to-head comparisons between models. This allows us to capture relative performance in a way that's responsive to improvements over time.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTrueSkill rating:\u003c/strong\u003e Originally developed for Xbox Live, TrueSkill offers a more nuanced rating that accounts for both a model's performance and the uncertainty in that performance. This is particularly useful for newer models or those with fewer evaluations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank percentages:\u003c/strong\u003e We track how often each model achieves each rank (1st through 5th) in direct comparisons. This provides insight into not just average performance, but consistency of top-tier results.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAverage rating:\u003c/strong\u003e A straightforward metric that gives an overall sense of model performance across all evaluations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to these key metrics, our methodology incorporates the following characteristics to ensure a balanced and fair evaluation:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eExpert evaluators:\u003c/strong\u003e Utilizing skilled professionals from our Alignerr platform to provide nuanced, context-aware assessments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComprehensive and novel datasets:\u003c/strong\u003e Curated to reflect real-world scenarios while minimizing contamination.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTransparent reporting:\u003c/strong\u003e Detailed insights into our methodologies and results without compromising proprietary information.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"4-continuously-updated-evaluations\"\u003e\u003cstrong\u003e4. Continuously updated evaluations\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOur leaderboard isn't static; we plan to regularly update our evaluations to include the latest models and evaluation metrics, ensuring stakeholders have access to current and relevant information.\u003c/p\u003e\u003ch2 id=\"leaderboard-insights-a-glimpse-into-the-image-generation-leaderboard\"\u003e\u003cstrong\u003eLeaderboard insights: A glimpse into the image generation leaderboard\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo illustrate the power of our comprehensive evaluation approach, let's explore the \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eimage generation leaderboard\u003c/a\u003e. For each evaluation of the latest image-generating models, we capture and publish four key pieces of data to help understand capabilities and areas of opportunity for each model.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Elo ratings:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXf_TDIoHT0uGUl1DnLLMI3qkaV4g9M9yrK4GVWRc1Ze495hZYjwXE5Yq0wZefgLQecqsXA8cS7bSmTNz923B8CYgza7d2PkPn25crjiCrd0I3W2MG53hWiTgo-N8BTL8y11b3vuog?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT Image 1 leads with 1069.17, followed by GPT 4.1 at 1039.62 and Recraft v3 at 1039.37\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2) TrueSkill ratings:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdIblItim2rjJ5kSmyMtNqbhzIQZM24C6TwogiUeuNFCwBNyUPydDpZ3vOlYPSimzksITCDgQ_ej4Czavte2eI8aT0OEjCB0FwDDagBBP-yQLgt2T_FV8dTy0DdFqWUfk4cbMSYcg?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT Image 1 again leads with 982.86, with GPT 4.1 following at 979.89\u003c/li\u003e\u003cli\u003eThis indicates high expected performance for GPT Image 1with relatively low uncertainty\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Rank percentages:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe8jjQCJegixK47fNhDT67NDHe8fhN6U0Kwm-mbkrIa9HWWFcwKbIubLDO7uy0_KtVVu1gHOcIk0Zh9VOHGgV6zrBBkGxbaCHW9a5Uu48lSo8L-J_48nAq6Q-bFqD9k-aYwPvY3?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT 4.1 achieves the top rank 60.14% of the time, followed by GPT Image 1 at 59.31%\u003c/li\u003e\u003cli\u003eThis shows GPT 4.1 consistency in achieving top results, but also highlights GPT Image 1 and DALL-E 3’s strong performance\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4) Average rank (lower better):\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemzdbe3m9BuWRyjcZTnpDJMSAn8a4UuMJTaepEJHsQwJ_JocoixeC4UQcftoenMWuZ3y9tToZU7UtSwxRMNCVdLwytjw72Tq_8bMC5MAJOxB0VYMpE4P4-EcVxs-uqZsEPlM6K?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eNote: Lower score is better here so GPT 4.1 leads in average rank.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT 4.1 slightly edges out GPT Image 1 with an average rating of 1.4 vs 1.41 (lower is better)\u003c/li\u003e\u003cli\u003eThis suggests GPT 4.1 performs well in direct comparisons despite lower Elo and TrueSkill ratings\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese metrics provide a multi-faceted view of model performance, allowing users to understand not just which model is \"best\" overall, but which might be most suitable for their specific use case. For instance, while GPT Image 1 and GPT 4.1 leads in most metrics, DALL-E 3’s and Imagen 3’s strong average rating suggests it is a reliable choice for consistent performance across a range of tasks.\u003c/p\u003e\u003ch2 id=\"join-the-revolution-beyond-the-benchmark\"\u003e\u003cstrong\u003eJoin the revolution: Beyond the benchmark\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Quantumworks Lab Leaderboards represent a significant advance in AI evaluation, pushing past traditional leaderboards by incorporating expert human evaluations for subjective generative AI models using comprehensive metrics. We are uniquely able to achieve this thanks to our modern AI data factory that combines human experts and our scalable platform with years of operational excellence evaluating AI models.\u003c/p\u003e\u003cp\u003eWe invite you to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCheck out the \u003ca href=\"http://labelbox.com/leaderboards?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e to explore our latest evaluations across various AI modalities and niche applications.\u003c/li\u003e\u003cli\u003e\u003cu\u003eLet us know\u003c/u\u003e if you have suggestions or want a specific model included in future assessments.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eContact us\u003c/u\u003e\u003c/a\u003e to learn more about how we can help you evaluate and improve your AI models across all modalities.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eReady to go beyond the benchmark? Let's redefine AI evaluation — together—and drive the field toward more meaningful, human-aligned progress that truly captures the capabilities of next-generation AI models.\u003c/p\u003e","comment_id":"684755a1e82e4e00013fe307","feature_image":"https://labelbox-guides.ghost.io/content/images/2025/06/guide_LeaderboxHero2.png","featured":false,"visibility":"public","created_at":"2025-06-09T21:44:01.000+00:00","updated_at":"2025-06-11T17:30:22.000+00:00","published_at":"2025-06-10T23:15:53.000+00:00","custom_excerpt":"Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"664e43b39132db0001f20548","name":"Michael Haag","slug":"michael","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/michael/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"664e43b39132db0001f20548","name":"Michael Haag","slug":"michael","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/michael/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/","excerpt":"Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Quantumworks Lab Leaderboards: Redefining AI Evaluation with Human Experts","meta_description":"Introducing our groundbreaking Quantumworks Lab leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bfded301c06500016e8b1c","uuid":"a2c40e88-6376-4824-b57f-e0e3673d8b1b","title":"Evaluating leading text-to-speech models","slug":"evaluating-leading-text-to-speech-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in speech generation evaluation, visit \u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe evolution of Text-to-Speech (TTS) models has been groundbreaking, with various companies offering high-quality solutions for a range of applications. Whether it's enhancing accessibility, automating customer service, or generating voiceovers, TTS models play a vital role in today's digital landscape. \u003c/p\u003e\u003cp\u003eWe recently evaluated the performance of six leading TTS models—Google TTS, Cartesia, AWS Polly, OpenAI TTS, Deepgram, and Eleven Labs—using key evaluation metrics such as Word Error Rate (WER) and human preference ratings.\u003c/p\u003e\u003cp\u003eTo provide an in-depth analysis, we evaluated each model on 500 prompts. Each prompt was reviewed by three expert labelers to ensure accuracy and provide diverse perspectives.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"evaluation-process\"\u003e\u003cstrong\u003eEvaluation Process\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate speech outputs across several key criteria:\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer\"\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWER measures the accuracy of the generated speech by calculating the number of insertions, deletions, and substitutions compared to a reference transcription. The lower the WER, the better the model performed in terms of generating accurate and coherent speech.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1120\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"word-errors\"\u003eWord errors\u003c/h3\u003e\u003cp\u003eRaters evaluated the presence of word errors in the generated TTS audio by counting the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Insertion Errors\u003c/strong\u003e: Additional words not present in the original text.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Deletion Errors\u003c/strong\u003e: Words from the original text that were omitted.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Substitution Errors\u003c/strong\u003e: Incorrect replacements for original words.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: No word errors or only minor errors (0-1 error).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some errors (2-3 errors).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Significant errors (4 or more errors).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"speech-naturalness\"\u003eSpeech naturalness\u003c/h3\u003e\u003cp\u003eRaters assessed how human-like the generated speech sounded.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Very human-like, natural flow with appropriate pauses and inflections.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some human qualities but with occasional robotic or awkward elements.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Clearly robotic or artificial, with choppy or monotone speech.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"pronunciation-accuracy\"\u003ePronunciation accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated how clearly and correctly words were pronounced in the TTS output.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: All words are pronounced clearly and correctly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: 1-2 words are mispronounced or unclear.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: 3 or more words are mispronounced or difficult to understand.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"noise\"\u003eNoise\u003c/h3\u003e\u003cp\u003eRaters evaluated the level of background noise or artifacts in the audio.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNone\u003c/strong\u003e: No detectable noise or artifacts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Minor noise, such as a faint hiss, but not distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Noticeable noise or artifacts that may be somewhat distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Significant noise or artifacts that interfere with comprehension.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"context-awareness\"\u003eContext awareness\u003c/h3\u003e\u003cp\u003eRaters assessed the TTS system's ability to adjust to context, including tone, emphasis, and punctuation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Excellent adaptation to context, with clear tonal shifts and pauses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Adequate adaptation but misses some nuances.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Little to no adaptation; reads text monotonously without context cues.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"prosody-accuracy\"\u003eProsody accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated the rhythm, stress, and intonation (prosody) of the generated speech.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Natural-sounding rhythm and intonation that closely mimics human speech.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Mostly natural but with occasional awkwardness in stress or rhythm.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Monotonous or robotic intonation with unnatural pauses and emphasis\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"prompts-curation-process\"\u003ePrompts curation process\u003c/h2\u003e\u003cp\u003eThe diverse set of prompts used in our TTS evaluation provides a comprehensive test of model capabilities across multiple dimensions. \u003c/p\u003e\u003cp\u003eConsider these examples:\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Wow! Did you see the fireworks last night? They were spectacular!\"\u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003e This short, exclamatory sentence tests the model's ability to convey excitement and emphasis. It challenges the TTS system to appropriately modulate tone and pitch to express enthusiasm.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"I wish you were on the right path...I-I-I truly do, but you're not,' he said sadly.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis prompt evaluates the model's handling of emotional nuance, hesitation (indicated by the stuttering \"I-I-I\"), and the use of ellipsis for pauses. It also tests the system's ability to convey sadness as specified in the dialogue tag.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Alright, listen up, team! The mission briefing is as follows: Operation Nighthawk commences at 0300 hours. Rendezvous at coordinates 51°30'26.5\"N 0°07'39.0\"W. Your login credentials for the secure server are: Username: NightOwl_Alpha, Password: 3ch3l0n_X9#. Remember, this is a stealth operation. Radio silence must be maintained except for emergency protocol Sierra-Tango-Seven. If compromised, use the failsafe code: Zulu-Yankee-4-7-2-Bravo. Any questions? No? Then synchronize your watches... now. Good luck, and godspeed.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis complex prompt tests multiple aspects:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical pronunciation of alphanumeric strings and military jargon\u003c/li\u003e\u003cli\u003eConveying urgency and authority\u003c/li\u003e\u003cli\u003eHandling of coordinates and precise numerical information\u003c/li\u003e\u003cli\u003eProper pacing for a lengthy, detailed instruction set\u003c/li\u003e\u003cli\u003eAppropriate pausing, indicated by punctuation and paragraph breaks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese prompts demonstrate how we cover a range of linguistic features, emotional tones, and formatting challenges. They span from simple exclamations to complex, technical instructions, ensuring a thorough assessment of prosody, emphasis, and versatility.\u003c/p\u003e\u003cp\u003eThis diversity in prompts matters because it enables a comprehensive evaluation that reflects real-world applicability. By covering such a broad spectrum of scenarios, we can identify specific strengths and weaknesses in each TTS model, allowing for fair comparisons and pinpointing areas for improvement. Whether it's expressing joy over fireworks, conveying emotional conflict, or delivering a detailed mission briefing, these prompts help us ensure that TTS models can handle the variety and complexity of human communication.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overall-ranking-best-to-worst-model-samples\"\u003eOverall Ranking best to worst model samples\u003c/h2\u003e\u003cp\u003eRaters ranked each TTS sample from best to worst based on performance across all categories above.\u003c/p\u003e\u003ch3 id=\"model-configurations-used-at-test-time\"\u003e\u003cstrong\u003eModel configurations used at test time\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: tts-1-hd\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: alloy\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle Text-to-Speech (TTS)\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel:\u003c/strong\u003e Default\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice: \u003c/strong\u003eDefault with neutral gender voice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eElevenLabs\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: eleven_monolingual_v1 (As of 30th September, eleven_monolingual_v1 is an outdated version and Quantumworks Lab will run a new experiment with\u0026nbsp;eleven_multilingual_v2 in the future)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: pMsXgVXv3BLzUgSXRplE\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: aura-asteria-en\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Default\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAmazon Polly\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Generative\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Ruth\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: sonic-english\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Teacher Lady\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe selected these models for our evaluation as they represent the cutting-edge offerings from leading companies in the TTS space, showcasing the latest advancements in voice synthesis technology.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"results\"\u003eResults\u003c/h1\u003e\u003cp\u003eOur evaluation of 500 diverse and complex prompts provided valuable insights into the capabilities of Google, Cartesia, AWS Polly, OpenAI, Deepgram, and Eleven Labs TTS models. Each prompt was meticulously assessed by three different raters to ensure accurate and diverse perspectives. This rigorous testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of these leading TTS models:\u003c/p\u003e\u003ch3 id=\"rank-distribution-percentages\"\u003eRank distribution percentages\u003cstrong\u003e\u0026nbsp; \u0026nbsp; \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"overall-ranking\"\u003eOverall ranking\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1:\u003c/strong\u003e\u0026nbsp; (Open AI TTS) - occurred 607 times, 42.93%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2:\u003c/strong\u003e\u0026nbsp; (Cartesia) - occurred 460 times, 32.53%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 410 times, 29.00%\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 4:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 338 times, 23.90%. Followed by Eleven labs as a close second with occurrence 335, 23.69%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 5:\u003c/strong\u003e\u0026nbsp; (Deepgram) - occurred 438 times, 30.98%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 6:\u003c/strong\u003e\u0026nbsp; (Google TTS) - occurred 868 times, 61.39%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote: Rank is determined by the most frequently selected model for each position, which is why some AWS Polly appears twice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1122\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003eFig: % of times each model was ranked first.\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer-results\"\u003eWord Error Rate (WER) Results\u003c/h3\u003e\u003cp\u003eThe WER scores offer an objective look at how accurately each model converts text to speech.\u003c/p\u003e\u003cul\u003e\u003cli\u003eDeepgram: 5.67%\u003c/li\u003e\u003cli\u003eOpenAI TTS: 4.19%\u003c/li\u003e\u003cli\u003eGoogle TTS: 3.36%\u003c/li\u003e\u003cli\u003eAWS Polly: 3.18%\u003c/li\u003e\u003cli\u003eCartesia: 3.87%\u003c/li\u003e\u003cli\u003eEleven Labs: 2.83%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs achieved the lowest WER at 2.83%, making it the most accurate model. Deepgram, however, had the highest WER at 5.67%, indicating room for improvement in transcription accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" title=\"Chart\" width=\"2000\" height=\"1126\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1123\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-specific-performance-results\"\u003eModel-Specific Performance Results\u003c/h2\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each TTS model based on key evaluation criteria.\u003c/p\u003e\u003ch3 id=\"openai-tts\"\u003eOpenAI TTS\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 4.1898%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 89.60% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 87.13% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 92.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 63.37% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 64.57% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOpenAI TTS excels at producing natural-sounding speech, making it a top choice for applications requiring lifelike vocal output. It demonstrates exceptional accuracy in pronunciation and performs excellently in producing clean audio with minimal background noise. The model shows strong capabilities in understanding and conveying contextual nuances in speech and is proficient in delivering appropriate intonation and rhythm. Despite ranking high in human preference, it shows moderate accuracy in word reproduction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cm24ur6wwf\" title=\"OpenAI: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gicqvkqc5e\" title=\"OpenAI: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kg9wqqftu4\" title=\"OpenAI: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"cartesia\"\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 58.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 53.11% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 93.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 80.98% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 77.02% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.8707%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCartesia demonstrates good accuracy in word reproduction. While not as consistent as OpenAI TTS in naturalness, it still performs very well in producing natural-sounding speech. It shows excellent accuracy in pronunciation and excels at producing clean audio with minimal background noise. The model displays good capabilities in understanding contextual nuances and performs well in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/3gukhi7z2j\" title=\"Cartesia: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y3lbes0sin\" title=\"Cartesia: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f0lmkto1xk\" title=\"Cartesia: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"aws-polly\"\u003e\u003cstrong\u003eAWS Polly\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 56.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 55.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.39% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 84.72% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Medium in 58.20% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.1770%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAWS Polly shows very good accuracy in word reproduction, ranking second among the models. It struggles more with producing natural-sounding speech compared to top performers but maintains excellent pronunciation accuracy. The model performs well in producing clean audio and shows good capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qawx1dwc9g\" title=\"Polly: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/klmnfcsno5\" title=\"AWS Polly: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ubzrqtjnc9\" title=\"AWS Polly: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"eleven-labs\"\u003e\u003cstrong\u003eEleven Labs\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 2.8302%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: Medium in 44.98% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 81.97% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 80.27% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 44.70% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 46.25% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs demonstrates the best accuracy in word reproduction among all models. It shows mixed results in producing natural-sounding speech but maintains excellent pronunciation accuracy. The model performs reasonably well in producing clean audio but has more noise issues than some competitors. It shows moderate capabilities in understanding contextual nuances and has room for improvement in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/x3qzhkt0ab\" title=\"Eleven Labs: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kl0lc7oqmz\" title=\"Eleven Labs: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bc19r3iqip\" title=\"Eleven Labs: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"deepgram\"\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 5.6743%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: High in 57.78% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 64.43% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 88.83% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 53.18% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 55.52% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDeepgram shows the highest error rate in word reproduction among all models. It performs moderately well in producing natural-sounding speech and demonstrates good accuracy in pronunciation, although lagging behind some competitors. The model performs well in producing clean audio with minimal background noise but shows moderate capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mngnvgtzx3\" title=\"Deepgram: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6h3bgmcg6d\" title=\"Deepgram: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uaj3x9bln8\" title=\"Deepgram: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"google-tts\"\u003e\u003cstrong\u003eGoogle TTS\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.3574%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Low in 78.01% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 77.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.46% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: Medium in 39.25% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: Low in 45.83% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z27ett9q8m\" title=\"Google TTS: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/06pqi5qc13\" title=\"Google TTS: Wow Did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kwotkto0u6\" title=\"Google TTS: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation reveals that while Eleven Labs leads in transcription accuracy with the lowest WER, OpenAI TTS emerges as the top choice when it comes to human preference, particularly for speech naturalness, pronunciation accuracy, and prosody.\u003c/p\u003e\u003cp\u003eConversely, Google TTS, despite a relatively low WER, ranks last in human preference due to poor performance in categories like speech naturalness and context awareness. This demonstrates the importance of a balanced evaluation that considers both quantitative metrics like WER and qualitative factors like user satisfaction.\u003c/p\u003e\u003cp\u003eAs the TTS field continues to evolve, the key to further advancements will lie in combining high transcription accuracy with natural, expressive, and context-aware speech. Models like OpenAI TTS and Eleven Labs are setting the standard, but there remains room for improvement across all models to fully meet the growing demands of diverse TTS applications.\u003c/p\u003e\u003cp\u003eIn summary, while WER is crucial, the broader context of user experience—focusing on factors like speech naturalness, context awareness, and prosody accuracy—is essential in evaluating the overall effectiveness of TTS models.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-speech models presented here represents a significant advancement in assessing AI-generated speech. By leveraging this methodology alongside Quantumworks Lab's cutting-edge platform, AI teams can dramatically accelerate the development and refinement of sophisticated, domain-specific speech generation models.\u003c/p\u003e\u003cp\u003eOur solution offers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eEfficient dataset curation\u003c/li\u003e\u003cli\u003eAutomated evaluation techniques\u003c/li\u003e\u003cli\u003eHuman-in-the-loop quality assurance\u003c/li\u003e\u003cli\u003eCustomizable workflows for TTS model assessment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-speech model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdohXa2KeUawTMTWS7MfKSUYC6fVPCHjtmcFCRt5ycmB7S4k4-JElTwnmRb6ufW2kR7F3Bqu1PwzbvL83LKcqgz8jbkoY02tefklFLp98wvqm2liOKBglEGaRITIk6Wx1tY3fi-43TaCJnpwEwRVCz7q101?key=Qy_gZn4bA_q73M6n7nwEIA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"1024\"\u003e\u003c/figure\u003e","comment_id":"66bfded301c06500016e8b1c","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-12.12.46-PM.png","featured":false,"visibility":"public","created_at":"2024-08-16T23:20:51.000+00:00","updated_at":"2024-11-26T00:11:38.000+00:00","published_at":"2024-08-17T19:13:25.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/evaluating-leading-text-to-speech-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bff4f801c06500016e8bba","uuid":"e5bf7da8-d960-4a06-9050-b06f63a8a4c0","title":"Metrics-based RAG Development with Quantumworks Lab","slug":"metrics-based-rag-development-with-labelbox","html":"\u003ch1 id=\"overview\"\u003e\u003cstrong\u003eOverview\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOne of the biggest challenges with RAG Application development is evaluating the quality of responses. As it currently stands, there are a series of different benchmark metrics, tools, and frameworks to help with RAG application evaluation.\u003c/p\u003e\u003cp\u003eLLM evaluation tools (such as RAGAS and DeepEval) provide a plethora of quality scores designed to evaluate the efficiency of the RAG model, ranging from retrieval to generation. In this guide, we’ll take a metrics based approach to enhance RAG development by focusing on 2 target metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext recall\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eThis measures (on a scale from 0-1) how well the retrieved content aligns with the Ground Truth Answer\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext precision\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eIdeally, the most relevant context chunks should be retrieved first. This metric measures whether the RAG application can return the most relevant chunks (with respect to the Ground Truth) at the top.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"generating-ground-truth-data-and-%E2%80%98synthetic%E2%80%99-data\"\u003e\u003cstrong\u003eGenerating ground truth data and ‘\u003cem\u003esynthetic’\u003c/em\u003e data\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo calculate performance metrics, we have to compare the RAG response with ground truth. Ground truths are factually verified responses to a given user query and must be linked back to a source document.\u003c/p\u003e\u003cul\u003e\u003cli\u003eOn the other hand, ground truth data can be automatically generated by pre-trained models. LLM Frameworks, such as LangChain, have modules that generate Question / Answer pairs based on a source document. While creating Synthetic ground truth is less labor intensive, drawbacks include quality issues, lacking real-world nuances, and inheriting model bias.\u003c/li\u003e\u003cli\u003eGround truth data is typically manually created by experts through the collection and verification of internal source documents. While labor intensive and slower to collect, manual data labeling optimizes for accuracy and quality.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo start off, we’ll use the Quantumworks Lab Platform to orchestrate a ground truth creation workflow with 2 approaches.\u003c/p\u003e\u003col\u003e\u003cli\u003eManual ground truth generation\u003c/li\u003e\u003cli\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"manual-ground-truth-generation\"\u003e\u003cstrong\u003eManual ground truth generation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eBy uploading common user questions (prompts) to the Quantumworks Lab Platform, we can set up an annotation project. Now subject matter experts can craft a thoughtful answer and provide the appropriate source.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"938\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"synthetic-ground-truth-generation-with-human-in-the-loop\"\u003e\u003cstrong\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1154\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeveraging the latest Foundation Models on Quantumworks Lab Foundry, we can combine an automated ground truth generation process with a Human in the Loop component for quality assurance.\u003c/p\u003e\u003cp\u003eTo start off, we will include the source document for each user query (PDF in this case). We can then prompt a foundation model (Google Gemini 1.5 Pro in this case) for each user question and attach the PDF context needed to answer the question.\u003c/p\u003e\u003cp\u003eThe provided prompt in this case is: *For the given text and PDF attachment, answer the following. Given the attached PDF, generate an answer using the information from the attachment. Respond with I don't know if the PDF attachment doesn't contain the answer to the question.*\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"724\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eNext, we can include the response generated from Gemini \u003cstrong\u003eas pre-labels\u003c/strong\u003e to an annotation project.\u0026nbsp;\u003c/p\u003e\u003cp\u003eInstead of starting from scratch while answering the question, human experts can modify and verify the Gemini response along with the corresponding document and user query, optimizing for quality and efficiency.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1480\" height=\"570\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1480w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"improving-rag\"\u003eImproving RAG\u003c/h1\u003e\u003cp\u003eA RAG based application consists of many components. Summarized in a recent research survey by Gao et al. (2023), a few factors that impact RAG system performance include (but not limited to):\u003c/p\u003e\u003cul\u003e\u003cli\u003eImproving the retriever (e.g. Reranking)\u003c/li\u003e\u003cli\u003eFine-tuning the embedding model for source documents\u003c/li\u003e\u003cli\u003eAdding document metadata tagging (for rerouting or semantic search)\u003c/li\u003e\u003cli\u003eQuery rewriting\u003c/li\u003e\u003cli\u003eOptimizing the chunking strategy\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’ll focus on the following strategies for improving metrics based RAG evaluation:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eOptimizing the chunking strategy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-tuning the embedding model\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"chunking-strategy\"\u003e\u003cstrong\u003eChunking strategy\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhile keeping LLM generation model (GPT Turbo 3.5) and Embeddings Model (sentence-transformers/distiluse-base-multilingual-cased-v2) consistent, we will compare the retrieval metrics derived from RAGAs.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will export the question and response pairs generated from Quantumworks Lab to a Python Environment.\u003c/li\u003e\u003cli\u003eNext we will generate the RAG response based on the following chunking strategies:\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRecursive character splitter\u003c/strong\u003e (500 token size with 150 chunk overlap)\u003cul\u003e\u003cli\u003eThis is a heuristic approach and involves dynamically splitting text based on a set of rules (e.g. new lines / tables / page breaks) while maintaining coherence.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpacy text splitter\u003c/strong\u003e\u003cul\u003e\u003cli\u003espaCy uses a rule-based approach combined with machine learning models to perform tokenization.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnce the RAG response and context have been generated, we create a table with the following fields required for RAGAs metrics evaluation: (Question, Ground Truth, RAG Answer, Context)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"432\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-5.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-5.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eBy calculating RAGAs metrics evaluation using the necessary Query, Response, Ground Truth, and Source information, we can derive performance results for the entire dataset and aggregate the findings. The Context Precision and Context Recall metrics for Recursive and Spacy are shown below:\u003c/p\u003e\u003ch3 id=\"recursive-text-chunking\"\u003e\u003cstrong\u003eRecursive text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1008\" height=\"624\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-4.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png 1008w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"spacy-text-chunking\"\u003e\u003cstrong\u003eSpacy text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1054\" height=\"652\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png 1054w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eFrom the results, we observe that Spacy yielded slightly better context precision. Given the volume of our dataset (~100 QA pairs), the difference is likely statistically insignificant. However, we can conclude that choosing the Chunking strategy and parameters will certainly affect the output of the chatbot.\u003c/p\u003e\u003cp\u003eGiven the distribution of our Recall and Precision metrics, both techniques seem to struggle with the same data rows. This indicates that the key semantics are not captured by our default embeddings (\u003cstrong\u003esentence-transformers/distiluse-base-multilingual-cased-v2\u003c/strong\u003e). We’ll test out another embedding model to see if we can improve RAG performance metrics.\u003c/p\u003e\u003ch2 id=\"embeddings-model\"\u003e\u003cstrong\u003eEmbeddings model\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAn Embedding Model is responsible for generating embeddings, which is a vector representation to any piece of information, such as a chunk of text in a RAG application. A solid embeddings model is able to better capture semantic meaning of a user question and the corpus of source documents, therefore returning the most relevant chunks as the context for answer generation.\u003c/p\u003e\u003cp\u003eWhile holding the chunking strategy constant (Recursive), we will change the underlying embeddings model in the RAG workflow before evaluating with RAGAs. The Huggingface\u003ca href=\"https://huggingface.co/spaces/mteb/leaderboard?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003eLeaderboard\u003c/u\u003e\u003c/a\u003e includes performance benchmarks for various embeddings models. Beyond retrieval performance, model size, latency, and context length are also important when choosing an embeddings model.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will generate responses using the \u003cstrong\u003ebge-base-en-v1.5\u003c/strong\u003e model and evaluate with RAGAs\u003cul\u003e\u003cli\u003eWhile ranking 35 on the leaderboards, this model is extremely lightweight (109 million parameters)\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1096\" height=\"632\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png 1096w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eFrom the results, changing the embeddings model (from distiluse-base-multilingual-cased-v2 to bge-base-en-v1.5) significantly improved the retrieval metrics, around ~10 points for both context recall and precision.\u003cul\u003e\u003cli\u003eThis indicates that the retrieved context chunks are more relevant to the ground truth and ranked higher on average as well.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"fine-tuning-an-embeddings-model\"\u003e\u003cstrong\u003eFine-tuning an embeddings model\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eMost embedding models are trained on general knowledge. Specific domains, such as internal organization information, may limit the effectiveness of these models. Therefore, we can choose to fine-tune an embeddings model with the goal of better understanding and generating domain specific information.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo fine tune embeddings, we need labeled examples of positive and negative context chunks.\u003cul\u003e\u003cli\u003eUsing a training dataset, we’ll upload the user query and the top X chunks to Quantumworks Lab\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing Quantumworks Lab Annotate, subject matter experts can determine if each chunk is relevant in answering the given question.\u003c/li\u003e\u003cli\u003e\u003c/li\u003e\u003cli\u003eUsing the Quantumworks Lab SDK we can now export our labels and convert to the following format for fine tuning: {\"query\": str, \"pos\": List[str], \"neg\":List[str]}\u003cul\u003e\u003cli\u003eThe fine tuning process for the BGE family of models is also described\u003ca href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/examples/finetune/README.md?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing the fine tuned embeddings model, we can generate updated responses and evaluate.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom the example in the screenshot, bge tends to incorrectly extract the citations section, which the expert labelers can correct and mark as irrelevant.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1005\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the resulting metrics, we can see that by fine tuning on 50 data rows is able to improve context recall and context precision by 2-3 points each. We expect this to improve as more labels are provided to continuously fine tune the embeddings model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1098\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image.png 1098w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eEvaluating the quality of responses in developing RAG Applications is essential for creating efficient and reliable systems. By incorporating the Quantumworks Lab platform in metrics based RAG development, from ground truth generation to feedback for embedding models, developers can enhance the overall performance and reliability of RAG applications. \u003c/p\u003e","comment_id":"66bff4f801c06500016e8bba","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.55.12-PM.png","featured":false,"visibility":"public","created_at":"2024-08-17T00:55:20.000+00:00","updated_at":"2024-09-03T20:00:44.000+00:00","published_at":"2024-08-17T01:07:55.000+00:00","custom_excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/metrics-based-rag-development-with-labelbox/","excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6656135f9132db0001f20551","uuid":"f173905b-1c7a-4f87-8064-ac8af0ccdaa7","title":"AI foundations: Understanding embeddings","slug":"ai-foundations-understanding-embeddings","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eEmbeddings transform complex data into meaningful vector representations, enabling powerful applications across various domains.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis post covers the theoretical foundations, practical examples, and demonstrates how embeddings enhance key use cases at Labelbox.\u003c/p\u003e\u003cp\u003eWhether you’re new to the concept or looking to deepen your understanding, this guide will provide valuable insights into the world of embeddings and their practical uses.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"what-is-an-embedding\"\u003eWhat is an embedding?\u003c/h1\u003e\u003cp\u003eAt its core, an embedding is a vector representation of information. This information can be of any modality—video, audio, text, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe process of generating embeddings involves a deep learning model trained on specific data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach index in a vector represents a numerical value, often a floating-point value between 0 and 1.\u003c/p\u003e\u003ch2 id=\"the-relationship-between-dimensions-and-detail\"\u003eThe relationship between dimensions and detail\u003c/h2\u003e\u003cp\u003eThe dimensions of a vector describe the level of detail it can capture. Higher dimensionality means higher detail and accuracy in the similarity score.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, a vector with only two dimensions is unlikely to store all relevant information, leading to simpler but less accurate representations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1362\" height=\"626\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1362w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"vector-representations-of-images\"\u003eVector representations of images\u003c/h2\u003e\u003cp\u003eFor instance, consider an image. The underlying compression algorithm converts this image into a vector representation, preserving all relevant information. These embeddings can then be used to determine the similarity between different pieces of data. For example, images of a cat and a dog would have different embeddings, resulting in a low similarity score.\u003c/p\u003e\u003cp\u003eAnother example: Let's consider a model trained on images of helicopters, planes, and humans. If we input an image of a helicopter, the model generates a vector representation reflecting this. If the image contains both a helicopter and humans, the representation changes accordingly. This process helps in understanding how embeddings are generated and used.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1240\" height=\"618\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1240w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Helicopter or not helicopter?\" captured via embeddings.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"importance-and-applications-of-embeddings\"\u003eImportance and applications of embeddings\u003c/h1\u003e\u003ch2 id=\"why-embeddings-matter\"\u003eWhy embeddings matter\u0026nbsp;\u003c/h2\u003e\u003cp\u003eAs we’ve described in the prior sections, understanding what embeddings are and how they work is crucial for several reasons:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCapturing Semantic Relationships\u003c/strong\u003e: Embeddings help capture complex relationships within data, enabling more accurate predictions and recommendations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Search Capabilities\u003c/strong\u003e: Embeddings facilitate efficient and accurate similarity searches, essential in applications like search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"applications-of-embeddings\"\u003eApplications of embeddings\u003c/h2\u003e\u003cp\u003eEmbeddings are extremely useful in search problems, especially within recommender systems.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHere are a few examples:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImage Similarity\u003c/strong\u003e: Finding images similar to an input image, like searching for all images of dogs based on a sample image.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eText-to-Image Search\u003c/strong\u003e: Using a textual query, such as \"image of a dog,\" to find relevant images.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContent Recommendations\u003c/strong\u003e: Identifying articles or movies similar to ones you've enjoyed, enhancing search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1296\" height=\"874\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1296w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEmbeddings are the essential ingredient to a smooth and enjoyable family movie night via recommendations systems, which can suggest movies similar to ones you've enjoyed in the past.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"how-embeddings-are-generated\"\u003eHow embeddings are generated\u003c/h1\u003e\u003cp\u003eDeep learning models, trained on either generalized or specific datasets, learn patterns from data to generate embeddings. Models trained on specific datasets, like cat images, are good at identifying different breeds of cats but not dogs. Generalized models, trained on diverse data, can identify various entities.\u003c/p\u003e\u003cp\u003eModern models produce embeddings of over 1024 dimensions, increasing accuracy and detail. However, this also raises challenges related to the cost of embedding generation, storage, and computational resources.\u003c/p\u003e\u003cp\u003eSome of the earliest work with creating embedding models included word2vec for NLP and convolutional neural networks for computer vision.\u003c/p\u003e\u003ch2 id=\"the-significance-of-word2vec\"\u003eThe significance of word2vec\u003c/h2\u003e\u003cp\u003eThe introduction of word2vec by Mikolov et al. in 2013 marked a significant milestone in creating word embeddings. Word2vec enabled the capture of semantic relationships between words, such as the famous king-queen and man-woman analogy, demonstrating how vectors can represent complex relationships.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis breakthrough revolutionized natural language processing (NLP), enhancing tasks like machine translation, sentiment analysis, and information retrieval.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWord2vec laid the foundation for subsequent embedding techniques and deep learning models in NLP.\u003c/p\u003e\u003ch2 id=\"equivalent-of-word2vec-for-images\"\u003eEquivalent of word2vec for Images\u003c/h2\u003e\u003cp\u003eFor images, convolutional neural networks (CNNs) serve as the equivalent of word2vec. Models like AlexNet, VGG, and ResNet revolutionized image processing by creating effective image embeddings.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese models convert images into high-dimensional vectors, preserving spatial hierarchies and semantic information.\u0026nbsp;\u003c/p\u003e\u003cp\u003eJust as word2vec transformed text data into meaningful vectors, CNNs transform images into embeddings that capture essential features, enabling tasks like object detection, image classification, and visual similarity search.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k_icon.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"250\" height=\"250\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eImageNet is a large-scale visual database designed for use in visual object recognition research, containing millions of labeled images across thousands of categories. Models like AlexNet, VGG, and ResNet demonstrated the power of CNNs using datasets like ImageNet.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"uploading-custom-embeddings-to-labelbox\"\u003eUploading custom embeddings to Quantumworks Lab\u0026nbsp;\u0026nbsp;\u003c/h2\u003e\u003cp\u003eLabelbox now supports importing and exporting custom embeddings seamlessly, allowing for better integration into workflows. This new feature provides flexibility in how embeddings are utilized, making it easier to leverage embeddings for various applications.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eimport Quantumworks Lab as lb\nimport transformers\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nimport requests\nimport numpy as np\n\n# Add your API key\nAPI_KEY = \"your_api_key_here\"\nclient = lb.Client(API_KEY)\n\n# Get images from a Quantumworks Lab dataset\nDATASET_ID = \"your_dataset_id_here\"\ndataset = client.get_dataset(DATASET_ID)\nexport_task = dataset.export_v2()\nexport_task.wait_till_done()\n\ndata_row_urls = [dr_url['data_row']['row_data'] for dr_url in export_task.result]\n\n# Get ResNet-50 from HuggingFace\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = transformers.ResNetModel.from_pretrained(\"microsoft/resnet-50\")\n\nimg_emb = []\n\nfor url in data_row_urls:\n    response = requests.get(url, stream=True)\n    image = Image.open(response.raw).convert('RGB').resize((224, 224))\n    img_hf = image_processor(image, return_tensors=\"pt\")\n    with torch.no_grad():\n        last_layer = model(**img_hf, output_hidden_states=True).last_hidden_state\n        resnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\n        resnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\n        img_emb.append(resnet_embeddings.cpu().numpy())\n\ndata_rows = []\n\nfor url, embedding in zip(data_row_urls, img_emb):\n    data_rows.append({\n        \"row_data\": url,\n        \"embeddings\": [{\"embedding_id\": new_custom_embedding_id, \"vector\": embedding[0].tolist()}]\n    })\n\ndataset = client.create_dataset(name='image_custom_embedding_resnet', iam_integration=None)\ntask = dataset.create_data_rows(data_rows)\nprint(task.errors)\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eTo compute your ow embeddings and send them to Quantumworks Lab, you can use models from Hugging Face. Here’s a brief example using ResNet-50:\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003cscript src=\"https://gist.github.com/mmbazel-labelbox/edb8efbab1904106009e1ed630199e29.js\"\u003e\u003c/script\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eThis code snippet demonstrates how to create and upload custom embeddings, which can then be used for similarity searches within Labelbox.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1694\" height=\"1176\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1694w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"why-you-should-care-about-custom-embeddings\"\u003eWhy you should care about custom embeddings\u003c/h3\u003e\u003cp\u003eCustom embeddings provide several advantages:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTailored Representations\u003c/strong\u003e: Custom embeddings can be tailored to your specific dataset, improving accuracy in domain-specific tasks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Performance\u003c/strong\u003e: They allow for more precise similarity searches and recommendations by capturing nuances specific to your data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFlexibility\u003c/strong\u003e: Custom embeddings offer flexibility in handling various types of data and use cases, making them a versatile tool in machine learning workflows.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"using-embeddings-for-better-search\"\u003eUsing embeddings for better search\u003c/h1\u003e\u003cp\u003eEarlier we mentioned that search (a core activity of search engines, recommendation systems, data curation, etc) is a common and important application of embeddings. How? By comparing how similar two (or many more) items are to each other.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"measuring-similarity\"\u003eMeasuring similarity\u003c/h2\u003e\u003cp\u003eThe most popular algorithm for measuring similarity between two vectors is cosine similarity. It calculates the angle between two vectors: the smaller the angle, the more similar they are. Cosine similarity scores range from 0 to 1, or -1 to 1 for dissimilarity.\u003c/p\u003e\u003cp\u003eFor example, if vectors W and V are close, their cosine similarity might be 0.79. If they are opposite, the similarity is lower, indicating dissimilarity. This method helps in visually and computationally understanding the similarity between different assets.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J0ha8suIAyD2qrFVEWI9TqDRLb2c8ForgqTmyvKQ8ZKsXLeHf5H96V0tQSMV_bCbqBsLWcBb3ljxSP8vKYDKElnlTv1zHq36uQG7mwzyP9HELFlHgGf7FTZO8AWH_ndg5OAXm3yLmbbEpVPg1Cvp108\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"718\" height=\"521\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOriginal source: \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Similarity-between-vectors-can-be-estimated-by-calculating-the-cosine-of-the-angle-th_fig1_333734049?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Similarity between vectors can be estimated by calculating the cosine of the angle θ between them.\"\u003c/span\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBecause embeddings are essentially vectors, we can apply many of the same operations used to analyze vectors to embeddings.\u003c/p\u003e\u003ch3 id=\"strategies-for-similarity-computation\"\u003eStrategies for similarity computation\u003c/h3\u003e\u003cp\u003eAdditional strategies for performing similarity computation include:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eBrute Force Search\u003c/strong\u003e: Comparing one asset against every other asset. This provides the highest accuracy but requires significant computational resources and time. For instance, running brute force searches on a dataset with millions of entries can lead to significant delays and high computational costs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eApproximate Nearest Neighbors (ANN)\u003c/strong\u003e: Comparing one asset against a subset of assets. This method reduces computational resources and latency while maintaining high accuracy. ANN algorithms like locality-sensitive hashing (LSH) and KD-trees provide faster search times, making them suitable for real-time applications, although they may sacrifice some accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHashing Methods\u003c/strong\u003e: Techniques like MinHash and SimHash provide efficient similarity searches by hashing data into compact representations, allowing quick comparison of hash values to estimate similarity. These methods are particularly useful for text and document similarity.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTree-based methods (KD-trees, R-trees, and VP-trees) and graph-based methods (like Hierarchical Navigable Small World, also known as HNSW) are also options.\u003c/p\u003e\u003ch2 id=\"leveraging-embeddings-for-data-curation-management-and-analysis-in-labelbox\"\u003eLeveraging embeddings for data curation, management, and analysis in Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThrough search, embeddings improve the efficiency and accuracy of data curation, management, and analysis in several ways:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eAutomated Data Organization\u003c/strong\u003e: Embeddings help group similar data points together, making it easier to organize and manage large datasets.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Search and Retrieval\u003c/strong\u003e: By transforming data into embeddings, search and retrieval operations become faster and more accurate, enabling quick access to relevant information.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Data Analysis\u003c/strong\u003e: Embeddings capture underlying patterns and relationships within data, facilitating more effective analysis and insights extraction.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAnd these are exactly the ways we use embeddings in our products at Quantumworks Lab to \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimprove search for data\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, in \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e, users can find images similar to an input image, such as searching for basketball players on a court. Our natural language search allows users to input a textual query, like \"dog,\" and find relevant images.\u003c/p\u003e\u003cp\u003eLabelbox offers several features to streamline embedding workflows, including:\u003c/p\u003e\u003ch3 id=\"data-curation-and-management\"\u003eData curation and management\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/custom-embeddings?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCustom Embeddings\u003c/u\u003e\u003c/a\u003e in Catalog: Surfaces high-impact data, with automatic computation for various data types.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/manage-selections?ref=labelbox-guides.ghost.io#smart-select\"\u003e\u003cu\u003eSmart Select\u003c/u\u003e\u003c/a\u003e in Catalog: Curates data using random, ordered, and cluster-based methods.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/cluster-view?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCluster View\u003c/u\u003e\u003c/a\u003e: Discovers similar data rows and identifies labeling or model mistakes.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1620\" height=\"838\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1620w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Cluster view to explore relationships between data rows, identify edge cases, and select rows for pre-labeling or human review.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eNatural Language\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSimilarity Search\u003c/u\u003e\u003c/a\u003e: Supports video and audio assets.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-metrics?ref=labelbox-guides.ghost.io#automatic-metrics\"\u003e\u003cu\u003ePrediction-level Custom Metrics\u003c/u\u003e\u003c/a\u003e: Filters and sorts by custom metrics, enhancing model error analysis.\u003c/li\u003e\u003cli\u003ePre-label Generation from \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFoundry\u003c/u\u003e\u003c/a\u003e Models: Streamlines project workflows.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/video-annotations?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnhanced Video Player\u003c/u\u003e\u003c/a\u003e: Aids in curating, evaluating, and visualizing video data.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1630\" height=\"904\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1630w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWould a rose smell as sweet if it was merely one of a thousand easily discovered in a flower dataset using a natural language search? Trick question: you can't smell photos. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"embeddings-schema\"\u003eEmbeddings schema\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/working-with-features?ref=labelbox-guides.ghost.io#create-features-from-the-schema-tab\"\u003e\u003cu\u003eSchema Tab\u003c/u\u003e\u003c/a\u003e: Includes an Embeddings subtab for viewing and creating custom embeddings.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"api-and-inferencing\"\u003eAPI and inferencing\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/run-foundry-apps?ref=labelbox-guides.ghost.io#run-app-using-rest-api\"\u003e\u003cu\u003eInferencing Endpoints\u003c/u\u003e\u003c/a\u003e: Generate predictions via REST API without creating data rows or datasets.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"additional-workflow-enhancements\"\u003eAdditional workflow enhancements\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBoost Express: Request additional labelers for data projects.\u003c/li\u003e\u003cli\u003eExport v2 Workflows and Improved SDK: Streamlines data management.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese features ensure that embeddings are utilized effectively, making your workflows more efficient and your models more accurate.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eEmbeddings are a powerful tool in machine learning, enabling efficient and accurate similarity searches and recommendations. We hope this post has clarified what embeddings are and how they can be utilized.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor more on uploading custom embeddings, check out our \u003ca href=\"https://docs.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/page/tutorials?ref=labelbox-guides.ghost.io\"\u003edeveloper guides\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAdditional resources on embeddings can be found here:\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jbwzrxh814\" title=\"Embeddings Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWatch the video: \u003ca href=\"https://labelbox.wistia.com/medias/jbwzrxh814?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eUnderstanding embeddings in machine learning\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eLearn more in our community about:\u003cu\u003e \u003c/u\u003e\u003ca href=\"https://community.labelbox.com/t/how-to-upload-custom-embedding-s-and-why-you-should-care/1169?ref=labelbox-guides.ghost.io#why-would-i-need-embedding-2\"\u003e\u003cu\u003eHow to upload custom embeddings and why you should care\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThanks to our contributors Tomislav Peharda, Paul Tancre, and Mikiko Bazeley! \u003c/p\u003e","comment_id":"6656135f9132db0001f20551","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/05/YT-Thumbnails--23-.jpg","featured":false,"visibility":"public","created_at":"2024-05-28T17:24:47.000+00:00","updated_at":"2024-09-04T18:02:27.000+00:00","published_at":"2024-05-28T20:32:29.000+00:00","custom_excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},"url":"https://labelbox-guides.ghost.io/ai-foundations-understanding-embeddings/","excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65c97ca417c84d0001395ccb","uuid":"ed465aa9-c5cf-4c1e-bf18-fa38cc037a18","title":"Distilling a faster and smaller custom LLM using Google Gemini","slug":"end-to-end-workflow-for-knowledge-distillation-with-nlp","html":"\u003cp\u003eThe race to both mimic and create competitor models to OpenAI’s GPT3.5 energized the interest in model compression and quantization techniques.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKnowledge distillation, \u003c/strong\u003ealso known as \u003cstrong\u003emodel distillation,\u003c/strong\u003e is one of many techniques that have grown in popularity and importance in enabling small teams to leverage foundation models in developing small (but mighty) custom models, used in intelligent applications.\u003c/p\u003e\u003cp\u003eIn “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, we illustrated some of the conceptual foundations for how model distillation works as well as why we even need smaller models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe also provided an in-depth guide with a worked example in the second part of our series,\u0026nbsp;“\u003ca href=\"https://labelbox.com/guides/end-to-end-workflow-with-model-distillation-for-computer-vision/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnd-to-end workflow with model distillation for computer vision\u003c/u\u003e\u003c/a\u003e”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eNow we turn our attention to demonstrating the flexibility and power of model distillation in another domain and use case, where increased efficiency through supervised training of a smaller model by a foundation model is necessary.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll demonstrate an end-to-end workflow for natural language processing, using model distillation to fine-tune a BERT model with labels created in Model Foundry using \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGoogle Gemini\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll show how easy it is to go from raw data to cutting-edge models, customized to your use case, using a \u003ca href=\"https://huggingface.co/datasets/SetFit/emotion?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esentiment dataset\u003c/u\u003e\u003c/a\u003e (additional public datasets can be found \u003ca href=\"https://labelbox.com/datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e or on sites like Kaggle or HuggingFace).\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn less than 30 min you’ll learn how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIngest\u003c/strong\u003e, \u003cstrong\u003eexplore\u003c/strong\u003e and \u003cstrong\u003eprepare\u003c/strong\u003e the sentiment dataset;\u003c/li\u003e\u003cli\u003ePick and configure \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany text-based foundation model\u003c/u\u003e\u003c/a\u003e to \u003cstrong\u003eautomatically label\u003c/strong\u003e data with just a few clicks;\u003c/li\u003e\u003cli\u003eExport the labeled predictions to a project as a potential set-up for \u003cstrong\u003emanual evaluation\u003c/strong\u003e;\u003c/li\u003e\u003cli\u003eUse the labeled predictions dataset to \u003cstrong\u003efine-tune a student model\u003c/strong\u003e using a cloud-based notebook provider;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEvaluate performance\u003c/strong\u003e of the fine-tuned student model.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAt the end of the tutorial we’ll also discuss advanced considerations in scaling your models up and out, such as automating data ingestion and labeling, and resources for incorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e\u0026nbsp; into your workflows.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-use-model-distillation-to-fine-tune-a-small-task-specific-language-model\"\u003eSee it in action: How to use model distillation to fine-tune a small, task-specific Language Model\u003c/h3\u003e\u003cp\u003e\u003cem\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab\u0026gclid=Cj0KCQiA7OqrBhD9ARIsAK3UXh1OL7LeJadsNvbP_43MnwLSpLlRPD0IkilIwyvf3VwKOzxQ5ioGon8aApXREALw_wcB\u0026referrer_url=https://www.google.com/\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial. You’ll also need to \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003ecreate API keys for accessing the SDK\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eNotebook: \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=1FMXvdze28AZ\"\u003e\u003cu\u003eText Bert Model Distillation\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003ch3 id=\"the-model-distillation-workflow-for-nlp\"\u003eThe Model Distillation Workflow for NLP\u003c/h3\u003e\u003cp\u003eIn our prior post on model distillation concepts, we discussed the different model distillation patterns, based on the following criteria:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe type of \u003cstrong\u003eTeacher\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eStudent\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eknowledge\u003c/strong\u003e being transferred from teacher to student model(s) (\u003cstrong\u003eresponse-based\u003c/strong\u003e,\u003cstrong\u003e feature-based\u003c/strong\u003e, \u003cstrong\u003erelation-based\u003c/strong\u003e knowledge);\u003c/li\u003e\u003cli\u003eThe cadence and scheme for how the student mode is trained (\u003cstrong\u003eoffline\u003c/strong\u003e, \u003cstrong\u003eonline\u003c/strong\u003e, \u003cstrong\u003eself\u003c/strong\u003e).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e“Benefits of Using Model Distillation”\u003c/strong\u003e, Source: “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, Fig \u003cstrong\u003e3.1\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll be demonstrating the most popular and easiest pattern to get started with: \u003cstrong\u003eoffline, response-based model\u003c/strong\u003e (or \u003cstrong\u003eknowledge\u003c/strong\u003e) \u003cstrong\u003edistillation\u003c/strong\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe teacher model we’ll be using to produce the responses is \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGoogle \u003c/u\u003eGemini\u003c/a\u003e and the student model is \u003ca href=\"https://huggingface.co/docs/transformers/en/model_doc/bert?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBERT\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://huggingface.co/distilbert/distilbert-base-uncased?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edistilbert-base-uncased\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs you’ll see, we could have chosen \u003ca href=\"https://labelbox.com/guides/how-to-fine-tune-large-language-models-with-labelbox/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany combination of teacher or student models\u003c/u\u003e\u003c/a\u003e, because the \u003cstrong\u003eoffline, response-based\u003c/strong\u003e pattern of model distillation is incredibly flexible.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen implementing this process for your own use case, it’s important to understand the relative strengths and weaknesses of each model and match them according to your requirements and use cases (whether it’s \u003ca href=\"https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edetecting and removing PII to be GDPR compliant\u003c/u\u003e\u003c/a\u003e or \u003ca href=\"https://labelbox.com/guides/how-to-build-a-content-moderation-model-to-detect-disinformation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edetecting unsavory content\u003c/u\u003e\u003c/a\u003e).\u003c/p\u003e\u003ch3 id=\"using-the-labelbox-platform-to-automate-model-distillation\"\u003eUsing The Quantumworks Lab Platform To Automate Model Distillation\u0026nbsp;\u003c/h3\u003e\u003cp\u003eLabelbox is the data factory for genAI, providing an end-to-end solution for curating, transforming, annotating, evaluating and orchestrating unstructured data for data science and machine learning.\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab platform supports the development of intelligent applications using the model distillation and fine-tuning workflow enabling AI developers to easily:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImport, curate, filter and eventually select the text that will be labeled for use in student model training and fine-tuning with \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e. Additionally you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/model-foundry-automating-data-tasks-with-foundation-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomate the labeling and annotation of the original text dataset using foundation models\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eor any pre-trained model, including custom models\u003c/u\u003e\u003c/a\u003e) in \u003ca href=\"https://labelbox.com/guides/guide-to-using-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e. The ability to leverage a variety of open source or third-party models to accelerate pre-labeling can cut labeling costs by up to 90% for teams with existing labeling initiatives (and kickstart new and budget constrained AI developers operating without\u0026nbsp; manual labeling capabilities).\u003c/li\u003e\u003cli\u003eLeverage \u003ca href=\"https://labelbox.com/customers/intuitive-surgical-customer-story/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman-in-the-loop\u003c/u\u003e\u003c/a\u003e evaluation (and \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e) through the Foundry to Annotate integration, so labels can be reviewed before being used for training or fine-tuning.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIntegrate with some of the most common cloud providers like \u003ca href=\"https://learn.labelbox.com/accelerate-retail-and-ecommerce-ai-roi/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Cloud Platform\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://labelbox.com/company/partnerships/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eDatabricks\u003c/u\u003e\u003c/a\u003e for additional MLOps services.\u003c/li\u003e\u003cli\u003eOrchestrate and schedule future automated labeling and model runs as \u003ca href=\"https://labelbox.com/blog/10x-faster-uploads-with-labelboxs-new-ingestion-upgrades/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enew data flows into the Quantumworks Lab platform\u003c/u\u003e\u003c/a\u003e through Model Apps (including \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emultimodal data labeling\u003c/u\u003e\u003c/a\u003e). \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"introduction-to-data-preparation-for-natural-language-processing-with-catalog\"\u003eIntroduction To Data Preparation for Natural Language Processing With Catalog\u003c/h2\u003e\u003cp\u003eBefore beginning the tutorial:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCreate a free HuggingFace account (in order to access the \u003ca href=\"https://huggingface.co/datasets/SetFit/emotion?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003e\"Setfit/emotion\" dataset\u003c/u\u003e\u003c/a\u003e)\u003cul\u003e\u003cli\u003eDownload the dataset locally\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab\u0026gclid=Cj0KCQiA7OqrBhD9ARIsAK3UXh1OL7LeJadsNvbP_43MnwLSpLlRPD0IkilIwyvf3VwKOzxQ5ioGon8aApXREALw_wcB\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eCreate a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e (in order to \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecreate a free API key\u003c/u\u003e\u003c/a\u003e)\u003cul\u003e\u003cli\u003eNavigate to “\u003cstrong\u003eCatalog\u003c/strong\u003e” in the sidebar\u003c/li\u003e\u003cli\u003eSelect “\u003cstrong\u003e+New\u003c/strong\u003e”\u003c/li\u003e\u003cli\u003eUpload the dataset from Kaggle\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNote\u003c/strong\u003e: The easiest method is to use \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#create-a-dataset-via-the-app-ui\"\u003e\u003cu\u003ethe Quantumworks Lab Web UI\u003c/u\u003e\u003c/a\u003e\u0026nbsp; to manually upload your dataset.\u0026nbsp;\u003cul\u003e\u003cli\u003eIf your goal is to scale the data ingestion process for future labeling or data refreshes, check out our SDK.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’re able to see your dataset in Quantumworks Lab Catalog, you’ll be able to do the following:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e for more granular data curation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003eFor additional details on how to use Catalog to enable data selection for downstream data-centric workflows (such as data labeling, model training, model evaluation, error analysis, and active learning), check out \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour documentation\u003c/u\u003e\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"using-a-large-nlp-model-or-llm-to-generate-and-distill-predictions-for-fine-tuning\"\u003eUsing A Large NLP Model Or LLM To Generate And Distill Predictions For Fine-Tuning\u003c/h2\u003e\u003cp\u003eThe first step of model distillation is to \u003cstrong\u003eidentify an appropriate teacher model\u003c/strong\u003e, which will be used to produce responses that, when combined with the original text, will serve as the \u003cstrong\u003efine-tuning dataset for the student model\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResponse-based\u003c/strong\u003e model distillation is powerful because it can be used even when access to the original model weights is limited (or the model is so big that downloading a copy of the model would take a really long time). Response-based distillation also doesn’t require the user to have trained the model themselves; just that the model was pre-trained.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox allows you to pick any of the \u003ca href=\"https://labelbox.com/blog/6-key-llms-to-power-your-text-based-ai-applications/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecurrently hosted, state-of-the-art models to use\u003c/u\u003e\u003c/a\u003e (as well as upload your own custom models) to use as the teacher model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor now, let’s get started with preparing the text we’ll be labeling, or generating predictions with, using Google Gemini. The combination of text and label pairs will be used for BERT.\u003c/p\u003e\u003ch3 id=\"step-1-select-text-assets-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select text assets and choose a foundation model of interest\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1002\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePreview of text in Quantumworks Lab Catalog\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to your uploaded Emotions dataset in Catalog.\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the text on which the predictions should be made.e.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “\u003cstrong\u003ePredict with Model Foundry\u003c/strong\u003e”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run (in this case \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGemini\u003c/u\u003e\u003c/a\u003e).\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task -\u0026nbsp; such as \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003etext classification, summarization, and text generation\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eTo locate a \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003especific model\u003c/u\u003e\u003c/a\u003e, you can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003eStep 2: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eWhen developing ML based applications, developers need to quickly and iteratively prepare and version training data, launch model experiments, and use the performance metrics to further refine the input data sources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe performance of a model can vary wildly depending on the data used, the quality of the annotations, and even the model architecture itself. A necessary requirement for replicability is being able to see the exact version of all the artifacts used or generated as a result of an experiment.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox will snapshot the experiment, the data artifacts as well as the trained model, as a saved process known as a \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis includes the types of items the model is supposed to identify and label, known as an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eontology\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of settings, which you can find in the Advanced model setting.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce you’ve located Gemini, you can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIn this case, we will enter the following prompt:\u0026nbsp;\u003cul\u003e\u003cli\u003e\u003cem\u003eFor the given text, answer the following. Classify emotions, pick one of the options: [sadness, joy, love, anger, fear, surprise]. Return the result as a JSON object. {\"emotions\" : \"\"}.\u0026nbsp;\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis prompt is designed to facilitate responses from the model with one of the following: \u003cem\u003esadness, joy, love, anger, fear, surprise\u003c/em\u003e.\u003c/p\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-review-predictions-in-the-model-tab\"\u003eStep 3: Review predictions in the Model tab \u003c/h3\u003e\u003cp\u003eBecause each \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run is submitted with a unique name\u003c/u\u003e\u003c/a\u003e, it’s easy to distinguish between each subsequent model run.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/foundry-view-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eView prediction results\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results \u003ca href=\"https://docs.labelbox.com/docs/comparing-model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eacross a variety of model runs\u003c/u\u003e\u003c/a\u003e different models\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-2.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-2.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eView predictions in Model tab for the model run\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-4.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-4.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-4.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eIn this case, Gemini Pro predicted this text to be \"joy\"\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThese generated labels are now ready to be used for fine-tuning the BERT student model.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-enriching-and-evaluating-predictions-using-human-in-the-loop-and-annotate\"\u003eStep 4: Enriching and evaluating predictions using human-in-the-loop and Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAlthough fine-tuning a foundation model requires less data than pre-training a large foundation model from scratch, the data (specifically the labels) need to be high-quality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEven big, powerful foundation models make mistakes or miss edge cases.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou might also find that there are additional categories that the parent model didn’t identify correctly because the ontology was incomplete.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce a parent model like Gemini has been used for the initial model-assisted labeling run, those predictions \u003ca href=\"https://docs.labelbox.com/docs/foundry-annotate-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then be sent\u003c/u\u003e\u003c/a\u003e to a \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eproject\u003c/u\u003e\u003c/a\u003e, a container \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhere all your labeling processes happen\u003c/u\u003e\u003c/a\u003e. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bpsrmes4da\" title=\"Distilling a faster and smaller LLM using BERT and Gemini_SendToAnnotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eIn this case, we feel fairly confident\u003cstrong\u003e \u003c/strong\u003ein how well Gemini performed so we’ll send the inferences to the corresponding Quantumworks Lab project and treat them as the ground truth that the student model will be fine-tuned on. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"fine-tuning-the-student-model-bert\"\u003eFine-Tuning The Student Model (BERT)\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWe’ve shown the first half of the model distillation to fine-tuning workflow.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe identified the categories we wanted the parent model (Gemini) to detect and label in the form of an ontology.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe used Gemini to automatically label the texts as \u003cem\u003e\"sadness\"\u003c/em\u003e or \u003cem\u003e\"fear\"\u003c/em\u003e (for example).\u003c/li\u003e\u003cli\u003eWe exported the generated labels to a Quantumworks Lab project, at which point we could review the labels manually and enrich them further using the Quantumworks Lab editor.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe next step is to use the generated labels, along with the original texts, to fine-tune a student model in Colab.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: You’ll now need the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=cclwx_KmJr3Z\u0026line=3\u0026uniqifier=1\"\u003e\u003cem\u003e\u003cu\u003eAPI keys from earlier\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to follow along with the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=1FMXvdze28AZ\"\u003e\u003cem\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003ch3 id=\"step-5-fetch-the-ground-truth-labels-from-the-project-via-labelbox-sdk\"\u003eStep 5: Fetch the ground truth labels from the project via Quantumworks Lab SDK\u003c/h3\u003e\u003cp\u003eFor brevity, we’ve omitted the surrounding code samples but you can copy or run the corresponding blocks in the provided notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our documentation to find out all the ways you can automate the model lifecycle (\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eincluding labeling\u003c/u\u003e\u003c/a\u003e) using our SDK. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore fine-tuning the BERT student model, we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=Jq-1tQs2QBrj\"\u003e\u003cu\u003eneed to fetch the generated labels\u003c/u\u003e\u003c/a\u003e from the Quantumworks Lab project using the Quantumworks Lab SDK as well as the \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=jKMysiHvUI1D\"\u003e\u003cu\u003eground truth\u003c/u\u003e\u003c/a\u003e.\u0026nbsp; \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-6-additionally-text-data-processing\"\u003eStep 6: Additionally Text Data Processing \u003c/h3\u003e\u003cp\u003eThere's additional processing that needs to happen, which we walk through below.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNext we’ll ensure the labels are \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=HFiQMbcSQSks\"\u003e\u003cu\u003eexported into a .csv file\u003c/u\u003e\u003c/a\u003e that contains two columns, the original ‘text’ and the generated ‘label’.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=B2_znE_XHbvG\u0026line=2\u0026uniqifier=1\"\u003e\u003cu\u003eread the csv file into a pandas dataframe\u003c/u\u003e\u003c/a\u003e, perform a series of aggregation operations to help us splits the text into train and test sets based on the category count.\u003c/li\u003e\u003cli\u003eWe’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=k9eNSPuLRAJx\"\u003e\u003cu\u003einitialize a tokenizer\u003c/u\u003e\u003c/a\u003e and encode the train and test texts.\u003c/li\u003e\u003cli\u003eFinally we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=PnHQrghbQ7tD\" rel=\"noreferrer\"\u003efinish creating the training \u0026amp; validation dataset\u003c/a\u003e. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNote\u003c/strong\u003e: Expand the “Export labels into .CSV file” block in the Colab notebook for the full code sample.\u0026nbsp; \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-7-fine-tune-student-bert-model-using-labels-generated-by-google-gemini\"\u003eStep 7: Fine tune student BERT model using labels generated by Google Gemini\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn Colab we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=gjMPaBgAIAGd\u0026line=2\u0026uniqifier=1\" rel=\"noreferrer\"\u003einstantiate a BERT model\u003c/a\u003e and \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=cmbMl5wXIB3I\u0026line=6\u0026uniqifier=1\" rel=\"noreferrer\"\u003etrain it using the data\u003c/a\u003e, which includes both text and labels. Specifically we'll fine-tune a text classifier model called \u003cem\u003e“distilbert-base-uncased”\u003c/em\u003e to classify text as one of the following categories in the ontology: \u003cem\u003esadness, joy, love, anger, fear, surprise\u003c/em\u003e. \u003c/li\u003e\u003cli\u003eWe’ll also \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=VzlNe83KRh98\"\u003esave the model\u003c/a\u003e and \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=RbPplpJdRuXL\"\u003etest the prediction\u003c/a\u003e.\u003cul\u003e\u003cli\u003eBy saving the model (or every model we create) we have the option of A/B testing models and using the models for downstream use cases (as well as share the models with other key stakeholders through a model registry, like MLFlow). \u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1339\" height=\"716\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 1339w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1262\" height=\"344\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 1262w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-8-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 8: Create a model run with predictions and ground truth\u003c/h3\u003e\u003cp\u003eOftentimes the initial training or fine-tuning step isn’t the final stop on the journey of developing a model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOne of the biggest differences between the traditional method of training models in the classroom versus the real-world is how much control you have over the quality of your data, and consequently the quality of the model produced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we mentioned earlier, developers can upload predictions and use the Model product to \u003ca href=\"https://labelbox.com/blog/how-to-confidently-compare-test-and-evaluate-models-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ediagnose performance issues with models and compare them across multiple experiments\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDoing so automatically populates model metrics that make it easy to evaluate the model’s performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e \u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, you’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=mjDaoyV3L3l1\"\u003egrab the model’s ID\u003c/a\u003e to \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=HsDr_g3qqUFo\"\u003ecreate a new model run\u003c/a\u003e (if needed).\u003c/li\u003e\u003cli\u003eThen you’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=K8D7lE9nrOLG\"\u003eget the ground truth from your project\u003c/a\u003e via the export as well as the \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=-cv2GJ6zrU5A\"\u003elabel IDs from ground truth\u003c/a\u003e. \u003c/li\u003e\u003cli\u003eNext you’ll create the predictions by \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=qxwq7kKZrfzL\"\u003erunning the fine-tuned BERT model on the original text assets\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eYou \u003ca href=\"https://docs.labelbox.com/docs/import-ground-truth?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then upload the ground truth labels\u003c/u\u003e\u003c/a\u003e (the labels generated by Gemini and used as the fine-tuning dataset) and the \u003ca href=\"https://docs.labelbox.com/docs/upload-model-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003epredictions\u003c/u\u003e\u003c/a\u003e from the fine-tuned BERT model \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=i7Xq-fldsiRY\"\u003eto the corresponding Quantumworks Lab model\u003c/a\u003e. \u003c/li\u003e\u003cli\u003eYou can see an example of how model metrics are automatically populated by Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"997\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-10.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-10.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-10.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-10.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAuto generated metrics\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"evaluating-model-performance\"\u003eEvaluating Model Performance\u003c/h2\u003e\u003cp\u003eThere’s no single metric to rule them all \u003ca href=\"https://labelbox.com/blog/gpt4-vs-palm-assessing-performance-of-llm-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhen evaluating how your fine-tuned LLM performs\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBoth qualitative and quantitative measures must be considered, combined with sampling and manual review.\u003c/p\u003e\u003cp\u003eWith that being said, Model offers a number of the most common out-of-the-box. With the \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io#metrics-view\"\u003e\u003cu\u003e‘Metrics view’\u003c/u\u003e\u003c/a\u003e users can drill into crucial model metrics, such as \u003ca href=\"https://docs.labelbox.com/docs/upload-model-metrics-1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econfusion matrix, precision, recall, F1 score, false positive, and more\u003c/u\u003e\u003c/a\u003e, to surface model errors.\u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive, which means you can click on any chart or metric to immediately open up the gallery view of the model run and see corresponding examples, as well as visually compare model predictions between multiple model runs. \u003c/p\u003e\u003ch3 id=\"step-9-evaluate-predictions-from-different-bert-model-runs-in-labelbox-model\"\u003eStep 9: Evaluate predictions from different BERT model runs in Quantumworks Lab Model \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBERT fine tuned on labels created by Gemini Pro vs ground truth labels \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to “\u003cstrong\u003eModel\u003c/strong\u003e”\u003c/li\u003e\u003cli\u003eIn this case, we fine-tuned two models, one using 1000 ground truth labels and the other with 1000 labels generated by the Gemini model. We see very similar results and leveraging an off the shelf model is almost as good as using ground truth labels.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"examples-of-predictions-from-fine-tuned-bert-model\"\u003e\u003cstrong\u003eExamples of predictions from fine tuned BERT model\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eHow does our fine-tuned model perform? \u003c/p\u003e\u003cp\u003eLet's manually inspect a few examples of predictions from the fine-tuned BERT model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-12.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-12.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-12.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-12.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “anger”.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-11.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-11.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-11.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-11.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “joy”.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “fear”.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"advanced-considerations\"\u003eAdvanced Considerations\u003c/h2\u003e\u003cp\u003eIn this step-by-step walkthrough, we’ve shown how anyone with any text-based dataset can leverage an LLM to label, fine-tune and analyze a smaller but mighty custom model.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"610\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e“Leveraging FMOps To Develop intelligent Applications”, Source: “\u003c/span\u003e\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eA pragmatic introduction to model distillation for AI developers\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e”, Fig 5.2.4\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAdditional considerations users should address for scaling similar projects include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCollecting feedback \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efrom users \u0026amp; human SME’s to improve\u003c/u\u003e\u003c/a\u003e the fine-tuning dataset quality on a continuous basis, including error analysis and \u003ca href=\"https://docs.labelbox.com/docs/llm-human-preference?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman preference modeling\u003c/u\u003e\u003c/a\u003e;\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003eStrategic planning for incorporating multiple data modalities besides text, including image, audio, and video;\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomating future data ingestion, curation, enrichment, and labeling when the fine-tuned model needs to be retrained due to drift via a \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erobust SDK\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eEasy-to-use, user interface that can be customized for various modalities of data when multiple users are involved (as well as \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM data generation\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eIn this tutorial we demonstrated an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language processing\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a BERT model with labels created in Model Foundry using \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Gemini\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHopefully you were able to see how easy it is to go from raw data to cutting-edge custom models in less than 30 min.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou learned how the Quantumworks Lab platform enables model distillation by allowing developers to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIngest, explore and prepare text-based datasets using \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eUse any \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM\u003c/u\u003e\u003c/a\u003e to automatically label data using \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e as well as how to incorporate human-in-the-loop evaluation using \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eExport these labeled predictions to a cloud-based training environment for fine-tuning;\u003c/li\u003e\u003cli\u003eAutomate the various workflows using the \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox SDK\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eEvaluate model performance and analyze model errors using \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox Model\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you’re interested in learning more about model distillation, check out the previous posts in this series: “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, “\u003ca href=\"https://labelbox.com/guides/end-to-end-workflow-with-model-distillation-for-computer-vision/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnd-to-end workflow with model distillation for computer vision\u003c/u\u003e\u003c/a\u003e”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLooking to implement a production-ready model distillation and fine-tuning in your organization but not sure how to get started leveraging your unstructured data?\u0026nbsp;\u003c/p\u003e\u003cp\u003eAsk \u003ca href=\"https://community.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour community\u003c/u\u003e\u003c/a\u003e or reach out to \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour solutions engineers\u003c/u\u003e\u003c/a\u003e!\u003c/p\u003e","comment_id":"65c97ca417c84d0001395ccb","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/distilling.png","featured":false,"visibility":"public","created_at":"2024-02-12T02:04:20.000+00:00","updated_at":"2024-11-20T23:09:07.000+00:00","published_at":"2024-02-15T19:25:02.000+00:00","custom_excerpt":"Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"681516a56e02120001d5ab84","name":"#mikiko","slug":"hash-mikiko","description":null,"feature_image":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/404/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"65bd4ff882e5680001e078f9","name":"Model Distillation","slug":"model-distillation","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/tag/model-distillation/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/end-to-end-workflow-for-knowledge-distillation-with-nlp/","excerpt":"Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. ","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65bd22d782e5680001e07877","uuid":"3e5a16b6-f3f5-494e-bb8b-4162abb3e830","title":"End-to-end workflow with model distillation for computer vision","slug":"end-to-end-workflow-with-model-distillation-for-computer-vision","html":"\u003cp\u003eModel distillation, also known as \u003cstrong\u003eknowledge distillation\u003c/strong\u003e, is a technique that focuses on creating efficient models by transferring knowledge from large, complex models to smaller, deployable ones.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel distillation is one of many techniques that have grown in popularity and importance in enabling small teams to leverage foundation models in developing small (but mighty) custom models, used in intelligent applications.\u003c/p\u003e\u003cp\u003eIn \u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e“A Pragmatic Introduction to Model Distillation for AI Developers\u003c/u\u003e\u003c/a\u003e”, we illustrated some of the conceptual foundations for how model distillation works.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe described how distillation can be leveraged in any domain or data modality requiring efficiency and model optimization, whether the use case is \u003cu\u003ecomputer vision or NLP related\u003c/u\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll demonstrate an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003ecomputer vision\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a YOLO model with labels created in Model Foundry using Amazon Rekognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll show how easy it is to go from raw data to cutting-edge models, customized to your use case, using a \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efashion products dataset\u003c/u\u003e\u003c/a\u003e (additional public datasets can be found \u003ca href=\"https://labelbox.com/datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e or on sites like Kaggle).\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn less than 30 min you’ll learn how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIngest\u003c/strong\u003e, \u003cstrong\u003eexplore\u003c/strong\u003e and \u003cstrong\u003eprepare\u003c/strong\u003e the products fashion dataset;\u003c/li\u003e\u003cli\u003ePick and configure\u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e any image based foundation model\u003c/u\u003e\u003c/a\u003e to \u003cstrong\u003eautomatically label\u003c/strong\u003e data with just a few clicks;\u003c/li\u003e\u003cli\u003eExport the labeled predictions to a project as a potential set-up for \u003cstrong\u003emanual evaluation\u003c/strong\u003e;\u003c/li\u003e\u003cli\u003eUse the labeled predictions dataset to \u003cstrong\u003efine-tune a student model\u003c/strong\u003e using a cloud-based notebook provider;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEvaluate performance\u003c/strong\u003e of the fine-tuned student model.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAt the end of the tutorial we’ll also discuss advanced considerations in scaling your models up and out, such as automating data ingestion and labeling, and resources for incorporating \u003ca href=\"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e\u0026nbsp; into your workflows.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-use-model-distillation-to-fine-tune-a-small-task-specific-computer-vision-model\"\u003eSee it in action: How to use model distillation to fine-tune a small, task-specific Computer Vision Model\u003c/h3\u003e\u003cp\u003e\u003cem\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial. You’ll also need to \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003ecreate API keys for accessing the SDK\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNotebook\u003c/strong\u003e: \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cu\u003eCV YOLO Model Distillation\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003ch3 id=\"the-model-distillation-workflow-for-computer-vision\"\u003eThe Model Distillation Workflow for Computer Vision\u003c/h3\u003e\u003cp\u003eIn our prior post on model distillation concepts, we discussed the different model distillation patterns, based on the following criteria:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe type of \u003cstrong\u003eTeacher\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eStudent\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eknowledge\u003c/strong\u003e being transferred from teacher to student model(s) (\u003cstrong\u003eresponse-based\u003c/strong\u003e,\u003cstrong\u003e feature-based\u003c/strong\u003e, \u003cstrong\u003erelation-based\u003c/strong\u003e knowledge);\u003c/li\u003e\u003cli\u003eThe cadence and scheme for how the student mode is trained (\u003cstrong\u003eoffline\u003c/strong\u003e, \u003cstrong\u003eonline\u003c/strong\u003e, \u003cstrong\u003eself\u003c/strong\u003e).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this tutorial we’ll be demonstrating the most popular and easiest pattern to get started with: \u003cstrong\u003eoffline, response-based model\u003c/strong\u003e (or \u003cstrong\u003eknowledge\u003c/strong\u003e) \u003cstrong\u003edistillation\u003c/strong\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe teacher model we’ll be using to produce the responses is Amazon Rekognition and the student model is \u003ca href=\"https://labelbox.com/product/model/foundry-models/yolov8-object-detection/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eYOLOv8 Object Detection\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs you’ll see, we could have chosen \u003ca href=\"https://labelbox.com/blog/6-cutting-edge-foundation-models-for-computer-vision-and-how-to-use-them/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany combination of teacher or student models\u003c/u\u003e\u003c/a\u003e, because the \u003cstrong\u003eoffline, response-based\u003c/strong\u003e pattern of model distillation is incredibly flexible.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen implementing this process for your own use case, it’s important to understand the relative strengths and weaknesses of each model and match them according to your requirements.\u003c/p\u003e\u003ch3 id=\"using-the-labelbox-platform-to-automate-model-distillation\"\u003eUsing The Quantumworks Lab Platform To Automate Model Distillation\u0026nbsp;\u003c/h3\u003e\u003cp\u003eLabelbox is the leading data-centric AI platform, providing an end-to-end platform for curating, transforming, annotating, evaluating and orchestrating unstructured data for data science, machine learning, and generative AI.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab platform supports the development of intelligent applications using the model distillation and fine-tuning workflow. \u003c/p\u003e\u003cp\u003eAI developers are enabled to easily:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImport, curate, filter and eventually select the images that will be labeled for use in student model training and fine-tuning with \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e. Additionally you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/model-foundry-automating-data-tasks-with-foundation-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomate the labeling and annotation of the original image dataset using foundation models\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eor any pre-trained model, including custom models\u003c/u\u003e\u003c/a\u003e) in \u003ca href=\"https://labelbox.com/guides/guide-to-using-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e. The ability to leverage a variety of open source or third-party models to accelerate pre-labeling can cut labeling costs by up to 90% for teams with existing labeling initiatives (and kickstart new and budget constrained AI developers operating without\u0026nbsp; manual labeling capabilities).\u003c/li\u003e\u003cli\u003eLeverage \u003ca href=\"https://labelbox.com/customers/intuitive-surgical-customer-story/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman-in-the-loop\u003c/u\u003e\u003c/a\u003e evaluation (and \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e) through the native Foundry to Annotate integration, so labels can be reviewed before being used for training or fine-tuning.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIntegrate with some of the most common cloud providers like \u003ca href=\"https://learn.labelbox.com/accelerate-retail-and-ecommerce-ai-roi/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Cloud Platform\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://labelbox.com/company/partnerships/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eDatabricks\u003c/u\u003e\u003c/a\u003e for additional MLOps services.\u003c/li\u003e\u003cli\u003eOrchestrate and schedule future automated labeling and model runs as \u003ca href=\"https://labelbox.com/blog/10x-faster-uploads-with-labelboxs-new-ingestion-upgrades/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enew data flows into the Quantumworks Lab platform\u003c/u\u003e\u003c/a\u003e through Model Apps (including \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emultimodal data labeling\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"introduction-to-data-preparation-for-computer-vision-with-catalog\"\u003eIntroduction To Data Preparation for Computer Vision With Catalog\u003c/h2\u003e\u003cp\u003eBefore beginning the tutorial:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eCreate a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e (in order to \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecreate a free API key\u003c/u\u003e\u003c/a\u003e)\u003c/li\u003e\u003cli\u003eCheck that you can access an \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eexisting fashion products dataset\u003c/u\u003e\u003c/a\u003e in Catalog\u003cul\u003e\u003cli\u003eIf not, check out sites like Kaggle for similar datasets.\u003c/li\u003e\u003cli\u003eDownload the images and their metadata.\u003c/li\u003e\u003cli\u003eChoose whether to upload data via \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#create-a-dataset-via-the-app-ui\"\u003e\u003cu\u003ethe Quantumworks Lab Web UI\u003c/u\u003e\u003c/a\u003e (preferred method) or the SDK.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’re able to see your dataset in Quantumworks Lab Catalog, you’ll be able to do the following:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e for more granular data curation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor additional details on how to use Catalog to enable data selection for downstream data-centric workflows (such as data labeling, model training, model evaluation, error analysis, and active learning), check out \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour documentation\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"using-a-large-computer-vision-model-to-generate-and-distill-predictions\"\u003eUsing A Large Computer Vision Model To Generate And Distill Predictions\u003c/h2\u003e\u003cp\u003eThe first step of model distillation is to \u003cstrong\u003eidentify an appropriate teacher model\u003c/strong\u003e, which will be used to produce responses that, when combined with the original images, will serve as the \u003cstrong\u003efine-tuning dataset for the student model\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResponse-based\u003c/strong\u003e model distillation is powerful because it can be used even when access to the original model weights is limited (or the model is so big that downloading a copy of the model would take a really long time). Response-based distillation also doesn’t require the user to have trained the model themselves; just that the model was pre-trained.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox allows you to pick any of the \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecurrently hosted, state-of-the-art models to use\u003c/u\u003e\u003c/a\u003e (as well as upload your own custom models) to use as the teacher model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor now, let’s get started with preparing the images we’ll be labeling, or generating predictions with, using Amazon Rekognition. The combination of image and label pairs will be used for YOLO.\u003c/p\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select images and choose a foundation model of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to the \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efashion products dataset\u003c/u\u003e\u003c/a\u003e in Catalog.\u003c/li\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “\u003cstrong\u003ePredict with Model Foundry\u003c/strong\u003e”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run (in this case Rekognition).\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task - such as \u003ca href=\"https://labelbox.com/usecases/computer-vision/image-classification/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eimage classification\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/usecases/computer-vision/object-detection/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eobject detection\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/guides/how-to-build-generative-captioning-using-foundation-models-for-product-listings/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage captioning\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003eStep 2: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eWhen developing ML based applications, developers need to quickly and iteratively prepare and version training data, launch model experiments, and use the performance metrics to further refine the input data sources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe performance of a model can vary wildly depending on the data used, the quality of the annotations, and even the model architecture itself. A necessary requirement for replicability is being able to see the exact version of all the artifacts used or generated as a result of an experiment.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox will snapshot the experiment, the data artifacts as well as the trained model, as a saved process known as a \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis includes the types of items the model is supposed to identify and label, known as an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eontology\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of settings, which you can find in the Advanced model setting.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce you’ve located Rekognition, you can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIn this case, we set the ontology to detect “Jacket” and we can see a preview of running the model on this ontology above.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo get an idea of how your current model settings affect the final predictions, you can generate preview predictions on up to five data rows. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-review-predictions-in-the-model-tab\"\u003eStep 3: Review predictions in the Model tab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eBecause each \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run is submitted with a unique name\u003c/u\u003e\u003c/a\u003e, it’s easy to distinguish between each subsequent model run.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/foundry-view-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eView prediction results\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results \u003ca href=\"https://docs.labelbox.com/docs/comparing-model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eacross a variety of model runs\u003c/u\u003e\u003c/a\u003e different models\u003c/li\u003e\u003cli\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe can see that for the most part, “jackets” were correctly identified and labeled.\u003c/li\u003e\u003cli\u003eThese generated labels are now ready to be used for fine-tuning the student model YOLO.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-enriching-and-evaluating-predictions-using-human-in-the-loop-and-annotate\"\u003eStep 4: Enriching and evaluating predictions using human-in-the-loop and Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAlthough fine-tuning a foundation model requires less data than pre-training a large foundation model from scratch, the data (specifically the labels) need to be high-quality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEven big, powerful foundation models make mistakes or miss edge cases.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou might also find that there are additional objects that the parent model didn’t identify because the items in the image weren’t initially identified as being important in the ontology (for example, “boots” or “ski hats”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce a parent model like Rekognition has been used for the initial model-assisted labeling run, those predictions \u003ca href=\"https://docs.labelbox.com/docs/foundry-annotate-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then be sent\u003c/u\u003e\u003c/a\u003e to a Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eproject\u003c/u\u003e\u003c/a\u003e, a container \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhere all your labeling processes happen\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/la022y1i78\" title=\"End-to-End Workflow for Model Distillation with Computer Vision - Send to annotate Jacket YOLO Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"484\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn this case, we feel fairly confident\u003cstrong\u003e \u003c/strong\u003ein how well Rekognition performed so we’ll send the inferences to the corresponding Quantumworks Lab project and treat them as the ground truth that the student model will be fine-tuned on. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"fine-tuning-the-student-model-yolo\"\u003eFine-Tuning The Student Model (YOLO)\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWe’ve shown the first half of the model distillation to fine-tuning workflow.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe identified the items we wanted the parent model (Rekognition) to detect and label in the form of an ontology.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe used Rekognition to automatically label items like “jackets”.\u003c/li\u003e\u003cli\u003eWe exported the generated labels to a Quantumworks Lab project, at which point we could review the labels manually and enrich them further using the Quantumworks Lab editor.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe next step is to use the generated labels, along with the original image, to fine-tune a student model in Colab.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: You’ll now need the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=PgdwI9SR5HHd\"\u003e\u003cem\u003e\u003cu\u003eAPI keys from earlier\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to follow along with the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cem\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003ch3 id=\"step-5-fetch-the-ground-truth-labels-from-the-project-via-labelbox-sdk-and-convert-the-images-into-the-relevant-format\"\u003eStep 5: Fetch the ground truth labels from the project via Quantumworks Lab SDK and convert the images into the relevant format.\u003c/h3\u003e\u003cp\u003eFor brevity, we’ve omitted the relevant code samples but you can copy or run the corresponding blocks in the provided notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our documentation to find out all the ways you can automate the model lifecycle (\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eincluding labeling\u003c/u\u003e\u003c/a\u003e) using our SDK.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"977\" height=\"542\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png 977w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore fine-tuning the YOLO student model, we’ll \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=7hs-oTEXOQKx\"\u003e\u003cu\u003eneed to fetch the generated labels\u003c/u\u003e\u003c/a\u003e from the Quantumworks Lab project using the Quantumworks Lab SDK as well as the \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=jKMysiHvUI1D\"\u003e\u003cu\u003eground truth\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe’ll also need to \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cu\u003econvert the images and ensure they’re in the right format\u003c/u\u003e\u003c/a\u003e for fine-tuning, specifically the COCO format.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-6-fine-tune-student-yolo-model-using-labels-generated-by-amazon-rekognition\"\u003eStep 6: Fine-tune student YOLO model using labels generated by Amazon Rekognition\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"608\" height=\"109\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png 608w\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn Colab we’ll \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=Becr0BZQO_Ze\u0026line=1\u0026uniqifier=1\" rel=\"noreferrer\"\u003einstantiate a YOLO model and train it using the data\u003c/a\u003e, which includes both images and labels.\u003c/li\u003e\u003cli\u003eWe’ll then \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=_quHpcfihY7s\" rel=\"noreferrer\"\u003erun the fine-tuned student YOLO model\u003c/a\u003e on the images to generate the predictions for analysis.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSee the example notebook for omitted code.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"923\" height=\"307\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png 923w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-7-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 7: Create a model run with predictions and ground truth\u0026nbsp;\u003c/h3\u003e\u003cp\u003eOftentimes the initial training or fine-tuning step isn’t the final stop on the journey of developing a model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOne of the biggest differences between the traditional method of training models in the classroom versus the real-world is how much control you have over the quality of your data, and consequently the quality of the model produced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we mentioned earlier, developers can upload predictions and use the Model product to \u003ca href=\"https://labelbox.com/blog/how-to-confidently-compare-test-and-evaluate-models-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ediagnose performance issues with models and compare them across multiple experiments\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDoing so automatically populates model metrics that make it easy to evaluate the model’s performance.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"857\" height=\"266\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png 857w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eHere we show how you \u003ca href=\"https://docs.labelbox.com/docs/import-ground-truth?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan upload the ground truth labels\u003c/u\u003e\u003c/a\u003e (the labels generated by Rekognition and used as the fine-tuning dataset) and the \u003ca href=\"https://docs.labelbox.com/docs/upload-model-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003epredictions\u003c/u\u003e\u003c/a\u003e from the fine-tuned YOLO model to the corresponding Quantumworks Lab model. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"evaluating-model-performance\"\u003eEvaluating Model Performance\u003c/h2\u003e\u003cp\u003eThere’s no single metric to rule them all when evaluating your \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecomputer vision model performance\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith that being said, Model offers a number of the most common out-of-the-box. With the \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io#metrics-view\"\u003e\u003cu\u003e‘Metrics view’\u003c/u\u003e\u003c/a\u003e users can drill into crucial model metrics, such as \u003ca href=\"https://docs.labelbox.com/docs/upload-model-metrics-1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econfusion matrix, precision, recall, F1 score, false positive, and more\u003c/u\u003e\u003c/a\u003e, to surface model errors.\u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive, which means you can click on any chart or metric to immediately open up the gallery view of the model run and see corresponding examples, as well as visually compare model predictions between multiple model runs.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-7-evaluate-predictions-from-different-yolo-model-runs-in-labelbox-model\"\u003eStep 7: Evaluate predictions from different YOLO model runs in Quantumworks Lab Model\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to “Model”\u003c/li\u003e\u003cli\u003eBy fine-tuning the Yolo v8 model with approximately 1000 images annotated using Amazon Rekognition, we can achieve performance similar to the Rekognition model within roughly one hour.\u003c/li\u003e\u003cli\u003eWe can now manually inspect examples of predictions from the fine-tuned YOLO model\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"advanced-considerations\"\u003eAdvanced Considerations\u003c/h2\u003e\u003cp\u003eIn this step-by-step walkthrough, we’ve shown how anyone with any image-based dataset can leverage an image-based foundation model to label, fine-tune and analyze a smaller but mighty custom model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAdditional considerations users should address for scaling similar projects include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAutomating future data ingestion, curation, enrichment, and labeling when the fine-tuned model needs to be retrained due to drift.\u003c/li\u003e\u003cli\u003eIncorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman evaluators and human-in-the-loop\u003c/u\u003e\u003c/a\u003e, as well as error analysis for identifying and addressing edge cases.\u0026nbsp;\u003c/li\u003e\u003cli\u003eEasy-to-use, user interface that can be customized for various modalities of data when multiple users are involved.\u0026nbsp;\u003c/li\u003e\u003cli\u003eA \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erobust SDK\u003c/u\u003e\u003c/a\u003e for integrating with any MLOps solutions provider, especially when incorporating model monitoring and complex deployment patterns.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEven with fine-tuned models, there’s no such thing as “setting and forgetting”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAll models eventually need to be retrained, with the data refreshed to account for changes.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eIn this tutorial, we demonstrated an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003ecomputer vision\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a YOLO model with labels created in Model Foundry using Amazon Rekognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHopefully you were able to see how easy it is to go from raw data to cutting-edge custom models in less than 30 min.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou learned how the Quantumworks Lab platform enables model distillation by allowing developers to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIngest, explore and prepare image-based datasets using Catalog;\u003c/li\u003e\u003cli\u003eUse any \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage based foundation model\u003c/u\u003e\u003c/a\u003e to automatically label data using Model Foundry as well as how to incorporate human-in-the-loop evaluation using Annotate;\u003c/li\u003e\u003cli\u003eExport these labeled predictions to a cloud-based training environment for fine-tuning;\u003c/li\u003e\u003cli\u003eAutomate the various workflows using the Quantumworks Lab SDK;\u003c/li\u003e\u003cli\u003eEvaluate model performance and analyze model errors using Quantumworks Lab Model.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn the next part of this series we replicate a very similar workflow for NLP, using model distillation to fine-tune a BERT model with labels created in Model Foundry using PaLM2. \u003c/p\u003e","comment_id":"65bd22d782e5680001e07877","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Social-Cards-Example--1-.jpg","featured":false,"visibility":"public","created_at":"2024-02-02T17:13:59.000+00:00","updated_at":"2024-10-02T00:02:14.000+00:00","published_at":"2024-02-01T20:23:00.000+00:00","custom_excerpt":"Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"681516a56e02120001d5ab84","name":"#mikiko","slug":"hash-mikiko","description":null,"feature_image":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/404/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"65bd4ff882e5680001e078f9","name":"Model Distillation","slug":"model-distillation","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/tag/model-distillation/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/end-to-end-workflow-with-model-distillation-for-computer-vision/","excerpt":"Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. ","reading_time":11,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"654bbab4016f5100016579c3","uuid":"6c33e4d1-fcaa-44de-b994-0fe65fce3dcc","title":"How to analyze customer reviews and improve customer care with NLP","slug":"how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp","html":"\u003cp\u003eCustomer reviews have become a critical tool for businesses looking to improve their products, services, and customer satisfaction. In today’s digital world, review sites like Yelp and social media make it easier than ever for customers to share their experiences with the world. Customer care can range in the services and support that businesses provide to their customers before, during, and after purchase. Great customer care can create positive brand experiences that lead to greater loyalty and customer satisfaction. In the ever-evolving world of retail, it also helps keep your business competitive and at the forefront of your customer’s sentiment and desires.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile companies now have access to a wealth of customer feedback data, sifting through all of these reviews can be incredibly time-consuming and manual. By leveraging AI, teams can analyze \u003ca href=\"https://birdeye.com/blog/review-management/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ecustomer reviews and feedback\u003c/a\u003e at scale, to gain insights into common review topics or customer sentiment. This allows businesses to identify common themes and pinpoint areas of improvement to enhance\u0026nbsp; the customer experience.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for customer care. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eImproving customer care requires a vast amount of data in the form of customer reviews. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape: \u003c/strong\u003eThe changing nature and format of customer review data from multiple sources (e.g webpages, apps, social media, etc.) poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their customer care through advanced natural language processing. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s assisted data enrichment and flexible training frameworks to quickly build AI systems that uncover actionable insights from customer reviews. Tackle unique customer care challenges with AI-driven insights to create more thoughtful and strategic customer interactions. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1444\" height=\"784\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 1444w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build an NLP model to improve customer care. Specifically, this guide will walk through how you can explore and better understand review topics and classify review sentiment to make more data-driven business decisions around customer care initiatives.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-accelerate-and-train-an-nlp-model-to-improve-customer-care\"\u003eSee it in action: How to accelerate and train an NLP model to improve customer care\u0026nbsp;\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below in either: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e or \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs customer reviews and feedback across channels proliferate, brands want to learn from customer feedback to foster positive experiences. For this use case, we’ll be working with a dataset of customer hotel reviews – with the goal of analyzing the reviews to demonstrate how a hospitality company could gain insight into how their customers feel about the quality of service they receive.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/q4dqjyg9xf\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"498\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003eFor the purpose of this tutorial, we’ve provided a sample open-source \u003ca href=\"https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe?ref=labelbox-guides.ghost.io\"\u003eKaggle dataset\u003c/a\u003e that can be downloaded.  \u003c/p\u003e\u003cp\u003ePlease \u003ca href=\"https://drive.google.com/file/d/1hh2MYzol6-4PSsqX7mbX2YSLLGA4jAYZ/view?usp=drivesdk\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edownload the dataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your text data as a CSV. \u003c/li\u003e\u003cli\u003eIf your text snippets sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your text data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xn3sj0uc8j\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExplore topics of interest\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith your data in Quantumworks Lab, you can begin to leverage Catalog to uncover interesting topics to get a sense of what customers are talking about from hotel reviews.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize your data –\u0026nbsp; you can click through individual data rows to get a sense for what customers are writing reviews on\u0026nbsp;\u003c/li\u003e\u003cli\u003eDrill into specific topics of interest\u003cstrong\u003e \u003c/strong\u003e– leverage a natural language search, for example searching “interior design,” to bring up all related reviews related to interior design. You can adjust the confidence threshold of your searches accordingly (this can be helpful in gauging the volume of data related to the topic of interest)\u0026nbsp;\u003c/li\u003e\u003cli\u003eBegin to surface subtopics or trends within your initial search – for example is the interior design review related to the style of design, attention to detail, or the type of environment created from the interior design\u003c/li\u003e\u003cli\u003eEasily find all instances of similar examples to data of interest\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate and save data slices\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf you have a search query that you’re interested in saving or reusing in the future, you can save it as \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003ea slice\u003c/a\u003e. You can construct a slice by using one or more filters to curate a collection of data rows. Users often combine filters to surface high-impact data and then save the results as a slice.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this example, we’ve surfaced reviews on the topic of breakfast that all talk about the value and price of the hotel’s breakfast. We can save this as a slice for future reference (“Breakfast_value”) and as we ingest more data that matches the slice’s criteria, they will automatically get filed into the slice.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-an-ontology\"\u003eCreate an ontology\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can create our ontology. \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eOntologies\u003c/a\u003e can be reused across different projects and they are required for data labeling, model training, and evaluation.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gdczmynqjt\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a new ontology:\u003c/p\u003e\u003cp\u003e1) Navigate to the ‘Schema’ tab\u003c/p\u003e\u003cp\u003e2) Hit ‘Create new ontology’\u003c/p\u003e\u003cp\u003e3) Select the media type that you wish to work with – for this use case ‘Text’\u003c/p\u003e\u003cp\u003e4) Give your ontology a name\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Add objects and classifications based on you use case\u003c/p\u003e\u003cp\u003e6) Objects are named entities\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003ePerson’s name\u0026nbsp;\u003c/li\u003e\u003cli\u003eLocation\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003e7) Classifications\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eReview sentiment such as positive or negative (radio)\u0026nbsp;\u003c/li\u003e\u003cli\u003eReview topics such as breakfast, dinner, location, staff, interior design (checklist)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAdd sub-classifications as desired\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003e8) Save and create your ontology\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter creating an ontology, you can begin labeling your data to fine-tune or train a model.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"label-data-of-interest\"\u003eLabel data of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003eWith Quantumworks Lab, you can label your data in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e: leverage our global network of\u0026nbsp; specialized labelers for a variety of tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWorkforce Boost provides a collaborative platform for labeling services in a self-serve manner — this is great for teams that don’t have the technical expertise to build a machine learning system yet are looking for an easy-to-use technology to get a quick turnaround on quality training data. You can learn more about our Boost offerings \u003ca href=\"https://docs.labelbox.com/docs/using-data-labeling-service?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create pre-labels with foundation models\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn addition to creating pre-labels for classification projects, you have the ability to send model predictions as pre-labels to your labeling project. This can be done in one of two ways:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel-assisted labeling\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eImport computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel Foundry\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eAutomate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePre-label data with Model Foundry\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModel Foundry\u003c/a\u003e acts as the copilot to create your training data –\u0026nbsp; instead of going into unstructured text datasets blindly, you can use pre-existing LLMs to pre-label data or pre-tag parts of it, reducing manual labeling efforts and cost.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9zspjgoau7\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1) Select data you wish to label in Catalog\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Hit \"Predict with Model Foundry\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Choose a foundation model\u003c/p\u003e\u003cul\u003e\u003cli\u003eYou can select a foundation model based on your use case to have the model take a first pass at labeling your data\u003c/li\u003e\u003cli\u003eThese pre-labels can be verified with human-in-the-loop review in Quantumworks Lab Annotate\u003c/li\u003e\u003cli\u003eFor this use case, we’ve selected the GPT-4 model\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e4) Configure the model’s settings\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect the previously created ontology in the earlier part of the tutorial\u0026nbsp;\u003c/li\u003e\u003cli\u003eLabelbox will auto-generate a prompt based on your ontology and use case – in this case we wish to classify the sentiment (positive or negative) and classify a topic with one or more options (breakfast, dinner, location, staff, room, facilities, value for money, or interior design)\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e5) Generate preview predictions\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore submitting the model run, you can generate prediction previews to understand how the model will perform\u003c/li\u003e\u003cli\u003eIt is recommended that you preview some predictions to confirm the model parameters are configured as desired\u003c/li\u003e\u003cli\u003eBased on the preview, you can then make any adjustments to the settings or choose to submit the model run as-is\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e6) Name and submit the model run\u0026nbsp;\u003c/p\u003e\u003cp\u003e7) View the model run in the Model tab to explore results\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce your model run is complete, you navigate to the Model tab\u003c/li\u003e\u003cli\u003eExplore the model’s results and click into each data row to dig deeper into the model’s predictions\u003c/li\u003e\u003cli\u003eFor this example, we can see that there are instances where GPT-4 has correctly tagged named entities and identified sentiment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve evaluated and are satisfied with GPT-4’s predictions, you can send them to a labeling project in Quantumworks Lab Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAdd a batch to a labeling project as pre-labels\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eBefore you can send these model predictions to a labeling project as pre-labels, you need to create a labeling project. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7zgl76n76w\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eCreate a new labeling project\u003c/em\u003e\u003c/p\u003e\u003cp\u003e1) Navigate to the Annotate tab\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Create a ‘New project’\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Select the project type – in this case we want to create a ‘Text’ project\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Name your project\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Attach your model’s ontology (created in a previous step)\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce you’ve created your labeling project and configured the ontology, head back to the Model tab to send your batch of data with pre-labels to that labeling project.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Highlight all data rows of interest\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Select ‘Manage selection’ \u0026gt; ‘Add batch to project’\u003c/p\u003e\u003cp\u003e3) Select the appropriate project that you created in the above step\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) You can give the batch a priority (from 1-5)\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Select the appropriate model run of the predictions you wish to send\u0026nbsp;\u003c/p\u003e\u003cp\u003e6) You can explore and select the various tags that have been applied and uncheck those that aren’t of interest\u0026nbsp;\u003c/p\u003e\u003cp\u003e7) Submit the batch\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can now navigate back to your project in Annotate and hit ‘Start labeling’.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"verify-data-quality-with-custom-workflows\"\u003eVerify data quality with custom workflows\u003c/h3\u003e\u003cp\u003eRather than starting from scratch, your internal or external team of labelers can now see predictions from the Model Foundry run. From here, you can validate or edit predictions as necessary and submit data rows to create ground truth labels.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/56bratjais\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs you begin to progress through your data rows, you’ll notice data rows that are initially marked up and reviewed by labelers in the ‘Initial review’ task (for your reviewers to verify and approve), with all submitted data rows falling into ‘Done’.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can create customizable, multi-step review and rework pipelines to drive efficiency and automation for your review tasks. Set a review task based on specific parameters that are unique to your labeling team or desired outcome.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eInitial labeling task: reserved for all data rows that have been queued for labeling\u003c/li\u003e\u003cli\u003eInitial review task: first review task for data rows with submitted labels\u003c/li\u003e\u003cli\u003eRework task: reserved for data rows that have been rejected\u003c/li\u003e\u003cli\u003eDone task: reserved for data rows that have a) moved through their qualified tasks in the workflow or b) did not qualify for any of the tasks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce all data rows have been reviewed and moved to the ‘Done’ step, you can begin the model training process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn Part 1 of this tutorial, we have looked at how we can leverage Catalog to understand the topics that exist within your dataset and construct an appropriate ontology. To accelerate our initial labeling job, we leveraged Model Foundry as part of our model-assisted labeling pipeline to use pre-labels from GPT-4 to our labeling workforce for validation. Those initial annotations can be exported via a model run and can be used to train or fine-tune a model outside of Labelbox.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"part-2-train-or-fine-tune-a-model-and-evaluate-model-performance\"\u003ePart 2: Train or fine-tune a model and evaluate model performance\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e or \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"train-a-custom-model-on-a-subset-of-data-outside-of-labelbox\"\u003eTrain a custom model on a subset of data outside of Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/07yc0p652w\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn the previous step, we leveraged Model Foundry to create pre-labels that were passed through Annotate for review with human-in-the-loop validation. Now that we have our appropriate annotation data, we can train a series of initial models on sentiment, topic classification, and named entity recognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eYou can reference \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io#scrollTo=EDmBNjYP_0u7\"\u003ethis step\u003c/a\u003e (Databricks) or \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io#scrollTo=EDmBNjYP_0u7\" rel=\"noreferrer\"\u003ethis step\u003c/a\u003e (Google Colab) in either notebook.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBring the trained model’s predictions back into a model run\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce the model has been trained, you can create an inference pipeline that leverages each model to classify different attributes for review. We can then leverage this for two things:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun inference on the model run dataset and upload it to Quantumworks Lab for evaluation\u003c/li\u003e\u003cli\u003eRun inference on our remaining dataset and use the predictions for model-assisted labeling, to be refined in the platform and used to accelerate labeling efforts\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ePlease follow \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io#scrollTo=B9_dYR_D_0vE\"\u003ethis step\u003c/a\u003e (Databricks) or \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io#scrollTo=B9_dYR_D_0vE\" rel=\"noreferrer\"\u003ethis step\u003c/a\u003e (Google Colab) to create an inference pipeline and to upload predictions to the model run and evaluate it against ground truth.\u003c/p\u003e\u003cp\u003eAfter following the notebook, you’ll be able to compare ground truth (green) versus the model’s predictions (red) for sentiment and topic.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness\"\u003eEvaluate and diagnose model effectiveness\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dqzdzj1seb\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 8 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eDiagnose model performance with model metrics\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn addition to visualizing the difference between model predictions and ground truth, you can click into the ‘Metrics’ view to get a better sense of how your model is performing.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the \"Metrics view\" to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor example, we can click into false negatives or false positives to narrow down situations where there might be false positives – where ‘negative’ sentiment is predicted whereas ground truth sentiment is ‘positive’.\u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select \"Find similar in Catalog\" from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled reviews that you can send to your model for labeling, you can filter on the \"Annotation is\" filter and select \"none.\" This will only show unlabeled text reviews that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all reviews that apply and select \"Add batch to project\"\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eUse model predictions as model-assisted labeling pipeline\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/r4p2h6iklg\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 9 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve filtered for and have selected reviews that you wish to label you can \"Add batch to project\" to send them to your labeling project in Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Name your batch\u003c/p\u003e\u003cp\u003e2) Select your labeling project from the dropdown\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Include model predictions (from your model run) – this will perform better than the initial GPT-4 run with Model Foundry since it has been trained on your custom data\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Select or uncheck any predictions as desired\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Submit the batch\u003c/p\u003e\u003cp\u003eWhen you return to Quantumworks Lab Annotate, you will now see the original batch that we added at the start of the project, as well as the newly added batch ready for labeling.\u0026nbsp;\u003c/p\u003e\u003cp\u003eRather than starting from scratch, similar to the predictions created by GPT-4 in Model Foundry, your labelers will now see the custom model predictions and validate them with human-in-the-loop review in the same manner. This workflow helps accelerate model iterations, allowing your team to bring in the latest model prediction as pre-labels for your project to reduce the amount of human labeling effort required to create ground truth labels.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model and can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003eCustomer reviews and feedback data represent an invaluable yet untapped opportunity for businesses. Manually analyzing this growing mountain of data is no longer practical. Instead, forward-thinking companies are turning to AI to efficiently sift through and extract actionable insights from reviews.\u003c/p\u003e\u003cp\u003eNatural language processing can help identify customer sentiment, pain points, and unmet needs. By leveraging AI to tap into this feedback treasure trove, businesses can drive measurable improvements in customer satisfaction, retention, and advocacy. They can refine products, enhance user experiences, and preemptively address concerns.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"654bbab4016f5100016579c3","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-2457--1-.png","featured":false,"visibility":"public","created_at":"2023-11-08T16:43:32.000+00:00","updated_at":"2024-06-25T16:28:21.000+00:00","published_at":"2023-11-08T21:47:13.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/","excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","reading_time":12,"access":true,"comments":false,"og_image":null,"og_title":"How to analyze customer reviews and improve customer care with NLP","og_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-2457--1--1.png","twitter_title":"How to analyze customer reviews and improve customer care with NLP","twitter_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","meta_title":"How to analyze customer reviews and improve customer care with NLP","meta_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"654407cdd96ee80001d8c876","uuid":"e29733c1-b966-4d1e-858d-f96deab4850e","title":"How to build a content moderation model to detect disinformation","slug":"how-to-build-a-content-moderation-model-to-detect-disinformation","html":"\u003cp\u003eAs user-generated content increases and the amount of data grows, trust and safety on digital platforms is becoming increasingly critical. Content that goes unmoderated can not only directly hurt brand reputation, but it can directly impact a businesses bottom line through lost users, advertisers, and revenue. Regulators worldwide are also implementing \u003ca href=\"https://insightplus.bakermckenzie.com/bm/data-technology/united-states-now-is-the-time-to-evaluate-your-online-content-moderation-program?ref=labelbox-guides.ghost.io\"\u003estricter rules\u003c/a\u003e around content moderation, online safety, misinformation, and disinformation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo address these growing risks, more businesses are looking to AI and machine learning as part of robust trust and safety strategies. State-of-the-art AI solutions enable unprecedented scale, nuance, consistency, and efficiency in identifying and taking action on high-risk user content.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for trust and safety. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDynamic content landscape: \u003c/strong\u003eModels are only as good as the data they are trained on. As new trends or content emerges, AI models need constant retraining on compelling diverse, unbiased, and large labeled datasets to reinforce content moderation.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEthical risks \u0026amp; biases: \u003c/strong\u003eWithout careful design, machine learning models risk exacerbating biases and are prone to \u003ca href=\"https://labelbox.com/blog/what-does-it-mean-when-an-llm-hallucinates/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ehallucination\u003c/a\u003e. Teams need a way to monitor and evaluate model training with ethical oversight.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that enables businesses to build state-of-the-art AI solutions for enhanced controls, transparency, efficiency in content moderation, and greater brand safety. Rather than spending valuable time building an in-house solution or relying on disparate systems, businesses can explore data, use foundation models for assisted-enrichment, and evaluate models to quickly build more accurate AI systems for analyzing user behavior, detecting disinformation, and enhancing ad-targeting. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1430\" height=\"786\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png 1430w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build a model for content moderation, such as detecting and classifying disinformation, allowing you to elevate brand trust and improve the trust and safety of your applications.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-build-a-content-moderation-model-to-detect-disinformation\"\u003eSee it in action: How to build a content moderation model to detect disinformation\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data: \u003ca href=\"https://colab.research.google.com/drive/1bTIKkQUHiccIy1adgKqQ_CVCm2KAxiHP?ref=labelbox-guides.ghost.io\"\u003eGoogle Colab Notebook\u0026nbsp;\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2: \u003c/strong\u003eCreate a model run, fine-tune an LLM, and evaluate model performance: \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1bTIKkQUHiccIy1adgKqQ_CVCm2KAxiHP?ref=labelbox-guides.ghost.io\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eWith the growing amount of user-generated content, businesses want to ensure that there is no inappropriate content or disinformation happening on their platform. To implement content moderation at scale, teams can leverage AI to analyze and detect harmful content and classify disinformation from existing data stored in a cloud bucket or a local folder.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9e76g8llbu\" title=\"How to enhance brand safety and content moderation with AI - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo upload a sample of your content to Quantumworks Lab for labeling, you have a few options:\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUpload a dataset through the SDK\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eUsing the \u003ca href=\"https://colab.research.google.com/drive/1bTIKkQUHiccIy1adgKqQ_CVCm2KAxiHP?ref=labelbox-guides.ghost.io\"\u003eGoogle Colab notebook\u003c/a\u003e, upload the sample dataset into Quantumworks Lab or use it to import data from various sources like Bigquery, Databricks, or Snowflake.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this notebook, we’re going to bring in two libraries of interest:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/sdk-fundamental-concepts-1?ref=labelbox-guides.ghost.io\"\u003eLabelbox SDK\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/Quantumworks Lab/labelpandas?ref=labelbox-guides.ghost.io\"\u003eLabelpandas\u003c/a\u003e (for bringing tabular data into Quantumworks Lab)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou’ll need your Quantumworks Lab API key to initiate the Quantumworks Lab Client and create a dataset. For this guide, we’ll be using a dataset stored in a Google Cloud bucket as a CSV and we can use Labelpandas to bring this data in.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe provided sample dataset includes:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAn article with a corresponding headline\u003c/li\u003e\u003cli\u003eWhen it was retrieved\u003c/li\u003e\u003cli\u003eMetadata (sorted by source)\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003ePre-labels based on if the article contains “disinformation” or not\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eUpload a dataset through the UI \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf you have a dataset from your local file, you can upload it through the Quantumworks Lab UI by clicking \"new dataset\" in Catalog.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce you’ve successfully uploaded your text, you can browse the dataset in Catalog — along its metadata. You can visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/vl9jr5463n\" title=\"How to enhance brand safety and content moderation with AI - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003eSearch across datasets\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003efind similar data\u003c/a\u003e in seconds with off-the-shelf embeddings\u0026nbsp;\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003enatural language\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003elayer structured and unstructured filters\u003c/a\u003e for more granular data curation\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate and save data slices\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ksmc9w7acz\" title=\"How to enhance brand safety and content moderation with AI - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIf you have a search query that you’re interested in saving or reusing in the future, you can save it as \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003ea slice\u003c/a\u003e. You can construct a slice by using one or more filters to curate a collection of data rows. Users often combine filters to surface high-impact data and then save the results as a slice.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this example, we are interested in saving the surfaced data rows as “Climate Articles” so that this filtered dataset can easily be surfaced later on for annotation or data discovery purposes.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-a-labeling-project-in-annotate\"\u003eCreate a labeling project in Annotate\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uxspsczkpn\" title=\"How to enhance brand safety and content moderation with AI - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1) Create a text project in \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Sample and send your uploaded dataset as a \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003ebatch\u003c/a\u003e to your newly created project. In this case we can send the two dataset slices that we created: “Climate related articles” and “Non-climate related articles”\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eontology\u003c/a\u003e to determine how to structure your data. If you have a previous ontology you’d like to use, you can do so. If not, you’ll need to create a new ontology. For this use case, our ontology consists of two classifications:\u003c/p\u003e\u003cul\u003e\u003cli\u003e“Does the article contain disinformation?” with two options\u003c/li\u003e\u003cli\u003e“Is the article climate related?” with two options\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e4) If you’re relying on an external team of labelers or want to provide your internal labeling team with more instructions, you can upload instructions as a PDF for your labelers during the ontology creation process.\u003c/p\u003e\u003ch3 id=\"label-the-data-of-interest\"\u003eLabel the data of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003eNow that we have a project with our data set up in Annotate, we’ll need to label this training data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eSince this project is a classification use case, we can also leverage \u003ca href=\"https://docs.labelbox.com/docs/bulk-classification?ref=labelbox-guides.ghost.io\"\u003ebulk classification\u003c/a\u003e to speed up our labeling process and maximize labeling efficiency. Teams who have used bulk classification in Quantumworks Lab have seen labeling time decrease from a full quarter to a few days. Since we’ve leveraged filters in Catalog to identify “Climate related articles,” we can send these articles to our newly created labeling project with pre-labels.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/eh5mwisy6q\" title=\"How to enhance brand safety and content moderation with AI - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo bulk classify and pre-label data rows, you can:\u003c/p\u003e\u003cp\u003e1) Highlight any data rows of interest, in our use case these would be data rows in the slice \"Climate related articles\",\u0026nbsp; and select \"Manage selection\" \u0026gt; \"Add classifications\"\u003c/p\u003e\u003cp\u003e2) Select the labeling project that you made in the previous step and determine a step of the project’s review workflow that you would like to send the classifications to. In the above demo, we are sending these to the \"Initial labeling task\" because we want to have a labeler verify that these are indeed all climate related articles\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Select the desired classification — in this case it would be \"Climate related\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) You can determine the batch’s data row priority (from 1-5) and submit the bulk classification job \u003c/p\u003e\u003cp\u003eRather than labeling from scratch, a team of labelers can now simply verify or correct the pre-labels used during this bulk classification step.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can label your data in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e: leverage our global network of\u0026nbsp; specialized labelers for a variety of tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWorkforce Boost provides a collaborative platform for labeling services in a self-serve manner — this is great for teams that don’t have the technical expertise to build a machine learning system yet are looking for an easy-to-use technology to get a quick turnaround on quality training data. You can learn more about our Boost offerings \u003ca href=\"https://docs.labelbox.com/docs/using-data-labeling-service?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create pre-labels with foundation models\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn addition to creating pre-labels for classification projects, you have the ability to send model predictions as pre-labels to your labeling project. This can be done in one of two ways:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel-assisted labeling\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eImport computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel Foundry\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eAutomate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"verify-data-quality-with-custom-workflows\"\u003eVerify data quality with custom workflows\u003c/h3\u003e\u003cp\u003eContent moderation relies heavily on training the model on accurate and verified data. To ensure that you’re producing the most reliable and high-quality training datasets, you can customize your \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003elabeling review workflow\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/lqa22gzj7o\" title=\"How to enhance brand safety and content moderation with AI - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou can create customizable, multi-step review and rework pipelines to drive efficiency and automation for your review tasks. Set a review task based on specific parameters that are unique to your labeling team or desired outcome.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eInitial labeling task: reserved for all data rows that have been queued for labeling\u003c/li\u003e\u003cli\u003eInitial review task: first review task for data rows with submitted labels\u003c/li\u003e\u003cli\u003eRework task: reserved for data rows that have been rejected\u003c/li\u003e\u003cli\u003eDone task: reserved for data rows that have a) moved through their qualified tasks in the workflow or b) did not qualify for any of the tasks\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-2-create-a-model-run-fine-tune-an-llm-and-evaluate-model-performance\"\u003ePart 2: Create a model run, fine-tune an LLM, and evaluate model performance\u0026nbsp;\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this part of the tutorial, we’ll be taking the ground truth labels created in Part 1 to fine-tune a large language model (LLM). From there, we’ll evaluate model performance in Quantumworks Lab Model to diagnose model strengths and weaknesses and look to continuously boost and improve model performance.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-a-new-model\"\u003eCreate a new model\u0026nbsp;\u003c/h3\u003e\u003cp\u003eOnce you have your labeled data in your project in Annotate, you’re ready to move on to creating a model run in \u003ca href=\"https://app.labelbox.com/mea?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003eLabelbox Model\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z31zew9hbd\" title=\"How to enhance brand safety and content moderation with AI - Part 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a new model, you’ll need to:\u003c/p\u003e\u003cp\u003e1) Navigate to the \"Experiments\" tab in Model. The \"Experiments\" tab will be where you can find all model experiments across iterations.\u003c/p\u003e\u003cp\u003e2) Create a new model by selecting the \"New model\" button.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eProvide a model name\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect the model ontology — in this case we will select the same ontology we used to create our labeling project that contains the corresponding ground truth data.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSubmit and create a model — before creating a model run, you will also be able to see and verify the number of data rows that are being submitted.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate a model run\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve created a new model, we will need to create a new \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003emodel run\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eA model run is a model training experiment — each model run provides a versioned data snapshot of the data rows, annotations, and \u003ca href=\"https://docs.labelbox.com/docs/curate-data-splits?ref=labelbox-guides.ghost.io\"\u003edata splits\u003c/a\u003e for that model run. You can upload predictions to the model run and compare its performance against other model runs in a model directory.\u003c/p\u003e\u003cp\u003eThe model run we create will be the initial model run for our LLM fine-tuning experiment. To add a new model run:\u003c/p\u003e\u003cp\u003e1) Select \"New model run\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Give the model run a name (e.g “model run #1”)\u003c/p\u003e\u003cp\u003e3) Set data splits for the model run (for train, validate, and test)\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Create the model run\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter creating a model run, you’ll be able to see the corresponding data rows with ground truth populated into the appropriate train, validate, and test splits. This model run will be the gateway for us to export ground truth data to fine-tune a large language model.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/bObh0Kje6BWPfrIPWcd3V0TFt09svuX0-7Wka4IQI9j-bKdmhJAEjTWsWPWOmdFUg-CgU9fLQC-p_vFdafFXv4nYhMZupffw7Bl6TN8Z-2j771nF4riavmSL-xiDAmjU8E32deblRc4eEmNjptF3GpI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"327\"\u003e\u003c/figure\u003e\u003ch3 id=\"export-ground-truth-from-the-model-run-experiment-for-fine-tuning\"\u003eExport ground truth from the model run experiment for fine-tuning\u0026nbsp;\u003c/h3\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/s2gzlnehyy\" title=\"How to enhance brand safety and content moderation with AI - Part 8 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWe’ll be using this \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e to fine-tune a model and bring back inferences from the fine-tuned model for evaluation and diagnosis.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor this step, you will need:\u003c/p\u003e\u003cul\u003e\u003cli\u003eYour API Key\u0026nbsp;\u003c/li\u003e\u003cli\u003eYour Model Run ID to export the corresponding ground truth and articles from the model run\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExport ground truth from the model run experiment\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLabelbox will return the ground truth export in a JSON format. With the provided \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e, we can visualize the exported JSON into a DataFrame format for us to view corresponding ground truth for each article.\u0026nbsp;\u003c/p\u003e\u003cp\u003eGiven that we want to fine-tune a Google Vertex model with this data, we’ll need to convert the ground truth export to a GCP vertex tuning format (JSONL):\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# build LLM prompt and convert to GCP vertex tuning format (jsonl)\n\nprompt = 'Given the following headline and content, determine whether the article is related to climate change or similar topics. Also determine whether the article contains inaccurate or disinformation. Answer in the following format with Yes/No Answers: [climate related? / disinformation?]'\ndf['input_text'] = prompt + df['content']\ndf['output_text'] = 'climate related: ' + df['climate_related'] + ' disinformation: ' + df['disinformation_flag']\n\n\nwith open('modelPrompt_GCP.jsonl', 'w') as file:\nfor _, row in df[['input_text', 'output_text']].iterrows():\njson_line = row.to_json()\nfile.write(json_line + '\\n')\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"fine-tune-an-llm-with-google-vertex-ai\"\u003eFine-tune an LLM with Google Vertex AI \u003c/h3\u003e\u003cp\u003eFine-tuning is a technique whereby we take an off-the-shelf open-source or proprietary model and retrain it on a variety of concrete examples, and save the updated weights as a new model checkpoint. You can learn more about other techniques to leverage LLMs \u003ca href=\"https://labelbox.com/guides/zero-shot-learning-few-shot-learning-fine-tuning/?ref=labelbox-guides.ghost.io#zero-shot-learning-few-shot-learning-and-fine-tuning-in-action\"\u003ein this guide\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/um0f8w1rzn\" title=\"How to enhance brand safety and content moderation with AI - Part 9 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eFor this use case, we’ll be using \u003ca href=\"https://cloud.google.com/vertex-ai?ref=labelbox-guides.ghost.io\"\u003eGoogle Vertex AI \u003c/a\u003eto fine-tune an LLM with the ground truth from Part 1 of this tutorial. Once in the Vertex AI console, we’ll want to create a tuned model:\u003c/p\u003e\u003cul\u003e\u003cli\u003eChoose a supervised learning task\u0026nbsp;\u003c/li\u003e\u003cli\u003eEnter additional model parameters (e.g model name)\u0026nbsp;\u003c/li\u003e\u003cli\u003eUpload the JSONL file from the previous step\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, we can start the model tuning process. Once the model fine-tuning job has been completed, we can head over to the Google Vertex sandbox and give the newly tuned model a prompt.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, we can ask if the article is climate related and if it contains disinformation and it will provide a response based on the training dataset we provided.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-inferences-with-the-tuned-model-and-evaluate-model-effectiveness-in-labelbox\"\u003eCreate inferences with the tuned model and evaluate model effectiveness in Quantumworks Lab\u003c/h3\u003e\u003cp\u003eNow that we’ve fine-tuned a model, we can use it to make predictions on the initial dataset and compare it with our ground truth data to assess the fine-tuned model’s performance.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cjbrcpktcr\" title=\"How to enhance brand safety and content moderation with AI - Part 10 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eCreate inferences with the tuned model\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWe’ll need to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eInstall Google Vertex and Google Cloud SDK\u0026nbsp;\u003c/li\u003e\u003cli\u003eProvide the endpoint ID for the tuned model\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe can then start creating model inferences and predictions from the tuned model on our news articles. Use Pandas to clean up the responses, to remove corresponding prompts, and save them as a DataFrame — this will return the model’s initial headline and the client’s response if the data row is climate related or contains disinformation. \u003c/p\u003e\u003cp\u003eOnce we have model inferences, we can send the inferences back to a model run in Quantumworks Lab for further evaluation and analysis.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eEvaluate and diagnose model effectiveness\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo evaluate the effectiveness of the fine-tuned model in Quantumworks Lab, we’ll need to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSpecify the model run ID\u0026nbsp;\u003c/li\u003e\u003cli\u003eUpload the list of model inferences for each specific data row\u0026nbsp;\u003c/li\u003e\u003cli\u003eAttach each list of data rows and submit it to a model run in Quantumworks Lab as an upload job via the SDK\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce that’s complete, you can hop back to the original Quantumworks Lab model run and view the corresponding ground truth data and model inferences on each data row. You can visually compare the effectiveness of the fine-tuned model predictions (in red) with ground truth (in green).\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the \"Metrics view\" to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor this use case, our goal is to minimize the spread of disinformation, so we can take a look at the metric that shows corresponding articles that are considered \"disinformation\" by labelers, but where the model incorrectly predicted articles as \"not disinformation\". \u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003ch3 id=\"curate-high-impact-data-to-drastically-improve-model-performance\"\u003eCurate high-impact data to drastically improve model performance\u003c/h3\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7podbew17q\" title=\"How to enhance brand safety and content moderation with AI - Part 11 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select \"Find similar in Catalog\" from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled articles that you can send to your model for labeling, you can filter on the \"Annotation is\" filter and select \"none\". This will only show unlabeled text articles that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all articles that apply and send them as a batch to your original labeling project. Labeling these in priority will help improve model performance. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model and can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eUnmoderated content poses mounting risks to businesses with the risk of spreading misinformation, disinformation, and an unsafe online environment. With responsible implementation, businesses can leverage AI for trust and safety to efficiently and consistently identify high-risk content at scale. This not only helps create an online environment that is safe for users, but also helps protect brand reputation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"654407cdd96ee80001d8c876","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Frame-2299--2-.png","featured":false,"visibility":"public","created_at":"2023-11-02T20:34:21.000+00:00","updated_at":"2024-07-17T20:55:32.000+00:00","published_at":"2023-11-02T21:18:18.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-build-a-content-moderation-model-to-detect-disinformation","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},"url":"https://labelbox-guides.ghost.io/how-to-build-a-content-moderation-model-to-detect-disinformation/","excerpt":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","reading_time":12,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Frame-2299--2--2.png","og_title":"How to build a content moderation model to detect disinformation","og_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Frame-2299--2--1.png","twitter_title":"How to build a content moderation model to detect disinformation","twitter_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","meta_title":"How to build a content moderation model to detect disinformation","meta_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64ff69ca935e200001ee0180","uuid":"89463a23-02d8-47e3-98db-49448eb94d96","title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","slug":"how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox","html":"\u003cp\u003eIn machine learning, fine-tuning pre-trained models is a powerful technique that adapts models to new tasks and datasets. Fine-tuning takes a model that has already learned representations on a large dataset, such as a large language model, and leverages prior knowledge to efficiently “teach” the model a new task. \u003c/p\u003e\u003cp\u003eThe key benefit of fine-tuning is that it allows you to take advantage of transfer learning. Rather than training a model from scratch, which requires massive datasets and compute resources, you can start with an existing model and specialize it to your use case with much less data and resources. Fine-tuning allows ML teams to efficiently adapt powerful models to new tasks with limited data and compute. It is essential for applying state-of-the-art models to real-world applications of AI. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning?ref=labelbox-guides.ghost.io\"\u003eOpen AI’s fine-tuning API\u003c/a\u003e allows teams to fine-tune the following models:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGPT-3.5 -turbo-0613 (recommended)\u003c/li\u003e\u003cli\u003eBabbage-002\u003c/li\u003e\u003cli\u003eDavinci-002\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we’ll cover how to leverage \u003ca href=\"https://www.ssw.com.au/rules/what-is-gpt/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eOpen AI’s GPT-3.5\u003c/a\u003e and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe goal of model fine-tuning is to improve the model’s performance against a specific task. Over other techniques to optimize model output, such as prompt design, fine-tuning can help achieve:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigher quality results: \u003c/strong\u003eFine-tuning allows the model to learn from a much larger and more diverse dataset than can fit into a prompt. The model can learn more granular patterns and semantics that are relevant to your use case through extensive fine-tuning training. Prompts are limited in how much task-specific context they can provide, while fine-tuning teaches the model your specific domain.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eToken savings: \u003c/strong\u003eFine-tuned models require less prompting to produce quality outputs. With fine-tuning, you can leverage a shorter, more general prompt since the model has learned your domain – saving prompt engineering effort and tokens. Whereas highly-specific prompts can often hit token limits.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLower latency: \u003c/strong\u003eHeavily engineered prompts can increase latency as they require more processing. As fine-tuned models are optimized for your specific task, they allow faster inference and can quickly retrieve knowledge for your domain.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFine-tuning is especially beneficial for adapting models to your specific use case and business needs. There are several common scenarios where fine-tuning really can help models capture the nuances required for an application:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStyle, tone, or format customization:\u003c/strong\u003e Fine-tuning allows you to adapt models to match the specific style or tone required for a use case, whether it be a particular brand voice or difference in tone for speaking to various audiences.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDesired output structure: \u003c/strong\u003eFine-tuning can teach models to follow a required structure or schema in outputs. For example, you can fine-tune a summarization model to consistently include key facts in a standardized template.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHandling edge cases: \u003c/strong\u003eReal-world data often contains irregularities and edge cases. Fine-tuning allows models to learn from a wider array of examples, including rare cases. You can fine-tune the model on new data samples so that it learns to handle edge cases when deployed to production.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn short, fine-tuning allows teams to efficiently adapt powerful models to new tasks and datasets, allowing ML teams to customize general models to their specific use cases and business needs through extensive training on curated data. High-quality fine-tuning datasets are crucial to improve performance by teaching models the nuances and complexity of the target domain more extensively than possible through prompts alone.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOpen AI’s recommended dataset guidelines\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo fine-tune an Open AI model, it is required to provide at least ten examples. Research has shown clear improvements from fine-tuning on 50 to 100 training examples with GPT-3.5-turbo. Data quality, over data quantity, is also critical to the success of the fine-tuned model. \u003c/p\u003e\u003cp\u003eYou can learn more about preparing a dataset in \u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?ref=labelbox-guides.ghost.io\"\u003eOpen AI’s documentation\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"how-to-use-labelbox-for-fine-tuning\"\u003eHow to use Quantumworks Lab for fine-tuning\u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform for building intelligent applications. With a suite of powerful data curation, labeling, and model evaluation tools, the platform is built to help continuously improve and iterate on model performance. For this example, we will use the Quantumworks Lab platform to create a high-quality fine-tuning dataset. \u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can prepare a dataset of prompts and responses to fine-tune large language models (LLMs). Quantumworks Lab supports dataset creation for a variety of fine-tuning tasks including summarization, classification, question-answering, and generation.\u003c/p\u003e\u003cp\u003eWhen you set up an \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003eLLM data generation project\u003c/a\u003e in Quantumworks Lab, you will be prompted to specify how you will be using the editor. You have three choices for specifying your LLM data generation workflow:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 1: Humans generate prompts and responses\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, the prompt and response fields will be required. This will indicate to your team that they should create a prompt and response from scratch.\u003c/p\u003e\u003cp\u003eYou can make a copy of this \u003ca href=\"https://colab.research.google.com/drive/1ElV4U68ZJCJ-wIMLFTQAyzXrSSz6aU2Y?ref=labelbox-guides.ghost.io#scrollTo=HUhjjPp0mnPq\"\u003eGoogle Colab Notebook\u003c/a\u003e to generate a prompt and response dataset in Quantumworks Lab and fine-tune GPT-3.5 Turbo in OpenAI. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 2: Humans generate prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, only the prompt field will be required. This will indicate to your team that they should create a prompt from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 3: Humans generate responses to uploaded prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, a previously uploaded prompt will appear. Your team will need to create responses for that prompt.\u003c/p\u003e\u003cp\u003eYou can make a copy of this \u003ca href=\"https://colab.research.google.com/drive/1J21Nb0fqbZCcnxyR4wOKLlADwcayOlSq?ref=labelbox-guides.ghost.io#scrollTo=vGHl-lKkZ9cK\"\u003eGoogle Colab Notebook\u003c/a\u003e to generate a dataset of responses to uploaded prompts in Quantumworks Lab and fine-tune GPT-3.5 Turbo in OpenAI. \u003c/p\u003e\u003cp\u003eIn the below example, we’ll be walking through a sample use case of summarizing and removing PII from customer support chats with Quantumworks Lab and OpenAI’s GPT-3.5 Turbo. Imagining we’re a company who wishes to summarize support logs without revealing personally identifiable information in the process, we’ll be fine-tuning an LLM to summarize and remove PII from customer support logs.\u003c/p\u003e\u003ch3 id=\"step-1-evaluate-how-gpt-35-performs-against-the-desired-task\"\u003eStep 1: Evaluate how GPT-3.5 performs against the desired task\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/e00fk9u59h\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBefore we begin the fine-tuning process, let’s first evaluate how ChatGPT (using GPT-3.5) performs against the desired task off-the-shelf. \u003c/p\u003e\u003cp\u003eWe uploaded the following sample chat log to ChatGPT:\u003c/p\u003e\u003cp\u003e“Summarize this chat log and remove any personally identifiable information in the summary:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTom: \u003c/strong\u003eI need to reset my account access.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUrsula: \u003c/strong\u003eI can help with that, Tom. What’s your account email?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTom: \u003c/strong\u003eIt’s \u003ca href=\"mailto:tom@example.com\"\u003etom@example.com\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUrsula: \u003c/strong\u003eGreat, Tom. I’ve sent you a link to update your credentials”\u003c/p\u003e\u003cp\u003eIn the above prompt, we’ve asked ChatGPT to summarize the chat log and remove any personally identifiable information. \u003c/p\u003e\u003cp\u003eUpon evaluation, the default GPT-3.5 model misses the mark for our desired use case. \u003c/p\u003e\u003cp\u003eThe summary includes both Tom and Ursula’s names and explicitly mentions Tom’s email address. In order to reliably use the model for our business use case, we need to fine-tune it so that it appropriately excludes elements of personally identifiable information. To do so, we will leverage Quantumworks Lab to generate our fine-tuning dataset and use it to fine-tune GPT-3.5 through OpenAI. \u003c/p\u003e\u003ch3 id=\"step-2-create-a-llm-data-generation-project-in-labelbox\"\u003eStep 2: Create a LLM data generation project in Quantumworks Lab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ldpv9nwh14\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to upload our support chat logs to \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Catalog \u003c/a\u003e– this will allow us to browse, curate, and send these data rows for labeling.\u003c/p\u003e\u003cp\u003eNext, we’ll need to create a LLM data generation labeling project in \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Annotate\u003c/a\u003e.\u003c/p\u003e\u003cul\u003e\u003cli\u003eSince we have an available dataset, this will be a ‘Humans generate response to uploaded prompts’ project.\u003c/li\u003e\u003cli\u003eWhen configuring the ontology, we will set the response type as ‘text’ and make the appropriate response to “summarize and remove personally identifiable information in the summary”.\u003c/li\u003e\u003cli\u003eDuring ontology creation, you can also define a character minimum or maximum and upload necessary instructions for the labeling team.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-label-data\"\u003eStep 3: Label data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/i8tzhezn70\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter successfully setting up an LLM data generation project, we can queue the uploaded chat logs in Catalog for labeling in Annotate. To label data, you have the option of leveraging \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost’s\u003c/a\u003e extensive workforce or use your own internal team to summarize and remove personally identifiable information in the summary.\u003c/p\u003e\u003cp\u003eFor larger or more complex fine-tuning tasks, you can scale up to hundreds or thousands of labeled data rows. Once all data has been labeled, you can review the corresponding summary to each prompt and export the data rows.\u003c/p\u003e\u003ch3 id=\"step-4-export-data-from-labelbox-and-fine-tune-it-in-openai\"\u003e\u003cbr\u003eStep 4: Export data from Quantumworks Lab and fine-tune it in OpenAI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bx1z0xy4db\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWith all necessary data labeled, we can export the dataset from Quantumworks Lab and upload it in a format that is readable by OpenAI. \u003c/p\u003e\u003cp\u003eOpenAI requires a dataset to be in the structure of their \u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create?ref=labelbox-guides.ghost.io\"\u003echat completions API\u003c/a\u003e, whereby each message has a role, content, and optional name. You can learn more about specific dataset requirements in OpenAI’s \u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. Using a script, we can convert the Quantumworks Lab export into OpenAI’s required conversational chat format.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e{\n \"messages\":\n \t{\"role\":\"system\",\n    \"content\":\"Given a chat log, summarize and remove personal \t\tidentifiable information in the summary.\"},\n    {\"role\":\"user\",\n    \"content\":\"Andy:Why has my order not shipped yet?! Bella: I \tapologize for the delay, Andy:May I have your order number? Andy: \t  It's ORDER5678. Please hurry! Bella: Thank you Andy. It's \t\t\texpedited and will ship today.\"},\n\t{\"role\":\"assistant\",\n    \"content\":\"Customer inquires about the delay in the shipment of his order. Support agent requests the order number and upon receiving \t  it, assures customer that the order has been expedited and will \t\tship that day.\"\n    }\n ]\n}\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003eAfter formatting our dataset, we can upload it and \u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model?ref=labelbox-guides.ghost.io\"\u003estart a fine-tuning job\u003c/a\u003e using the OpenAI SDK.\u003c/p\u003e\u003cp\u003eYou can use a copy of the following \u003ca href=\"https://colab.research.google.com/drive/1J21Nb0fqbZCcnxyR4wOKLlADwcayOlSq?ref=labelbox-guides.ghost.io#scrollTo=vGHl-lKkZ9cK\"\u003eGoogle Colab notebook\u003c/a\u003e to export data from Quantumworks Lab in a format compatible with fine-tuning GPT-3.5 Turbo and begin a fine-tuning job. \u003c/p\u003e\u003ch3 id=\"step-5-assess-the-fine-tuned-model%E2%80%99s-performance-in-openai\"\u003eStep 5: Assess the fine-tuned model’s performance in OpenAI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9kdb8m0xm0\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 5) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter the fine-tuning job has succeeded, you can navigate to the OpenAI playground and select the newly fine-tuned model for evaluation. Similarly to evaluating the initial GPT-3.5 model, we can enter a sample chat log and see how the newly fine-tuned model performs.\u003c/p\u003e\u003cp\u003eCompared to the off-the-shelf GPT-3.5 model, this model that has been fine-tuned on our training data is performing as expected. We can see that all names and relevant information that would be considered as personally identifiable information has been retracted. \u003c/p\u003e\u003cp\u003eWe can also compare the fine-tuned model to the initial GPT-3.5 model and see how it performs on the same prompt. Again, we can see that while GPT-3.5 excludes some aspects of personally identifiable information, it still includes the user’s first name, so it doesn’t quite meet the expectations for our business use case.\u003c/p\u003e\u003cp\u003eThe newly fine-tuned model has allowed us to adapt GPT-3.5 to our specific use case of concealing personally identifiable information. With Quantumworks Lab, teams can iteratively identify gaps and outdated samples in the fine-tuning data, then generate fresh high-quality data, allowing model accuracy to be maintained over time. Updating fine-tuning datasets through this circular feedback process is crucial for adapting to new concepts and keeping models performing at a high level within continuously changing environments.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eTo improve LLM performance, Quantumworks Lab simplifies the process for subject matter experts to generate high-quality datasets for fine-tuning with leading model providers and tools, like OpenAI. \u003c/p\u003e\u003cp\u003eUnlock the full potential of large language models with Quantumworks Lab’s end-to-end platform and a \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox.ghost.io\"\u003enew suite of LLM tools\u003c/a\u003e to generate high-quality training data and optimize LLMs for your most valuable AI use cases.\u003c/p\u003e","comment_id":"64ff69ca935e200001ee0180","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Group-3081.jpg","featured":false,"visibility":"public","created_at":"2023-09-11T19:26:02.000+00:00","updated_at":"2024-05-28T17:02:27.000+00:00","published_at":"2023-09-11T19:58:50.000+00:00","custom_excerpt":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/","excerpt":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","og_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Group-3081-1.jpg","twitter_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","twitter_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","meta_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","meta_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64fb3e9f935e200001ee0106","uuid":"9f549cdf-80e9-4441-8b4e-f1ce92572f56","title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","slug":"how-to-fine-tune-vertex-ai-models-with-labelbox","html":"\u003cp\u003eIn machine learning, fine-tuning pre-trained models is a powerful technique that adapts models to new tasks and datasets. Fine-tuning takes a model that has already learned representations on a large dataset, such as a large language model, and leverages prior knowledge to efficiently “teach” the model a new task. \u003c/p\u003e\u003cp\u003eThe key benefit of fine-tuning is that it allows you to take advantage of transfer learning. Rather than training a model from scratch, which requires massive datasets and compute resources, you can start with an existing model and specialize it to your use case with much less data and resources. Fine-tuning allows ML teams to efficiently adapt powerful models to new tasks with limited data and compute. It is essential for applying state-of-the-art models to real-world applications of AI.\u003c/p\u003e\u003cp\u003eVertex AI provides \u003ca href=\"https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models?ref=labelbox-guides.ghost.io#supported_models\"\u003eseveral base models\u003c/a\u003e that can be fine-tuned:\u003c/p\u003e\u003cul\u003e\u003cli\u003etext-bison@001\u003c/li\u003e\u003cli\u003ecode-bison@001\u003c/li\u003e\u003cli\u003ecodechat-bison@001\u003c/li\u003e\u003cli\u003echat-bison@001\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.\u003c/p\u003e\u003chr\u003e\u003cp\u003eThe goal of model fine-tuning is to improve the model’s performance against a specific task. Over other techniques to optimize model output, such as prompt design, fine-tuning can help achieve:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigher quality results: \u003c/strong\u003eFine-tuning allows the model to learn from a much larger and more diverse dataset than can fit into a prompt. The model can learn more granular patterns and semantics that are relevant to your use case through extensive fine-tuning training. Prompts are limited in how much task-specific context they can provide, while fine-tuning teaches the model your specific domain.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eToken savings:\u003c/strong\u003e Fine-tuned models require less prompting to produce quality outputs. With fine-tuning, you can leverage a shorter, more general prompt since the model has learned your domain – saving prompt engineering effort and tokens. Whereas highly-specific prompts can often hit token limits.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLower latency: \u003c/strong\u003eHeavily engineered prompts can increase latency as they require more processing. As fine-tuned models are optimized for your specific task, they allow faster inference and can quickly retrieve knowledge for your domain.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFine-tuning is especially beneficial for adapting models to your specific use case and business needs. There are several common scenarios where fine-tuning really can help models capture the nuances required for an application:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStyle, tone, or format customization:\u003c/strong\u003e Fine-tuning allows you to adapt models to match the specific style or tone required for a use case, whether it be a particular brand voice or difference in tone for speaking to various audiences.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDesired output structure: \u003c/strong\u003eFine-tuning can teach models to follow a required structure or schema in outputs. For example, you can fine-tune a summarization model to consistently include key facts in a standardized template. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHandling edge cases: \u003c/strong\u003eReal-world data often contains irregularities and edge cases. Fine-tuning allows models to learn from a wider array of examples, including rare cases. You can fine-tune the model on new data samples so that it learns to handle edge cases when deployed to production.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn short, fine-tuning allows teams to efficiently adapt powerful models to new tasks and datasets, allowing ML teams to customize general models to their specific use cases and business needs through extensive training on curated data. High-quality fine-tuning datasets are crucial to improve performance by teaching models the nuances and complexity of the target domain more extensively than possible through prompts alone.\u003c/p\u003e\u003ch2 id=\"how-to-use-labelbox-for-fine-tuning\"\u003eHow to use Quantumworks Lab for fine-tuning\u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform for building intelligent applications. With a suite of powerful data curation, labeling, and model evaluation tools, the platform is built to help continuously improve and iterate on model performance. We will use the Quantumworks Lab platform to create a high-quality fine-tuning dataset. \u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can prepare a dataset of prompts and responses to fine-tune large language models (LLMs). Quantumworks Lab supports dataset creation for a variety of fine-tuning tasks including summarization, classification, question-answering, and generation.\u003c/p\u003e\u003ch3 id=\"step-1-evaluate-how-a-model-performs-against-the-desired-task\"\u003eStep 1: Evaluate how a model performs against the desired task\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/n4ob00h4cj\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-2-create-an-llm-data-generation-dataset-in-labelbox\"\u003eStep 2: Create an LLM data generation dataset in Quantumworks Lab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1248\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 1248w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eWhen you set up an \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003eLLM data generation project\u003c/a\u003e in Quantumworks Lab, you will be prompted to specify how you will be using the editor. You have three choices for specifying your LLM data generation workflow:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 1: Humans generate prompts and responses\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1252\" height=\"578\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 1252w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn the editor, the prompt and response fields will be required. This will indicate to your team that they should create a prompt and response from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 2: Humans generate prompts\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1260\" height=\"576\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 1260w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn the editor, only the prompt field will be required. This will indicate to your team that they should create a prompt from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 3: Humans generate responses to uploaded prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, a previously uploaded prompt will appear. Your team will need to create responses for that prompt.\u003c/p\u003e\u003cp\u003eFor our project, we'll want to create a \"Humans generate responses to uploaded prompts\" project. Namely, we want humans to create responses in the form of a list of airlines. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/irnaoi0u64\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-3-upload-data-to-vertex-ai\"\u003eStep 3: Upload data to Vertex AI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1dwwdeggyj\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eExport the Quantumworks Lab fine-tuning dataset \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve constructed a fine-tuning dataset with Quantumworks Lab, you can export it using our \u003ca href=\"https://colab.research.google.com/drive/1imCvNhd1rZNEf_dCIsPT-wLfnHkv3o_A?ref=labelbox-guides.ghost.io\"\u003eLabelbox to Vertex AI conversion script\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStart a model tuning job using Vertex AI \u0026amp; deploy the model\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAfter exporting the fine-tuned dataset, start a \u003ca href=\"https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models?ref=labelbox-guides.ghost.io\"\u003emodel tuning job using Vertex\u003c/a\u003e. When a fine-tuning job is run, the model learns additional parameters that help it encode the necessary information to perform the desired behavior or learn the desired behavior. \u003c/p\u003e\u003cp\u003eThe output of the tuning job is a new model, which is effectively a combination of the newly learned parameters and the original model. Once the fine-tuning job is complete, you can deploy the model and return to Quantumworks Lab for model evaluation.\u003c/p\u003e\u003ch3 id=\"step-4-evaluate-and-iterate-on-fine-tuning-dataset-quality\"\u003eStep 4: Evaluate and iterate on fine-tuning dataset quality\u003c/h3\u003e\u003cp\u003eA well-performing fine-tuned model indicates the effective optimization of model architecture, training data, and hyperparameters. It signifies that the training dataset used for fine-tuning is high-quality and is representative of the real-world use case. This allows for the fine-tuned model to achieve better performance on tasks compared to the base model in less time than it would have to train a model from scratch. \u003c/p\u003e\u003cp\u003eReal-world conditions and data are often dynamic. As the use case evolves, it's crucial to maintain representativeness and relevance in the fine-tuning data. Continuous evaluation of the fine-tuned model’s performance can help detect edge cases or model errors.  You can evaluate model performance and debug errors leveraging \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e. Utilize interactive auto-populated model metrics, such as a confusion matrix, precision, recall, F1 score, and more to surface model errors. Detect and visualize corner-cases where the model is underperforming and generate high-impact data to drastically improve model performance. After running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1258\" height=\"632\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 1258w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eLabelbox Model\u003c/em\u003e\u003c/i\u003e\u003c/a\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e allows teams to debug models and iteratively improve model performance\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBy iteratively identifying gaps and outdated samples in the fine-tuning data, then generating fresh high-quality data, model accuracy can be maintained over time. Updating fine-tuning datasets through this circular feedback process is crucial for adapting to new concepts and keeping models performing at a high level within continuously changing environments.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eTo improve LLM performance, Quantumworks Lab simplifies the process for subject matter experts to generate high-quality datasets for fine-tuning with leading model providers and tools, like Google Vertex AI. \u003c/p\u003e\u003cp\u003eUnlock the full potential of large language models with Quantumworks Lab’s end-to-end platform and a \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox.ghost.io\"\u003enew suite of LLM tools\u003c/a\u003e to generate high-quality training data and optimize LLMs for your most valuable AI use cases. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..\u0026ref=labelbox-guides.ghost.io\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"64fb3e9f935e200001ee0106","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299.png","featured":false,"visibility":"public","created_at":"2023-09-08T15:32:47.000+00:00","updated_at":"2024-03-26T16:25:35.000+00:00","published_at":"2023-09-08T18:03:36.000+00:00","custom_excerpt":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-vertex-ai-models-with-labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-vertex-ai-models-with-labelbox/","excerpt":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","reading_time":6,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299-1.png","og_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","og_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299-2.png","twitter_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","twitter_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","meta_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","meta_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":21,"tag":{"slug":"train-fine-tune-ai","id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","count":{"posts":21},"url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},"slug":"train-fine-tune-ai","currentPage":"1"},"__N_SSG":true},"page":"/guides/tag/[id]/page/[pagenum]","query":{"id":"train-fine-tune-ai","pagenum":"1"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script>
    

    

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>

                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><span style="color: inherit; cursor: default;">Docs</span></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="/static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <span style="color: inherit; cursor: default;">Terms of Service</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Privacy Notice</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Copyright Dispute Policy</span>
                </div>
            </div>
        </div>
    </footer>
</body>
<!-- Mirrored from labelbox.com/guides/tag/train-fine-tune-ai/page/1/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:53:14 GMT -->
</html>