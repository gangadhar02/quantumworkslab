<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/tag/train-fine-tune-ai/page/2/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:20:34 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../../../static/scripts/munchkin.js"></script><script src="../../../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/guides/tag/%5bid%5d/page/%5bpagenum%5d-da4e9ee1c105845a.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../../../index.html"><img width="106" height="24" alt="logo" src="../../../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../../../build-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Build AI</a><a href="../../../use-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Use AI</a><a href="../../../explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="../../../label-data-for-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Label data for AI</a><a href="../../index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../../../mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-uncover-model-errors-with-labelbox-model/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexdb1e.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3065--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-uncover-model-errors-with-labelbox-model/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to uncover model errors with Quantumworks Lab Model</p><p class="text-base max-w-2xl undefined line-clamp-3">Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index38a9.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Meta&#x27;s Segment Anything (SAM) model on video with Quantumworks Lab&#x27;s model-assisted labeling</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-fine-tune-large-language-models-with-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index528e.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FFrame-2299--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-fine-tune-large-language-models-with-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to fine-tune large language models (LLMs) with Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../intro-to-model-metrics/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexebef.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../intro-to-model-metrics/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">An introduction to model metrics</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index3e9d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Quantumworks Lab and Weights &amp; Biases to fine tune your computer vision projects</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can use Quantumworks Lab and Weights &amp; Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-train-evaluate-and-improve-your-ml-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexcf8d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-train-evaluate-and-improve-your-ml-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to get started in Quantumworks Lab Model: Train, evaluate, and improve your ML models</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we&#x27;ll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../the-guide-to-getting-started-with-active-learning/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2963--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2963--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2963--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2963--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2963--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2963--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2963--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2963--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index413d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2963--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../the-guide-to-getting-started-with-active-learning/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Get started with active learning</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to get started with active learning by leveraging the 3 techniques that consistently help ML teams more quickly identify what data will most dramatically improve model performance.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-find-and-fix-label-errors/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2791--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2791--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2791--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2791--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2791--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2791--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2791--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2791--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index4e92.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2791--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-find-and-fix-label-errors/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to find and fix label errors</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-curate-and-version-your-training-datasets-and-hyperparameters/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FGroup-2721.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FGroup-2721.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FGroup-2721.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FGroup-2721.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FGroup-2721.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FGroup-2721.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FGroup-2721.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FGroup-2721.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexf415.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FGroup-2721.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-curate-and-version-your-training-datasets-and-hyperparameters/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to curate and version your training datasets and hyperparameters</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. Easily track and reproduce model experiments to observe the differences and share best practices with your team. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-find-and-fix-model-errors/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2818--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2818--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2818--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2818--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2818--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2818--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2818--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2818--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index3fd1.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F10%2FGroup-2818--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-find-and-fix-model-errors/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to find and fix model errors</p><p class="text-base max-w-2xl undefined line-clamp-3">A great way to boost model performance is to surface edge cases on which the model might be struggling. You can fix those model failures with targeted improvements to your training data so that the model is better trained on these edge cases. </p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8"><a class="mr-9 text-neutral-700 mb-1" href="../1/index.html">&lt;</a>Page 2 of 3<a class="ml-9 text-neutral-700 mb-1" href="../3/index.html">&gt;</a></div></div></div></div></div></div>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><span style="color: inherit; cursor: default;">Docs</span></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <span style="color: inherit; cursor: default;">Terms of Service</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Privacy Notice</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Copyright Dispute Policy</span>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"648a3024f45029000168a9d5","uuid":"df001e73-f4a2-4a45-8bc7-8ebbaa093778","title":"How to uncover model errors with Quantumworks Lab Model","slug":"how-to-uncover-model-errors-with-labelbox-model","html":"\u003cp\u003eMachine learning models are only as good as the quality of their predictions. But how do you ensure that your model is making accurate predictions, and more importantly, how do you identify and address the errors your model might be making? In this guide, we will be exploring how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.\u003c/p\u003e\u003cp\u003eIn the examples below, we’ll start by uploading predictions from YOLOv8 along with confidence scores to Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003emodel run\u003c/a\u003e. We’ll also use an open source dataset to provide a diverse dataset for testing the model. You can get instructions on how to upload model inferences to Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/page/tutorials?ref=labelbox-guides.ghost.io#upload-predictions-to-a-model-run\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"gallery-view-identifying-disagreements\"\u003eGallery View: Identifying disagreements\u003c/h2\u003e\u003cp\u003eGallery View is a powerful tool for identifying disagreements between model predictions and ground truth labels. Here's how to use it for error analysis:\u003c/p\u003e\u003cp\u003e1) Go to the Gallery View within a model run. You may choose to focus on the Validation or Test split if you prefer.\u003c/p\u003e\u003cp\u003e2) Apply a Metrics filter to identify images with metrics that could indicate disagreements between model predictions and ground truth annotations. Users can sort the assets based on any combination of IOU, confidence, recall, false negative, and false positive etc.\u003c/p\u003e\u003cp\u003e3) Sort the data rows either by increasing metrics or increasing order of confidence. This can help surface rows where the model is least confident or is likely to be erroneous.\u003c/p\u003e\u003cp\u003e4) Inspect the surfaced data rows in detail to identify patterns of edge cases where the model is struggling. This may involve manually inspecting hundreds of data rows.\u003c/p\u003e\u003cp\u003eThe video below shows an uncertainty sampling based on a low-confidence example.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7mair0juaw\" title=\"[Uncover model errors] MEA low confidence Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe video below shows how to find model errors for a particular class by sorting in ascending order for IOU. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/tlgntztpau\" title=\"[Uncover model errors] IOU ascending Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eIdentifying incorrect model prediction based on low intersection over union\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-metrics-identifying-struggling-classes\"\u003eModel Metrics: Identifying struggling classes\u003c/h2\u003e\u003cp\u003eThe Metrics View provides a comprehensive overview of how your model is performing. You can leverage this view to quickly identify classes that your model might be struggling with by:\u003c/p\u003e\u003col\u003e\u003cli\u003eInspecting the metrics in the Metrics View. In this example, we’re finding predictions that are false negatives and have recall value for a person between .2 to .3.\u003c/li\u003e\u003cli\u003eClicking on the recall value for person between .2 to .3 will open the Gallery View, which will have filtering and sorting activated to show assets associated with the particular class.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xumtw94n2i\" title=\"[Uncover model errors] Metrics view Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch2 id=\"fix-model-errors\"\u003eFix model errors\u003c/h2\u003e\u003cp\u003eOnce you have identified a pattern of incorrect model predictions, you can find similar assets that the model will also struggle with and send them to be labeled before retraining your model.\u003c/p\u003e\u003col\u003e\u003cli\u003eSelect data rows on which your model is struggling.\u003c/li\u003e\u003cli\u003eOpen the selected data rows in Catalog by clicking on [n] selected \u0026gt; View in Catalog. You will then be redirected to a filtered view of your Catalog showing only the previously selected data rows.\u003c/li\u003e\u003cli\u003eUse \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003esimilarity search\u003c/a\u003e to surface data similar to this pattern of model failures among all of the data in your Catalog. Optionally, you could create a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e that will automatically collect any similar data uploaded in the future.\u003c/li\u003e\u003cli\u003eNext, you could filter on Annotation \u0026gt; is none to surface only unlabeled data rows. Labeling this high-impact data and then re-training your model is a powerful way to boost model performance. Create a \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003ebatch\u003c/a\u003e and send it to a labeling project.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/htzhdxduhh\" title=\"[Uncover model errors] Curating a batch Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLabelbox offers powerful tools and workflows to not only uncover model errors but also to improve the performance of your machine learning models over time. By leveraging the Gallery View, Model Metrics, and Projector View, you can identify where model might be struggling. Additionally, with the ability to fix these errors through data-centric iterations, you can ensure that your model becomes more accurate and reliable with each iteration.\u003c/p\u003e","comment_id":"648a3024f45029000168a9d5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3065--2-.png","featured":false,"visibility":"public","created_at":"2023-06-14T21:24:52.000+00:00","updated_at":"2023-10-27T17:14:26.000+00:00","published_at":"2023-06-15T17:22:00.000+00:00","custom_excerpt":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-uncover-model-errors-with-labelbox-model/","excerpt":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":"How to uncover model errors with Gallery View and Model Metrics","og_description":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","twitter_image":null,"twitter_title":"How to uncover model errors with Gallery View and Model Metrics","twitter_description":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","meta_title":"How to uncover model errors with Gallery View and Model Metrics","meta_description":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6478fbf1f06ecd0001d70bc5","uuid":"506ee668-bb83-4bc8-80b1-8c9b4c76e759","title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling","slug":"using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWhile Yolov8 is no longer supported on Quantumworks Lab, this blog remains relevant if you are working with other object detection models. Alternatives such as OWL-ViT, Rekognition, GroundingDINO, and GroundingDINO + SAM can still be found and used on Quantumworks Lab’s platform.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this guide, we will demonstrate the application of foundation models, such as Meta’s Segment Anything and YOLOv8, to automatically detect, classify and draw masks on objects of interest in a video. This is a follow-up to earlier guide: \u003ca href=\"https://labelbox.com/guides/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/?ref=labelbox-guides.ghost.io\"\u003eUsing Meta’s Segment Anything with YOLOv8 to Automatically Classify Masks\u003c/a\u003e. In this guide, we’ll automatically detect and segment objects in a video.\u003c/p\u003e\u003cp\u003eVideos have many frames and are tedious to label. Segmentation masks are even more time consuming to label as they vary ever so slightly frame-by-frame, requiring manual fine-tuning each time. With foundation models, you can automate and significantly speed up the labeling process to label more video data, in less time. This allows you to focus valuable time on review, simply correcting the AI models’ output.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"498\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we will be walking through a simple semantic segmentation task: drawing masks around a person as they skateboard.\u003c/p\u003e\u003cp\u003eHere’s a high-level summary of the process that we will be walking through step-by-step below, with code:\u003c/p\u003e\u003cp\u003e1) Load YOLOv8, SAM and Quantumworks Lab Python SDK\u003c/p\u003e\u003cp\u003e2) For each frame of the video:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun an object detector to generate bounding boxes with classifications for specified classes\u003c/li\u003e\u003cli\u003eFeed the bounding boxes as inputs to Meta’s Segment Anything model which will produce segmentation masks\u003c/li\u003e\u003cli\u003ePrepare mask predictions in a format that Quantumworks Lab Python SDK expects\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e3) Upload all frames at once to Quantumworks Lab via prediction import\u003c/p\u003e\u003cp\u003e4) Open up video editor and review or modify the pre-labels as you usually do\u003c/p\u003e\u003cp\u003eYou can run all of the above out-of-the-box on your video(s) using our \u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/sam/meta_sam_labelbox_video.ipynb?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e. Simply load in your video and get automatically segmented masks, with classes in Quantumworks Lab, in minutes!\u003c/p\u003e\u003cp\u003eFor this guide, we will use the following video:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-orig.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-orig.gif 600w\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-load-yolov8\"\u003eStep 1: Load YOLOv8\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.ultralytics.com/?ref=labelbox-guides.ghost.io\"\u003eYOLOv8\u003c/a\u003e is a state-of-the-art object detector that produces bounding boxes and classes around common objects. It's the latest iteration of the YOLO (You Only Look Once) family of models, and it boasts some impressive features. YOLOv8 is known for its speed and accuracy, making it an invaluable tool for a wide range of applications. Here, we use YOLOv8 to automatically detect and localize the person skateboarding in the video.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport ultralytics\nultralytics.checks()\nfrom ultralytics import YOLO\nmodel = YOLO(f'{HOME}/yolov8n.pt')\n\n# each class id is assigned a different color\ncolors = np.random.randint(0, 256, size=(len(model.names), 3))\nprint(model.names)\n\n# Specify which classes you care about. The rest of classes will be filtered out.\nchosen_class_ids = [0] # 0 refers to person, as per model.names\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-2-load-sam\"\u003eStep 2: Load SAM\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eMeta's SAM model\u003c/a\u003e is a state-of-the-art computer vision model that is designed to accurately segment images and videos into distinct objects. Using advanced deep learning techniques, Segment Anything is able to identify and segment objects in images, making it a powerful tool for a wide range of applications. The SAM model is able to generate segmentation masks based on prompts, including bounding box prompts, which we will use in the code below.\u003c/p\u003e\u003cp\u003e\u003cbr\u003eFor an in-editor experience of SAM, please read our other blog post \u003ca href=\"https://labelbox.com/blog/coming-soon-auto-segment-powered-by-sam/?ref=labelbox-guides.ghost.io\"\u003eAuto-Segment 2.0 powered by Meta’s Segment Anything Model\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport torch\nimport matplotlib.pyplot as plt\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nsam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\nmask_predictor = SamPredictor(sam)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-3-load-labelboxs-python-sdk\"\u003eStep 3: Load Quantumworks Lab's Python SDK\u003c/h2\u003e\u003cp\u003eLabelbox’s Python SDK gives you easy methods to create ontologies, projects and datasets, and upload masks to a video.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab as lb\nimport labelbox.types as lb_types\n\n# Create a Quantumworks Lab API key for your account by following the instructions here:\n# https://docs.labelbox.com/reference/create-api-key\n# Then, fill it in here\nAPI_KEY = \"\"\nclient = lb.Client(API_KEY)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-4-run-yolov8-and-sam-per-frame\"\u003eStep 4: Run YOLOv8 and SAM per-frame\u003c/h2\u003e\u003cp\u003eHere we run the models on each frame and generate masks automatically.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003ecap = cv2.VideoCapture(VIDEO_PATH)\n\n# This will contain the resulting mask predictions for upload to Quantumworks Lab\nmask_frames = []\n\nframe_num = 1\nwhile cap.isOpened():\n  ret, frame = cap.read()\n  if not ret:\n    break\n\n  # Run frame through YOLOv8 to get detections\n  detections = model.predict(frame, conf=0.7)\n \n  # Run frame and detections through SAM to get masks\n  transformed_boxes = mask_predictor.transform.apply_boxes_torch(detections[0].boxes.xyxy, list(get_video_dimensions(cap)))\n  mask_predictor.set_image(frame)\n  masks, scores, logits = mask_predictor.predict_torch(\n    boxes = transformed_boxes,\n    multimask_output=False,\n    point_coords=None,\n    point_labels=None\n  )\n\n  # Combine mask predictions into a single mask, each with a different color\n  class_ids = detections[0].boxes.cpu().cls\n  merged_with_colors = add_color_to_mask(masks[0][0], colors[int(class_ids[0])]).astype(np.uint8)\n  for i in range(1, len(masks)):\n    curr_mask_with_colors = add_color_to_mask(masks[i][0], colors[int(class_ids[i])])\n    merged_with_colors = np.bitwise_or(merged_with_colors, curr_mask_with_colors)\n\n  # Upload multi-colored combined mask to temp location\n  # to get temp instance uri\n  instance_uri = get_instance_uri(client, global_key, merged_colored_mask)\n\n  # Create MaskFrame object to be uploaded to Quantumworks Lab\n  mask_frame = lb_types.MaskFrame(index=frame_num, instance_uri=instance_uri)\n  mask_frames.append(mask_frame)\n\n  frame_num += 1\n\ncap.release()\n\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-bboxes.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-bboxes.gif 600w\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-masks.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-masks.gif 600w\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-5-upload-the-predicted-masks-as-pre-labels-onto-labelbox\"\u003eStep 5: Upload the predicted masks as pre-labels onto Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThe predicted masks can be easily and seamlessly integrated into Quantumworks Lab via our SDK.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Create MaskInstance per unique class predicted / chosen\ninstances = []\n for cid in chosen_class_ids:\n   color = get_color(colors[int(cid)])\n   name = model.names[int(cid)]\n   instances.append(lb_types.MaskInstance(color_rgb=color, name=name))\n\n# Create list of VideoMaskAnnotation objects, one for each unique class\nannotations = []\nfor instance in instances:\n  video_mask_annotation = lb_types.VideoMaskAnnotation(\n       frames=mask_frames,\n       instances=[instance]\n   )\n  annotations.append(video_mask_annotation)\n\n# Create Label object\nlabels = [\nlb_types.Label(data=lb_types.VideoData(global_key=global_key),\n                  annotations=annotations))\n]\n\n# Run import job\nupload_job = lb.MALPredictionImport.create_from_objects(\n   client=client,\n   project_id=project.uid,\n   name=\"mal_import_job\" + str(uuid.uuid4()),\n   predictions=labels\n)\nupload_job.wait_until_done()\n\nprint(f\"Errors: {upload_job.errors}\", )\nprint(f\"Status of uploads: {upload_job.statuses}\")\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-LB.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"374\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-LB.gif 600w\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eCreating segmentation masks on video data can be tedious and time-consuming. Using the power of foundation models in Quantumworks Lab, you can easily generate masks with classifications in a matter of minutes. Rather than spending hours labeling video data, you now have a way to accelerate video labeling and not only reduce time to market, but also the cost of developing your models.\u003c/p\u003e","comment_id":"6478fbf1f06ecd0001d70bc5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074.png","featured":false,"visibility":"public","created_at":"2023-06-01T20:13:37.000+00:00","updated_at":"2024-11-25T21:18:27.000+00:00","published_at":"2023-06-01T22:14:48.000+00:00","custom_excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/","excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074-1.png","og_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","og_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074-2.png","twitter_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","twitter_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","meta_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","meta_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64134c4235aad5003db6f2b2","uuid":"72160526-3ea8-42ee-ab34-75b7b0fa3878","title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","slug":"how-to-fine-tune-large-language-models-with-labelbox","html":"\u003ch2 id=\"what-are-large-language-models-llms\"\u003eWhat are large language models (LLMs)?\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/usecases/large-language-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLarge language models\u003c/a\u003e leverage deep learning techniques to recognize, classify, analyze, generate and even predict text. Critical in \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003enatural language processing (NLP)\u003c/a\u003e applications like AI voice assistants, chatbots, translation, and sentiment analysis — large language models rely upon large volumes of data to consistently comprehend, capture and convey the nuances of human language.\u003c/p\u003e\u003ch2 id=\"why-should-you-consider-using-one\"\u003eWhy should you consider using one?\u003c/h2\u003e\u003cp\u003eRecent advancements and accessibility of large language models can serve as a powerful starting point for your machine learning team. Although you’ll still need to retrain these base models on data that is contextually-relevant to your use case, leveraging a foundational model saves significant times and costs. Large language models significantly improve with Reinforcement Learning from Human Preferences (RLHP). This guide provides a framework for using Quantumworks Lab to fine-tune OpenAI 's popular GPT-3 large language model for your use case. \u003c/p\u003e\u003ch2 id=\"getting-started\"\u003e\u003cstrong\u003eGetting Started\u003c/strong\u003e \u003c/h2\u003e\u003cp\u003eThis guide and Colab notebook will walk you through how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect your own relevant classification ontology to use with foundational large language models like GPT-3.\u003c/li\u003e\u003cli\u003eCreate a Project in Quantumworks Lab Annotate to generate labeled training data.\u003c/li\u003e\u003cli\u003eLeverage iterative model runs to rapidly tune OpenAI large language models.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model to diagnose performance, find high impact data, get the data labeled, and create another model run for the next iteration of fine tuning.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/deya57pbun\" title=\"How to fine-tune large language models (LLMs) with Quantumworks Lab Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"1-import-colab-notebook-packages\"\u003e1. Import Colab notebook packages\u003c/h3\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1p3nwxBHUpnUMP4B3mWAACwL-g4uOpjzw?ref=labelbox-guides.ghost.io\" class=\"kg-btn kg-btn-accent\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003eTo help you get started, for this workflow, we’ve created a \u003ca href=\"https://colab.research.google.com/drive/1p3nwxBHUpnUMP4B3mWAACwL-g4uOpjzw?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e which has installed Open AI and Quantumworks Lab packages within the same python notebook. Import all of the packages and use the corresponding Open AI and Quantumworks Lab API keys to connect to your instances. \u003c/p\u003e\u003cp\u003e\u003ccode\u003e!pip install Quantumworks Lab[data] --upgrade -q !pip install openai -q \u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"2-create-a-project-based-on-your-desired-data-ontology\"\u003e\u003cstrong\u003e2\u003c/strong\u003e.\u003cstrong\u003e Create a Project based on your desired data ontology\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNext, \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003ecreate a Project\u003c/a\u003e in the platform that matches the defined ontology for the data you want to classify using Open AI’s GPT-3 model. Add this ontology to a Project. Our Colab notebook example focuses on classifying e-commerce assets into the following four categories that comprise 80% of all e-commerce assets sold:\u003c/p\u003e\u003cul\u003e\u003cli\u003e“Electronics”\u003c/li\u003e\u003cli\u003e“Household”\u003c/li\u003e\u003cli\u003e“Books”\u003c/li\u003e\u003cli\u003e“Clothing \u0026amp; Accessories” up  common e-commerce use case.  \u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e``import os \nimport openai\nfrom labelbox.schema.ontology import OntologyBuilder, Tool, Classification, Option\nfrom Quantumworks Lab import Client, LabelingFrontend, LabelImport, MALPredictionImport``\n\n``from labelbox.data.annotation_types import (\n    Label, ImageData, ObjectAnnotation, MaskData,\n    Rectangle, Point, Line, Mask, Polygon,\n    Radio, Checklist, Text,\n    ClassificationAnnotation, ClassificationAnswer\n    )``\n\n``from labelbox.data.serialization import NDJsonConverter\nimport pandas as pd\nimport shutil\nimport labelbox.data\nimport json\nimport uuid ``\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"3-generate-training-data\"\u003e3. Generate training data\u003c/h3\u003e\u003cp\u003eOnce you’ve determined the ontology and added it to your Quantumworks Lab Project, you can begin curating the training data that supports your requirements. This will ultimately play a critical role towards adapting GPT-3 to support your use case. Once you add your ontology to a project, you have multiple options for creating training data, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab Annotate to label the data yourself\u003c/li\u003e\u003cli\u003eImporting existing annotations you have in your data lake\u003c/li\u003e\u003cli\u003eLeveraging our experts from \u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edata labeling services\u003c/a\u003e for support rapidly curating training-quality data \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"4-push-the-batch-of-training-data-labels-to-model-runs\"\u003e4. Push the batch of training data labels to model runs\u003c/h3\u003e\u003cp\u003eOnce you've created the training data, select the appropriate, corresponding data rows and export them to \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Model\u003c/a\u003e for model training. Note that you'll need to implement model training settings that connect your model training environment and cloud service provider (CSP) to Labelbox. Our Colab Notebook demo uses Google Cloud Platform (GCP) but this same workflow and Quantumworks Lab’s cloud-agnostic platform works well with any model training environment. \u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1148\" height=\"586\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png 1148w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eYou will iteratively run the last section of this notebook as you build momentum fine-tuning your model and improving performance.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"5-iteratively-improve-and-fine-tune-the-model\"\u003e5. Iteratively improve and fine-tune the model\u003c/h3\u003e\u003cp\u003eNext the Colab notebook features a list of prompts to iteratively train the Open AI GPT-3 model based on the annotations that fit your unique use case. As you review the model predictions in Quantumworks Lab Model, the platform will help you easily identify mis-predictions and target areas where the model consistently performs poorly. \u003c/p\u003e\u003cp\u003eIn Quantumworks Lab Catalog, you can leverage embeddings and similarity search features to find training data samples that exhibit similar characteristics to data where your model is consistently performing poorly – then queue that data for labeling so it can be incorporated during your next retraining iteration. You'll do this by iteratively \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003esubmitting a batch\u003c/a\u003e – featuring these newly-curated Data Rows – to the same Project you created earlier (in step two) for fine-tuning your LLM. Ultimately, this iterative loop of exposing the model to new prompts will allow you to continuously fine-tune the GPT-3 model to perform based on your own data priorities.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1656\" height=\"834\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 1656w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFind and prioritize data from Catalog most likely to have the highest impact on your next training iteration.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe notebook feeds the output results from Open AI back into the Quantumworks Lab Model tab. This iterative back and forth workflow empowers you to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLeverage Quantumworks Lab Model to measure model performance \u003c/li\u003e\u003cli\u003eEvaluate model output predictions and identify areas where the model performs poorly\u003c/li\u003e\u003cli\u003eLeverage Catalog to prioritize data from your Catalog that will have the most maximal impact towards addressing your edge cases\u003c/li\u003e\u003cli\u003eFine-tune the Open AI large language model by iteratively feeding it relevant data addressing your ontology, finding erroneous model predictions and fixing areas where the model performs poorly.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eGet started today by following the prompts in this \u003ca href=\"https://colab.research.google.com/drive/1p3nwxBHUpnUMP4B3mWAACwL-g4uOpjzw?ref=labelbox-guides.ghost.io#scrollTo=KSsXvqbaeLXs\"\u003eColab notebook\u003c/a\u003e. To learn more about Quantumworks Lab Model, check out the following guides below:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-train-evaluate-and-improve-your-ml-models/?ref=labelbox-guides.ghost.io\"\u003eHow to get started in Quantumworks Lab Model: Train, evaluate, and improve your ML models\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-run-model-assisted-labeling-with-active-learning-on-ner-data-with-a-hugging-face-model/?ref=labelbox-guides.ghost.io\"\u003eHow to run model-assisted labeling and active learning on NER data with a Hugging Face model\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"64134c4235aad5003db6f2b2","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Frame-2299--2-.png","featured":false,"visibility":"public","created_at":"2023-03-16T17:05:06.000+00:00","updated_at":"2024-10-02T22:30:29.000+00:00","published_at":"2023-03-22T23:00:29.000+00:00","custom_excerpt":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-large-language-models-with-Labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-large-language-models-with-labelbox/","excerpt":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","og_description":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","twitter_image":null,"twitter_title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","twitter_description":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","meta_title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","meta_description":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6375c52617f6c9003d7b0fd9","uuid":"5f8b37d4-a683-4515-a584-0116c30aa131","title":"An introduction to model metrics","slug":"intro-to-model-metrics","html":"\u003cp\u003eModel metrics help you evaluate the performance of a model and allows you to qualitatively compare different models. You can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. \u003c/p\u003e\u003ch3 id=\"why-does-model-accuracy-not-give-a-complete-picture-of-the-models-performance\"\u003eWhy does model accuracy not give a complete picture of the model's performance?\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"392\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAccuracy tells us the model's overall performance, but this metric doesn't provide all the information needed to accurately assess a model's performance. For a more holistic picture, we'll need to consider other metrics, based on the specific context that the model is used in.\u003c/p\u003e\u003cp\u003eGenerally, accuracy tends to be high in situations where a class has a very low probability of occurring, so a model can achieve high accuracy by simply predicting the most common class. For instance, the probability of finding cancer in computed topography scans or of finding swimming pools from satellite images of homes is low, so the model's accuracy can be high even if the model's ability to detect true positives is very poor.\u003c/p\u003e\u003ch3 id=\"precision\"\u003ePrecision\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1522\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png 1522w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003ePrecision metrics focus on consistency and agreement among labelers, which are primarily derived from Quantumworks Lab's built-in consensus capability. Quantumworks Lab uses and tests the effectiveness of over 15 similar metrics for a wide range of supported annotations.\u003c/p\u003e\u003cp\u003ePrecision is a valuable metric when the negative cost of a false positive is high. For example, in spam detection models, a false positive would cause a vital email to be hidden and marked as spam when in fact, it is non-spam. A false positive in this case would negatively impact the user experience for seeing essential and urgent emails on time.\u003c/p\u003e\u003cp\u003ePopular precision metrics include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eKrippendorff's Alpha is a popular metric used to assess the agreement among raters because it works well for two or more raters, can handle missing data, and supports nominal, ordinal, and ranking data types.\u003c/li\u003e\u003cli\u003eStandard deviation measures the dispersion of a set of ratings from their mean (average) value. In the context of AI data quality, it quantifies how much variation or spread exists in the ratings given by different AI trainers for the same item or task.\u003c/li\u003e\u003cli\u003ePercent agreement is a straightforward measure of inter-rater reliability that calculates the proportion of times different raters agree in their judgments. This is particularly useful in classification tasks (enums).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"\"\u003e\u003c/h3\u003e\u003ch3 id=\"recall\"\u003eRecall\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1616\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 1616w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eRecall is a helpful metric to use when the cost of false negative is high, and you want to minimize it. For example, in fraud detection models, a false negative would cause a fraudulent transaction to be successfully processed when it should have been flagged as fraudulent. This would obviously have a negative impact on the finances of the user. Recall is also helpful for most medical condition predictions, where you would minimize false negatives to increase recall.\u003c/p\u003e\u003ch3 id=\"f-1\"\u003eF-1\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-03-at-12.47.58-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"938\" height=\"316\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-03-at-12.47.58-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-03-at-12.47.58-PM.png 938w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn some cases with imbalanced data problems, both precision and recall are important – we can consider the F1 score as an evaluation metric. An F1 score helps in the detection of skewed datasets and rare classes. Generally, it is best to have high precision and recall so that your F1 score is high. \u003c/p\u003e\u003cp\u003eTo demonstrate how accuracy only provides a partial assessment of a model's performance, we can compare the model metrics of two models below: \u003c/p\u003e\u003cfigure class=\"kg-card kg-gallery-card kg-width-wide\"\u003e\u003cdiv class=\"kg-gallery-container\"\u003e\u003cdiv class=\"kg-gallery-row\"\u003e\u003cdiv class=\"kg-gallery-image\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png\" width=\"1142\" height=\"808\" loading=\"lazy\" alt=\"\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/div\u003e\u003cdiv class=\"kg-gallery-image\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png\" width=\"1142\" height=\"792\" loading=\"lazy\" alt=\"\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/figure\u003e\u003cp\u003eThe accuracy in model A is 73.65%, and model B is 83.69%. Based on accuracy alone, model B seems to perform better. However, if you compare their recall scores, then model A has a better recall of 87.38% vs model B's 82.97% recall. Taking this into account, model A performs better since the cost of a false negative is high. \u003c/p\u003e\u003ch3 id=\"what-do-model-metrics-look-like-in-labelbox\"\u003eWhat do model metrics look like in Quantumworks Lab? \u003c/h3\u003e\u003cp\u003eRather than having you manually compute and upload metrics, \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e auto-computes metrics such as precision, recall, F-1, confusion matrix, etc. on individual predictions for you. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/2022-12-19_22-08-24--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1773\" height=\"884\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/2022-12-19_22-08-24--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/2022-12-19_22-08-24--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/2022-12-19_22-08-24--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/2022-12-19_22-08-24--1-.gif 1773w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLabelbox Model will auto-generate metrics on individual predictions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eYou can simply upload your model predictions and ground truths to receive auto-generated metrics on model precision, recall, F1-score, TP/TN/FP/FN, and confusion matrix.\u003c/li\u003e\u003cli\u003eIf the auto-generated metrics aren’t sufficient for your use case, you can upload your own custom metrics as well.\u003c/li\u003e\u003cli\u003eVisualize, filter, sort, and drill into your metrics, confidence scores, predictions, and annotations. This allows you to easily surface mispredictions, mislabeled data, and allows you to quickly identify improvements to your training data.\u003c/li\u003e\u003cli\u003eYou can interact and click into the NxN confusion matrix or click into the IOU / Precision / Recall histograms to surface and view specific data rows in “gallery view.” For instance, you can understand where your model is not performing well, where your labels are off, or where your model is the least confident.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1031\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAuto-computed confusion matrix\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eUpload confidence scores alongside every prediction and tune the confidence and IOU thresholds in the Quantumworks Lab Model UI to see how model metrics change as the thresholds change.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eFind the distribution of annotations and predictions in every model run via histograms\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/histogram-1.jpeg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1980\" height=\"1337\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/histogram-1.jpeg 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/histogram-1.jpeg 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/histogram-1.jpeg 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/histogram-1.jpeg 1980w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse the prediction and annotation distribution histogram to surface important information about your model runs\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn addition, you can easily understand the distribution of your annotations and predictions via histograms. This makes curating datasets for labeling and analyzing model performance easier than ever. You can now use distributions to find the most predicted or least-predicted class and surface classes represented in training data, but rarely predicted by the model. \u003c/p\u003e\u003ch3 id=\"labelbox-leaderboards-a-new-era-of-evaluation-for-generative-ai\"\u003e\u003cstrong\u003eLabelbox leaderboards: A new era of evaluation for generative AI\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn the rapidly evolving landscape of artificial intelligence, traditional benchmarks are no longer sufficient to capture the full capabilities of AI models. As AI grows increasingly complex, challenges like data contamination, overfitting to public benchmarks, scalability issues, and the absence of standardized evaluation criteria necessitate a more advanced approach to model metrics. \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://labelbox.com/leaderboards/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e are the first to tackle these challenges by conducting structured evaluations on subjective AI model outputs using human experts and a scientific process that provides detailed feature-level metrics and multiple ratings. Leaderboards are available for \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eImage Generation\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/speech-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSpeech Generation\u003c/u\u003e\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/leaderboards/video-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eVideo Generation\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eBy combining expert human evaluations with our \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAlignerr\u003c/u\u003e\u003c/a\u003e workforce, reliable methodology, and continuous updates, Quantumworks Lab is redefining AI evaluation. Our approach complements traditional leaderboards by offering a  comprehensive and human-based assessment of AI models. \u003c/p\u003e\u003cp\u003eRead more about Quantumworks Lab leaderboards on our blog \u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cstrong\u003e \u003c/strong\u003eLabelbox offers a robust platform coupled with expert human evaluation services to efficiently generate and visualize these metrics, empowering you to make informed decisions and improve your models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can learn more about Quantumworks Lab auto-metrics in our \u003ca href=\"https://docs.labelbox.com/docs/evaluate-model-performance?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e or by reviewing \u003ca href=\"https://docs.labelbox.com/reference/upload-image-predictions?ref=labelbox-guides.ghost.io\"\u003ehow to upload image predictions in Quantumworks Lab\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your model evaluation, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e","comment_id":"6375c52617f6c9003d7b0fd9","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-2721--1-.png","featured":false,"visibility":"public","created_at":"2022-11-17T05:22:46.000+00:00","updated_at":"2024-11-27T03:03:43.000+00:00","published_at":"2023-03-03T17:53:39.000+00:00","custom_excerpt":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/intro-to-model-metrics","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/intro-to-model-metrics/","excerpt":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":"An introduction to model metrics","og_description":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-2721--1--2.png","twitter_title":"An introduction to model metrics","twitter_description":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","meta_title":"An introduction to model metrics","meta_description":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63ff856122e0a7003dacb795","uuid":"6f3a4aaf-3582-413a-857c-b7686a5bdb48","title":"Using Quantumworks Lab and Weights \u0026 Biases to fine tune your computer vision projects","slug":"using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects","html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\u003cp\u003eRepeatable, scalable, and diagnosable production artificial intelligence (AI), requires a sophisticated machine learning operations (MLOps) ecosystem. MLOps is the backbone of machine learning (ML) engineering, focused on streamlining development of AI/ML models and deploying, and monitoring those models in production. \u003c/p\u003e\u003cp\u003eIn this post, we’ll explore an MLOps architecture that uses both Quantumworks Lab and \u003ca href=\"https://wandb.ai/site?ref=labelbox-guides.ghost.io\"\u003eWeights \u0026amp; Biases\u003c/a\u003e to develop a computer vision model for a manufacturing focused use case. The goal of the model is to reduce defects on a production manufacturing line using automated visual inspection and the model requires human judgment to curate (supervise) the training data for model development.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/rx0-34Eba9ac5-xI9vIRIt8eZW7uCrVspPzHvmlLuXsF18qJCo-lWBZinRHHwm48UXGvnS3pdYgXOkEBqaeIkH5hk6epd9RfIHPtOEeVgZ1S2p8q_yMVOkOhXIypGKnl8zrwT4JpkzKXRW0IT70pBb6sDnYnl3pG1jDBCMPfbGFPbcnC9M6ai5gWBcwDhw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"465\" height=\"346\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample image of reducing defects on a production manufacturing line via the Quantumworks Lab interface.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eKey Components of AI Development\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eDeveloping a production caliber model is an extremely iterative process. Rarely, if ever, is a model trained once and ready for production. Typically, there are a series of experiments that need to be run across several combinations of datasets and models, followed by expert analysis to determine which one yields the best results. Each experiment must be meticulously tracked and analyzed as ML teams cycle through the development process. \u003c/p\u003e\u003cp\u003eIt’s not uncommon for data science teams to jump into model training without a clear understanding of their data. This can greatly increase the number of model training iterations needed to achieve the desired result, drive up development cost and increase the risks of achieving shipping ML-powered products on time.  \u003c/p\u003e\u003cp\u003eAn efficient, data centric model development approach is valuable, and consists of two key ML Ops components, a data-centric AI platform (Quantumworks Lab) and a model diagnostics platform (Weights \u0026amp; Biases).\u003c/p\u003e\u003ch3 id=\"improving-your-data-and-models\"\u003eImproving your data and models\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eWhen your model is being developed/trained or when it is in production, there will be indicators that your model is either drifting or performing poorly. This will be noticed in the different statistics your model outputs - confidence, accuracy, loss, a fluctuation in the number of detections or the classes that are being detected, all of which will be tracked and triggered in a model diagnostics platform such as Weights \u0026amp; Biases. \u003c/p\u003e\u003cp\u003eBut what about improving the actual dataset? Or understanding which specific data my model is performing poorly on and determining which new data will most improve my model? How do I quickly get that data labeled and back into model training? When transitioning from model diagnostics to data diagnostics, we recommend leveraging tools to quickly understand the data that your model is performing poorly on, find more data similar to that, and curate subsequent datasets in order to iterate through the model development process faster. \u003c/p\u003e\u003cp\u003eThe bottom line is that model diagnostic tools and AI platforms supplement each other and should be used together. Let's dive deeper into what this looks like in practice.\u003c/p\u003e\u003ch2 id=\"how-it-looks-in-practice\"\u003eHow it looks in practice\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eOnce Quantumworks Lab and Weights \u0026amp; Biases have been installed and are set up in your pipeline, what does it look like to embark on the model development journey for the first time (or subsequent times)? Let’s look at an example workflow, most of which can be performed in a seamless process by leveraging both the \u003ca href=\"https://labelbox-python.readthedocs.io/en/latest/index.html?ref=labelbox-guides.ghost.io\"\u003eLabelbox\u003c/a\u003e and \u003ca href=\"https://docs.wandb.ai/quickstart?ref=labelbox-guides.ghost.io\"\u003eW\u0026amp;B\u003c/a\u003e SDKs:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1300\" height=\"706\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 1300w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003col\u003e\u003cli\u003eBegin by first \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003econnecting your data source(s)\u003c/a\u003e in Quantumworks Lab Catalog for easy data visualization and curation.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/View-your-dataset-in-Catalog--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"995\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/View-your-dataset-in-Catalog--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/View-your-dataset-in-Catalog--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/View-your-dataset-in-Catalog--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/03/View-your-dataset-in-Catalog--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eQuickly search, explore and manage your data in one place. View raw data, metadata, and ground truth labels as shown in the mdetal cast defect example above.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e2. Label and review your data in Quantumworks Lab Annotate\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/MTDzpAqLb7Fnqe5_3GbURP2RlB56Dp4lGfprQf_difu_gRWtUxUWw3Ftkn-nhEjC25ZC66rxrZuoIc0WN7U66lBBVXL4z-p1iU1WjlFvAdiSHjOHygivHBtyVI5z2LN1IUpwBVF-FONNhYSfTUlPEvoY8RVKUp91u31dNmeDqANDdvIQShZCOBJpKgEcNQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"339\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEasily set up a consistent ontology for your defect detection use case.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/ezgif.com-optimize--6-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1491\" height=\"976\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/ezgif.com-optimize--6-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/ezgif.com-optimize--6-.gif 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/ezgif.com-optimize--6-.gif 1491w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLeverage auto-annotation to speed up your manual labeling tasks.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e3. Easily split data into \u003ca href=\"https://docs.labelbox.com/docs/curate-data-splits?ref=labelbox-guides.ghost.io\"\u003etrain, test, and validate\u003c/a\u003e sets in Quantumworks Lab Model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Model_Run_Comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"880\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Model_Run_Comparison--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Model_Run_Comparison--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/Model_Run_Comparison--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/03/Model_Run_Comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eYou can use SDK to customize how you want to split your data rows for a given model run.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003e4. Send your data to Weights \u0026amp; Biases from Quantumworks Lab for handling model training and hyperparameter search and tuning.\u003c/p\u003e\u003cp\u003e5. Weights \u0026amp; Biases will handle model training and do a hyperparameters search to run a series of model experiments to be compared with each other. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/StJ0uM0O4-VRXym-dfdNG4JF2ArLtF3qHyoed2yp-F6fYE7dB7ztXzVn5u6AtaPlwp-eSu9fO6VugHVYneYoyOmfZSxk8cCjoEq9YdIkjJ43g7vbZg5B7U49VpraoHWt-uY2WHbDyb4EgTxtxTylEnw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"327\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eTo run a Hyperparameter search using Weights \u0026amp; Biases Sweeps you can \u003c/span\u003e\u003ca href=\"https://colab.research.google.com/drive/13eKhoSbn13kHRRQexbXEfQPz8OwjjcEh?ref=labelbox-guides.ghost.io#scrollTo=EhOKTaHcg4Xl\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003erun this colab notebook here\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e6. Visualize your results in W\u0026amp;B dashboards to quickly diagnose your model performance and create reports in order to share with colleagues and streamline communication. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/ugAyjHH-6i8xJCCdujBBbmC8L7NKrivhJ2j7_UW6Fd5rMf0S7EFPRT8r_v9x9yVrqxRxjqFqYiaikYIrzhdOY_IOjpLt_Qf9hYzBCsMVVxS-SUbKbsSQSSJYKEvuGf1qI9trJwCy_Y4HLDuh6Hgc7JE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"313\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHere we can see the results of a search over 40 experiments to find the best training settings. \u003c/span\u003e\u003ca href=\"https://wandb.ai/morgan/industrial-images/sweeps/t88dzqwe?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eClick here to see this live Weights \u0026amp; Biases dashboard\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e7. Quickly visualize your results in Quantumworks Lab Model to quickly diagnose the data that is related to your model, rapidly query all of your data sources to find data similar to where your model performs poorly (edge cases), and seamlessly queue that data for labeling.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/01/Model-metrics-view--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1753\" height=\"959\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003e8. Rapidly repeat this iterative model development process until you have a production-worthy model, deploy that model, and continuously monitor\u003c/p\u003e\u003ch2 id=\"the-labelbox-ai-platform\"\u003eThe Quantumworks Lab AI platform\u003c/h2\u003e\u003cp\u003eOne of the biggest mistakes made when creating an AI ecosystem is not integrating a data-centric approach into your MLOps pipeline and serves as a foundational tool for data curation and fast iterative improvement through the model development lifecycle. \u003c/p\u003e\u003cp\u003eUsing Quantumworks Lab, ML Teams can easily connect to their sources (i.e. file systems, data lake platforms etc.) of unstructured data and quickly begin exploring and prioritizing their data curation efforts using Quantumworks Lab \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003eOnce data has been assessed and prioritized for curation \u0026amp; labeling, a combination of weak supervision and human supervised labeling campaigns are supported with the use of Quantumworks Lab \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003eFinally, the training data is curated into data splits for test, train and validation and can be easily integrated with a model diagnostics solution (Weights and Biases) to efficiently manage the first of many model training experiments and iterations. This integration is accomplished with the Quantumworks Lab \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eMode\u003c/a\u003el product.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/03/Screen-Shot-2023-03-01-at-8.26.03-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1568\" height=\"898\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe Quantumworks Lab platform allows teams to more quickly go from data curation, annotation and fast iterative improvement through the model development lifecycle.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"weights-biases%E2%80%99-model-diagnostics-platform\"\u003eWeights \u0026amp; Biases’ Model Diagnostics Platform\u003c/h2\u003e\u003cp\u003eA model diagnostics platform should offer quick iteration, easy collaboration, and a centralized system of record for ML teams. Because ML development is often closer to a science than traditional software engineering, experimentation is at its core, and tracking the progress of these experiments is critical.\u003c/p\u003e\u003cp\u003eUsing Weights \u0026amp; Biases’ experiment tracking, ML teams can easily log their work in a standardized way, knowing that they can return to the results of their experiments days, months, or years later. \u003c/p\u003e\u003cp\u003eEase of collaboration is critical for ML teams so that they can move quickly, and Weights \u0026amp; Biases’ Reports enables colleagues to share quick notes, training journals, and polished analysis to teammates and managers to unlock decision making and keep the team moving forward.\u003c/p\u003e\u003cp\u003eFinally, knowing that your multiple model checkpoints are securely stored gives you the full picture of which model is best to select for deployment and to send to the W\u0026amp;B Model Registry, where your MLOps or DevOps team can pull it down for deployment.\u003cbr\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/4PUY0WUteKeig2EtGDEgFP_VVBSQSWkEV3ILX0b_rPUIFMDC2LsEdajU6cykdxVjjinMfRd9dEzEijMLgj0MkaS78BN4g_-yiPwbh3deCfVAWW7IEML2h_dI-YUUwCfUYG4LwErUmaIouAyL-ZqLR0TDH_l_cjnu4_ZhaXNiEzQ8Ds5We-caYw7dQdwc8A\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"144\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003eAI model development is an iterative process, which can be tedious and time consuming without a reliable MLOps pipeline. An efficient model development environment can be broken into two parts: a data-centric workflow for data ingestion, exploration, understanding, and preparation, and a model diagnostics platform for model training, tracking, evaluation, and versioning. Quantumworks Lab and Weights \u0026amp; Biases are leading platforms that were designed to complement this iterative process in mind, and when combined, can scale and help AI development teams build better models, faster than ever before.\u003c/p\u003e\u003cp\u003eWe'll be releasing more technical guides in the coming weeks on how to best utilize Quantumworks Lab and Weights \u0026amp; Biases so stay tuned! \u003cbr\u003e\u003c/p\u003e","comment_id":"63ff856122e0a7003dacb795","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-8.57.36-AM.png","featured":false,"visibility":"public","created_at":"2023-03-01T17:03:29.000+00:00","updated_at":"2023-12-18T22:47:42.000+00:00","published_at":"2023-03-01T18:56:05.000+00:00","custom_excerpt":"Learn how you can use Quantumworks Lab and Weights \u0026 Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},"url":"https://labelbox-guides.ghost.io/using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/","excerpt":"Learn how you can use Quantumworks Lab and Weights \u0026 Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Using Quantumworks Lab and Weights \u0026 Biases | Quantumworks Lab","meta_description":"Use Quantumworks Lab and Weights \u0026 Biases to build better computer vision models with step-by-step workflows of data curation, annotation, \u0026 diagnostics","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6348256660b562003d250b50","uuid":"f78ce8ff-29aa-4034-9953-1b6b50823eea","title":"How to get started in Quantumworks Lab Model: Train, evaluate, and improve your ML models","slug":"how-to-train-evaluate-and-improve-your-ml-models","html":"\u003cp\u003eThe quality of your data will dictate your model’s performance. ML teams have historically had to rely on manual methods of curating data and debugging model errors. For teams who are looking to go through fast, data-centric iterations, this is not an ideal way to quickly scale and reach production AI. \u003c/p\u003e\u003cp\u003eIn order to ship performant models, you need to be able to quickly train models with collaborative data-centric tools. Quantumworks Lab Model can help you ship better models faster by leveraging collaborative tools to curate, debug, diagnose, and optimize your machine learning data and models.\u003c/p\u003e\u003cp\u003eThis guide will walk you through how to get started with \u003ca href=\"https://app.labelbox.com/models?ref=labelbox-guides.ghost.io\"\u003eModel in the Quantumworks Lab platform\u003c/a\u003e. We’ll walk through a COCO object detection example and show you how to get onboarded in Model with your first project, model, and model run. \u003c/p\u003e\u003cp\u003eBy the end of the tutorial, you will have learned how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUpload and version a dataset in a Quantumworks Lab Model run\u003c/li\u003e\u003cli\u003eExport and train an object detection model from a pre-trained model\u003c/li\u003e\u003cli\u003eVisualize model predictions against ground truth annotations\u003c/li\u003e\u003cli\u003eView auto-generated metrics (F1, precision, recall, IOU, confusion matrix, etc.) and the distribution of annotations and predictions\u003c/li\u003e\u003cli\u003eEvaluate model performance and improve your model and data with error analysis and active learning\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eKey definitions in Quantumworks Lab Model:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eA \u003ca href=\"https://docs.labelbox.com/docs/create-a-model?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003emodel\u003c/em\u003e\u003c/a\u003e\u0026nbsp;is a large language model (LLM) integrated into Quantumworks Lab Model or your custom configuration specified by an ontology of data.\u003c/li\u003e\u003cli\u003eAn \u003ca href=\"https://docs.labelbox.com/docs/experiments?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cem\u003eexperiment\u003c/em\u003e \u003c/u\u003e\u003c/a\u003eis a directory where you can create, manage, and compare a set of \u003cem\u003emodel runs\u003c/em\u003e related to the same machine learning task (e.g object detection on COCO). \u003c/li\u003e\u003cli\u003eA \u003ca href=\"https://docs.labelbox.com/docs/create-a-model-run?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003emodel run\u003c/em\u003e\u003c/a\u003e is a model training experiment within a model directory. Each model run provides a versioned data snapshot of the data rows, annotations, and training/validation/test splits for that model run\u003c/li\u003e\u003cli\u003eYou can specify \u003ca href=\"https://docs.labelbox.com/docs/add-model-run-config-to-track-hyperparameters?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003emodel run configuratio\u003c/em\u003ens\u003c/a\u003e to create, version, and track your hyperparameters and any training-related configurations for a model run\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"part-1-seed-a-coco-dataset-and-create-a-project-and-model-run\"\u003ePart 1: Seed a COCO dataset and create a project and model run\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/174ebhOCA8XeQ-WavIDZw3U1EeUHh8MaV?ref=labelbox-guides.ghost.io#scrollTo=UxrfeTs6fint\" class=\"kg-btn kg-btn-accent\"\u003ePart 1: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/74ypf3u3ba\" title=\"How to get started in Quantumworks Lab Model (Part 1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"640\" height=\"360\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eFor this onboarding tutorial, we’ll be working with a COCO dataset example. You can follow along by using the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/174ebhOCA8XeQ-WavIDZw3U1EeUHh8MaV?ref=labelbox-guides.ghost.io#scrollTo=UxrfeTs6fint\"\u003e\u003cem\u003eGoogle Colab Notebook\u003c/em\u003e\u003c/a\u003e\u003cem\u003e and the accompanying video tutorials, but feel free to also bring your own dataset into Quantumworks Lab Model to create a Quantumworks Lab project, model and model run for your use case.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBefore you begin this tutorial, you’ll need to sign into your Quantumworks Lab account or \u003c/strong\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003ecreate a free account\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"step-1-upload-the-coco-dataset-to-labelbox\"\u003eStep 1: Upload the COCO dataset to Quantumworks Lab\u003c/h3\u003e\u003cul\u003e\u003cli\u003eUse the provided helper functions in the Colab Notebook to upload the provided COCO dataset to Quantumworks Lab\u003c/li\u003e\u003cli\u003eOnce you’ve successfully run the helper function, you should be able to see the COCO dataset appear in \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1068\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-2-upload-the-coco-dataset%E2%80%99s-bounding-box-labels-to-labelbox\"\u003eStep 2: Upload the COCO dataset’s bounding box labels to Quantumworks Lab\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWhile labels are typically created by an internal or external team of labelers, we are going to use an already labeled public dataset for this tutorial\u003c/li\u003e\u003cli\u003eUse the provided helper function to directly upload the dataset’s labels to Quantumworks Lab\u003c/li\u003e\u003cli\u003eSimilarly to the step above, you should be able to see your COCO data rows and the appropriate labels in \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-populate-a-model-run-version-model-run-data-and-model-hyperparameters\"\u003eStep 3: Populate a model run: Version model run data and model hyperparameters\u003c/h3\u003e\u003cul\u003e\u003cli\u003eNow, we’re ready to populate the above data (dataset + labels) in a model run\u003c/li\u003e\u003cli\u003eFollow the steps in the Colab Notebook to configure the ontology (containing classes of objects that we want to detect in the image dataset)\u003c/li\u003e\u003cli\u003eYou can also specify your model run’s hyperparameters\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-view-your-populated-model-run-in-the-model-tab\"\u003eStep 4: View your populated model run in the Model tab\u003c/h3\u003e\u003cul\u003e\u003cli\u003eOnce you’ve completed these steps, you should be able to see your versioned data in the Model tab\u003c/li\u003e\u003cli\u003eYou can view your data rows, labels, and inspect your model run configuration (hyperparameters) in the Quantumworks Lab app \u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1052\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eNext, we’re going to use the model run data that we’ve uploaded in Quantumworks Lab to train a model.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"part-2-fine-tune-a-faster-r-cnn-model-upload-predictions-to-model-and-run-error-analysis\"\u003ePart 2: Fine-tune a Faster R-CNN model, upload predictions to Model, and run error analysis\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1mSl3RU8tz2NMQUbE5RZC9rjsmXCTjgjT?ref=labelbox-guides.ghost.io#scrollTo=kvrRY4pJk1ul\" class=\"kg-btn kg-btn-accent\"\u003ePart 2: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/yxk8wmgpmg\" title=\"How to get started in Quantumworks Lab Model (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-1-train-an-object-detection-model-from-a-pre-trained-faster-r-cnn-model\"\u003eStep 1: Train an object detection model from a pre-trained Faster R-CNN model\u003c/h3\u003e\u003cul\u003e\u003cli\u003eExport the labels from the model run you created – the labels will be versioned by the model run\u003c/li\u003e\u003cli\u003eUse the provided helper function to transform the labels into a format that the Pytorch model can accept\u003c/li\u003e\u003cli\u003eWe’ve provided a Faster R-CNN model for fine-tuning – this replaces the last box prediction head with a new layer so that it can predict the model ontology and classes that we are concerned with\u003c/li\u003e\u003cli\u003eTrain the Faster R-CNN model for 1 epoch (4 minutes) – after this, you’ll have a model that performs decently well for this use case\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-generate-predictions-with-the-trained-model\"\u003eStep 2: Generate predictions with the trained model \u003c/h3\u003e\u003cul\u003e\u003cli\u003eGenerating predictions will help us visualize model performance in Quantumworks Lab Model and can help identify model errors\u003c/li\u003e\u003cli\u003eAssemble predictions and results of the model into a format that can be ingested back into Quantumworks Lab\u003c/li\u003e\u003cli\u003eTurn predictions into NumPy arrays and create annotation payloads for each object \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWant to upload a \u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/bring-your-own-models-to-labelbox-with-new-custom-model-integration/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003ecustom model\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e instead?\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAt Quantumworks Lab, we want to ensure you are able to upload your own models quickly and easily, with no manual onboarding. With just a few clicks, you can seamlessly integrate your own custom models into our platform to enhance prediction, accelerate model evaluation, and improve data enrichment.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdlv2idAhfmVrY-LfZhsvYcU2L7e9a5gJ-9XLJW_f98QwSSm6SlOvKk36PC-8parhUQmEc02JMpOhffaz9IjfDYGNOzfzGZNsHb9qCRDqi3r4_KUoVr2KvLV6Tgo8PHbHL1DtUM?key=d5f31BHEZOcEnpQg1zrr_5zJ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"339\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\u0026nbsp;Import a Custom Model from the Model homepage\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"step-3-upload-predictions-back-into-your-labelbox-model-run\"\u003eStep 3: Upload predictions back into your Quantumworks Lab model run \u003c/h3\u003e\u003cul\u003e\u003cli\u003eUpload the model predictions to the Quantumworks Lab app in order to visualize how the model is performing\u003c/li\u003e\u003cli\u003eOther than confidence scores, you don’t have to worry about computing metrics. Quantumworks Lab Model will auto-compute model metrics such as F1 scores, precision, IOU,  confusion matrix, false positives/false negatives/true positives/true negatives, etc\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-visualize-model-predictions-and-auto-generated-metrics-in-labelbox-model\"\u003eStep 4: Visualize model predictions and auto-generated metrics in Quantumworks Lab Model \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/GIF--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/GIF--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/GIF--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/GIF--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/GIF--1-.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAfter you’ve completed the steps above, you should now be able to go to the Models tab to visualize model predictions and model metrics\u003c/li\u003e\u003cli\u003eClick on “Metrics View” to inspect model metrics and the model’s performance on each class\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can run \u003ca href=\"https://docs.labelbox.com/docs/find-model-errors?ref=labelbox-guides.ghost.io\"\u003eerror analysis\u003c/a\u003e on specific classes or data rows of interest:\u003c/p\u003e\u003cul\u003e\u003cli\u003eStart by inspecting how the model is doing on each class\u003c/li\u003e\u003cli\u003eIf you notice the model is struggling on a specific class, you can click into the histogram to view data rows on which the model is struggling. You can refine the search query to further drill into and inspect model performance\u003c/li\u003e\u003cli\u003eLeverage “detailed view” to better inspect disagreements and find patterns of model failures on images\u003c/li\u003e\u003cli\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"learn-more\"\u003eLearn More \u003c/h2\u003e\u003cp\u003eBy following this step-by-step tutorial, you’ve now successfully created a model, a model run, and have uploaded model predictions into Quantumworks Lab for further analysis. \u003c/p\u003e\u003cp\u003eYou can also refer to the below guides for a more in-depth walkthrough on how to improve data selection and model performance:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eHow to search, surface, and prioritize data within a project\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/active-learning?ref=labelbox-guides.ghost.io\"\u003eHow to prioritize high-value data\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-label-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix label errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-model-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix model errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003eHow to find similar data in one click\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’re happy to help answer any questions. Reach out to us anytime on our \u003ca href=\"https://labelbox.com/sales?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e page. \u003c/p\u003e","comment_id":"6348256660b562003d250b50","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2966--1-.png","featured":false,"visibility":"public","created_at":"2022-10-13T14:49:10.000+00:00","updated_at":"2024-11-27T02:19:06.000+00:00","published_at":"2023-01-25T23:49:15.000+00:00","custom_excerpt":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-train-evaluate-and-improve-your-ML-models","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-train-evaluate-and-improve-your-ml-models/","excerpt":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":"How to get started in Quantumworks Lab Model: train, evaluate, and improve your ML models","og_description":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","twitter_image":null,"twitter_title":"How to get started in Quantumworks Lab Model: train, evaluate, and improve your ML models","twitter_description":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","meta_title":"How to get started in Quantumworks Lab Mode: Train evaluate, and improve your ML models","meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6373f09e17f6c9003d7b0dbc","uuid":"157143d6-16c6-451c-83ea-73c8d192eb26","title":"Get started with active learning","slug":"the-guide-to-getting-started-with-active-learning","html":"\u003ch2 id=\"what-is-active-learning\"\u003eWhat is active learning?\u003c/h2\u003e\u003cp\u003eMachine learning (ML) teams working with large volumes of unstructured data in order to generate \u003ca href=\"https://labelbox.com/blog/what-does-it-mean-to-make-high-quality-training-data/?ref=labelbox-guides.ghost.io\"\u003ehigh-quality training data\u003c/a\u003e typically encounter a critical question: If not all my data is created equal, among all of my unlabeled data, what should I label in priority? Active learning in machine learning is the process of answering this very question, by identifying what data will most dramatically improve model performance and feeding these insights into the prioritization of data for labeling.\u003c/p\u003e\u003ch2 id=\"why-is-active-learning-important-in-machine-learning\"\u003eWhy is active learning important in machine learning?\u003c/h2\u003e\u003ch3 id=\"1-avoid-overspending\"\u003e#1: Avoid overspending\u003c/h3\u003e\u003cp\u003eYour goal should be to label as little data as possible by focusing your data labeling and data debugging efforts on the data that will most dramatically improve model performance. By doing so, you'll be able to save significant time and resources upfront. Active learning in machine learning helps with this prioritization.\u003c/p\u003e\u003ch3 id=\"2-better-prioritization\"\u003e#2: Better prioritization\u003c/h3\u003e\u003cp\u003eWhen ML teams possess a wealth of unlabeled data and cannot label everything at once, the best option is to select a fraction of your assets to label, and choose from the highest impact assets first.\u003c/p\u003e\u003ch3 id=\"3-boost-model-performance\"\u003e#3: Boost model performance\u003c/h3\u003e\u003cp\u003eIt turns out that being very intentional about what to label next is one of the best ways to improve the performance of your machine learning models. This is evidenced by other leading AI companies such as Tesla, who have stated that, “\u003cem\u003ein academia, we often see that people keep data constant, but at Tesla it’s very much the opposite. We see time and again that data is one of the best, if not the most deterministic levers to solving these [challenging cases].” - Source: \u003ca href=\"https://youtu.be/ODSJsviD_SU?t=6921\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eTesla AI Day 2022\u003c/em\u003e\u003c/a\u003e \u003c/em\u003e\u003c/p\u003e\u003cp\u003eIn addition, leading AGI research organizations such as OpenAI have also utilized active learning to better train, deploy, and maintain popular models such as GPT-3 and DALL-E 2. To train DALL-E 2, they utilized two different active learning techniques to iterate on the image classifiers: one to find and fix false positives, and another to find and fix false negatives. The organization has published a \u003ca href=\"https://openai.com/blog/dall-e-2-pre-training-mitigations/?ref=labelbox-guides.ghost.io\"\u003eblog post\u003c/a\u003e about the active learning techniques used to modify and process the data used to train DALL-E 2 to ensure that no violent or lewd images are included in the training dataset. Their active learning process used classifiers to select a handful of unlabeled images that are likely to improve classifier performance. Finally, humans produced labels for these images, adding them to the labeled dataset. The process was repeated to iteratively improve the classifier’s performance.\u003c/p\u003e\u003cp\u003eThe ML engineering team at Doordash also \u003ca href=\"https://doordash.engineering/2020/08/28/overcome-the-cold-start-problem-in-menu-item-tagging/?ref=labelbox-guides.ghost.io\"\u003eshared\u003c/a\u003e how they utilized active learning techniques to select low-confidence samples and then built a human-in-the-loop workflow for better menu item labeling. By leveraging active learning based on low confidence predictions, they were able to utilize similarity search and fix model performance on low performing slices of data.\u003c/p\u003e\u003cp\u003eBuilding off these examples, the goal of your AI team and your labeling team should be to identify the unique assets that will boost model performance the most, and to focus all of your labeling time, efforts and budget on these high impact assets. The benefits of this data-centric approach yields higher model performance – allowing teams to rapidly ship models to production, continuously improve models once in production, and effortlessly accumulate competitive advantages faster than ever before.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/wjFWwmp5eggt1gSYX0yLfZgq2suwhXkJCrRyh7IhxFcwAub5VjACL2-pOJXPBM9cxPBJLYhNyDIyw6JxYPqjxw4qjKrWmeBu4PW5JsqBHR3fi4Sl3VzwMTWfcwOTkdss-cdbqMwm6aeEiAacgLq5h8rZkhxRyK6iV3prOJlBSbvAKI1XApYTp6Cjfscjvw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"321\" height=\"247\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAn illustrative Active Learning workflow and the key steps associated with the process.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"how-active-learning-unlocks-value-for-all-ml-teams\"\u003eHow active learning unlocks value for all ML teams\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eWithin the model development lifecycle, there are generally two phases of iterative and data-centric performance improvements. Although both encounter a set of different challenges, active learning is highly relevant for both stages:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eModels in development: \u003c/strong\u003ethese models are not yet deployed into production or accepting live / new ingested data. However, ML teams continuously evaluate and improve these models, trying to reach the performance metrics required to get them into production as quickly and as confidently as possible. Active learning will help you select the right data to include in your training sets so you can reach desired model performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eModels deployed in production: T\u003c/strong\u003ehese models have either met or exceeded the initial minimum viable performance levels required for them to be deployed into production. At this point, they are generally accepting live / newly-ingested data in the “real world.”\u003c/p\u003e\u003cp\u003eEven after a model has been deployed to production, ML teams still need to keep improving their models for two key reasons:\u003c/p\u003e\u003cp\u003e1. Improving their product/service relies upon continuously improving their model\u003c/p\u003e\u003cp\u003e2. The data from production changes over time; hence model performance tends to decrease over time if no steps are taken to tune the model based on these changes\u003cstrong\u003e.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eActive learning will help you select what to label, among all this production data, in order to keep improving model performance.\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;\"\u003e\u003ccolgroup\u003e\u003ccol width=\"98\"\u003e\u003ccol width=\"267\"\u003e\u003ccol width=\"298\"\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#262626;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eModels in development\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#262626;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eModels deployed in production\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:14.14892578125pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #0b5394 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;background-color:#0b5394;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #0b5394 1pt;border-right:solid #0b5394 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;background-color:#0b5394;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #0b5394 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;background-color:#0b5394;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:49.19677734375pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#262626;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eFor ML teams working with:\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eLabeled training data (train/validation/test splits)\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eUnlabeled data, from production,\u003c/span\u003e\u003c/p\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003ewith model predictions \u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#262626;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eKey questions:\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAmong the pool of unlabeled data sitting unused in my company, \u003c/span\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#1c4587;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003ewhat should I add to my training data to boost model performance?\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:10.5pt;font-family:'IBM Plex Sans',sans-serif;color:#262626;background-color:transparent;font-weight:400;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAmong this constant stream of unlabeled data with model predictions, coming from production, \u003c/span\u003e\u003cspan style=\"font-size:10.5pt;font-family:'IBM Plex Sans',sans-serif;color:#1c4587;background-color:transparent;font-weight:700;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003ewhat is the high impact data that I should incorporate into my training data to update and keep improving model performance? \u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003ch2 id=\"key-challenges-to-overcome-when-leveraging-active-learning\"\u003eKey challenges to overcome when leveraging active learning\u003c/h2\u003e\u003cp\u003eEven with an organized data lake, selecting the most useful data can be a complex challenge. \u003ca href=\"https://labelbox.com/blog/how-to-improve-model-performance-with-less-data/?ref=labelbox-guides.ghost.io\"\u003eTo improve model performance\u003c/a\u003e, you don’t need volumes of just any data – you need the most relevant data. Filtering out the value from the noise becomes even more challenging and time consuming when deployed models confront the continuously evolving landscape of real data.\u003cbr\u003e\u003cbr\u003eHowever, implementing this data-centric strategy poses a challenge as it requires ML teams to have purpose-built tools with the ability to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize assets, model predictions, and ground truths alongside each other. Identify model predictions.\u003c/li\u003e\u003cli\u003eUnderstand where models are performing well and struggling. Evaluate the performance of a model on the training data splits, on incoming production data, and on any data slice.\u003c/li\u003e\u003cli\u003eUnderstand the distribution of your data.\u003c/li\u003e\u003cli\u003eAnalyze model confidence scores.\u003c/li\u003e\u003cli\u003eEnsure that every ML experiment is reproducible.\u003c/li\u003e\u003cli\u003eCompare models against each other, as you go through data-centric iterations. The goal is to track whether models are actually making progress with each successive iteration.\u003c/li\u003e\u003cli\u003eTrack model performance over time as performance requirements and real ground truth conditions change.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe complexity compounds because the workflows typically need to be continuously repeated in order to meet the demands of surging capacity to address real world changes or data conditions that suddenly and significantly reduce model performance. Fortunately, dedicated tooling now exists to support these critical active learning workflows as we will introduce later in this post. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-16-at-1.24.07-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1466\" height=\"534\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-16-at-1.24.07-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-16-at-1.24.07-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-16-at-1.24.07-PM.png 1466w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAn example Active Learning workflow that depicts how ML teams can identify and execute targeted improvements to your data and subsequently, model performance.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"current-active-learning-strategies\"\u003eCurrent active learning strategies\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eThe three dominant active learning strategies we’ve encountered in the field can be grouped into broad categories:\u003c/p\u003e\u003cul\u003e\u003cli\u003eActive learning that leverages your data distribution\u003c/li\u003e\u003cli\u003eActive learning that leverages your model predictions\u003c/li\u003e\u003cli\u003eActive learning that leverages model error analysis\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"active-learning-strategies-that-leverage-your-data-distribution\"\u003eActive learning strategies that leverage your data distribution\u003c/h3\u003e\u003cp\u003eActive learning techniques of this type can be applied even before you have labeled data or a trained a ML model, as the primary focus of these techniques is to analyze your data distribution.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003ePrioritize diverse data\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eSelecting diverse data is one of the critical steps to follow before you begin training your model. The process focuses on picking a small, diverse subset of data, e.g. by using random sampling, or e.g. by clustering your unlabeled data and sampling from each cluster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh6.googleusercontent.com/3zHUw2hm3BhK33iyPMUft8COSIh34O5ZmLVM9C6mpKp0XCfik-HJLhkzZOeJsoCy1zaMqrzd8zwtbsjoMexKZg2exBhJZt3atSG-Ta4chCv61KBOwOJbCOedLkfW7_TT1ANZKesmoSpKjQvEAmBGoNO_WTHh0fPkPcjdzV8SCvnBj8UeMcAoY5BA9T35JU5L\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"464\" height=\"304\"\u003e\u003c/figure\u003e\u003cp\u003e2.\u003cstrong\u003e Prioritize out-of-distribution data\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eBased on the nature of continuous change in the real world, once models are delivered and deployed, they are often exposed to data samples they were not trained on. As a function of time and volume, the model will make predictions on data with increasingly more varied characteristics that often outpace the data diversity of the initial training data set. \u003c/p\u003e\u003cp\u003eIn essence, once deployed, the model becomes stale and outdated quickly –— posing inherent challenges for improving, let alone maintaining machine learning models. \u003c/p\u003e\u003cp\u003eActive learning workflows alert ML teams to indications of change or differences in what’s happening lately compared to what had been happening before.\u003c/p\u003e\u003cp\u003eFor example, using \u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\"\u003eimage segmentation\u003c/a\u003e to identify geographic features after a natural disaster may be a much more nuanced challenge to the model than it was pre-disaster. As a natural language processing (NLP) example, the model can report that it is seeing many more instances of deceptive fake news articles than it has previously encountered during past model runs on real / live data.  \u003c/p\u003e\u003cp\u003eA model reporting a sharp change in sentiment analysis can alert end users to new service disruptions experienced by customers. Active learning helps not only to identify these changes, but also expedite retraining on areas that are outside the training distribution.\u003c/p\u003e\u003cp\u003eML Teams will have to confront this data drift – when the underlying data distribution changes over time. If the model was engaged within an Active Learning workflow, the model would:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAlert the ML team that it is encountering data with new qualities than previously observed.\u003c/li\u003e\u003cli\u003eTriage and identify data that is outside the training distribution and cue labeling for data samples that exhibit similar characteristics.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Out-of-distribution-data.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Out-of-distribution-data.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Out-of-distribution-data.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/12/Out-of-distribution-data.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/12/Out-of-distribution-data.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"active-learning-strategies-that-leverage-your-model-predictions\"\u003eActive learning strategies that leverage your model predictions\u003c/h3\u003e\u003cp\u003eOnce you have a trained model, you can use it as a guide to select high impact data that will boost model performance. The following active learning techniques can be applied to your unlabeled data, as long as you generate model predictions on them. This is a typical setting during model development (with your unlabeled data that sits unused in a data bucket) and once your model is in production (with real data that your model is being confronted with in production).\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003ePrioritize low confidence predictions (uncertainty sampling)\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eActive learning workflows of this type quickly identify and triage areas where model confidence is low. For models that have already been deployed and spent time confronting real world data, active learning identifies areas where model confidence has declined based on a variety of factors, such as shifts in the data itself. \u003c/p\u003e\u003cp\u003eActive learning workflows select the data necessary to boost model performance among these areas of uncertainty as well as track the impact of each retraining on model performance, across all data splits, data slices and model classes.\u003cbr\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-15-at-12.22.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"972\" height=\"568\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-15-at-12.22.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-15-at-12.22.35-PM.png 972w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e2. Prioritize rare predictions\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003eWe've observed that active learning workflows become even more critical and valuable for teams that work with rare data — defined as data where there are minimal observable or found samples to use for training even the initial model. These use cases demand that ML teams bring to  bear every bit of data they have, at any given time, in order to attain any sort of reliable model performance. \u003cbr\u003e\u003cbr\u003eActive learning workflows select the data necessary to improve performance in the prioritized model prediction classes or metadata tags, as well as track the impact of each retraining on model performance, across all data splits, data slices and model classes.\u003c/p\u003e\u003cp\u003eImprovement cycles for rare predictions and rare metadata tags can happen at the moment that new real and rare data becomes available, rather than waiting a longer period of time until a larger quantity batch has become available.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Rare-predictions.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1773\" height=\"973\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Rare-predictions.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Rare-predictions.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/12/Rare-predictions.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/12/Rare-predictions.gif 1773w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"active-learning-strategies-that-leverage-model-error-analysis\"\u003eActive learning strategies that leverage model error analysis\u003c/h3\u003e\u003cp\u003eSome advanced active learning techniques may require both model predictions and ground truth data. While training data typically is labeled, it is rarely the case for production data. The solution is to label a subset of your production data and then to apply the following active learning techniques on this subset of labeled data.\u003c/p\u003e\u003ch3 id=\"prioritize-data-on-which-your-model-is-struggling\"\u003ePrioritize data on which your model is struggling\u003c/h3\u003e\u003cp\u003eActive learning techniques help find and fix model errors by:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIdentifying and surfacing areas of disagreement that occur between the model predictions.\u003c/li\u003e\u003cli\u003eBucketing model errors into patterns of model failures.\u003c/li\u003e\u003cli\u003eMining all of your unlabeled data, looking for similar challenging assets, and sending them to labeling.\u003c/li\u003e\u003cli\u003eTracking the impact of each retraining on model performance, across all of the data splits, model classes and data slices. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLearn more about how to find and fix model errors in our “\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-model-errors/?ref=labelbox-guides.ghost.io\"\u003eFind and Fix Model Errors\u003c/a\u003e” Guide.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1344\" height=\"1024\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/11/image-1.png 1344w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"prioritize-labeling-mistakes-that-undermine-your-model-training\"\u003ePrioritize labeling mistakes that undermine your model training\u003c/h3\u003e\u003cp\u003eActive learning techniques help find and fix label errors by:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIdentifying and surfacing areas of disagreement that occur between the model and labels, but where the model also shows high confidence.\u003c/li\u003e\u003cli\u003eVisualizing these candidate label errors.\u003c/li\u003e\u003cli\u003eMining all of your labeled data, looking for similar annotation mistakes, and sending them to re-labeling.\u003c/li\u003e\u003cli\u003eTracking the impact of each retraining on model performance, across all of the data splits, model classes and data slices.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLearn more about how to find and fix label errors in our “\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-label-errors/?ref=labelbox-guides.ghost.io\"\u003eFind and Fix Label Errors\u003c/a\u003e” Guide.\u003c/p\u003e\u003ch3 id=\"combining-active-learning-techniques-to-compound-your-gains\"\u003eCombining active learning techniques to compound your gains\u003c/h3\u003e\u003cp\u003e\u003cbr\u003eHaving shared some of the multiple techniques to implement active learning, we recommend evaluating these techniques based on your use case and seeing these building blocks as tools in your team’s ML workbench to solve different model performance challenges. We have seen that ML teams that mix together the core building blocks for active learning techniques are able to compound the value of these techniques to deliver further model improvements. By setting up the infrastructure and workflows that allow you to do this easily – and even automatically – apply all of these active learning techniques, ML teams can unlock compounding and exponential value, especially compared to doing just one active learning technique (or not doing any active learning at all).\u003c/p\u003e\u003ch2 id=\"leverage-labelbox-for-active-learning\"\u003eLeverage Quantumworks Lab for active learning\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1972\" height=\"1490\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/image.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/image.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/image.png 1972w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox introduces a convenient way to work with your data and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003emodels\u003c/a\u003e to help you get started faster with your active learning workflows. Whether the most relevant strategy calls for retraining on edge cases and outliers; updating human labeling queues; adopting \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003emodel-assisted labeling\u003c/a\u003e; or refining data splits, Quantumworks Lab empowers your team with a diverse workbench of active learning tools necessary to accelerate, streamline, and scale implementation of these model improvement techniques above.\u003c/p\u003e\u003cul\u003e\u003cli\u003eAmplify impact by going through many active learning iterations, as frequently as possible.\u003c/li\u003e\u003cli\u003eInteract with data visually; compare model predictions to ground truth; identify edge cases and identify trends in model behavior.\u003c/li\u003e\u003cli\u003eCurate high-quality, balanced training data. Split labeled data into training, validation, and test datasets, including adjusting the configuration of these datasets across versions.\u003c/li\u003e\u003cli\u003eCluster visually similar data to understand trends in real data distribution and how those trends impact model performance.\u003c/li\u003e\u003cli\u003eComprehend the nuances of a model's performance and add crucial context to key decision-making metrics like Intersection over Union (IoU); Precision and Recall, and more.\u003c/li\u003e\u003cli\u003eVersion your model experiments and model metrics. Easily track each data version to reproduce model results, by preserving the original datasets used to train the model.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eML teams can leverage Quantumworks Lab's data engine to deliver targeted improvements to your training data, so that your team can save crucial time and achieve better and faster model performance.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-16-at-11.17.21-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1432\" height=\"944\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-16-at-11.17.21-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-16-at-11.17.21-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-16-at-11.17.21-AM.png 1432w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"active-learning-customer-spotlight-cape-analytics\"\u003eActive Learning customer spotlight: Cape Analytics\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://capeanalytics.com/?ref=labelbox-guides.ghost.io\"\u003eCape Analytics\u003c/a\u003e enables insurers and other property stakeholders to access valuable property attributes during underwriting, by using \u003ca href=\"https://labelbox.com/guides/computer-vision/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision\u003c/a\u003e algorithms to extract information from geospatial imagery. The power of this solution lies in combining the accuracy and detail of property information traditionally relegated to in-person inspections, with the lightning speed of a living database covering the entire property base of the US, and delivering this information in a matter of seconds.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/customers/cape-analytics/?ref=labelbox-guides.ghost.io\"\u003eCape Analytics\u003c/a\u003e currently uses active learning loops to study the uncertainty of their models. They first select data where the model is uncertain and feed it into Quantumworks Lab, and then back into their model and execute this loop as many times as needed in order to hit their model performance goals. The aggregate time saved from utilizing Quantumworks Lab’s active learning workflows represented an estimated 30%+ increase in total time savings, as well as months of custom development work in engineering hours.\u003c/p\u003e\u003cp\u003eAccording to their principal data scientist, Giacomo Vianello, \"we've found that some active learning techniques are more successful than others depending on the project at hand. Also, it’s not just always about performance, but also robustness. Active learning finds corner cases that may not move the needle for model performance, but improve robustness. This is important to our customers because we're making our models more reliable.\"\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/image-3.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/image-3.png 2048w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"active-learning-customer-spotlight-deque\"\u003eActive learning customer spotlight: Deque \u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://www.deque.com/blog/deque-furthers-digital-accessibility-through-machine-learning/?ref=labelbox-guides.ghost.io\"\u003eDeque\u003c/a\u003e is a leader in digital accessibility which is the practice of making digital documents, web and mobile apps accessible to everyone, including people with disabilities. Using the power of machine learning, the Deque team is now focused on the next generation of accessibility testing. Building out the components of their ML program, Deque leverages a sophisticated data engine using model diagnostics and active learning that’s capable of prioritizing the most performant classes of data, discovering model errors quickly, and fueling their iterations with high-quality data.\u003c/p\u003e\u003cp\u003eThe ML team at Deque were able to make considerable improvements to model performance by seeing the ability to evaluate and visualize model performance. According to Noe Barrell, Deque's machine learning engineer, “we detected some noise issues in our dataset and thanks to model diagnostics, we were able to filter out about one-third of data points we considered less trustworthy. By doing so, model performance went up 5%. We re-labeled some data and we saw the performance went up again after we added the re-labeled points. It was challenging data for humans to label and for the model to understand. Being able to target the data we already had in Quantumworks Lab and make changes and fixes to it was really helpful to us as a team to save time and target where we knew it would make a difference in our model’s performance.”\u003c/p\u003e\u003cp\u003eThe Deque team made huge leaps in several areas via \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003emodel diagnostics\u003c/a\u003e and were able to target their data collection in a way that addresses model failures more quickly. For instance, they boosted performance on classes of data such as improving detection of checkboxes from 47% accuracy to 75% accuracy, presentational tables from 66% accuracy to 79% accuracy, and radio buttons from 37.9% accuracy to 74% accuracy. \u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eTry Quantumworks Lab and its active learning features yourself for free\u003c/a\u003e, or \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e to learn how it works.\u003cbr\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"6373f09e17f6c9003d7b0dbc","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2963--1-.png","featured":false,"visibility":"public","created_at":"2022-11-15T20:03:42.000+00:00","updated_at":"2023-10-27T17:17:50.000+00:00","published_at":"2022-11-16T17:17:06.000+00:00","custom_excerpt":"Discover how to get started with active learning by leveraging the 3 techniques that consistently help ML teams more quickly identify what data will most dramatically improve model performance.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/the-guide-to-getting-started-with-active-learning/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/the-guide-to-getting-started-with-active-learning/","excerpt":"Discover how to get started with active learning by leveraging the 3 techniques that consistently help ML teams more quickly identify what data will most dramatically improve model performance.","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":"Getting started with active learning","og_description":"Discover how to leverage the 3 active learning techniques that consistently help ML teams more quickly identify what data will most dramatically improve model performance.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2963--1--2.png","twitter_title":"Getting started with active learning","twitter_description":"Discover how to leverage the 3 active learning techniques that consistently help ML teams more quickly identify what data will most dramatically improve model performance.","meta_title":"Getting started with active learning | Quantumworks Lab","meta_description":"Get started with active learning by leveraging 3 techniques that will help identify what data will most dramatically improve model performance.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63336dc3875dd8004d80eb50","uuid":"4b053b4c-821c-41ff-8494-93ff9ef80fd2","title":"How to find and fix label errors","slug":"how-to-find-and-fix-label-errors","html":"\u003cp\u003eIn this guide, we'll be walking you through how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/8hpp57mhw0\" title=\"How to find and fix label errors Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTry the Colab: \u003ca href=\"https://colab.research.google.com/drive/1SRcoEo4-UMiKiX4CPBP82UlLi6OtItn5?ref=labelbox-guides.ghost.io#scrollTo=8bedb521\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eML models are only as good as the data they are trained on and it's actually more common than you think for there to be labeling errors in your training data. In order to improve your ML models, you'll need to improve your training data by finding label errors and then sending them to be corrected. \u003c/p\u003e\u003cp\u003eOnce you upload your model predictions and model metrics to Quantumworks Lab, you can unlock powerful workflows to label high-impact data, faster and more efficiently. This not only helps speed up your labeling efforts and increases label quality, but can help reduce your labeling budget. \u003c/p\u003e\u003cp\u003eLearn more about our methods to drive data-centric iterations by improving your training data:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/improve-model-performance?ref=labelbox-guides.ghost.io\"\u003eFind model errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/active-learning?ref=labelbox-guides.ghost.io\"\u003eActive learning: Prioritize high-value data\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"63336dc3875dd8004d80eb50","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2791--2-.png","featured":false,"visibility":"public","created_at":"2022-09-27T21:40:19.000+00:00","updated_at":"2023-10-27T17:20:00.000+00:00","published_at":"2022-10-10T23:18:00.000+00:00","custom_excerpt":"Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-find-and-fix-label-errors/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-find-and-fix-label-errors/","excerpt":"Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. ","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2791-1.png","og_title":"How to find and fix label errors","og_description":"Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2791.png","twitter_title":"How to find and fix label errors","twitter_description":"Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. ","meta_title":"How to find and fix label errors | Quantumworks Lab","meta_description":"Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"632c712a16e912003d39b1ea","uuid":"43d97c24-4694-4599-b510-00cf170294a5","title":"How to curate and version your training datasets and hyperparameters","slug":"how-to-curate-and-version-your-training-datasets-and-hyperparameters","html":"\u003cp\u003eIt's important for teams to be able to manage their model input and outputs in a single place. Being able to understand and visualize how different models compare to each other is a crucial aspect of a successful data engine. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9mas8rc1d7\" title=\"How to curate and version your training datasets and hyperparameters Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTry the Colab: \u003ca href=\"https://colab.research.google.com/drive/1nWoWb-EwuQFZK1UpuF2y-q6VVETU-GyH?ref=labelbox-guides.ghost.io#scrollTo=QDMX8CvqWcGk\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eRather than flying blind, teams can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. You can easily track and reproduce model experiments to observe the differences and share best practices with your team. \u003c/p\u003e\u003cp\u003eML teams may want to kick off multiple model runs with different sets of hyperparameters and compare performance. Having a single place to track model configurations and performance is crucial to model iteration and for reproducing model results. Learn how you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/create-a-model?ref=labelbox-guides.ghost.io\"\u003eCreate a model\u003c/a\u003e \u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/create-a-model-run?ref=labelbox-guides.ghost.io\"\u003eCreate a model run\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/curate-data-splits?ref=labelbox-guides.ghost.io\"\u003eCurate data splits\u003c/a\u003e \u003c/li\u003e\u003cli\u003eCreate and modify a model run configuration \u003ca href=\"https://docs.labelbox.com/docs/add-model-run-config-to-track-hyperparameters?ref=labelbox-guides.ghost.io#create-and-modify-model-run-config-in-the-ui\"\u003ethrough our UI\u003c/a\u003e \u003c/li\u003e\u003cli\u003eCreate and modify a model run configuration \u003ca href=\"https://docs.labelbox.com/docs/add-model-run-config-to-track-hyperparameters?ref=labelbox-guides.ghost.io#create-and-modify-model-run-config-in-sdk\"\u003ethrough the SDK\u003c/a\u003e \u003c/li\u003e\u003cli\u003eUse our \u003ca href=\"https://colab.research.google.com/drive/1nWoWb-EwuQFZK1UpuF2y-q6VVETU-GyH?ref=labelbox-guides.ghost.io#scrollTo=QDMX8CvqWcGk\"\u003eGoogle Colab Notebook\u003c/a\u003e (as shown in the video) to curate and version your training datasets and hyperparameters\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo learn more about model run configurations, feel free to visit our \u003ca href=\"https://docs.labelbox.com/docs/add-model-run-config-to-track-hyperparameters?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e","comment_id":"632c712a16e912003d39b1ea","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2721.png","featured":false,"visibility":"public","created_at":"2022-09-22T14:28:58.000+00:00","updated_at":"2023-10-27T17:23:25.000+00:00","published_at":"2022-10-10T15:36:00.000+00:00","custom_excerpt":"Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. Easily track and reproduce model experiments to observe the differences and share best practices with your team. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-curate-and-version-your-training-datasets-and-hyperparameters/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-curate-and-version-your-training-datasets-and-hyperparameters/","excerpt":"Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. Easily track and reproduce model experiments to observe the differences and share best practices with your team. ","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2721-2.png","og_title":"How to curate and version your training datasets and hyperparameters","og_description":"Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. Easily track and reproduce model experiments to observe the differences and share best practices with your team. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2721-1.png","twitter_title":"How to curate and version your training datasets and hyperparameters","twitter_description":"Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. Easily track and reproduce model experiments to observe the differences and share best practices with your team. ","meta_title":"How to curate and version your training datasets and hyperparameters | Quantumworks Lab","meta_description":"Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"633c4d4bc39859004d2e16cf","uuid":"ed5983f1-aa56-41ca-8d13-4287a88a28b2","title":"How to find and fix model errors","slug":"how-to-find-and-fix-model-errors","html":"\u003cp\u003eA great way to boost model performance is to surface edge cases on which the model is struggling. You can fix those model failures with targeted improvements to your training data so that the model becomes better on these edge cases. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jzt02qjlts\" title=\"How to find and fix model errors Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-center\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1SRcoEo4-UMiKiX4CPBP82UlLi6OtItn5?ref=labelbox-guides.ghost.io#scrollTo=8bedb521\" class=\"kg-btn kg-btn-accent\"\u003eTry the Google Colab\u003c/a\u003e\u003c/div\u003e\u003cp\u003eOnce you upload your model predictions and model metrics to Quantumworks Lab, you can use your trained model as a guide to find model failure and edge cases. With quantitative and visual inspection, you can identify challenging edge cases and then use \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Catalog\u003c/a\u003e to find similar unlabeled data to improve your model. \u003c/p\u003e\u003cp\u003e\u003cu\u003eHere's a systematic process that can help teams easily surface and fix model errors:\u003c/u\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStep 1\u003c/strong\u003e: Look for assets where your model predictions and labels disagree – one way to do that is to look at model metrics and surface clusters of data points where the model is struggling.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStep 2:\u003c/strong\u003e Visualize these challenge cases and identify patterns of model failures. Prioritize the most important model failures to fix.  \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStep 3:\u003c/strong\u003e Now that you've identified edge cases that need fixing, the goal is to find unlabeled data points that are most similar to the challenge case – these are high-impact data points that you want to label in priority.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStep 4:\u003c/strong\u003e When you re-train the model on this newly labeled data, the model will learn to make better predictions on the newly added data points and won't struggle as it did before. In a few steps, you've fixed your model errors and have boosted model performance. \u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow else can you benefit from a data-engine like Quantumworks Lab?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTeams can also drive data-centric iterations by finding and fixing label errors to improve model performance. Learn more about how to find and fix label errors in in this \u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-label-errors/?ref=labelbox-guides.ghost.io\"\u003eguide\u003c/a\u003e our \u003ca href=\"https://docs.labelbox.com/docs/identify-labeling-mistakes?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e","comment_id":"633c4d4bc39859004d2e16cf","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2818--1-.png","featured":false,"visibility":"public","created_at":"2022-10-04T15:12:11.000+00:00","updated_at":"2023-10-26T18:23:24.000+00:00","published_at":"2022-10-05T13:43:59.000+00:00","custom_excerpt":"A great way to boost model performance is to surface edge cases on which the model might be struggling. You can fix those model failures with targeted improvements to your training data so that the model is better trained on these edge cases. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-find-and-fix-model-errors/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-find-and-fix-model-errors/","excerpt":"A great way to boost model performance is to surface edge cases on which the model might be struggling. You can fix those model failures with targeted improvements to your training data so that the model is better trained on these edge cases. ","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2818-2.png","og_title":"How to find and fix model errors","og_description":"A great way to boost model performance is to surface edge cases on which the model might be struggling. You can fix those model failures with targeted improvements to your training data so that the model is better trained on these edge cases. ","twitter_image":null,"twitter_title":"How to find and fix model errors","twitter_description":"A great way to boost model performance is to surface edge cases on which the model might be struggling. You can fix those model failures with targeted improvements to your training data so that the model is better trained on these edge cases. ","meta_title":"How to find and fix model errors | Quantumworks Lab","meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":21,"tag":{"slug":"train-fine-tune-ai","id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","count":{"posts":21},"url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},"slug":"train-fine-tune-ai","currentPage":"2"},"__N_SSG":true},"page":"/guides/tag/[id]/page/[pagenum]","query":{"id":"train-fine-tune-ai","pagenum":"2"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script>
    

    

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>

                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><span style="color: inherit; cursor: default;">Docs</span></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="/static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <span style="color: inherit; cursor: default;">Terms of Service</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Privacy Notice</span>
                    <div class="footer-divider"></div>
                    <span style="color: inherit; cursor: default;">Copyright Dispute Policy</span>
                </div>
            </div>
        </div>
    </footer>
</body>
<!-- Mirrored from labelbox.com/guides/tag/train-fine-tune-ai/page/2/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:20:40 GMT -->
</html>