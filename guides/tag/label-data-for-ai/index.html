<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/tag/label-data-for-ai/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:32:14 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../static/scripts/munchkin.js"></script><script src="../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../_next/static/chunks/pages/guides/tag/%5bid%5d-cc2bd40a983392d8.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../index.html"><img width="106" height="24" alt="logo" src="../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../build-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Build AI</a><a href="../use-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Use AI</a><a href="../explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Label data for AI</a><a href="../train-fine-tune-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="mb-10"><h2 class="text-3xl md:text-4xl font-medium mb-4">Label data for AI</h2><p class="text-base max-w-2xl font-medium text-neutral-500"></p></div><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../evaluating-leading-text-to-speech-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index6ff8.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../evaluating-leading-text-to-speech-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Evaluating leading text-to-speech models</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../metrics-based-rag-development-with-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexd10c.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-16-at-5.55.12-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../metrics-based-rag-development-with-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Metrics-based RAG Development with Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index96b1.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-2.55.05-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Unlocking precision: The &quot;Needle-in-a-Haystack&quot; test for LLM evaluation</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our &quot;Needle-in-a-Haystack&quot; experiment. Learn how to enhance accuracy and efficiency in complex data annotations.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../a-comprehensive-approach-to-evaluating-text-to-video-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index9db2.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-06-at-10.13.57-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../a-comprehensive-approach-to-evaluating-text-to-video-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">A comprehensive approach to evaluating text-to-video models</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../a-comprehensive-approach-to-evaluating-text-to-image-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexea06.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-30-at-4.35.13-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../a-comprehensive-approach-to-evaluating-text-to-image-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">A comprehensive approach to evaluating text-to-image models</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../using-labelbox-to-improve-data-quality-via-autoqa-advanced-labeler-review/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-19-at-9.36.05-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-19-at-9.36.05-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-19-at-9.36.05-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-19-at-9.36.05-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-19-at-9.36.05-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-19-at-9.36.05-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-19-at-9.36.05-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-19-at-9.36.05-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index5684.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2FScreenshot-2024-07-19-at-9.36.05-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../using-labelbox-to-improve-data-quality-via-autoqa-advanced-labeler-review/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Quantumworks Lab to improve data quality via AutoQA &amp; advanced labeler review</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can use Quantumworks Lab to accelerate the quality review process for creating better data for generative AI use cases using autoQA and advanced labeler reviews.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../using-multimodal-chat-to-enhance-a-customers-online-support-experience/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexdc84.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Fmmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../using-multimodal-chat-to-enhance-a-customers-online-support-experience/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using multimodal chat to enhance a customer’s online support experience</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../harnessing-ai-for-efficient-video-labeling/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index23ca.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fimage.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../harnessing-ai-for-efficient-video-labeling/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to harness AI for efficient video labeling</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-improve-search-relevance/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index4a80.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Fthumbnail--6-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-improve-search-relevance/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to use AI to improve website search relevance</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can dramatically improve search relevance for any website or app by using AI and foundation models to better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-improve-your-task-specific-chatbot-for-safety-relevancy-and-user-feedback/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--3-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--3-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--3-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--3-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--3-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--3-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--3-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--3-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index9c23.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Fthumbnail--3-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-improve-your-task-specific-chatbot-for-safety-relevancy-and-user-feedback/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to improve your task-specific chatbot for better safety, relevancy, and user feedback</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage Quantumworks Lab for optimizing your task-specific LLM chatbot for better safety, relevancy, and user feedback.</p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8">Page 1 of 5<a class="ml-9 text-neutral-700 mb-1" href="page/2/index.html">&gt;</a></div></div></div></div></div></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"66bfded301c06500016e8b1c","uuid":"a2c40e88-6376-4824-b57f-e0e3673d8b1b","title":"Evaluating leading text-to-speech models","slug":"evaluating-leading-text-to-speech-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in speech generation evaluation, visit \u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe evolution of Text-to-Speech (TTS) models has been groundbreaking, with various companies offering high-quality solutions for a range of applications. Whether it's enhancing accessibility, automating customer service, or generating voiceovers, TTS models play a vital role in today's digital landscape. \u003c/p\u003e\u003cp\u003eWe recently evaluated the performance of six leading TTS models—Google TTS, Cartesia, AWS Polly, OpenAI TTS, Deepgram, and Eleven Labs—using key evaluation metrics such as Word Error Rate (WER) and human preference ratings.\u003c/p\u003e\u003cp\u003eTo provide an in-depth analysis, we evaluated each model on 500 prompts. Each prompt was reviewed by three expert labelers to ensure accuracy and provide diverse perspectives.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"evaluation-process\"\u003e\u003cstrong\u003eEvaluation Process\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate speech outputs across several key criteria:\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer\"\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWER measures the accuracy of the generated speech by calculating the number of insertions, deletions, and substitutions compared to a reference transcription. The lower the WER, the better the model performed in terms of generating accurate and coherent speech.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1120\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"word-errors\"\u003eWord errors\u003c/h3\u003e\u003cp\u003eRaters evaluated the presence of word errors in the generated TTS audio by counting the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Insertion Errors\u003c/strong\u003e: Additional words not present in the original text.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Deletion Errors\u003c/strong\u003e: Words from the original text that were omitted.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Substitution Errors\u003c/strong\u003e: Incorrect replacements for original words.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: No word errors or only minor errors (0-1 error).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some errors (2-3 errors).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Significant errors (4 or more errors).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"speech-naturalness\"\u003eSpeech naturalness\u003c/h3\u003e\u003cp\u003eRaters assessed how human-like the generated speech sounded.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Very human-like, natural flow with appropriate pauses and inflections.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some human qualities but with occasional robotic or awkward elements.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Clearly robotic or artificial, with choppy or monotone speech.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"pronunciation-accuracy\"\u003ePronunciation accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated how clearly and correctly words were pronounced in the TTS output.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: All words are pronounced clearly and correctly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: 1-2 words are mispronounced or unclear.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: 3 or more words are mispronounced or difficult to understand.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"noise\"\u003eNoise\u003c/h3\u003e\u003cp\u003eRaters evaluated the level of background noise or artifacts in the audio.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNone\u003c/strong\u003e: No detectable noise or artifacts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Minor noise, such as a faint hiss, but not distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Noticeable noise or artifacts that may be somewhat distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Significant noise or artifacts that interfere with comprehension.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"context-awareness\"\u003eContext awareness\u003c/h3\u003e\u003cp\u003eRaters assessed the TTS system's ability to adjust to context, including tone, emphasis, and punctuation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Excellent adaptation to context, with clear tonal shifts and pauses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Adequate adaptation but misses some nuances.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Little to no adaptation; reads text monotonously without context cues.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"prosody-accuracy\"\u003eProsody accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated the rhythm, stress, and intonation (prosody) of the generated speech.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Natural-sounding rhythm and intonation that closely mimics human speech.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Mostly natural but with occasional awkwardness in stress or rhythm.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Monotonous or robotic intonation with unnatural pauses and emphasis\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"prompts-curation-process\"\u003ePrompts curation process\u003c/h2\u003e\u003cp\u003eThe diverse set of prompts used in our TTS evaluation provides a comprehensive test of model capabilities across multiple dimensions. \u003c/p\u003e\u003cp\u003eConsider these examples:\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Wow! Did you see the fireworks last night? They were spectacular!\"\u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003e This short, exclamatory sentence tests the model's ability to convey excitement and emphasis. It challenges the TTS system to appropriately modulate tone and pitch to express enthusiasm.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"I wish you were on the right path...I-I-I truly do, but you're not,' he said sadly.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis prompt evaluates the model's handling of emotional nuance, hesitation (indicated by the stuttering \"I-I-I\"), and the use of ellipsis for pauses. It also tests the system's ability to convey sadness as specified in the dialogue tag.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Alright, listen up, team! The mission briefing is as follows: Operation Nighthawk commences at 0300 hours. Rendezvous at coordinates 51°30'26.5\"N 0°07'39.0\"W. Your login credentials for the secure server are: Username: NightOwl_Alpha, Password: 3ch3l0n_X9#. Remember, this is a stealth operation. Radio silence must be maintained except for emergency protocol Sierra-Tango-Seven. If compromised, use the failsafe code: Zulu-Yankee-4-7-2-Bravo. Any questions? No? Then synchronize your watches... now. Good luck, and godspeed.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis complex prompt tests multiple aspects:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical pronunciation of alphanumeric strings and military jargon\u003c/li\u003e\u003cli\u003eConveying urgency and authority\u003c/li\u003e\u003cli\u003eHandling of coordinates and precise numerical information\u003c/li\u003e\u003cli\u003eProper pacing for a lengthy, detailed instruction set\u003c/li\u003e\u003cli\u003eAppropriate pausing, indicated by punctuation and paragraph breaks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese prompts demonstrate how we cover a range of linguistic features, emotional tones, and formatting challenges. They span from simple exclamations to complex, technical instructions, ensuring a thorough assessment of prosody, emphasis, and versatility.\u003c/p\u003e\u003cp\u003eThis diversity in prompts matters because it enables a comprehensive evaluation that reflects real-world applicability. By covering such a broad spectrum of scenarios, we can identify specific strengths and weaknesses in each TTS model, allowing for fair comparisons and pinpointing areas for improvement. Whether it's expressing joy over fireworks, conveying emotional conflict, or delivering a detailed mission briefing, these prompts help us ensure that TTS models can handle the variety and complexity of human communication.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overall-ranking-best-to-worst-model-samples\"\u003eOverall Ranking best to worst model samples\u003c/h2\u003e\u003cp\u003eRaters ranked each TTS sample from best to worst based on performance across all categories above.\u003c/p\u003e\u003ch3 id=\"model-configurations-used-at-test-time\"\u003e\u003cstrong\u003eModel configurations used at test time\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: tts-1-hd\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: alloy\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle Text-to-Speech (TTS)\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel:\u003c/strong\u003e Default\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice: \u003c/strong\u003eDefault with neutral gender voice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eElevenLabs\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: eleven_monolingual_v1 (As of 30th September, eleven_monolingual_v1 is an outdated version and Quantumworks Lab will run a new experiment with\u0026nbsp;eleven_multilingual_v2 in the future)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: pMsXgVXv3BLzUgSXRplE\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: aura-asteria-en\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Default\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAmazon Polly\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Generative\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Ruth\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: sonic-english\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Teacher Lady\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe selected these models for our evaluation as they represent the cutting-edge offerings from leading companies in the TTS space, showcasing the latest advancements in voice synthesis technology.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"results\"\u003eResults\u003c/h1\u003e\u003cp\u003eOur evaluation of 500 diverse and complex prompts provided valuable insights into the capabilities of Google, Cartesia, AWS Polly, OpenAI, Deepgram, and Eleven Labs TTS models. Each prompt was meticulously assessed by three different raters to ensure accurate and diverse perspectives. This rigorous testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of these leading TTS models:\u003c/p\u003e\u003ch3 id=\"rank-distribution-percentages\"\u003eRank distribution percentages\u003cstrong\u003e\u0026nbsp; \u0026nbsp; \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"overall-ranking\"\u003eOverall ranking\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1:\u003c/strong\u003e\u0026nbsp; (Open AI TTS) - occurred 607 times, 42.93%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2:\u003c/strong\u003e\u0026nbsp; (Cartesia) - occurred 460 times, 32.53%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 410 times, 29.00%\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 4:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 338 times, 23.90%. Followed by Eleven labs as a close second with occurrence 335, 23.69%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 5:\u003c/strong\u003e\u0026nbsp; (Deepgram) - occurred 438 times, 30.98%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 6:\u003c/strong\u003e\u0026nbsp; (Google TTS) - occurred 868 times, 61.39%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote: Rank is determined by the most frequently selected model for each position, which is why some AWS Polly appears twice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1122\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003eFig: % of times each model was ranked first.\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer-results\"\u003eWord Error Rate (WER) Results\u003c/h3\u003e\u003cp\u003eThe WER scores offer an objective look at how accurately each model converts text to speech.\u003c/p\u003e\u003cul\u003e\u003cli\u003eDeepgram: 5.67%\u003c/li\u003e\u003cli\u003eOpenAI TTS: 4.19%\u003c/li\u003e\u003cli\u003eGoogle TTS: 3.36%\u003c/li\u003e\u003cli\u003eAWS Polly: 3.18%\u003c/li\u003e\u003cli\u003eCartesia: 3.87%\u003c/li\u003e\u003cli\u003eEleven Labs: 2.83%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs achieved the lowest WER at 2.83%, making it the most accurate model. Deepgram, however, had the highest WER at 5.67%, indicating room for improvement in transcription accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" title=\"Chart\" width=\"2000\" height=\"1126\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1123\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-specific-performance-results\"\u003eModel-Specific Performance Results\u003c/h2\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each TTS model based on key evaluation criteria.\u003c/p\u003e\u003ch3 id=\"openai-tts\"\u003eOpenAI TTS\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 4.1898%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 89.60% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 87.13% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 92.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 63.37% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 64.57% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOpenAI TTS excels at producing natural-sounding speech, making it a top choice for applications requiring lifelike vocal output. It demonstrates exceptional accuracy in pronunciation and performs excellently in producing clean audio with minimal background noise. The model shows strong capabilities in understanding and conveying contextual nuances in speech and is proficient in delivering appropriate intonation and rhythm. Despite ranking high in human preference, it shows moderate accuracy in word reproduction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cm24ur6wwf\" title=\"OpenAI: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gicqvkqc5e\" title=\"OpenAI: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kg9wqqftu4\" title=\"OpenAI: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"cartesia\"\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 58.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 53.11% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 93.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 80.98% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 77.02% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.8707%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCartesia demonstrates good accuracy in word reproduction. While not as consistent as OpenAI TTS in naturalness, it still performs very well in producing natural-sounding speech. It shows excellent accuracy in pronunciation and excels at producing clean audio with minimal background noise. The model displays good capabilities in understanding contextual nuances and performs well in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/3gukhi7z2j\" title=\"Cartesia: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y3lbes0sin\" title=\"Cartesia: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f0lmkto1xk\" title=\"Cartesia: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"aws-polly\"\u003e\u003cstrong\u003eAWS Polly\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 56.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 55.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.39% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 84.72% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Medium in 58.20% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.1770%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAWS Polly shows very good accuracy in word reproduction, ranking second among the models. It struggles more with producing natural-sounding speech compared to top performers but maintains excellent pronunciation accuracy. The model performs well in producing clean audio and shows good capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qawx1dwc9g\" title=\"Polly: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/klmnfcsno5\" title=\"AWS Polly: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ubzrqtjnc9\" title=\"AWS Polly: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"eleven-labs\"\u003e\u003cstrong\u003eEleven Labs\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 2.8302%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: Medium in 44.98% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 81.97% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 80.27% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 44.70% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 46.25% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs demonstrates the best accuracy in word reproduction among all models. It shows mixed results in producing natural-sounding speech but maintains excellent pronunciation accuracy. The model performs reasonably well in producing clean audio but has more noise issues than some competitors. It shows moderate capabilities in understanding contextual nuances and has room for improvement in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/x3qzhkt0ab\" title=\"Eleven Labs: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kl0lc7oqmz\" title=\"Eleven Labs: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bc19r3iqip\" title=\"Eleven Labs: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"deepgram\"\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 5.6743%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: High in 57.78% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 64.43% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 88.83% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 53.18% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 55.52% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDeepgram shows the highest error rate in word reproduction among all models. It performs moderately well in producing natural-sounding speech and demonstrates good accuracy in pronunciation, although lagging behind some competitors. The model performs well in producing clean audio with minimal background noise but shows moderate capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mngnvgtzx3\" title=\"Deepgram: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6h3bgmcg6d\" title=\"Deepgram: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uaj3x9bln8\" title=\"Deepgram: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"google-tts\"\u003e\u003cstrong\u003eGoogle TTS\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.3574%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Low in 78.01% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 77.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.46% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: Medium in 39.25% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: Low in 45.83% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z27ett9q8m\" title=\"Google TTS: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/06pqi5qc13\" title=\"Google TTS: Wow Did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kwotkto0u6\" title=\"Google TTS: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation reveals that while Eleven Labs leads in transcription accuracy with the lowest WER, OpenAI TTS emerges as the top choice when it comes to human preference, particularly for speech naturalness, pronunciation accuracy, and prosody.\u003c/p\u003e\u003cp\u003eConversely, Google TTS, despite a relatively low WER, ranks last in human preference due to poor performance in categories like speech naturalness and context awareness. This demonstrates the importance of a balanced evaluation that considers both quantitative metrics like WER and qualitative factors like user satisfaction.\u003c/p\u003e\u003cp\u003eAs the TTS field continues to evolve, the key to further advancements will lie in combining high transcription accuracy with natural, expressive, and context-aware speech. Models like OpenAI TTS and Eleven Labs are setting the standard, but there remains room for improvement across all models to fully meet the growing demands of diverse TTS applications.\u003c/p\u003e\u003cp\u003eIn summary, while WER is crucial, the broader context of user experience—focusing on factors like speech naturalness, context awareness, and prosody accuracy—is essential in evaluating the overall effectiveness of TTS models.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-speech models presented here represents a significant advancement in assessing AI-generated speech. By leveraging this methodology alongside Quantumworks Lab's cutting-edge platform, AI teams can dramatically accelerate the development and refinement of sophisticated, domain-specific speech generation models.\u003c/p\u003e\u003cp\u003eOur solution offers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eEfficient dataset curation\u003c/li\u003e\u003cli\u003eAutomated evaluation techniques\u003c/li\u003e\u003cli\u003eHuman-in-the-loop quality assurance\u003c/li\u003e\u003cli\u003eCustomizable workflows for TTS model assessment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-speech model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdohXa2KeUawTMTWS7MfKSUYC6fVPCHjtmcFCRt5ycmB7S4k4-JElTwnmRb6ufW2kR7F3Bqu1PwzbvL83LKcqgz8jbkoY02tefklFLp98wvqm2liOKBglEGaRITIk6Wx1tY3fi-43TaCJnpwEwRVCz7q101?key=Qy_gZn4bA_q73M6n7nwEIA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"1024\"\u003e\u003c/figure\u003e","comment_id":"66bfded301c06500016e8b1c","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-12.12.46-PM.png","featured":false,"visibility":"public","created_at":"2024-08-16T23:20:51.000+00:00","updated_at":"2024-11-26T00:11:38.000+00:00","published_at":"2024-08-17T19:13:25.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/evaluating-leading-text-to-speech-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bff4f801c06500016e8bba","uuid":"e5bf7da8-d960-4a06-9050-b06f63a8a4c0","title":"Metrics-based RAG Development with Quantumworks Lab","slug":"metrics-based-rag-development-with-labelbox","html":"\u003ch1 id=\"overview\"\u003e\u003cstrong\u003eOverview\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOne of the biggest challenges with RAG Application development is evaluating the quality of responses. As it currently stands, there are a series of different benchmark metrics, tools, and frameworks to help with RAG application evaluation.\u003c/p\u003e\u003cp\u003eLLM evaluation tools (such as RAGAS and DeepEval) provide a plethora of quality scores designed to evaluate the efficiency of the RAG model, ranging from retrieval to generation. In this guide, we’ll take a metrics based approach to enhance RAG development by focusing on 2 target metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext recall\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eThis measures (on a scale from 0-1) how well the retrieved content aligns with the Ground Truth Answer\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext precision\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eIdeally, the most relevant context chunks should be retrieved first. This metric measures whether the RAG application can return the most relevant chunks (with respect to the Ground Truth) at the top.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"generating-ground-truth-data-and-%E2%80%98synthetic%E2%80%99-data\"\u003e\u003cstrong\u003eGenerating ground truth data and ‘\u003cem\u003esynthetic’\u003c/em\u003e data\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo calculate performance metrics, we have to compare the RAG response with ground truth. Ground truths are factually verified responses to a given user query and must be linked back to a source document.\u003c/p\u003e\u003cul\u003e\u003cli\u003eOn the other hand, ground truth data can be automatically generated by pre-trained models. LLM Frameworks, such as LangChain, have modules that generate Question / Answer pairs based on a source document. While creating Synthetic ground truth is less labor intensive, drawbacks include quality issues, lacking real-world nuances, and inheriting model bias.\u003c/li\u003e\u003cli\u003eGround truth data is typically manually created by experts through the collection and verification of internal source documents. While labor intensive and slower to collect, manual data labeling optimizes for accuracy and quality.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo start off, we’ll use the Quantumworks Lab Platform to orchestrate a ground truth creation workflow with 2 approaches.\u003c/p\u003e\u003col\u003e\u003cli\u003eManual ground truth generation\u003c/li\u003e\u003cli\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"manual-ground-truth-generation\"\u003e\u003cstrong\u003eManual ground truth generation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eBy uploading common user questions (prompts) to the Quantumworks Lab Platform, we can set up an annotation project. Now subject matter experts can craft a thoughtful answer and provide the appropriate source.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"938\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"synthetic-ground-truth-generation-with-human-in-the-loop\"\u003e\u003cstrong\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1154\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeveraging the latest Foundation Models on Quantumworks Lab Foundry, we can combine an automated ground truth generation process with a Human in the Loop component for quality assurance.\u003c/p\u003e\u003cp\u003eTo start off, we will include the source document for each user query (PDF in this case). We can then prompt a foundation model (Google Gemini 1.5 Pro in this case) for each user question and attach the PDF context needed to answer the question.\u003c/p\u003e\u003cp\u003eThe provided prompt in this case is: *For the given text and PDF attachment, answer the following. Given the attached PDF, generate an answer using the information from the attachment. Respond with I don't know if the PDF attachment doesn't contain the answer to the question.*\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"724\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eNext, we can include the response generated from Gemini \u003cstrong\u003eas pre-labels\u003c/strong\u003e to an annotation project.\u0026nbsp;\u003c/p\u003e\u003cp\u003eInstead of starting from scratch while answering the question, human experts can modify and verify the Gemini response along with the corresponding document and user query, optimizing for quality and efficiency.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1480\" height=\"570\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1480w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"improving-rag\"\u003eImproving RAG\u003c/h1\u003e\u003cp\u003eA RAG based application consists of many components. Summarized in a recent research survey by Gao et al. (2023), a few factors that impact RAG system performance include (but not limited to):\u003c/p\u003e\u003cul\u003e\u003cli\u003eImproving the retriever (e.g. Reranking)\u003c/li\u003e\u003cli\u003eFine-tuning the embedding model for source documents\u003c/li\u003e\u003cli\u003eAdding document metadata tagging (for rerouting or semantic search)\u003c/li\u003e\u003cli\u003eQuery rewriting\u003c/li\u003e\u003cli\u003eOptimizing the chunking strategy\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’ll focus on the following strategies for improving metrics based RAG evaluation:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eOptimizing the chunking strategy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-tuning the embedding model\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"chunking-strategy\"\u003e\u003cstrong\u003eChunking strategy\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhile keeping LLM generation model (GPT Turbo 3.5) and Embeddings Model (sentence-transformers/distiluse-base-multilingual-cased-v2) consistent, we will compare the retrieval metrics derived from RAGAs.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will export the question and response pairs generated from Quantumworks Lab to a Python Environment.\u003c/li\u003e\u003cli\u003eNext we will generate the RAG response based on the following chunking strategies:\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRecursive character splitter\u003c/strong\u003e (500 token size with 150 chunk overlap)\u003cul\u003e\u003cli\u003eThis is a heuristic approach and involves dynamically splitting text based on a set of rules (e.g. new lines / tables / page breaks) while maintaining coherence.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpacy text splitter\u003c/strong\u003e\u003cul\u003e\u003cli\u003espaCy uses a rule-based approach combined with machine learning models to perform tokenization.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnce the RAG response and context have been generated, we create a table with the following fields required for RAGAs metrics evaluation: (Question, Ground Truth, RAG Answer, Context)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"432\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-5.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-5.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eBy calculating RAGAs metrics evaluation using the necessary Query, Response, Ground Truth, and Source information, we can derive performance results for the entire dataset and aggregate the findings. The Context Precision and Context Recall metrics for Recursive and Spacy are shown below:\u003c/p\u003e\u003ch3 id=\"recursive-text-chunking\"\u003e\u003cstrong\u003eRecursive text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1008\" height=\"624\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-4.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png 1008w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"spacy-text-chunking\"\u003e\u003cstrong\u003eSpacy text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1054\" height=\"652\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png 1054w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eFrom the results, we observe that Spacy yielded slightly better context precision. Given the volume of our dataset (~100 QA pairs), the difference is likely statistically insignificant. However, we can conclude that choosing the Chunking strategy and parameters will certainly affect the output of the chatbot.\u003c/p\u003e\u003cp\u003eGiven the distribution of our Recall and Precision metrics, both techniques seem to struggle with the same data rows. This indicates that the key semantics are not captured by our default embeddings (\u003cstrong\u003esentence-transformers/distiluse-base-multilingual-cased-v2\u003c/strong\u003e). We’ll test out another embedding model to see if we can improve RAG performance metrics.\u003c/p\u003e\u003ch2 id=\"embeddings-model\"\u003e\u003cstrong\u003eEmbeddings model\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAn Embedding Model is responsible for generating embeddings, which is a vector representation to any piece of information, such as a chunk of text in a RAG application. A solid embeddings model is able to better capture semantic meaning of a user question and the corpus of source documents, therefore returning the most relevant chunks as the context for answer generation.\u003c/p\u003e\u003cp\u003eWhile holding the chunking strategy constant (Recursive), we will change the underlying embeddings model in the RAG workflow before evaluating with RAGAs. The Huggingface\u003ca href=\"https://huggingface.co/spaces/mteb/leaderboard?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003eLeaderboard\u003c/u\u003e\u003c/a\u003e includes performance benchmarks for various embeddings models. Beyond retrieval performance, model size, latency, and context length are also important when choosing an embeddings model.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will generate responses using the \u003cstrong\u003ebge-base-en-v1.5\u003c/strong\u003e model and evaluate with RAGAs\u003cul\u003e\u003cli\u003eWhile ranking 35 on the leaderboards, this model is extremely lightweight (109 million parameters)\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1096\" height=\"632\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png 1096w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eFrom the results, changing the embeddings model (from distiluse-base-multilingual-cased-v2 to bge-base-en-v1.5) significantly improved the retrieval metrics, around ~10 points for both context recall and precision.\u003cul\u003e\u003cli\u003eThis indicates that the retrieved context chunks are more relevant to the ground truth and ranked higher on average as well.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"fine-tuning-an-embeddings-model\"\u003e\u003cstrong\u003eFine-tuning an embeddings model\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eMost embedding models are trained on general knowledge. Specific domains, such as internal organization information, may limit the effectiveness of these models. Therefore, we can choose to fine-tune an embeddings model with the goal of better understanding and generating domain specific information.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo fine tune embeddings, we need labeled examples of positive and negative context chunks.\u003cul\u003e\u003cli\u003eUsing a training dataset, we’ll upload the user query and the top X chunks to Quantumworks Lab\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing Quantumworks Lab Annotate, subject matter experts can determine if each chunk is relevant in answering the given question.\u003c/li\u003e\u003cli\u003e\u003c/li\u003e\u003cli\u003eUsing the Quantumworks Lab SDK we can now export our labels and convert to the following format for fine tuning: {\"query\": str, \"pos\": List[str], \"neg\":List[str]}\u003cul\u003e\u003cli\u003eThe fine tuning process for the BGE family of models is also described\u003ca href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/examples/finetune/README.md?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing the fine tuned embeddings model, we can generate updated responses and evaluate.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom the example in the screenshot, bge tends to incorrectly extract the citations section, which the expert labelers can correct and mark as irrelevant.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1005\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the resulting metrics, we can see that by fine tuning on 50 data rows is able to improve context recall and context precision by 2-3 points each. We expect this to improve as more labels are provided to continuously fine tune the embeddings model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1098\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image.png 1098w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eEvaluating the quality of responses in developing RAG Applications is essential for creating efficient and reliable systems. By incorporating the Quantumworks Lab platform in metrics based RAG development, from ground truth generation to feedback for embedding models, developers can enhance the overall performance and reliability of RAG applications. \u003c/p\u003e","comment_id":"66bff4f801c06500016e8bba","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.55.12-PM.png","featured":false,"visibility":"public","created_at":"2024-08-17T00:55:20.000+00:00","updated_at":"2024-09-03T20:00:44.000+00:00","published_at":"2024-08-17T01:07:55.000+00:00","custom_excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/metrics-based-rag-development-with-labelbox/","excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66b28fe6c2887d000107cdde","uuid":"40792083-45ad-4606-8583-7781bc74c305","title":"Unlocking precision: The \"Needle-in-a-Haystack\" test for LLM evaluation","slug":"unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation","html":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\u003cp\u003eSelecting the optimal large language model (LLM) for specific tasks is crucial for maximizing efficiency and accuracy. One of the key challenges faced by teams is selecting the best models for pre-labeling tasks, especially when dealing with large datasets and complex annotations.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox’s Model Foundry provides a robust platform for evaluation and determining the most suitable model for various applications. To illustrate this, the Quantumworks Lab Labs team conducted an experiment simulating the \"Needle-in-a-Haystack\" test. This test involves identifying specific elements within vast amounts of data, ensuring the model’s precision and reliability.\u003c/p\u003e\u003cp\u003eBy utilizing Quantumworks Lab Model Foundry’s advanced experiments and evaluation tools, teams can compare multiple LLMs to identify the one that delivers the highest accuracy and efficiency for pre-labeling on complex tasks, thus saving time and enhancing the quality of predictions.\u003c/p\u003e\u003cp\u003eIn this blog post, we’ll dive into the intricacies of the \"Needle-in-a-Haystack\", exploring how to leverage Foundry to find the best model for your pre-labeling or data enrichment needs.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"the-needle-in-a-haystack-test\"\u003eThe \"Needle-in-a-Haystack\" test\u003c/h1\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test is a specialized evaluation method designed to gauge the performance of large language models (LLMs) in identifying specific, often infrequent, elements in large datasets.\u0026nbsp;\u003c/p\u003e\u003cp\u003eImagine you have a massive dataset filled with a mix of common and rare pieces of information, similar to a haystack with a few needles hidden inside. The challenge is to determine how effectively a model can find those needles (rare information) without getting distracted by the surrounding hay (common information ).\u0026nbsp; This rare information could be anything from specific keywords in a text document to unique objects in a video.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"669\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/08/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePrevious analysis of Claude-2.1 \u003c/span\u003e\u003ca href=\"https://twitter.com/GregKamradt/status/1722386725635580292?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eperformed by Greg Kamradt\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e, member of the ARC Prize team.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"628\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/08/image-2.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePrevious analysis of GPT-4 \u003c/span\u003e\u003ca href=\"https://twitter.com/GregKamradt/status/1722386725635580292?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eperformed by Greg Kamradt\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e, member of the ARC Prize team.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"why-is-this-a-good-test-for-model-foundry\"\u003e\u003cstrong\u003eWhy is this a good test for Model Foundry?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test is a great fit for Model Foundry for several reasons:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReal-world relevance:\u003c/strong\u003e This test simulates real-world conditions, where critical information is buried in a large dataset. This ensures that models are being simulated in environments that match actual applications.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComprehensive evaluation:\u003c/strong\u003e Quantumworks Lab Model Foundry offers advanced tools that make setting up experiments, running evaluations, and comparing results efficient and easy.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eBetter decision making:\u003c/strong\u003e The insights gained from the Needle in a Haystack test can facilitate stronger decision-making when we are choosing the most suitable LLM for a task. This ensures investment in models that offer the best performance for application.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch1 id=\"creating-the-needle-in-a-haystack-internally\"\u003eCreating the \"Needle-in-a-Haystack\" internally\u003c/h1\u003e\u003cp\u003eThe first step in our experiment was to create a detailed labeling instructions set that we could eventually send to LLMs for pre-labeling. It is important to note that we decided to use Text data for our study. Various other asset types such as Video and Image can also emulate a similar test.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1254\" height=\"698\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 1254w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"instructions-overview\"\u003e\u003cstrong\u003eInstructions overview\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe wanted to build a dataset that consisted of conversations between users and a customer support chatbot, focusing on banking and financial transactions. Each conversation would be categorized into specific issues related to accounts, banking services, and transactions.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs a result, our instruction set would include detailed descriptions of each category, example conversations to guide the labeling process, and clear decision-making guidelines to help annotators distinguish between closely-related issues.\u003c/p\u003e\u003ch2 id=\"ontology\"\u003e\u003cstrong\u003eOntology\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe ontology included categories such as:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eACCOUNT_TERMINATION\u003c/li\u003e\u003cli\u003eACCOUNT_RECOVERY\u003c/li\u003e\u003cli\u003eACCOUNT_SECURITY_BREACH\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_UPDATE_DETAILS\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eBANKING_LOAN_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_FEE_DISPUTE\u003c/li\u003e\u003cli\u003eBANKING_OVERDRAFT_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_WIRE_TRANSFER_HELP\u003c/li\u003e\u003cli\u003eBANKING_SAVINGS_PLANS\u003c/li\u003e\u003cli\u003eBANKING_INVESTMENT_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eBANKING_INSURANCE_PRODUCTS\u003c/li\u003e\u003cli\u003eBANKING_MOBILE_APP_SUPPORT\u003c/li\u003e\u003cli\u003eBANKING_DEBIT_CARD_ACTIVATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eTRANSACTION_DISPUTE\u003c/li\u003e\u003cli\u003eTRANSCATION_REFUND\u003c/li\u003e\u003cli\u003eTRANSACTION_VERIFICATION\u003c/li\u003e\u003cli\u003eTRANSACTION_FRAUD_REPORT\u003c/li\u003e\u003cli\u003eTRANSACTION_LIMIT_INCREASE\u003c/li\u003e\u003cli\u003eTRANSACTION_HISTORY_QUERY\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAs seen, we chose closely-correlated categories and provided precise instructions so that while there were many similarities between subcategories, there were slight differences and nuances that our chosen LLM would have to notice and use to drive the decision-making process.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"building-the-dataset\"\u003e\u003cstrong\u003eBuilding the dataset\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCreating the dataset involved curating and structuring the data row to reflect real-world scenarios that modeled the above ontology. This ensured the dataset was comprehensive and challenging for the models.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"dataset-composition\"\u003e\u003cstrong\u003eDataset composition\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData Row Content: \u003c/strong\u003eEach data row represented a conversation between a user and customer support chatbot.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"sample-data-rows\"\u003e\u003cstrong\u003eSample data rows\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eACCOUNT_TERMIATION: User conversations requesting closure of their account\u003c/li\u003e\u003cli\u003eBANKING_LOAN_SERVICES: Inquiries about applying for or managing loans\u003c/li\u003e\u003cli\u003eTRANSACTION_FRAUD_REPORT: Reports of suspected fraudulent activities\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo view our labeling instructions, click the link \u003ca href=\"https://storage.googleapis.com/labelbox-datasets/lb_rahul/pdfs/Customer%20Support%20Ticket%20LLM%20Instructions.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"model-evaluation-using-foundry-llms\"\u003eModel evaluation using Foundry LLMs\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1246\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 1246w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"model-selection\"\u003e\u003cstrong\u003eModel selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe decided to evaluate our dataset on four leading LLMs currently on the market:\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"gemini-15-pro\"\u003e\u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eReleased by Google as part of the Gemini series;\u003c/li\u003e\u003cli\u003eKnown for its strong multimodal capabilities;\u003c/li\u003e\u003cli\u003eDesigned for complex reasoning and task completion.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gpt-4o\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eDeveloped by OpenAI;\u003c/li\u003e\u003cli\u003eAn advanced iteration of the GPT (Generative Pre-trained Transformer) series;\u003c/li\u003e\u003cli\u003eKnown for its strong natural language understanding and generation;\u003c/li\u003e\u003cli\u003eOptimized for faster response times and efficient computational resource usage.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"claude-35-sonnet\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eCreated by Anthropic;\u003c/li\u003e\u003cli\u003ePart of the Claude 3 model family;\u003c/li\u003e\u003cli\u003eKnown for its strong performance in writing and complex tasks;\u003c/li\u003e\u003cli\u003eCapable of engaging in nuanced conversations and providing detailed explanations.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gemini-15-flash\"\u003e\u003cstrong\u003eGemini 1.5 Flash\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eAnother model in the Google Gemini series;\u003c/li\u003e\u003cli\u003eOptimized for speed and efficiency;\u003c/li\u003e\u003cli\u003eDesigned for tasks requiring quick responses;\u003c/li\u003e\u003cli\u003eSuitable for applications where real-time responses are crucial.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"analysis-and-insights\"\u003e\u003cstrong\u003eAnalysis and insights\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1314\" height=\"712\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 1314w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce we had created Model Foundry Predictions on our dataset for all four LLMs, we placed them into a Model Experiment for model evaluation. Creating an experiment allowed us to dive deeply into the intricacies of each model to determine their overall performance on a needle in a haystack application.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFrom the exhibits above, we can see which models performed best from an precision perspective:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eGemini 1.5 Pro (81.55%)\u003c/li\u003e\u003cli\u003eClaude 3.5 Sonnet (80.98%)\u003c/li\u003e\u003cli\u003eGPT-4o (79.02%)\u003c/li\u003e\u003cli\u003eGemini 1.5 Flash (76.96%)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1220\" height=\"772\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 1220w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eConfusion matrices and Precision graphs are also available in Model Experiment, giving us a better understanding of the above precision scores.\u003c/p\u003e\u003cp\u003eFrom the graphs and further analysis, we can see the categories in the ontology that each model struggled with. Note that a struggle indicates a precision score of less than 0.75.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1306\" height=\"734\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 1306w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"gemini-15-pro-1\"\u003e\u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"claude-35-sonnet-1\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gpt-4o-1\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eBANKING_MISC_QUERY\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gemini-15-flash-1\"\u003e\u003cstrong\u003eGemini 1.5 Flash\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_FEE_DISPUTE\u003c/li\u003e\u003cli\u003eBANKING_INSURANCE_PRODUCTS\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eBANKING_MISC_QUERY\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBased on the performance breakdown, we can draw several insights:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTop performers\u003c/strong\u003e: Gemini 1.5 Pro and Claude 3.5 Sonnet emerge as the leading models for this particular needle in a haystack task, with very similar performance profiles.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCommon challenges\u003c/strong\u003e: All models struggled with certain categories, particularly ACCOUNT_ID_CONFIRMATION and BANKING_CREDIT_CARD_ISSUES. This suggests these categories may be inherently more difficult to classify or may require more specific training data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrecision vs. Speed\u003c/strong\u003e: While Gemini 1.5 Pro achieved the highest accuracy, teams should consider their specific needs. If real-time responses are crucial, Gemini 1.5 Flash might be a better choice despite its lower accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRoom for improvement\u003c/strong\u003e: Even the top-performing models have areas where they struggle. This information can be valuable for fine-tuning models or adjusting the labeling instructions for future iterations.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch1 id=\"leveraging-model-foundry-for-decision-making-and-pre-labeling\"\u003eLeveraging Model Foundry for decision making and pre-labeling\u003c/h1\u003e\u003cp\u003eThe experiment demonstrates the power of Quantumworks Lab Model Foundry in facilitating data-driven decision-making for model selection and optimizing the pre-labeling process. By providing comprehensive evaluation tools and visualizations, Model Foundry enables teams to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompare \u003cstrong\u003emultiple\u003c/strong\u003e models \u003cstrong\u003esimultaneously\u003c/strong\u003e\u003c/li\u003e\u003cli\u003eIdentify \u003cstrong\u003especific\u003c/strong\u003e strengths and weaknesses of \u003cstrong\u003eeach\u003c/strong\u003e \u003cstrong\u003emodel\u003c/strong\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eMake informed decisions based on \u003cstrong\u003eprecision\u003c/strong\u003e, \u003cstrong\u003erecall\u003c/strong\u003e, and \u003cstrong\u003eoverall accuracy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePinpoint\u003c/strong\u003e areas for potential model \u003cstrong\u003eimprovement\u003c/strong\u003e or \u003cstrong\u003efine-tuning\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to model evaluation, Model Foundry significantly enhances the pre-labeling workflow:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEfficient pre-labeling\u003c/strong\u003e: Once the best-performing model is identified, it can be seamlessly integrated into the pre-labeling pipeline, significantly reducing manual labeling efforts\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eQuality assurance\u003c/strong\u003e: By understanding model strengths and weaknesses, teams can strategically allocate human resources to review and correct pre-labels in categories where models struggle\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIterative improvement\u003c/strong\u003e: As more data is labeled and models are retained, teams can continuously evaluate and update their pre-labeling model, ensuring ongoing optimization of the labeling process.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost reduction\u003c/strong\u003e: By selecting the most accurate model for pre-labeling, teams can minimize the need for manual corrections, leading to substantial time and cost savings in large-scale labeling projects.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy leveraging Model Foundry for both decision-making and pre-labeling processes, teams can significantly enhance the efficiency and accuracy of their entire data labeling pipeline.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"next-steps\"\u003eNext Steps\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/2024/08/image-1.png 1000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo further improve model performance and decision-making, consider the following steps:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eFine-tune models on challenging categories.\u003c/li\u003e\u003cli\u003eConduct additional experiments with different data types or industry-specific datasets.\u003c/li\u003e\u003cli\u003eImplement regular evaluations and feedback loops to identify areas for improvement and adapt to changing requirements.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy continually refining your approach and leveraging the insights gained from Model Foundry, you can ensure that your team is always using the most effective LLM for your specific needs, driving efficiency and accuracy in your AI-powered workflows.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test, as implemented through Quantumworks Lab Model Foundry, proves to be an effective method for evaluating LLM performance on complex, nuanced tasks. By simulating real-world scenarios and leveraging Model’s advanced evaluation tools, teams can select the most suitable model for their specific pre-labeling needs.\u003c/p\u003e\u003cp\u003eIn our experiment, Gemini 1.5 Pro and Claude 3.5 Sonnet demonstrated superior performance, but the choice between them (or other models) would depend on the specific requirements of the project, including factors like speed, resource efficiency, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs the field of AI continues to evolve rapidly, tools like Quantumworks Lab Model Foundry become increasingly valuable, enabling teams to stay at the forefront of the space by consistently evaluating and selecting the best models for their unique challengers.\u003c/p\u003e","comment_id":"66b28fe6c2887d000107cdde","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-2.55.05-PM.png","featured":false,"visibility":"public","created_at":"2024-08-06T21:04:38.000+00:00","updated_at":"2024-09-03T20:04:14.000+00:00","published_at":"2024-08-06T22:08:47.000+00:00","custom_excerpt":"Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our \"Needle-in-a-Haystack\" experiment. Learn how to enhance accuracy and efficiency in complex data annotations.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"66b29da3c2887d000107ce2e","name":"Raj Jain","slug":"raj","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/raj/"},{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"66b29da3c2887d000107ce2e","name":"Raj Jain","slug":"raj","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/raj/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation/","excerpt":"Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our \"Needle-in-a-Haystack\" experiment. Learn how to enhance accuracy and efficiency in complex data annotations.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66b1410331089400019364ee","uuid":"2207c60e-9f86-4ef2-9613-168f8707aee5","title":"A comprehensive approach to evaluating text-to-video models","slug":"a-comprehensive-approach-to-evaluating-text-to-video-models","html":"\u003cp\u003eThe emergence of text-to-video AI models has marked a significant milestone in artificial intelligence, with models from Runway ML (Gen-3), Luma Labs, and Pika transforming written descriptions into dynamic and lifelike videos. This technology is reshaping industries from video production to digital marketing, democratizing visual storytelling.\u003c/p\u003e\u003cp\u003eHowever, despite their impressive capabilities, these models often fall short of human expectations, producing results that lack prompt adherence, realism, or fidelity to the input text. To accelerate the development of text-to-video models, it is crucial to establish comprehensive evaluation methodologies to pinpoint areas for improvement.\u003c/p\u003e\u003cp\u003eThis article presents a rigorous approach to assessing the strengths and limitations of Runway ML (Gen-3), Luma Labs, and Pika using human preference ratings. Let’s dive into how we systematically analyzed these leading models.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"human-preference-evaluation\"\u003e\u003cstrong\u003eHuman preference evaluation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate video outputs across several key criteria:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1245\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 2246w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eChecklist for evaluating text-to-video models using human preferences.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"prompt-adherence\"\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRaters assessed how well each generated video matched the given text prompt on a scale of high, medium, or low. For example, in the given prompt: “A peaceful Zen garden with carefully raked sand, bonsai trees, and a small koi pond.” Raters looked to see if there was prompt adherence by looking at the presence of key concepts for the prompt.\u003c/p\u003e\u003cul\u003e\u003cli\u003eIs there a garden?\u003c/li\u003e\u003cli\u003eDoes it look peaceful?\u003c/li\u003e\u003cli\u003eIs the sand present, and is it raked?\u003c/li\u003e\u003cli\u003eAre there bonsai trees?\u003c/li\u003e\u003cli\u003eIs the a small koi pond?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: If all or most of the key concepts are present.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e:\u0026nbsp; If half the key concepts are present.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: If less than half of key concepts are present .\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdENKR1QEaokNDikPODa-kTi209qknyV3PBGvEm5xa4QWqugwI5sx0zmlAZOIH2XrpQRjB9xuAAb4gshoWZfSl7mjgNmNDIWcL_bXxK4pqk0TnzP0-Xn7EG0LTznK4sBhXk1ZxuiG4p_WwoCXqx2d3dj5V_?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"509\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 1: \"A romantic Parisian street scene with couples walking and street musicians playing\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"video-realism\"\u003e\u003cstrong\u003eVideo realism\u003c/strong\u003e \u003c/h4\u003e\u003cp\u003eRaters assessed how closely the video resembled reality.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Realistic lighting, textures, and proportions.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Somewhat realistic but with slight issues in shadows or textures.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Animated or artificial appearance.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd4QhDzWX5l35W_ecjUSerUNd0sTSA_DtOLU2DjwPLenslTtGrRyf4tPjhEcwQdIxFw50JqfnPwk86brTWRtowMUFRHbMdG1hr-dqGZ69-nYFebJ9KVMslxPNvHQdjrBWPA6soRH5LZ3uUDZBggfO_tJSFl?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"504\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 2: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"video-resolution\"\u003e\u003cstrong\u003eVideo resolution\u003c/strong\u003e \u003c/h4\u003e\u003cp\u003eThis criterion evaluates the level of detail and overall clarity in the video.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Fine details visible (e.g., individual leaves, fabric textures).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Major elements are clear, but finer details are somewhat lacking.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Overall blurry or lacking significant detail.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdbrYYgTHolz9LOsLQzJHJVpzFx7TWbyIXrutMv52jbQl6nu_qHjrEV5kFXbCH01iYVLRTgoJV3BDjOIdZKU8TOod8uA_Ev-5QemuPmYTYiymsr2XN7nMs6F2AbWoVph_NnkZ54BRHe2Ldq8-IJK2J1ggNj?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"502\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 3: \"An ancient temple in the jungle with hidden traps and treasures waiting to be discovered\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"artifacts\"\u003e\u003cstrong\u003eArtifacts \u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRaters identified any visible artifacts, distortions, or errors, such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnnatural distortions in objects or backgrounds\u003c/li\u003e\u003cli\u003eMisplaced or floating elements\u003c/li\u003e\u003cli\u003eInconsistent lighting or shadows\u003c/li\u003e\u003cli\u003eUnnatural repeating patterns\u003c/li\u003e\u003cli\u003eUnnatural movements\u003c/li\u003e\u003cli\u003eBlurred or pixelated areas\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: If all or 5 of the errors are present.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: If 2 or 3 errors are present.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: \u0026nbsp;If 1 or 0 errors are present.\u0026nbsp;\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcOz4iP5QSqnQJpBB5F4neKuaS-jElkj40E21sIahJZmgm2qEE2oBxWOhDBTriFa26xOPwHbpQUGPjzUMFIfIjzWxl8I8vI8Z904UjiO-Jut14igYMNzk55VBU0U82is1tavV3FJA7uSU1kXtjCEuAmhB6I?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"502\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 4: \"Cat following a mouse\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"results\"\u003e\u003cstrong\u003eResults\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOur evaluation of 25 diverse set of complex prompts, generated by GPT-4, was stress-tested and provided valuable insights into the capabilities of Runway ML, Luma Labs and Pika. Each prompt was assessed by three different raters to ensure more accurate and diverse perspectives. This rigorous stress testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of the top models: Runway ML (Gen-3), Luma Labs, and Pika.\u003c/p\u003e\u003ch3 id=\"overall-ranking-for-human-preference-evaluations\"\u003e\u003cstrong\u003eOverall ranking for human-preference evaluations\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1\u003c/strong\u003e: Runway ML (Gen-3) ranked 1st in 65.22% of cases, Luma Labs in 18.84% and Pika in 15.94%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2\u003c/strong\u003e: Luma Labs ranked 2nd in 59.42% of cases, Runway ML (Gen-3) in 21.74% and Pika in 18.84%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3\u003c/strong\u003e: Pika ranked 3rd in 65.22% of cases, Luma Labs in 21.74% and\u0026nbsp; Runway ML in 13.04%\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1248\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRank 1 standings: Runway ML (Gen-3), Luma Labs, Pika\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"model-specific-performance-results\"\u003e\u003cstrong\u003eModel-Specific Performance results\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1248\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch4 id=\"runway-ml-gen-3\"\u003e\u003cstrong\u003eRunway ML (Gen-3)\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 59.42% of cases. This model excels at accurately reflecting the input prompts, making it a reliable choice for generating intended content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 17.39% of cases. Although it performs well relative to other models, it still has room for improvement in minimizing artifacts and errors.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 46.38% of cases. Runway ML generates realistic videos nearly half the time, indicating strong capabilities in producing lifelike content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 56.52% of cases. This model is proficient in delivering high-resolution videos, enhancing the viewing experience.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uyngtw0382\" title=\"RUNWAYML_____Gen-3 Alpha 2891026367, A grand fantasy cast Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"576\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRunway ML (Gen-3): \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"luma-labs\"\u003e\u003cstrong\u003eLuma Labs\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 37.68% of cases. While not as consistent as Runway ML, it still performs reasonably well in adhering to prompts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 17.39% of cases. Similar to Runway ML, it needs improvements to reduce visual defects.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 20.29% of cases. This model struggles more with realism, making it less suitable for applications requiring lifelike video content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 30.43% of cases. Luma Labs offers moderate video resolution quality but lags behind Runway ML.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/pyz5tqupa7\" title=\"LUMA_______A_grand_fantasy_castle_surrounded_by_lush_landscapes_and_mythical_creatures__85043b Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"530\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLuma Labs: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"pika\"\u003e\u003cstrong\u003ePika\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 36.23% of cases. Comparable to Luma Labs, Pika maintains a fair level of consistency with input prompts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 43.48% of cases. Pika has the highest occurrence of artifacts and errors, indicating significant areas for enhancement.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 20.29% of cases. Like Luma Labs, Pika also faces challenges in producing realistic videos.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 23.19% of cases. Pika offers the least in terms of video resolution among the three models, suggesting a need for improvement in this aspect.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/k1kvephgc9\" title=\"PIKA____A_grand_fantasy_castle_surrounded_by_lush_landscapes_and_mythical_creatures._seed3125151661828847 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePika: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIt's worth noting that the scope of this study was constrained by two key factors: \u003c/p\u003e\u003cul\u003e\u003cli\u003eThe absence of a public API for large-scale video generation from prompts; \u003c/li\u003e\u003cli\u003eOur deliberate use of a diverse prompt dataset. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis dataset encompassed a wide range of complexity, from simple to intricate descriptions. Additionally, we attempted to use automatic evaluations, such as assessing video quality based on all video frames and evaluations by large language models (LLMs) that support video. However, due to conflicting results, these methods were omitted from the blog post.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation of state-of-the-art text-to-video models reveals a clear preference hierarchy among Runway ML (Gen-3), Luma Labs, and Pika. Runway ML (Gen-3) emerges as the top performer, securing the first rank in 65.22% of cases, thanks to its high prompt adherence and superior video resolution. However, it still exhibits a notable occurrence of artifacts and errors, suggesting room for enhancement.\u003c/p\u003e\u003cp\u003eLuma Labs, while trailing behind Runway ML, demonstrates moderate performance, particularly in maintaining prompt consistency and video resolution. Its primary weakness lies in generating realistic videos, which is crucial for lifelike content applications. On the other hand, Pika, ranking third, shows the highest need for improvement, especially in minimizing artifacts and enhancing video resolution.\u003c/p\u003e\u003cp\u003eWhile each model has its strengths and weaknesses, Runway ML (Gen-3) stands out for its robust performance across most evaluation criteria, making it the preferred choice for generating high-quality, realistic videos. As the field of text-to-video generation continues to evolve, addressing the identified shortcomings will be key to advancing the capabilities of these models.\u003c/p\u003e\u003cp\u003eBy targeting these key areas, we can drive the next wave of innovations in text-to-video technology, creating more sophisticated and versatile text-to-video systems that cater to a broader range of applications and user needs.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-video models presented here represents a significant advance in assessing AI-generated videos. When combined with Quantumworks Lab's platform, AI teams can accelerate the development and refinement of sophisticated, domain-specific video generation models with greater efficiency and quality through our dataset curation, automated evaluation techniques, and human-in-the-loop QA.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-video model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more. \u003c/p\u003e\u003cp\u003eWe'd love to hear from you and discuss how we can support your AI evaluation needs.\u003c/p\u003e","comment_id":"66b1410331089400019364ee","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.13.57-AM.png","featured":false,"visibility":"public","created_at":"2024-08-05T21:15:47.000+00:00","updated_at":"2024-09-03T20:08:05.000+00:00","published_at":"2024-08-05T22:14:00.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/a-comprehensive-approach-to-evaluating-text-to-video-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66a978d931089400019364a5","uuid":"bbe7d436-1e06-4439-bce3-780f72151bdf","title":"A comprehensive approach to evaluating text-to-image models","slug":"a-comprehensive-approach-to-evaluating-text-to-image-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in image generation evaluation, visit\u0026nbsp;\u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAs text-to-image AI models continue to evolve, it's become increasingly important to develop robust evaluation methods that can assess their performance across multiple dimensions. In this post, we'll explore a comprehensive approach to evaluating 3 leading text-to-image models - \u003cem\u003eDALL-E 3, Stable Diffusion, and Imagen 2 \u003c/em\u003e- using both human preference ratings and automated evaluation techniques.\u003c/p\u003e\u003ch2 id=\"the-rise-of-text-to-image-models\"\u003eThe rise of text-to-image models\u003c/h2\u003e\u003cp\u003eText-to-image generation has seen remarkable progress in recent years. Models like \u003cem\u003eDALL-E 3, Stable Diffusion, and Imagen 2\u003c/em\u003e can now produce strikingly realistic and creative images from natural language descriptions. This technology has many useful applications, from graphic design and content creation to scientific visualization and beyond.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe5C5TuOA7TCKRT_7O_PQ5hnqXPPAlM4jOO4KvRk2DR0y0k9OWNYJzNhGIL5rqanTMlYK9reVzCb_pwx__rvW6rTmRkRljBX6aIjnA3DuEH1L_-ahgR0MnJU9JD-vWdQnDi8pZeoRIvpXD0kskzRf3WdSxH?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"723\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemOVe_M8fg0ddRqLYwN1MaKDcuJtPGMXgkP1hlaXtbjU7ZKsbunWQZMVM34ttGlsa8ulDjWoCxW-KVagWUiNG4xdUc3yCYdWMcEKsC1Pl8da6kyk0UgjjL66-qxaua9sYL4IUNTOSNsyggkztxBhXbIur6?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"733\"\u003e\u003c/figure\u003e\u003cp\u003eAs these models become more advanced, having reliable ways to compare their performance and identify areas for improvement are paramount. Let’s next dive into how we developed a two-fold evaluation approach for getting more granularity into their performance.\u003c/p\u003e\u003ch2 id=\"1-human-preference-evaluation\"\u003e[1] Human preference evaluation\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfTKedTbSWhlq5swtSUlaHwVE--84lNrAPDAGpko3p_AnEz0jooz4-lin5Mpg-SwHG3CJktHiRqSHTCJcuavjLCJOau4-GxczB2Odq4mQOXSaH7ijz1wCWZic_o0B_npX0x5qgxAqiHULmNouzCOv2l8nMy?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"902\"\u003e\u003c/figure\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human labelers per data row, allowing us to tap into a network of expert raters to evaluate image outputs across several key criteria:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAlignment with prompt: \u003c/strong\u003eRaters assessed how well each generated image matched the given text prompt on a scale of high, medium, or low. For example, for the prompt \"A red apple on a wooden table\":\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Image shows a clear, realistic red apple on a wooden table\u003c/li\u003e\u003cli\u003eMedium: Image shows an apple, but it's green or the table isn't clearly wooden\u003c/li\u003e\u003cli\u003eLow: Image shows an unrelated scene\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePhotorealism: \u003c/strong\u003eThis criterion evaluates how closely the image resembled a real photograph:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Realistic lighting, textures, and proportions\u003c/li\u003e\u003cli\u003eMedium: Somewhat realistic but with slight issues in shadows or textures\u003c/li\u003e\u003cli\u003eLow: Cartoonish or artificial appearance\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eDetail: \u003c/strong\u003eRaters then determined the level of detail and overall clarity:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Fine details visible (e.g., individual leaves, fabric textures)\u003c/li\u003e\u003cli\u003eMedium: Major elements clear, but finer details somewhat lacking\u003c/li\u003e\u003cli\u003eLow: Overall blurry or lacking significant detail\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eArtifacts: \u003c/strong\u003eFinally, raters identified any visible artifacts, distortions, or errors, such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnnatural distortions in objects or backgrounds\u003c/li\u003e\u003cli\u003eMisplaced or floating elements\u003c/li\u003e\u003cli\u003eInconsistent lighting or shadows \u003c/li\u003e\u003cli\u003eUnnatural repeating patterns\u003c/li\u003e\u003cli\u003eBlurred or pixelated areas\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"2-automated-evaluations\"\u003e[2] Automated evaluations\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXduxNvNrJswCQvT7DGZfSyEoIGUOLdlCVN1c3UTP_Bbz-s-sL-P246muageVr1aPVyH7DlaXtqkmGS6Lf9fm_MuztOdJAddLr2muQLG7MzHCC4BqHfBLlN9YAO7teoaN1Qma8wTq8kRv53xfh-V1iGRVMqO?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"907\"\u003e\u003c/figure\u003e\u003cp\u003eTo complement human ratings, we implemented several automated evaluation techniques. Here’s an example for one \u003ca href=\"https://storage.googleapis.com/text_image_eval/gen_images/014ac7aa527c953fd0a7aeb08e238dea/results.json?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eImage Quality Score: \u003c/strong\u003eWe calculated an objective image quality score based on several key metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSharpness: Using Laplacian variance to assess image clarity.\u003c/li\u003e\u003cli\u003eContrast: Evaluating the range between minimum and maximum pixel values.\u003c/li\u003e\u003cli\u003eNoise Estimation: Employing a filter-based approach to quantify image noise.\u003c/li\u003e\u003cli\u003eStructural Similarity Index (SSIM): Comparing the image to a slightly blurred version to assess structural integrity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese metrics were combined into a comprehensive quality score in order to provide an objective measure of the image's technical attributes.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePrompt adherence: \u003c/strong\u003eWe utilized the CLIP (Contrastive Language-Image Pre-training) model to measure similarity between the text prompt and the generated image in a shared embedding space. This approach provides an automated assessment of how well the image aligns with the given prompt, offering insights into the model's ability to accurately interpret and visualize textual descriptions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDetailed scoring: \u003c/strong\u003eWe employed Claude, an advanced AI model, to provide detailed scoring and analysis of the generated images. This multifaceted evaluation includes several key components.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cu\u003eElement accuracy scores\u003c/u\u003e: For each key element in the prompt, Claude assesses its presence, provides a description, and assigns an accuracy score out of 10 reflecting how well these elements matched the prompt.\u003c/li\u003e\u003cli\u003e\u003cu\u003eCategory scores\u003c/u\u003e: Claude evaluates images across various categories such as objects, colors, spatial relations, activities, and materials. Each category receives a score out of 10, providing a comprehensive view of the image's content accuracy.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eClaude prompt adherence\u003c/u\u003e: Claude assigns an overall similarity score, expressed as a percentage, indicating how closely the entire image matches the given prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eUnexpected elements and inconsistencies\u003c/u\u003e: Claude identifies any unexpected elements or inconsistencies in the image that do not align with the intended prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eOverall impression:\u003c/u\u003e Claude provides an overall impression of the image, summarizing how well it captures the essence of the prompt. This includes a qualitative assessment of the image's strengths and areas for improvement.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"results\"\u003eResults\u0026nbsp;\u003c/h2\u003e\u003cp\u003eOur evaluation of 100 images across a diverse set of complex prompts, generated by GPT-4, stress-tested and provided valuable insights into the capabilities of Stable Diffusion, DALL-E, and Imagen 2.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHere’s a summary of the human-preference evaluations:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eModel rankings and initial findings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eStable Diffusion ranked first in 50.7% of cases, DALL-E in 38%, and Imagen 2 in 11.3%.\u003c/li\u003e\u003cli\u003eStable Diffusion ranked second (37%), followed by DALL-E (33%) and Imagen 2 (30%)\u003c/li\u003e\u003cli\u003eImagen 2 was ranked third (59%), while DALL-E and Stable Diffusion were ranked last less often (29% and 12% respectively\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ePerformance metrics (as percentages of maximum possible scores):\u003c/p\u003e\u003cul\u003e\u003cli\u003eStable Diffusion: 84.3% prompt alignment, 85.3% photorealism, 91.7% detail/clarity.\u003c/li\u003e\u003cli\u003eDALL-E: 84.3% prompt alignment, 58.3% photorealism, 83.7% detail/clarity.\u003c/li\u003e\u003cli\u003eImagen 2: 61.3% prompt alignment, 74.7% photorealism, 71.3% detail/clarity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eHere’s a summary of the detailed auto evaluation metrics:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eStable Diffusion emerged as a consistent performer across various metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003eClaude’s prompt adherence score: 82.27%. More specifically, attributed to accurate/realistic colors (89.40%) and depicting objects (89.30%)\u003c/li\u003e\u003cli\u003eHowever, image quality (34.98%) was relatively low\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDALL-E excelled in prompt interpretation and visualization:\u003c/p\u003e\u003cul\u003e\u003cli\u003eClaude’s prompt adherence score: 87.04%. More specifically, attributed to depicting objects well (91.60%) and displaying moving activities accurately (81.10%)\u003c/li\u003e\u003cli\u003eStrongest in translating textual descriptions into visual elements\u003c/li\u003e\u003cli\u003eHowever, image quality (30.11%) was lowest\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eImagen 2 performed the worst, but had a higher technical quality for images:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLowest Claude prompt adherence score (76.86%) and aspect accuracy (70.92%)\u003c/li\u003e\u003cli\u003eMuch weaker in moving activities (72.20%) and detailed attributes (77.70%)\u003c/li\u003e\u003cli\u003eHigher image quality (55.55%) than the other two models\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eAnalysis and insights:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"831\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eComparing the auto evaluation metrics to the human preference evaluations reveals some additional interesting findings:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStable Diffusion's balanced performance:\u003c/strong\u003e Stable Diffusion emerged as the top performer overall in human evaluations, ranking first in 50.7% of cases. It showed consistent high scores across human-evaluated metrics, particularly excelling in detail/clarity (91.7%) and photorealism (85.3%). However, the auto evaluation revealed a relatively low image quality score (34.98%), suggesting that technical image quality doesn't always correlate with human perception of quality.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDALL-E's strengths and weaknesses:\u003c/strong\u003e While DALL-E ranked first in 38% of human evaluations, it showed a significant weakness in human-perceived photorealism (58.3%). Interestingly, it had the highest Claude prompt adherence score (87.04%) in the auto evaluation, which aligns with its strong performance in human-evaluated prompt alignment (84.3%). This suggests DALL-E excels at interpreting and executing prompts, but may struggle with realistic rendering.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImagen 2's technical quality vs. human preference:\u003c/strong\u003e Imagen 2 consistently ranked lower in human preferences, struggling particularly with prompt alignment (61.3%). However, it had the highest technical image quality score (55.55%) in the auto evaluation. This discrepancy highlights that technical image quality doesn't necessarily translate to human preference or perceived prompt adherence.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrompt alignment discrepancies\u003c/strong\u003e: While human evaluations showed Stable Diffusion and DALL-E tied in prompt alignment (84.3% each), the auto evaluation gave DALL-E a higher score (87.04%) compared to Stable Diffusion (82.27%). This suggests that human and AI perceptions of prompt adherence may differ slightly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePhotorealism and image quality\u003c/strong\u003e: The human-evaluated photorealism scores don't align with the auto-evaluated image quality scores. Stable Diffusion led in human-perceived photorealism (85.3%) but had low technical image quality (34.98%). Conversely, Imagen 2 had the highest technical image quality (55.55%) but ranked second in human-perceived photorealism (74.7%).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDetail and clarity vs. technical metrics\u003c/strong\u003e: Stable Diffusion stood out in human-evaluated detail and clarity (91.7%), which aligns with its high auto-evaluated scores in depicting objects (89.30%) and accurate colors (89.40%). This suggests a correlation between these technical aspects and human perception of detail and clarity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIt's also important to note that this study didn't include Midjourney due to its Discord-only integration, which made it challenging to implement into our evaluation study. While Midjourney is recognized for its high-quality output, its unconventional access method can be a barrier for users seeking traditional API or web-based interactions. Additionally, Google’s Imagen 2 implements strict safety and content filters across a wide range of topics, which did limit versatility and required additional pre-processing. Such factors, alongside the technical and human-based perceptual metrics evaluated in our study, also influence the overall usability and adoption of AI image generation models in real-world scenarios.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eOur comprehensive evaluation of leading text-to-image models, combining human preference ratings with automated metrics, reveals intriguing contrasts between quantitative performance and human perception. Stable Diffusion emerged as the overall top performer in human evaluations, excelling in detail/clarity and photorealism despite a lower technical image quality score. This underscores the complex relationship between technical metrics and human perception of quality. DALL-E demonstrated strength in prompt interpretation and adherence across both human and automated evaluations, although it showed weakness in human-perceived photorealism. Imagen 2, while scoring highest in technical image quality, consistently ranked lower in human preferences, particularly struggling with prompt alignment.\u003c/p\u003e\u003cp\u003eAs these technologies continue to evolve, our results indicate that each model has distinct strengths and areas for improvement. Stable Diffusion offers balanced performance across various criteria, making it suitable for a wide range of applications. DALL-E excels in prompt interpretation and execution, making it ideal for tasks requiring precise visualization of detailed descriptions. Imagen 2's high technical quality suggests it could be particularly useful in applications where image fidelity is prioritized, although improvements in prompt adherence would enhance its overall performance. Future research should focus on bridging the gap between technical metrics and human perception, as well as addressing specific weaknesses identified in each model, such as DALL-E’s photorealism or Imagen 2’s prompt alignment. By refining these aspects, we can push the boundaries of AI-generated imagery and develop more versatile and capable text-to-image systems that better meet the needs of various applications and user preferences.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003eGet started today\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-image models presented here represents a significant advance in assessing AI-generated images. When combined with Quantumworks Lab's platform, AI teams can accelerate the development and refinement of sophisticated, domain-specific image generation models with greater efficiency and quality through our dataset curation, automated evaluation techniques, and human-in-the-loop QA.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-image model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more. We'd love to hear from you and discuss how we can support your AI evaluation needs.\u003c/p\u003e","comment_id":"66a978d931089400019364a5","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-4.35.13-PM.png","featured":false,"visibility":"public","created_at":"2024-07-30T23:35:53.000+00:00","updated_at":"2024-11-26T00:11:57.000+00:00","published_at":"2024-07-31T00:02:09.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"66a9822f31089400019364c9","name":"Krish Maniar","slug":"krish","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/krish/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"66a9822f31089400019364c9","name":"Krish Maniar","slug":"krish","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/krish/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/a-comprehensive-approach-to-evaluating-text-to-image-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"669a9314e017de000190bda5","uuid":"88ed362b-5dd4-48eb-950c-0e2798e34a3f","title":"Using Quantumworks Lab to improve data quality via AutoQA \u0026 advanced labeler review","slug":"using-labelbox-to-improve-data-quality-via-autoqa-advanced-labeler-review","html":"\u003cp\u003eWorking with leading generative AI teams who are building frontier models and developing task-specific generative AI products, we've seen first hand the importance of data quality and robust QA processes in order to deliver performant models. The availability of human-evaluated data sets the companies apart in their AI offerings. In this solution accelerator, we will guide you through the different workflows and demonstrate how Quantumworks Lab can expedite the quality review process for creating better data for generative AI use cases.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dkbjlz0nnn\" title=\"Kushal AudoQA - 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe generative AI use cases we’ll focus on for this guide is multi-turn conversations, similar to what you’d find interacting with an LLM. As an introduction, Quantumworks Lab offers multimodal chat for two main options:\u003c/p\u003e\u003ch3 id=\"1-live-online-based-evaluation\"\u003e\u003cstrong\u003e1. Live online-based evaluation\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn this scenario, Quantumworks Lab provides an experience similar to a \u003ca href=\"https://labelbox.com/blog/announcing-multimodal-chat-for-genai-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003echatbot arena\u003c/a\u003e where multiple models or multiple versions of the same model are compared against each other. A labeler then ranks the responses to determine which one is better. For example, you can input a prompt and receive three different responses from different versions of a model, such as Gemini, GPT, or Claude. It's important to note that the labelers don’t know which response corresponds to which model to prevent bias.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExample:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eInput a prompt.\u003c/li\u003e\u003cli\u003eReceive three responses from different model versions.\u003c/li\u003e\u003cli\u003eThe labeler then ranks the responses based on quality (defined as per your business-specific needs).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"2-offline-multimodal-chat\"\u003e\u003cstrong\u003e2. Offline multimodal chat\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThis option allows you to upload existing multi-turn conversations to Quantumworks Lab for evaluation (e.g. accuracy, relevance, tone, etc.).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExample:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eUpload a multi-turn conversation between a human and a chatbot.\u003c/li\u003e\u003cli\u003eEvaluate and label the conversation across different axes, such as relevance, factuality, and fluency.\u003c/li\u003e\u003cli\u003eThis can be done on a per-message level or for the entire conversation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy leveraging these workflows, teams can effectively utilize Quantumworks Lab for various chatbot-based use cases.\u003c/p\u003e\u003ch2 id=\"prompt-and-response-generation-walkthrough\"\u003e\u003cstrong\u003ePrompt and response generation walkthrough\u003c/strong\u003e\u003cbr\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mfgloie4dw\" title=\"Kushal AutoQA - 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a first step, let’s walk through a common workflow in Quantumworks Lab involving prompt and response generation, which is crucial for training a model. This process involves creating question and answer pairs and offers three main options: workforce-generated prompts and responses, guided prompt and response creation, and responding to uploaded prompts.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor workforce-generated prompts, labelers input questions and corresponding responses, selecting the appropriate category for each pair.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXemODUkHK4G4d1-GDQpXuY-GpXvdZKokbuOZ7pqhwH4YtEqYP-c2UNwakRhkJKp_jja7mEgD-pvdSUAe0F4QAVu5DX6cAB3pqCww2cWOGqc3fG9nMwl99D5VqUJnPyoh2nzblS1VQo3EKAvIJ0t4690fUET?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"823\"\u003e\u003c/figure\u003e\u003cp\u003eAs shown above, the guided prompt and response creation option uses an image, code snippet, video, or text as a basis for labelers to generate relevant question and answer pairs.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rk8jhk0ro6\" title=\"Kushal AutoQA - 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLastly, if you have existing prompts, labelers can evaluate these prompts on multi-turn conversations and provide suitable responses. This flexible workflow supports various input types, enabling efficient model training through comprehensive prompt and response generation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXfp0NYehNcvRTlOFRnRnvl_c88o9_XZO4MQcTGAhlxO0zFJiKJhOxoSC-SnfAb2rPOQAovFmjRKWEW422hl6gqrONAYrDTo_dasu3GxgQ71K3_bVCYrJay_cdRd3fEyQz-89-puoPPKlghrbm1l49dhsSxV?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"819\"\u003e\u003c/figure\u003e\u003cp\u003eAfter identifying the data to be labeled and setting up the labeling schema, which includes fields for the prompt, response, and category, you can attach detailed instructions via PDF. These instructions are accessible to labelers during the labeling process and labelers can start labeling from scratch by generating prompt-response combinations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXf-pM7Cev56U0tGeAKUQ2qsmDjGPHfwGn_sLk8yb_7sKCuzuWhystmTA2--SOTMp5C9uPmIvxzr1KnkMmbAfuD6YG6GzDmfU_tVf6l7cJ3TU1BoJfLNqyAILHuD7q-QFx4CMYBInrek29GFll6Q3lupNyYW?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"885\"\u003e\u003c/figure\u003e\u003ch2 id=\"ai-assisted-alignment-ai-assisted-labeling\"\u003e\u003cstrong\u003eAI-assisted alignment (AI-assisted labeling)\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dxv4cpcs3z\" title=\"Kushal AutoQA - 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLabelbox allows you to use a large language model or a custom model for pre-labeling by using our model-assisted labeling option.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can select any available model such as \u003cem\u003eGPT, Gemini, Claude\u003c/em\u003e, etc or bring in your own custom one. By generating a preview, you receive pre-labels that labelers can then modify and correct without having to create labels from scratch. While this speeds up the labeling process, it can introduce bias.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf your task requires unbiased labeling, you can enforce labelers to create labels from scratch without any model assistance. The platform offers flexibility, enabling you to choose between speed and accuracy based on your task requirements.\u003c/p\u003e\u003ch2 id=\"labelbox-ai-assisted-alignment-autoqa-aka-ai-critic\"\u003e\u003cstrong\u003eLabelbox AI-assisted alignment (AutoQA aka AI critic)\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ugdbth629w\" title=\"Kushal AutoQA - 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a next step, let’s walk through how you can leverage Quantumworks Lab’s platform to utilize an LLM to act as an AI critic or judge to help auto QA your data during the review process. Quantumworks Lab employs an LLM to review prompt and response pairs, providing scores and feedback and critique on the quality of labels and why things were good or bad. This feedback helps identify insights and scores to figure out which labels require further review.\u003c/p\u003e\u003ch3 id=\"scoring-and-feedback\"\u003e\u003cstrong\u003eScoring and feedback\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXd49bah2g-Y-dJi7rv07qxkQxH3QVspBNwHJMYT06TKhZPS61KEjDIKdfGsMAWaMLXwVFHHXSAgnfePp1TjzyBtG_15Xmz91syjfZN2XDU34jovG1Akyao2_7o_h1LvX-CBG6oJnJWuYDOJ05rvpGL4-sK-?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"829\"\u003e\u003c/figure\u003e\u003cp\u003eAs an example shown above, you can see that for this specific data row, the labeling score is 4.75,\u0026nbsp; and includes ideas for improving the reasoning. With this view, you can go through all of your different data rows and filter the data based on scores to identify labels that need additional scrutiny. For example, you might filter for scores below 4 and set a range, and move these labels to a custom queue for further review. Creating custom queues for specific score ranges, like between 3 and 4, allows for a more organized review and QA process.\u003c/p\u003e\u003cp\u003eReviewers in the loop can view label instructions, scores, and improvement ideas for each label. This information is crucial for understanding the quality of your labels and making necessary adjustments. You can filter and select labels based on their scores and move them to appropriate queues for further action. As a best practice, the Quantumworks Lab team works closely with our customers to come up with the best scoring for your review and evaluation needs.\u003c/p\u003e\u003cp\u003eFurthermore, Quantumworks Lab allows you to create custom review workflows tailored to your specific needs. As shown below, you can define different metrics and criteria for scoring, such as \u003cem\u003eBLEU\u003c/em\u003e scores, which is common specifically for Gen AI use cases around free-form text.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXdDrCCs3pFr5vZc1XiH4NKSRy10Z52AwFMxspY8rlrM8VwWSD97jtCucEWQl9y6siSKXRVDwnrZBP78CPRobkNjdp2MhwpGUt9PrZGbmPdFlTo0vkvDOD5XnRx1DL9vKuYXJ5FKFvv4gx3AIdgU9eNOpiKs?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1096\"\u003e\u003c/figure\u003e\u003cp\u003eThis flexibility ensures that the review process aligns with all of your project requirements and quality standards.\u003c/p\u003e\u003ch3 id=\"exporting-your-data\"\u003e\u003cstrong\u003eExporting your data\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXe3mrwO8jvCOqnuLQNzGBZcY_h4iVwJygVYyRXE0JUDEk4yijWUglS6tjJI29fGsZESN1kSyiIqbFoBcH4LqQdUHaf72pb5Az2PMV0SAYzNQE4JaXaJ6PncP_wGBU17MHmu7pHCpuvioc-OXqB9VvN2yj38?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"786\"\u003e\u003c/figure\u003e\u003cp\u003eOnce labels are finished with the review process, labels are moved to the done stage and are ready for export out of the Quantumworks Lab platform. You can export the data by selecting it and triggering the export JSON. This process ensures that you have high-quality data ready.\u003c/p\u003e\u003cp\u003eBecause Quantumworks Lab tracks in-depth metrics around labeling operations (average label time, average review time, etc.) disaggregated by labeling member, you can very easily calculate additional inter-annotator agreement metrics, such as Krippendorff’s Alpha Score and Cohen’s Cappa for qualitative metrics (e.g. likert scales), beyond the off-the-shelf benchmark and consensus metrics that Quantumworks Lab already calculates.\u003c/p\u003e\u003ch3 id=\"providing-feedback-to-labelers-and-ensuring-quality\"\u003e\u003cstrong\u003eProviding feedback to labelers and ensuring quality\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXdNzGDU92vOA2K8yP34ISKLkN1FD-qRMenZxYSsJZFRfI3mdgVMAWymkkZV4eukzSUuxyt1nEobCbJJoVPlXIrACbfz6c6-HCDCjf9KqIlAkIVw2D_ivX6SVpfzJMeUc2MwOfW4YYhrpBwuriI-2aSEkJkc?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"817\"\u003e\u003c/figure\u003e\u003cp\u003eCustomers and reviewers can also easily collaborate and provide real-time feedback to labelers by raising any issues or instructions, and labelers can be notified to make necessary changes. As an example, you can instruct your labelers to “incorporate the feedback from the LLM to make this response better.”\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcD95jtRW_w_sxoXYDqzq5_U2KUBcaMwhKlfyFedseFMCRNbUjTEm0RNeb4TqIghead-cgTt1AosJ-yELJpndZfwL25GZOjIFJPPX_lDNF0gMhRMnmeiTg2jVCmjMBUiJl0lILmbeytnwpi3Y3bcZa5HaRI?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"862\"\u003e\u003c/figure\u003e\u003cp\u003eThe review and feedback loop helps maintain high-quality data and can be tailored to include as many review steps as needed to meet your quality control standards, allowing you to select how much data you want auto-QA’d, how much data you want Quantumworks Lab to internally review amongst themselves or whether you want to review it on your own. Quantumworks Lab provides full control and typically works with customers to determine how many review steps you want and what are the different scores you want to evaluate against.\u003c/p\u003e\u003cp\u003eThis ongoing process ensures a consistent and efficient labeling engine; as new data comes in, it gets quickly labeled and if it meets the criteria, it moves to an appropriate queue which then gets reviewed by subject matter experts, and finally gets you the batch of high-quality labels delivered.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003e\u003c/h2\u003e\u003cp\u003eWe hope that this walkthrough gives you a better understanding of how Quantumworks Lab makes it easier than ever before to iterate quickly with real-time, granular visibility into labels and implement autoQA workflows for data quality. In addition, AI labs and generative AI companies can benefit from tapping into \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ediverse pools\u003c/a\u003e of expertise in order to improve the underlying data and model performance with Labelbox.\u0026nbsp;\u003c/p\u003e\u003cp\u003eReady to improve the data quality for your generative AI initiatives? \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eContact us\u003c/u\u003e\u003c/a\u003e to learn more and we’d love to hear from you.\u0026nbsp;\u003c/p\u003e","comment_id":"669a9314e017de000190bda5","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-19-at-9.36.05-AM.png","featured":false,"visibility":"public","created_at":"2024-07-19T16:23:48.000+00:00","updated_at":"2024-07-19T18:03:53.000+00:00","published_at":"2024-07-19T16:32:08.000+00:00","custom_excerpt":"Learn how you can use Quantumworks Lab to accelerate the quality review process for creating better data for generative AI use cases using autoQA and advanced labeler reviews.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/using-labelbox-to-improve-data-quality-via-autoqa-advanced-labeler-review/","excerpt":"Learn how you can use Quantumworks Lab to accelerate the quality review process for creating better data for generative AI use cases using autoQA and advanced labeler reviews.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"668c2f59878a56000112a4e0","uuid":"93014cdf-3555-4d2b-b480-2b9db686e2db","title":"Using multimodal chat to enhance a customer’s online support experience","slug":"using-multimodal-chat-to-enhance-a-customers-online-support-experience","html":"\u003cp\u003eCustomer service systems are prime targets for multimodal chat as this new technology leverages an intuitive interface for answering user questions\u0026nbsp; while enabling companies to unlock previously hidden insights about customer journeys and sentiment.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can streamline the process of labeling and managing data for various use cases, including intent classification, dialogue retrieval, state tracking, and model evaluation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. We’ll illustrate the various labeling tasks that Quantumworks Lab can accomplish to ensure the chatbot is intelligent and effective, providing customers with relevant and timely answers.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://files.readme.io/ce6e056-Labelbox_2024-06-04_10-37-061.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"678\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eLabelbox offers a \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/docs/live-multimodal-chat-evaluation?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003enumber of editor types\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, including the live multimodal chat editor.\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"labeling-tasks\"\u003e\u003cstrong\u003eLabeling Tasks\u003c/strong\u003e\u003c/h1\u003e\u003ch2 id=\"multimodal-intent-classification-dialogue-retrieval-and-state-tracking\"\u003e\u003cstrong\u003eMultimodal Intent Classification, Dialogue Retrieval and State Tracking\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xs53lmro56\" title=\"Customer Query Resolution Training - Part 1 - Training Data Collection + Using Classifications Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 1: Generate prompt-response pairs for supervised fine-tuning of agentic workflows.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eLabel the customer’s intent and sentiments in a given turn of the chat, so you can identify the appropriate next steps accordingly.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1427\" height=\"1333\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/first-one.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/first-one.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png 1427w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe customer appears frustrated, so the sentiment is labeled as “frustrated.”\u003c/p\u003e\u003cp\u003eFurthermore, the customer appears to be reporting their order was damaged and is requesting a new one, so both of those requests are labeled using checklist classifications.\u003c/p\u003e\u003cp\u003eWith these labels, you know the customer’s intent (get a new piece) and can also record the current state of the conversation (damage reported but no action taken yet). As the conversation proceeds, you can update the state based on what actions are taken and keep checking the intent and tone of the customer to ensure they are satisfied.\u003c/p\u003e\u003cp\u003eLabelbox allows you to label every aspect of every turn-by-turn conversation, enabling you to develop fine-grained data sets of annotated real-world conversations.\u003c/p\u003e\u003cp\u003eYou can use these labels to train your agent to retrieve similar past interactions and follow similar response patterns or simply be inspired by them.\u003c/p\u003e\u003ch2 id=\"model-evaluation-ranking-and-selection\"\u003e\u003cstrong\u003eModel evaluation, ranking and selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eYou trained a few models offline with the previously generated data using supervised fine-tuning or RLHF. How do you know which of the models is the best performing, and how often do they make mistakes?\u003c/p\u003e\u003cp\u003eLabelbox allows you to connect these models hosted behind your custom endpoints. Then, with the Live Multimodal Chat editor, labelers can interact with all of them simultaneously by feeding prompts, comparing and contrasting responses, and even rewriting the best ones to be even better.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1vpskixih4\" title=\"Customer Query Resolution Training - Part 2 - LLM Development \u0026amp; Evaluation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 2: Evaluate a single model, generate multiple responses and write your own gold standard response.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eWhen evaluating a single model, Quantumworks Lab allows you to generate multiple responses and also add your own response. This is very useful for use cases where all of the model’s responses are lacking in quality and a human-written response provides the gold standard for further downstream training. Furthermore, in the course of the labeling conversation, you can choose one of these responses to be passed in as prior context into later turns, allowing you to choose the path of the conversation in real-time for better training data generation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uzesocscxk\" title=\"Customer Query Resolution Training - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 3: Evaluate multiple models at once in “chat arena” style.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/secone-one-1-1-1-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"450\" height=\"640\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox also supports evaluating multiple models in a “chat arena” style setup.\u003c/p\u003e\u003cp\u003eSay you have trained candidate models and now want to compare them against each other and also foundation models like GPT 4o. Connect your own models with Quantumworks Lab as \u003cem\u003ecustom\u003c/em\u003e models, and easily choose any latest and greatest foundation model from Foundry within Labelbox. The chat arena excludes information about which response was generated from which model so as not to bias the labeling process.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWith Quantumworks Lab's multimodal chat functionalities, you can generate high-quality datasets to train, evaluate, and test your custom LLMs or pit off-the-shelf LLMs like GPT4o and Gemini 1.5 against each other in a Chatbot Arena-style competition to see which one suits your needs best.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/b3f423vxob\" title=\"Customer Query Resolution Training - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 4: Survey of all LLM capabilities in Quantumworks Lab\u003c/em\u003e\u003c/p\u003e\u003cp\u003eAs the landscape of LLMs evolves rapidly, Quantumworks Lab will continue to build functionality to ensure our customers can align models to their use cases quickly and reliably. With differentiated data, streamlined operations, and transparent orchestration, you can build and refine foundation models that meet your specific needs.\u003c/p\u003e\u003ch2 id=\"evaluate-multimodal-chat-data-today-for-free\"\u003eEvaluate multimodal chat data today for free\u003c/h2\u003e\u003cp\u003eReady to deliver on the next-generation of generative AI?\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eSign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more and we’d love to hear from you.\u003c/p\u003e","comment_id":"668c2f59878a56000112a4e0","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/mmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp","featured":false,"visibility":"public","created_at":"2024-07-08T18:26:33.000+00:00","updated_at":"2024-07-10T16:09:14.000+00:00","published_at":"2024-07-09T16:06:15.000+00:00","custom_excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-multimodal-chat-to-enhance-a-customers-online-support-experience/","excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"662804d1d733f0000145b1c4","uuid":"3db0aafe-d6b7-470a-8ff0-37835d066fe4","title":"How to harness AI for efficient video labeling","slug":"harnessing-ai-for-efficient-video-labeling","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eWorking in collaboration with numerous leading companies in artificial intelligence, we're observing a surge in enthusiasm for using advanced models to initially label data, integrating human expertise later to refine and tailor previously labor-intensive and time-consuming tasks.\u003c/p\u003e\u003cp\u003eThese AI models are transforming one of the most daunting tasks in machine learning—the creation of high-quality video datasets. Utilizing such models allows machine learning teams to leverage automated tools to pre-label or enrich data, facilitating a range of applications from monitoring driver behavior to detecting objects in manufacturing environments.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis blog post will explore how models like Gemini 1.5 Pro, Grounding DINO, and SAM are redefining the video labeling landscape, while boosting efficiency and speed. \u003c/p\u003e\u003cp\u003eBy automating the labor-intensive labeling tasks, these models not only accelerate the workflow, but also liberate time for users and decrease labeling costs.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"steps\"\u003eSteps\u003c/h2\u003e\u003ch3 id=\"step-1-select-video\"\u003eStep 1: Select video\u0026nbsp;\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eTo select a video: \u003c/p\u003e\u003cul\u003e\u003cli\u003eNarrow in on a subset of data. Users can use Quantumworks Lab Catalog filters, including media attributes, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eUsers can click “Predict with Foundry” once the data of interest is selected.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-choose-a-model-of-interest\"\u003eStep 2: Choose a model of interest\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eTo select a model: \u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, users will be prompted to choose a model of interest for a model run.\u003c/li\u003e\u003cli\u003eThen, select a model from the ‘model gallery’ based on the type of task - such as video classification, video object detection, and video segmentation.\u003c/li\u003e\u003cli\u003eTo locate a specific model, users can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine-learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-configure-model-settings-and-submit-a-model-run\"\u003eStep 3: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eOnce the model of interest is selected, users can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of settings, which can be found in the Advanced model setting.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can generate preview predictions on up to five data rows to understand how the current model settings affect the final predictions.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows users to confirm the configuration settings confidently:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf users are unhappy with the generated preview predictions, they can edit the model settings and continue to generate preview predictions until they're satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eAfter users are satisfied with the preview predictions, a model run can be submitted.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-send-the-images-to-annotate\"\u003eStep 4: Send the images to Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eUsers can transfer the results to a labeling project using the UI via the \"Send to Annotate\" feature. Labelers can then quickly review labels for accuracy.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"pre-labeling-use-cases\"\u003ePre-labeling Use Cases\u003c/h2\u003e\u003ch3 id=\"example-1-segmentation-mask-using-grounding-dino-sam\"\u003eExample 1: Segmentation mask using Grounding DINO + SAM\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eSegmentation masks\u003c/a\u003e are used for autonomous vehicles, medical imagery, retail applications, face recognition and analysis, video surveillance, satellite image analysis, etc. Masks are some of the most time-consuming annotations to make for video. Below, we see an example of how this can be automated with Grounding DINO + SAM so the reviewers can make small edits if needed instead of starting from scratch.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7k184d0h65\" title=\"Pre-labeling Use Case: Segmentation mask using Grounding DINO + SAM Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"example-2-bounding-box-using-grounding-dino\"\u003eExample 2: Bounding box using Grounding DINO\u003c/h3\u003e\u003cp\u003eBounding boxes are utilized in similar scenarios as segmentation masks, but these scenarios demand less precision than those requiring pixel-level (masks) detail. Bounding boxes can be automated using Grounding DINO, as illustrated below with detection of a person in video.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ie9s2r3zff\" title=\"Pre-labeling Use Case: Bounding Box using Grounding DINO Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"example-3-global-classification-using-gemini-15-pro\"\u003eExample 3: Global classification using Gemini 1.5 Pro\u003c/h3\u003e\u003cp\u003eGlobal classification for video is used when the overall classification for video is required like when a driver safety system needs to detect if a driver is distracted. Gemini 1.5 Pro can analyze an hour long video and provide answers about events that took place in the video. This automation reduces the need for human intervention, allowing personnel to focus on reviewing videos only when they are flagged with specific classifications. \u003c/p\u003e\u003ch3 id=\"example-4-frame-based-classification-using-gemini-15-pro\"\u003eExample 4: Frame based classification using Gemini 1.5 Pro\u003c/h3\u003e\u003cp\u003eFrame-based classification is utilized in scenarios similar to segmentation masks. Gemini 1.5 Pro can analyze an hour-long video and identify the specific timestamps for a particular event. \u003c/p\u003e\u003ch2 id=\"additional-considerations\"\u003e\u003cstrong\u003eAdditional considerations\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAdditional considerations as users incorporate Foundry labels into their projects and workflows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo produce frame based binary classifications in Gemini 1.5 Pro, users are recommended to experiment with the prompt and provide as much context as possible to get the best results. \u003cul\u003e\u003cli\u003eFor example, the following yields better results:\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003c!--kg-card-begin: html--\u003e\n“For the given video, what timestamps have a banana and be as thorough as possible about checking each second for a banana. Make sure there is no overlap in timestamps. Return the result as a dictionary without any new lines. {“banana”: “\u003ctimestamps-with-banana\u003e“, “no_banana”:  “\u003ctimestamps-without-banana\u003e“}” than using “For the given video, what timestamps have a banana. Return the result as a dictionary without any new lines. {“banana”: “\u003ctimestamps-with-banana\u003e” }”\n\u003c!--kg-card-end: html--\u003e\n\u003cul\u003e\u003cli\u003eUsers can also \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA/B test different models \u003c/u\u003e\u003c/a\u003efrom Foundry to find the model that best fits the use case using Quantumworks Lab Model.\u003c/li\u003e\u003cli\u003eIf we do not support your use case or any questions arise, feel free to contact our \u003ca href=\"https://docs.labelbox.com/docs/contacting-customer-support?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esupport team\u003c/u\u003e\u003c/a\u003e, as we would love to hear your feedback to improve Foundry.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAnnotating video data has traditionally been a tedious and time-consuming task. The integration of advanced AI models from Quantumworks Lab Foundry into the video labeling process marks a significant transformation in how video data is annotated. By leveraging Foundry's capabilities, users can drastically speed up their video labeling projects. This acceleration not only diminishes the time required to bring products to market but also substantially reduces the costs involved in model development.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our additional resources on how to utilize state-of-the-art AI models in Foundry, including using \u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003emodel distillation\u003c/a\u003e and fine-tuning to leverage the power of foundation models:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-accelerate-labeling-projects/?ref=labelbox-guides.ghost.io#conclusion\"\u003e\u003cu\u003eHow to accelerate labeling projects using GPT–4V in Foundry\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io#what-are-the-benefits-of-using-image-segmentation-for-my-machine-learning-model\"\u003e\u003cu\u003eHow to create high-quality image segmentation masks quickly and easily\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://event.on24.com/wcc/r/4460171/5D5357846BEAB1FD1E12A5ED146DE6E1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomated Labeling With Any Foundation Model\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFine tune LLMs\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"662804d1d733f0000145b1c4","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/image.png","featured":false,"visibility":"public","created_at":"2024-04-23T18:58:25.000+00:00","updated_at":"2024-11-22T23:53:12.000+00:00","published_at":"2024-04-23T23:40:19.000+00:00","custom_excerpt":"Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/harnessing-ai-for-efficient-video-labeling/","excerpt":"Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66183ccc8d5c4a00014061cc","uuid":"2afa1d4e-85b9-48d3-ae01-81c57db2916b","title":"How to use AI to improve website search relevance","slug":"how-to-improve-search-relevance","html":"\u003cp\u003eWith the latest advances in foundation models, organizations can now enhance search relevance for websites by better matching between user intent with product listings. While companies now have access to a wealth of search queries, sifting through all of these search results can be incredibly time-consuming and resource-intensive. By leveraging AI, teams can now analyze search queries and feedback at scale, to gain insights into common topics or customer sentiment. This allows businesses to identify common themes and pinpoint areas of improvement to enhance\u0026nbsp;their overall website experience to maximize for key metrics such as user retention, conversion and revenue.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for search relevance. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eImproving search relevance requires a vast amount of data in the form of search queries and accurate product descriptions. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape: \u003c/strong\u003eThe changing nature and format of customer review data from multiple sources (e.g webpages, apps, social media, etc.) poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their website search relevance for product descriptions and listings. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s assisted data enrichment and flexible training frameworks to quickly build AI systems that uncover actionable insights from customer searches. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-full\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1746\" height=\"952\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1746w\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to dramatically improve search relevance for any website or app. Specifically, this guide will walk through how you can explore and better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-use-ai-to-improve-search-relevance-for-your-website\"\u003eSee it in action: How to use AI to improve search relevance for your website\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below in either: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1-kTALPqchHXGogUdRVnya7GklKHrsWVL?ref=labelbox-guides.ghost.io#scrollTo=Vw_nNPl8baNJ\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1-kTALPqchHXGogUdRVnya7GklKHrsWVL?ref=labelbox-guides.ghost.io#scrollTo=Vw_nNPl8baNJ\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs customer queries and product descriptions across channels proliferate, brands want to learn from customer feedback to build the most user-friendly experience on their website or app. For this use case, we’ll be working with a dataset of e-commerce website queries – with the goal of analyzing the queries to demonstrate how a company could gain insight into how their customers search for products and how to optimize for relevance.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/vpl1wf0vui\" title=\"Search relevance 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003eFor the purpose of this tutorial, we’ve provided a sample open-source \u003ca href=\"https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe?ref=labelbox-guides.ghost.io\"\u003eKaggle dataset\u003c/a\u003e that can be downloaded.  \u003c/p\u003e\u003cp\u003ePlease \u003ca href=\"https://drive.google.com/file/d/1hh2MYzol6-4PSsqX7mbX2YSLLGA4jAYZ/view?usp=drivesdk\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edownload the dataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your text data as a CSV. \u003c/li\u003e\u003cli\u003eIf your text snippets sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your text data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data-by-clustering\"\u003eSearch and curate data by clustering\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"708\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 1538w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Smart select to cluster data and focus your model improvement on specific data rows\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExplore topics of interest\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith your data in Quantumworks Lab, you can begin to leverage Catalog to uncover interesting topics to get a sense of what customers are searching for. \u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize your data –\u0026nbsp; you can click through individual data rows to get a sense for which queries are the most popular.\u003c/li\u003e\u003cli\u003eDrill into specific topics of interest\u003cstrong\u003e \u003c/strong\u003e– leverage natural language search, for example searching “beds, mirrors, etc,” to bring up all related queries related to that topic. You can adjust the confidence threshold of your searches accordingly which can be helpful in gauging the volume of data related to the topic of interest.\u0026nbsp;\u003c/li\u003e\u003cli\u003eEasily find all instances of similar examples to data of interest.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-2-create-an-initial-model-run-of-search-relevance-assessments\"\u003ePart 2: Create an initial model run of search relevance assessments \u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/t3er59wcfk\" title=\"Search relevance 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can proceed to using Quantumworks Lab's Foundry product to model run an initial model run to accelerate search relevance assessments.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, you'll need to set up your ontology for search relevance assessment based on your project's requirements.\u003c/li\u003e\u003cli\u003eAfterwards, you can define the criteria for rating the relevance of search results to each type of query.\u003c/li\u003e\u003cli\u003eNext, you can communicate the business definition of relevance to the models directly into the prompt. You can use Foundry to add context to the prompt, allowing it to rank results as if it were part of your respective business. In this example, we'll include the prompts for what \"good relevance\", \"excellent relevance\", etc and help the model predict what would fit under this criteria. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAs an illustrative example, you can set up \"excellent relevance\" as a result that perfectly matches the search query, including all specific attributes (category, material, color, purpose, etc). This indicates that the term is exactly what the user is searching for. For the query, \"kitchen blender stainless steel\", a result for \"stainless steel countertop blender\" is highly relevant, matching the user's intended category.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"673\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eGenerate an initial preview to assess how well the adjusted prompt performs and you can save the adjusted prompt as an app, including data type (text), ontology, and the original prompt. This allows for easy re-use and the ability to build upon the saved app for future assessments of search relevance criteria. \u003c/p\u003e\u003cp\u003eAfter this has been set up, you can now generate the next preview to ensure quality before submitting the model run for assessments.\u003c/p\u003e\u003ch3 id=\"view-search-relevance-assessments-results\"\u003eView search relevance assessments results \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gq8nnv2ykw\" title=\"Search relevance 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce your model inferencing job has been completed, you can then navigate to the model tab and locate a variety of foundation models (e.g., Claude 3 will be used in this tutorial) to view the completed model run with your rankings.\u003c/li\u003e\u003cli\u003eOptionally, you can add an explanation classification or review the results of the ranking, which includes all 560 items/data rows.\u003c/li\u003e\u003cli\u003eBy adding the results to your project, you can next perform further analytics such as analyzing the distributions of predictions from the Metrics view.\u003c/li\u003e\u003cli\u003eNext, select all items and you can send them to Annotate, and choose \"search relevance assessment\", where you'll then be able to have humans review as an additional quality check.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"further-analyze-and-optimize-your-search-relevance-assessments-results\"\u003eFurther analyze and optimize your search relevance assessments results \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kw7in8bosa\" title=\"Search relevance 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe last part of the walkthrough is to analyze your distribution of relevance categories within your project, noting varying levels of relevance and to review the query class for all 560 data rows to identify trends in relevance. You can do this by using automated approaches to understand query types and relevance patterns, as we show in the video above.\u003c/li\u003e\u003cli\u003eBy filtering your dataset by the search relevance assessment project, you can navigate to the Analytics view to identify trends and examples of excellent relevance and poor relevance within specific query classes (as shown below).\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"883\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample analytics distribution showing excellent relevance on terms like kids wall decor, sectionals and area rugs.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"838\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample analytics distribution showing poor relevance for beds, furniture cushions, mirrors\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAs this time, you can consider adjusting prompts to accurately reflect relevance criteria, or use metadata fields, such as query class, to further analyze relevance.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo further evaluate and enrich the data, teams can also explore incorporating human supervision in the labeling process, with a hybrid or combination approaches: fully automated, half human in the loop, half automated, or all human-in-the-loop.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can improve your data further in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eAlignerr\u003c/a\u003e: Leverage our global network of\u0026nbsp;specialized labelers for a variety of tasks.\u0026nbsp;This community of subject matter experts from several disciplines align AI models by creating high-quality data in their field of expertise. The community spans nearly every major discipline of sciences, industries and languages, worldwide.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eBy tapping into the most recent developments in foundation models, businesses can transform the effectiveness of website searches by refining the alignment between user intent and product offerings. Given the abundance of search queries that a prospective customer may use, the process of sorting through them manually is labor-intensive and time-consuming. \u003c/p\u003e\u003cp\u003eBy harnessing the power of AI, organizations can efficiently examine search queries and feedback on a large scale, uncovering recurring themes and gauging customer sentiment. \u003c/p\u003e\u003cp\u003eThis enables enterprises to detect prevalent trends and target areas for enhancement, allowing them to optimizing the overall website experience to drive key metrics like user retention, conversion rates, and revenue. Remember to optimize the \u003ca href=\"https://www.web4business.com.au/portfolio-item/the-most-important-24-pages-to-include-on-website/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ewebsite content\u003c/a\u003e as well to ensure it's meeting your end user's goals. Give the walkthrough a try and we also recommend checking out our other solution accelerators such as \u003ca href=\"https://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003epersonalized experiences\u003c/a\u003e for retail to improve customer experiences.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful search relevance websites. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"66183ccc8d5c4a00014061cc","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/thumbnail--6-.png","featured":false,"visibility":"public","created_at":"2024-04-11T19:41:00.000+00:00","updated_at":"2024-09-12T23:45:53.000+00:00","published_at":"2024-04-12T17:02:16.000+00:00","custom_excerpt":"Learn how you can dramatically improve search relevance for any website or app by using AI and foundation models to better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-improve-search-relevance/","excerpt":"Learn how you can dramatically improve search relevance for any website or app by using AI and foundation models to better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6601bf732822410001bac2ba","uuid":"f740e28a-2932-4536-b087-575a1edd5fde","title":"How to improve your task-specific chatbot for better safety, relevancy, and user feedback","slug":"how-to-improve-your-task-specific-chatbot-for-safety-relevancy-and-user-feedback","html":"\u003cp\u003eBuilding task-specific chatbots requires a structured approach when it comes to improving their everyday usefulness, specifically for better safety, relevancy, and user feedback. Given the highly subjective tasks LLM-powered chat applications are expected to perform, a common denominator for how well they do in real-world settings depend on the availability of reliable high-quality training data and how closely aligned they are to human preferences. Working hand-in-hand with leading AI teams, we've observed a set of best practices that we wanted to share in order to help you improve the performance of your task-specific chatbots.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo ensure high levels of trust \u0026amp; safety, an LLM-powered chatbot should be able to detect intentions, entities, and topics when interacting with a user. With quality labeled examples, a task driven application can steer away from conversations that are not relevant to its intended task, ensuring a safe and smooth user experience.\u003c/li\u003e\u003cli\u003eThe large language model (LLM) at the heart of the chat application must be fine tuned with relevant responses or enhanced with human feedback from RAG techniques (e.g. \u003ca href=\"https://labelbox.com/guides/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eReranking\u003c/a\u003e) in order to ensure that the user receives the most relevant information. Examples of \u003ca href=\"https://www.ssw.com.au/rules/train-gpt/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLLMs that can be used, include GPT\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eTo prevent a chatbot from drifting or getting stale with outdated responses, ML teams must continuously monitor and evaluate model performance using ground-truth responses and / or human feedback.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this tutorial guide, we'll walk through some of these top considerations and how Quantumworks Lab can be used as a platform to help accelerate chatbot development.\u003c/p\u003e\u003ch3 id=\"part-1-trust-and-safety-%E2%80%94-understanding-intentions\"\u003e\u003cstrong\u003ePart 1: Trust and Safety — Understanding Intentions\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/u8nd7zpy0o\" title=\"Chat Pt.1 Safety Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo ensure the best user experience, an LLM-based chatbot must be properly scoped to deliver on its intended area of expertise. For example, a chatbot application for an airline company should not be responding to off topic questions, such as politics. Therefore, a chatbot that understands user intent can steer the user towards its intended areas of expertise and away from potentially harmful or unrelated conversations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/image--10-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1720\" height=\"792\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/image--10-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/image--10-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/image--10-.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/image--10-.png 1720w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAn example visual of an LLM-based chatbot which without guardrails, would go off and answer questions that are not pertinent to their intended area of expertise.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn this section, we'll leverage Quantumworks Lab to classify the intent of historical conversations as on-topic (coffee / tea) or off-topic (politics). To start off, let’s load a subset of the \u003ca href=\"https://github.com/thunlp/UltraChat?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eUltrachat\u003c/u\u003e\u003c/a\u003e dataset to Quantumworks Lab Catalog. Ultrachat is an open-source dialogue dataset powered by Turbo APIs to train powerful language models with general conversational capability.\u003c/p\u003e\u003cp\u003eTo being, let's first identify political conversations that your chatbot shouldn’t be able to answer.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing the \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eCatalog\u003c/a\u003e filters, we can cast an initial wide net of examples.\u003cul\u003e\u003cli\u003eSemantic Search — utilize the underlying vector embeddings to return relevant prompts (Politics and opinionated text)\u003c/li\u003e\u003cli\u003eOther functions (e.g. find text) — this will be used to identify targeted keywords or phrases\u003c/li\u003e\u003cli\u003eSave the filters as a \"Slice\" for further reference\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/dHdLvbrx9IA9Uxtlz5Ka2lcRVwT-U6PqOr3Dn7rJ8RtNjv5UQIys4KXULZiOl-8O80BV7jqvifcV4c7evkfG-zIi4RGiI034Ou6_iC9NixZjw90SIxjXbzFS93YzT6PJT323LhJRuORDokUxr_df2rU\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"294\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Quantumworks Lab Catalog filters to help you identify target keywords\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can see that using semantic search and labeling functions have produced promising initial results. As a next step, let’s validate your results further with foundation models.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eWith Quantumworks Lab, you can use a pre-trained foundation model to verify the results from semantic search and other functions.\u003c/li\u003e\u003cli\u003eBy selecting the targeted Data Rows within the slice, you can apply Google’s Gemini Pro to check for political and opinionated classification.\u003c/li\u003e\u003cli\u003eThe results will be returned to you as pre-labels.\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/U-RLJxDtPSYJ99SfjhdUoAv4Nhi6B9O0eaaQMtBtrhesVDrNHvcfUYLym-q_xzn8jZPdpvunuAVrP5VY_fWOaigJH1vIMCg4vxi17LLJ_Rs82Mz759wDEGTss97OxRH_xBJh8Vkdfg4TFZpNUoQlag4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"758\"\u003e\u003c/figure\u003e\u003cp\u003eTo ensure completeness, you can next leverage a human-in-the-loop (HITL) approach\u0026nbsp;to conduct a final review for intent.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eAnnotate\u003c/a\u003e, you create an Annotation Project for Text.\u003c/li\u003e\u003cli\u003eYou can now send the Prompts and responses (as pre-labels) to your Annotation Project.\u003c/li\u003e\u003cli\u003eA human-in-the-loop (HITL) approach ensures that the prompts are checked for complex nuances by skilled professionals that the models may have missed.\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/ES6aNgATSLyk3CKSOqxwuDHPqLq0KsG7K8QeH9MGZKEFwcPqk-CmR5pNKgVfUyJ6TL0LeLSJx-YH-HMEYo-lmAILem_GiFLmUgNI36ntbiPSPdndCgq7cc2fz2aNEoS3pmRF3-h_JPCiyD7Yu36i8BM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"529\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eUtilizing the \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox SDK\u003c/a\u003e, these prompts, along with their labeled classifications, can now be seamlessly integrated with trust and safety frameworks, such as\u003ca href=\"https://github.com/NVIDIA/NeMo-Guardrails?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003eNeMo Guardrails\u003c/u\u003e\u003c/a\u003e, an open source toolkit for controlling the output of a large language model.\u003c/li\u003e\u003cli\u003eBased on the prompts and classifications labeled in the previous steps, we can feed these examples into NeMo to ensure that the application does not respond to potentially sensitive political topics.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Q00lf_YPoPlL_11VaJyfLtcYK9e7VBgb55ZvOv0SmvhWAq5McNJ4wjyului_alK0F0M4rWoUHLXpETRZAlE_zUbTg7_WrDvuptD_WQWExyiMR-EusJL58Fs8YYtoqudO0KGN6mCKcgk1M_oxDnuu-ow\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"300\"\u003e\u003c/figure\u003e\u003ch3 id=\"part-2-generating-quality-responses-for-your-llm-based-chatbot\"\u003e\u003cstrong\u003ePart 2: Generating Quality Responses for your LLM-based Chatbot \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y8zb2m8mt1\" title=\"Chatbot Pt2. Fine Tuning Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWhen your intelligent chat application is powered by an underlying large language model, you can customize these LLMs to a defined task with 2 key approaches: Fine-tuning or Retrieval Augmented Generation (RAG).\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor this exercise, you will fine tune a task-specific LLM, and you can follow along via text below or from the video above. It’s worth noting that Quantumworks Lab can also be used for optimizing RAG based systems with techniques such as \u003ca href=\"https://labelbox.com/guides/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eReranking\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo start off, let's first generate quality responses to selected prompts. Ideally, we’ll want the chatbot to replicate the responses provided by your annotators.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab Catalog, you'll identify the relevant prompts related to coffee, and you can combine filters such as:\u003cul\u003e\u003cli\u003eSemantic search with input prompt\u003c/li\u003e\u003cli\u003eSimilarity search with ideal example data rows\u003c/li\u003e\u003cli\u003eKeywords matching\u003c/li\u003e\u003cli\u003eExclude already classified prompts that were shown in Part 1 of this tutorial\u003c/li\u003e\u003cli\u003eSave your filters as a Slice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/4lzE9AcYtkeXWiz6ZpFvV65fK1hOJbnY4OYDoe-7a5kCzFILWHQ3yH3GXGe7rfqC4pP1QjJ4V44OwPhiSdg3YgCJXBVlwMqmvUqSgkxVYXAl2WfnWpQVsWp9yqz-02wzymgp_2hRFgn4iPuDzzrOTiU\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"703\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce you have the initial set of LLM prompts, you will next create an annotation project for \u003cstrong\u003eHumans Responses to Uploaded Prompts.\u003c/strong\u003e This allows you to quickly generate quality responses to each of the selected prompts.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/6dGzWO9-wbbS2WJnqxtXYsBuOY21s9CJAqsPAks6LH4WvUiy9Ld7aIwTcz8ZQzvGmHTWX9bIaHNq8PA6ZKGmGl2-wVtpuHIHWe3dYry75aXJeJFATMnt5uLxWJgROWD312eqX1Sh-7-FCrOS185jvvs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"776\" height=\"472\"\u003e\u003c/figure\u003e\u003cp\u003eYour team of annotators can now produce specific responses for the LLM to learn from.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eIf you need help in this domain, Quantumworks Lab provides on-demand teams of skilled professional labelers with LLM experience via Quantumworks Lab’s \u003ca href=\"https://labelbox.com/product/boost/workforce/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBoost Workforce\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/W7qsBM44eDsFDtQEHnWHzKt9pACdNp3o5MOrLpCFnVfrXmRiL-ZTk3yao-WHkSPO6SocQ02waG2WkbYVJNo-BWuPbjSzgzZNmqr0xWmSoBIXIlasNWi-pQnWRXeGWFALhB9czIz7U4OXZDhNhlbMQbQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"654\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce your dataset is generated, you can use the Python SDK to export the labels and convert it to a format for model fine-tuning (e.g. JSONL format for GCP Vertex AI)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy identifying relevant prompts and generating quality responses, you can now train a task specific LLM model and output predictions for any new requests.\u003c/p\u003e\u003ch3 id=\"part-3-model-evaluation-and-deployment\"\u003e\u003cstrong\u003ePart 3: Model Evaluation and Deployment\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9a0o6o5vne\" title=\"Chatbot pt3. Evaluation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"558\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBefore deploying your model into production, let’s evaluate the performance of the responses when compared to your ground-truth data (expected output). Using holdout prompts and responses pairs not used to train the model, you can evaluate the performance of your fine-tuned LLM.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModel\u003c/a\u003e, let’s first create a new experiment as shown below.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/dS7V4NoliqteLrHDgvgOAnvjWmyl5BvWfBFXpQpI0JfwAbnHPdtSj4riVFvYjGgSqwIJur5n8cW2POzvJWDnkNi_MBXsmsw73gRLvwFabtjJIJKT5p69clEPPP1SiPY67SqwYN1kO1AqDVZosSS7Sso\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"759\"\u003e\u003c/figure\u003e\u003cp\u003eAfterwards, you can create a model run to add the holdout dataset containing the ground-truth responses.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/PYteE32jXIijEUpH5HqvAk8I__xzRMkv9d30sqZU0ztL1lbdWBQp6GwJenkqDmcN3LkuJh-QlbQB4wV-0eisZOIUN9j7FIz4s1ODAf7zkh-vhm1h2WGjFPbOZlqt72GZEmc-4EgWo8Jo2EYU0mcW1SM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"557\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eUsing the Python SDK, you can next export the prompts to generate responses from your fine-tuned model from Google's Vertex AI.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eUsing 3rd-party libraries, you can calculate custom metrics for evaluating NLP tasks, such as the BLEU score\u003c/li\u003e\u003cli\u003eThe predictions and custom metrics can now be uploaded back into your model run\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/9DYocSmDHD410N5L4APC2cQ_LoWrzK380GO8LpvaLysfihLwLse2S7sTkFNQiui5RfRYTr7Uy5oCIL9dqAZ7PW3KTScFZ_WbLbR8rq7iu5384DxzF3932GBLTQuaiPkboL6D8hjpPHMQIP5RblomLXw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"588\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWith the metrics and predictions uploaded, you can filter by various metrics to identify the highest and lowest performing prompts, along with overall model performance.\u003c/li\u003e\u003cli\u003eYou can then select these data rows (such as this high performing prompt below) to find similar prompts within the catalog, which will be used to train the next iteration of your chat application.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/yZHhwpGt5t1TM9EoNUufy7G239bLWVWYpWMcX8m5nfhIcRTE_XZ3FRdq5tiCCpM-T9WrsDQd6Qo7XVQgduIc-r2_n1_JN0nk13UJ7XfFwpgYFzdE5_RX_tqWXsSnhBAM-nzVssr_ZQ4xoKG6H5u2qaE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"938\"\u003e\u003c/figure\u003e\u003ch3 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn this guide, we highlighted a few best practices for ensuring better safety, relevancy, and user feedback when building a task-specific chatbot. Regardless of the techniques used and the models chosen, it is crucial to generate and classify data to the highest quality for optimizing chatbot outputs. \u003c/p\u003e\u003cp\u003eBy incorporating semantic search, labeling functions, foundation models, and a human-in-the-loop approach, you will be able to generate and classify data  in higher qualities consistently. Combined with an SDK-driven approach, you can more easily train models and enhance LLM performance through faster iterations. Give the tutorial a try and we'd love to hear your feedback or ideas on how we can help you improve your LLM-based chatbot applications.\u003c/p\u003e","comment_id":"6601bf732822410001bac2ba","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/03/thumbnail--3-.png","featured":false,"visibility":"public","created_at":"2024-03-25T18:16:19.000+00:00","updated_at":"2024-05-28T17:01:29.000+00:00","published_at":"2024-03-25T19:16:18.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab for optimizing your task-specific LLM chatbot for better safety, relevancy, and user feedback.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/how-to-improve-your-task-specific-chatbot-for-safety-relevancy-and-user-feedback/","excerpt":"Learn how to leverage Quantumworks Lab for optimizing your task-specific LLM chatbot for better safety, relevancy, and user feedback.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":48,"allPosts":[{"id":"66bfded301c06500016e8b1c","uuid":"a2c40e88-6376-4824-b57f-e0e3673d8b1b","title":"Evaluating leading text-to-speech models","slug":"evaluating-leading-text-to-speech-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in speech generation evaluation, visit \u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe evolution of Text-to-Speech (TTS) models has been groundbreaking, with various companies offering high-quality solutions for a range of applications. Whether it's enhancing accessibility, automating customer service, or generating voiceovers, TTS models play a vital role in today's digital landscape. \u003c/p\u003e\u003cp\u003eWe recently evaluated the performance of six leading TTS models—Google TTS, Cartesia, AWS Polly, OpenAI TTS, Deepgram, and Eleven Labs—using key evaluation metrics such as Word Error Rate (WER) and human preference ratings.\u003c/p\u003e\u003cp\u003eTo provide an in-depth analysis, we evaluated each model on 500 prompts. Each prompt was reviewed by three expert labelers to ensure accuracy and provide diverse perspectives.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"evaluation-process\"\u003e\u003cstrong\u003eEvaluation Process\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate speech outputs across several key criteria:\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer\"\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWER measures the accuracy of the generated speech by calculating the number of insertions, deletions, and substitutions compared to a reference transcription. The lower the WER, the better the model performed in terms of generating accurate and coherent speech.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1120\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"word-errors\"\u003eWord errors\u003c/h3\u003e\u003cp\u003eRaters evaluated the presence of word errors in the generated TTS audio by counting the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Insertion Errors\u003c/strong\u003e: Additional words not present in the original text.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Deletion Errors\u003c/strong\u003e: Words from the original text that were omitted.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Substitution Errors\u003c/strong\u003e: Incorrect replacements for original words.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: No word errors or only minor errors (0-1 error).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some errors (2-3 errors).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Significant errors (4 or more errors).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"speech-naturalness\"\u003eSpeech naturalness\u003c/h3\u003e\u003cp\u003eRaters assessed how human-like the generated speech sounded.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Very human-like, natural flow with appropriate pauses and inflections.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some human qualities but with occasional robotic or awkward elements.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Clearly robotic or artificial, with choppy or monotone speech.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"pronunciation-accuracy\"\u003ePronunciation accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated how clearly and correctly words were pronounced in the TTS output.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: All words are pronounced clearly and correctly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: 1-2 words are mispronounced or unclear.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: 3 or more words are mispronounced or difficult to understand.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"noise\"\u003eNoise\u003c/h3\u003e\u003cp\u003eRaters evaluated the level of background noise or artifacts in the audio.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNone\u003c/strong\u003e: No detectable noise or artifacts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Minor noise, such as a faint hiss, but not distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Noticeable noise or artifacts that may be somewhat distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Significant noise or artifacts that interfere with comprehension.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"context-awareness\"\u003eContext awareness\u003c/h3\u003e\u003cp\u003eRaters assessed the TTS system's ability to adjust to context, including tone, emphasis, and punctuation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Excellent adaptation to context, with clear tonal shifts and pauses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Adequate adaptation but misses some nuances.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Little to no adaptation; reads text monotonously without context cues.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"prosody-accuracy\"\u003eProsody accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated the rhythm, stress, and intonation (prosody) of the generated speech.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Natural-sounding rhythm and intonation that closely mimics human speech.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Mostly natural but with occasional awkwardness in stress or rhythm.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Monotonous or robotic intonation with unnatural pauses and emphasis\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"prompts-curation-process\"\u003ePrompts curation process\u003c/h2\u003e\u003cp\u003eThe diverse set of prompts used in our TTS evaluation provides a comprehensive test of model capabilities across multiple dimensions. \u003c/p\u003e\u003cp\u003eConsider these examples:\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Wow! Did you see the fireworks last night? They were spectacular!\"\u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003e This short, exclamatory sentence tests the model's ability to convey excitement and emphasis. It challenges the TTS system to appropriately modulate tone and pitch to express enthusiasm.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"I wish you were on the right path...I-I-I truly do, but you're not,' he said sadly.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis prompt evaluates the model's handling of emotional nuance, hesitation (indicated by the stuttering \"I-I-I\"), and the use of ellipsis for pauses. It also tests the system's ability to convey sadness as specified in the dialogue tag.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Alright, listen up, team! The mission briefing is as follows: Operation Nighthawk commences at 0300 hours. Rendezvous at coordinates 51°30'26.5\"N 0°07'39.0\"W. Your login credentials for the secure server are: Username: NightOwl_Alpha, Password: 3ch3l0n_X9#. Remember, this is a stealth operation. Radio silence must be maintained except for emergency protocol Sierra-Tango-Seven. If compromised, use the failsafe code: Zulu-Yankee-4-7-2-Bravo. Any questions? No? Then synchronize your watches... now. Good luck, and godspeed.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis complex prompt tests multiple aspects:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical pronunciation of alphanumeric strings and military jargon\u003c/li\u003e\u003cli\u003eConveying urgency and authority\u003c/li\u003e\u003cli\u003eHandling of coordinates and precise numerical information\u003c/li\u003e\u003cli\u003eProper pacing for a lengthy, detailed instruction set\u003c/li\u003e\u003cli\u003eAppropriate pausing, indicated by punctuation and paragraph breaks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese prompts demonstrate how we cover a range of linguistic features, emotional tones, and formatting challenges. They span from simple exclamations to complex, technical instructions, ensuring a thorough assessment of prosody, emphasis, and versatility.\u003c/p\u003e\u003cp\u003eThis diversity in prompts matters because it enables a comprehensive evaluation that reflects real-world applicability. By covering such a broad spectrum of scenarios, we can identify specific strengths and weaknesses in each TTS model, allowing for fair comparisons and pinpointing areas for improvement. Whether it's expressing joy over fireworks, conveying emotional conflict, or delivering a detailed mission briefing, these prompts help us ensure that TTS models can handle the variety and complexity of human communication.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overall-ranking-best-to-worst-model-samples\"\u003eOverall Ranking best to worst model samples\u003c/h2\u003e\u003cp\u003eRaters ranked each TTS sample from best to worst based on performance across all categories above.\u003c/p\u003e\u003ch3 id=\"model-configurations-used-at-test-time\"\u003e\u003cstrong\u003eModel configurations used at test time\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: tts-1-hd\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: alloy\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle Text-to-Speech (TTS)\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel:\u003c/strong\u003e Default\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice: \u003c/strong\u003eDefault with neutral gender voice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eElevenLabs\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: eleven_monolingual_v1 (As of 30th September, eleven_monolingual_v1 is an outdated version and Quantumworks Lab will run a new experiment with\u0026nbsp;eleven_multilingual_v2 in the future)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: pMsXgVXv3BLzUgSXRplE\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: aura-asteria-en\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Default\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAmazon Polly\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Generative\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Ruth\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: sonic-english\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Teacher Lady\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe selected these models for our evaluation as they represent the cutting-edge offerings from leading companies in the TTS space, showcasing the latest advancements in voice synthesis technology.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"results\"\u003eResults\u003c/h1\u003e\u003cp\u003eOur evaluation of 500 diverse and complex prompts provided valuable insights into the capabilities of Google, Cartesia, AWS Polly, OpenAI, Deepgram, and Eleven Labs TTS models. Each prompt was meticulously assessed by three different raters to ensure accurate and diverse perspectives. This rigorous testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of these leading TTS models:\u003c/p\u003e\u003ch3 id=\"rank-distribution-percentages\"\u003eRank distribution percentages\u003cstrong\u003e\u0026nbsp; \u0026nbsp; \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"overall-ranking\"\u003eOverall ranking\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1:\u003c/strong\u003e\u0026nbsp; (Open AI TTS) - occurred 607 times, 42.93%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2:\u003c/strong\u003e\u0026nbsp; (Cartesia) - occurred 460 times, 32.53%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 410 times, 29.00%\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 4:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 338 times, 23.90%. Followed by Eleven labs as a close second with occurrence 335, 23.69%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 5:\u003c/strong\u003e\u0026nbsp; (Deepgram) - occurred 438 times, 30.98%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 6:\u003c/strong\u003e\u0026nbsp; (Google TTS) - occurred 868 times, 61.39%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote: Rank is determined by the most frequently selected model for each position, which is why some AWS Polly appears twice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1122\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003eFig: % of times each model was ranked first.\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer-results\"\u003eWord Error Rate (WER) Results\u003c/h3\u003e\u003cp\u003eThe WER scores offer an objective look at how accurately each model converts text to speech.\u003c/p\u003e\u003cul\u003e\u003cli\u003eDeepgram: 5.67%\u003c/li\u003e\u003cli\u003eOpenAI TTS: 4.19%\u003c/li\u003e\u003cli\u003eGoogle TTS: 3.36%\u003c/li\u003e\u003cli\u003eAWS Polly: 3.18%\u003c/li\u003e\u003cli\u003eCartesia: 3.87%\u003c/li\u003e\u003cli\u003eEleven Labs: 2.83%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs achieved the lowest WER at 2.83%, making it the most accurate model. Deepgram, however, had the highest WER at 5.67%, indicating room for improvement in transcription accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" title=\"Chart\" width=\"2000\" height=\"1126\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1123\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-specific-performance-results\"\u003eModel-Specific Performance Results\u003c/h2\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each TTS model based on key evaluation criteria.\u003c/p\u003e\u003ch3 id=\"openai-tts\"\u003eOpenAI TTS\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 4.1898%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 89.60% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 87.13% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 92.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 63.37% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 64.57% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOpenAI TTS excels at producing natural-sounding speech, making it a top choice for applications requiring lifelike vocal output. It demonstrates exceptional accuracy in pronunciation and performs excellently in producing clean audio with minimal background noise. The model shows strong capabilities in understanding and conveying contextual nuances in speech and is proficient in delivering appropriate intonation and rhythm. Despite ranking high in human preference, it shows moderate accuracy in word reproduction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cm24ur6wwf\" title=\"OpenAI: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gicqvkqc5e\" title=\"OpenAI: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kg9wqqftu4\" title=\"OpenAI: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"cartesia\"\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 58.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 53.11% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 93.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 80.98% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 77.02% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.8707%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCartesia demonstrates good accuracy in word reproduction. While not as consistent as OpenAI TTS in naturalness, it still performs very well in producing natural-sounding speech. It shows excellent accuracy in pronunciation and excels at producing clean audio with minimal background noise. The model displays good capabilities in understanding contextual nuances and performs well in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/3gukhi7z2j\" title=\"Cartesia: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y3lbes0sin\" title=\"Cartesia: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f0lmkto1xk\" title=\"Cartesia: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"aws-polly\"\u003e\u003cstrong\u003eAWS Polly\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 56.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 55.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.39% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 84.72% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Medium in 58.20% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.1770%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAWS Polly shows very good accuracy in word reproduction, ranking second among the models. It struggles more with producing natural-sounding speech compared to top performers but maintains excellent pronunciation accuracy. The model performs well in producing clean audio and shows good capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qawx1dwc9g\" title=\"Polly: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/klmnfcsno5\" title=\"AWS Polly: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ubzrqtjnc9\" title=\"AWS Polly: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"eleven-labs\"\u003e\u003cstrong\u003eEleven Labs\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 2.8302%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: Medium in 44.98% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 81.97% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 80.27% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 44.70% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 46.25% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs demonstrates the best accuracy in word reproduction among all models. It shows mixed results in producing natural-sounding speech but maintains excellent pronunciation accuracy. The model performs reasonably well in producing clean audio but has more noise issues than some competitors. It shows moderate capabilities in understanding contextual nuances and has room for improvement in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/x3qzhkt0ab\" title=\"Eleven Labs: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kl0lc7oqmz\" title=\"Eleven Labs: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bc19r3iqip\" title=\"Eleven Labs: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"deepgram\"\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 5.6743%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: High in 57.78% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 64.43% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 88.83% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 53.18% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 55.52% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDeepgram shows the highest error rate in word reproduction among all models. It performs moderately well in producing natural-sounding speech and demonstrates good accuracy in pronunciation, although lagging behind some competitors. The model performs well in producing clean audio with minimal background noise but shows moderate capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mngnvgtzx3\" title=\"Deepgram: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6h3bgmcg6d\" title=\"Deepgram: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uaj3x9bln8\" title=\"Deepgram: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"google-tts\"\u003e\u003cstrong\u003eGoogle TTS\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.3574%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Low in 78.01% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 77.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.46% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: Medium in 39.25% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: Low in 45.83% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z27ett9q8m\" title=\"Google TTS: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/06pqi5qc13\" title=\"Google TTS: Wow Did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kwotkto0u6\" title=\"Google TTS: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation reveals that while Eleven Labs leads in transcription accuracy with the lowest WER, OpenAI TTS emerges as the top choice when it comes to human preference, particularly for speech naturalness, pronunciation accuracy, and prosody.\u003c/p\u003e\u003cp\u003eConversely, Google TTS, despite a relatively low WER, ranks last in human preference due to poor performance in categories like speech naturalness and context awareness. This demonstrates the importance of a balanced evaluation that considers both quantitative metrics like WER and qualitative factors like user satisfaction.\u003c/p\u003e\u003cp\u003eAs the TTS field continues to evolve, the key to further advancements will lie in combining high transcription accuracy with natural, expressive, and context-aware speech. Models like OpenAI TTS and Eleven Labs are setting the standard, but there remains room for improvement across all models to fully meet the growing demands of diverse TTS applications.\u003c/p\u003e\u003cp\u003eIn summary, while WER is crucial, the broader context of user experience—focusing on factors like speech naturalness, context awareness, and prosody accuracy—is essential in evaluating the overall effectiveness of TTS models.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-speech models presented here represents a significant advancement in assessing AI-generated speech. By leveraging this methodology alongside Quantumworks Lab's cutting-edge platform, AI teams can dramatically accelerate the development and refinement of sophisticated, domain-specific speech generation models.\u003c/p\u003e\u003cp\u003eOur solution offers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eEfficient dataset curation\u003c/li\u003e\u003cli\u003eAutomated evaluation techniques\u003c/li\u003e\u003cli\u003eHuman-in-the-loop quality assurance\u003c/li\u003e\u003cli\u003eCustomizable workflows for TTS model assessment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-speech model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdohXa2KeUawTMTWS7MfKSUYC6fVPCHjtmcFCRt5ycmB7S4k4-JElTwnmRb6ufW2kR7F3Bqu1PwzbvL83LKcqgz8jbkoY02tefklFLp98wvqm2liOKBglEGaRITIk6Wx1tY3fi-43TaCJnpwEwRVCz7q101?key=Qy_gZn4bA_q73M6n7nwEIA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"1024\"\u003e\u003c/figure\u003e","comment_id":"66bfded301c06500016e8b1c","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-12.12.46-PM.png","featured":false,"visibility":"public","created_at":"2024-08-16T23:20:51.000+00:00","updated_at":"2024-11-26T00:11:38.000+00:00","published_at":"2024-08-17T19:13:25.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/evaluating-leading-text-to-speech-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bff4f801c06500016e8bba","uuid":"e5bf7da8-d960-4a06-9050-b06f63a8a4c0","title":"Metrics-based RAG Development with Quantumworks Lab","slug":"metrics-based-rag-development-with-labelbox","html":"\u003ch1 id=\"overview\"\u003e\u003cstrong\u003eOverview\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOne of the biggest challenges with RAG Application development is evaluating the quality of responses. As it currently stands, there are a series of different benchmark metrics, tools, and frameworks to help with RAG application evaluation.\u003c/p\u003e\u003cp\u003eLLM evaluation tools (such as RAGAS and DeepEval) provide a plethora of quality scores designed to evaluate the efficiency of the RAG model, ranging from retrieval to generation. In this guide, we’ll take a metrics based approach to enhance RAG development by focusing on 2 target metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext recall\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eThis measures (on a scale from 0-1) how well the retrieved content aligns with the Ground Truth Answer\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext precision\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eIdeally, the most relevant context chunks should be retrieved first. This metric measures whether the RAG application can return the most relevant chunks (with respect to the Ground Truth) at the top.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"generating-ground-truth-data-and-%E2%80%98synthetic%E2%80%99-data\"\u003e\u003cstrong\u003eGenerating ground truth data and ‘\u003cem\u003esynthetic’\u003c/em\u003e data\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo calculate performance metrics, we have to compare the RAG response with ground truth. Ground truths are factually verified responses to a given user query and must be linked back to a source document.\u003c/p\u003e\u003cul\u003e\u003cli\u003eOn the other hand, ground truth data can be automatically generated by pre-trained models. LLM Frameworks, such as LangChain, have modules that generate Question / Answer pairs based on a source document. While creating Synthetic ground truth is less labor intensive, drawbacks include quality issues, lacking real-world nuances, and inheriting model bias.\u003c/li\u003e\u003cli\u003eGround truth data is typically manually created by experts through the collection and verification of internal source documents. While labor intensive and slower to collect, manual data labeling optimizes for accuracy and quality.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo start off, we’ll use the Quantumworks Lab Platform to orchestrate a ground truth creation workflow with 2 approaches.\u003c/p\u003e\u003col\u003e\u003cli\u003eManual ground truth generation\u003c/li\u003e\u003cli\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"manual-ground-truth-generation\"\u003e\u003cstrong\u003eManual ground truth generation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eBy uploading common user questions (prompts) to the Quantumworks Lab Platform, we can set up an annotation project. Now subject matter experts can craft a thoughtful answer and provide the appropriate source.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"938\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"synthetic-ground-truth-generation-with-human-in-the-loop\"\u003e\u003cstrong\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1154\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeveraging the latest Foundation Models on Quantumworks Lab Foundry, we can combine an automated ground truth generation process with a Human in the Loop component for quality assurance.\u003c/p\u003e\u003cp\u003eTo start off, we will include the source document for each user query (PDF in this case). We can then prompt a foundation model (Google Gemini 1.5 Pro in this case) for each user question and attach the PDF context needed to answer the question.\u003c/p\u003e\u003cp\u003eThe provided prompt in this case is: *For the given text and PDF attachment, answer the following. Given the attached PDF, generate an answer using the information from the attachment. Respond with I don't know if the PDF attachment doesn't contain the answer to the question.*\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"724\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eNext, we can include the response generated from Gemini \u003cstrong\u003eas pre-labels\u003c/strong\u003e to an annotation project.\u0026nbsp;\u003c/p\u003e\u003cp\u003eInstead of starting from scratch while answering the question, human experts can modify and verify the Gemini response along with the corresponding document and user query, optimizing for quality and efficiency.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1480\" height=\"570\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1480w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"improving-rag\"\u003eImproving RAG\u003c/h1\u003e\u003cp\u003eA RAG based application consists of many components. Summarized in a recent research survey by Gao et al. (2023), a few factors that impact RAG system performance include (but not limited to):\u003c/p\u003e\u003cul\u003e\u003cli\u003eImproving the retriever (e.g. Reranking)\u003c/li\u003e\u003cli\u003eFine-tuning the embedding model for source documents\u003c/li\u003e\u003cli\u003eAdding document metadata tagging (for rerouting or semantic search)\u003c/li\u003e\u003cli\u003eQuery rewriting\u003c/li\u003e\u003cli\u003eOptimizing the chunking strategy\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’ll focus on the following strategies for improving metrics based RAG evaluation:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eOptimizing the chunking strategy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-tuning the embedding model\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"chunking-strategy\"\u003e\u003cstrong\u003eChunking strategy\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhile keeping LLM generation model (GPT Turbo 3.5) and Embeddings Model (sentence-transformers/distiluse-base-multilingual-cased-v2) consistent, we will compare the retrieval metrics derived from RAGAs.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will export the question and response pairs generated from Quantumworks Lab to a Python Environment.\u003c/li\u003e\u003cli\u003eNext we will generate the RAG response based on the following chunking strategies:\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRecursive character splitter\u003c/strong\u003e (500 token size with 150 chunk overlap)\u003cul\u003e\u003cli\u003eThis is a heuristic approach and involves dynamically splitting text based on a set of rules (e.g. new lines / tables / page breaks) while maintaining coherence.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpacy text splitter\u003c/strong\u003e\u003cul\u003e\u003cli\u003espaCy uses a rule-based approach combined with machine learning models to perform tokenization.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnce the RAG response and context have been generated, we create a table with the following fields required for RAGAs metrics evaluation: (Question, Ground Truth, RAG Answer, Context)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"432\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-5.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-5.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eBy calculating RAGAs metrics evaluation using the necessary Query, Response, Ground Truth, and Source information, we can derive performance results for the entire dataset and aggregate the findings. The Context Precision and Context Recall metrics for Recursive and Spacy are shown below:\u003c/p\u003e\u003ch3 id=\"recursive-text-chunking\"\u003e\u003cstrong\u003eRecursive text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1008\" height=\"624\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-4.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png 1008w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"spacy-text-chunking\"\u003e\u003cstrong\u003eSpacy text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1054\" height=\"652\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png 1054w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eFrom the results, we observe that Spacy yielded slightly better context precision. Given the volume of our dataset (~100 QA pairs), the difference is likely statistically insignificant. However, we can conclude that choosing the Chunking strategy and parameters will certainly affect the output of the chatbot.\u003c/p\u003e\u003cp\u003eGiven the distribution of our Recall and Precision metrics, both techniques seem to struggle with the same data rows. This indicates that the key semantics are not captured by our default embeddings (\u003cstrong\u003esentence-transformers/distiluse-base-multilingual-cased-v2\u003c/strong\u003e). We’ll test out another embedding model to see if we can improve RAG performance metrics.\u003c/p\u003e\u003ch2 id=\"embeddings-model\"\u003e\u003cstrong\u003eEmbeddings model\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAn Embedding Model is responsible for generating embeddings, which is a vector representation to any piece of information, such as a chunk of text in a RAG application. A solid embeddings model is able to better capture semantic meaning of a user question and the corpus of source documents, therefore returning the most relevant chunks as the context for answer generation.\u003c/p\u003e\u003cp\u003eWhile holding the chunking strategy constant (Recursive), we will change the underlying embeddings model in the RAG workflow before evaluating with RAGAs. The Huggingface\u003ca href=\"https://huggingface.co/spaces/mteb/leaderboard?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003eLeaderboard\u003c/u\u003e\u003c/a\u003e includes performance benchmarks for various embeddings models. Beyond retrieval performance, model size, latency, and context length are also important when choosing an embeddings model.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will generate responses using the \u003cstrong\u003ebge-base-en-v1.5\u003c/strong\u003e model and evaluate with RAGAs\u003cul\u003e\u003cli\u003eWhile ranking 35 on the leaderboards, this model is extremely lightweight (109 million parameters)\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1096\" height=\"632\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png 1096w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eFrom the results, changing the embeddings model (from distiluse-base-multilingual-cased-v2 to bge-base-en-v1.5) significantly improved the retrieval metrics, around ~10 points for both context recall and precision.\u003cul\u003e\u003cli\u003eThis indicates that the retrieved context chunks are more relevant to the ground truth and ranked higher on average as well.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"fine-tuning-an-embeddings-model\"\u003e\u003cstrong\u003eFine-tuning an embeddings model\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eMost embedding models are trained on general knowledge. Specific domains, such as internal organization information, may limit the effectiveness of these models. Therefore, we can choose to fine-tune an embeddings model with the goal of better understanding and generating domain specific information.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo fine tune embeddings, we need labeled examples of positive and negative context chunks.\u003cul\u003e\u003cli\u003eUsing a training dataset, we’ll upload the user query and the top X chunks to Quantumworks Lab\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing Quantumworks Lab Annotate, subject matter experts can determine if each chunk is relevant in answering the given question.\u003c/li\u003e\u003cli\u003e\u003c/li\u003e\u003cli\u003eUsing the Quantumworks Lab SDK we can now export our labels and convert to the following format for fine tuning: {\"query\": str, \"pos\": List[str], \"neg\":List[str]}\u003cul\u003e\u003cli\u003eThe fine tuning process for the BGE family of models is also described\u003ca href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/examples/finetune/README.md?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing the fine tuned embeddings model, we can generate updated responses and evaluate.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom the example in the screenshot, bge tends to incorrectly extract the citations section, which the expert labelers can correct and mark as irrelevant.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1005\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the resulting metrics, we can see that by fine tuning on 50 data rows is able to improve context recall and context precision by 2-3 points each. We expect this to improve as more labels are provided to continuously fine tune the embeddings model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1098\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image.png 1098w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eEvaluating the quality of responses in developing RAG Applications is essential for creating efficient and reliable systems. By incorporating the Quantumworks Lab platform in metrics based RAG development, from ground truth generation to feedback for embedding models, developers can enhance the overall performance and reliability of RAG applications. \u003c/p\u003e","comment_id":"66bff4f801c06500016e8bba","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.55.12-PM.png","featured":false,"visibility":"public","created_at":"2024-08-17T00:55:20.000+00:00","updated_at":"2024-09-03T20:00:44.000+00:00","published_at":"2024-08-17T01:07:55.000+00:00","custom_excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/metrics-based-rag-development-with-labelbox/","excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66b28fe6c2887d000107cdde","uuid":"40792083-45ad-4606-8583-7781bc74c305","title":"Unlocking precision: The \"Needle-in-a-Haystack\" test for LLM evaluation","slug":"unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation","html":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\u003cp\u003eSelecting the optimal large language model (LLM) for specific tasks is crucial for maximizing efficiency and accuracy. One of the key challenges faced by teams is selecting the best models for pre-labeling tasks, especially when dealing with large datasets and complex annotations.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox’s Model Foundry provides a robust platform for evaluation and determining the most suitable model for various applications. To illustrate this, the Quantumworks Lab Labs team conducted an experiment simulating the \"Needle-in-a-Haystack\" test. This test involves identifying specific elements within vast amounts of data, ensuring the model’s precision and reliability.\u003c/p\u003e\u003cp\u003eBy utilizing Quantumworks Lab Model Foundry’s advanced experiments and evaluation tools, teams can compare multiple LLMs to identify the one that delivers the highest accuracy and efficiency for pre-labeling on complex tasks, thus saving time and enhancing the quality of predictions.\u003c/p\u003e\u003cp\u003eIn this blog post, we’ll dive into the intricacies of the \"Needle-in-a-Haystack\", exploring how to leverage Foundry to find the best model for your pre-labeling or data enrichment needs.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"the-needle-in-a-haystack-test\"\u003eThe \"Needle-in-a-Haystack\" test\u003c/h1\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test is a specialized evaluation method designed to gauge the performance of large language models (LLMs) in identifying specific, often infrequent, elements in large datasets.\u0026nbsp;\u003c/p\u003e\u003cp\u003eImagine you have a massive dataset filled with a mix of common and rare pieces of information, similar to a haystack with a few needles hidden inside. The challenge is to determine how effectively a model can find those needles (rare information) without getting distracted by the surrounding hay (common information ).\u0026nbsp; This rare information could be anything from specific keywords in a text document to unique objects in a video.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"669\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/08/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePrevious analysis of Claude-2.1 \u003c/span\u003e\u003ca href=\"https://twitter.com/GregKamradt/status/1722386725635580292?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eperformed by Greg Kamradt\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e, member of the ARC Prize team.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"628\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/08/image-2.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePrevious analysis of GPT-4 \u003c/span\u003e\u003ca href=\"https://twitter.com/GregKamradt/status/1722386725635580292?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eperformed by Greg Kamradt\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e, member of the ARC Prize team.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"why-is-this-a-good-test-for-model-foundry\"\u003e\u003cstrong\u003eWhy is this a good test for Model Foundry?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test is a great fit for Model Foundry for several reasons:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReal-world relevance:\u003c/strong\u003e This test simulates real-world conditions, where critical information is buried in a large dataset. This ensures that models are being simulated in environments that match actual applications.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComprehensive evaluation:\u003c/strong\u003e Quantumworks Lab Model Foundry offers advanced tools that make setting up experiments, running evaluations, and comparing results efficient and easy.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eBetter decision making:\u003c/strong\u003e The insights gained from the Needle in a Haystack test can facilitate stronger decision-making when we are choosing the most suitable LLM for a task. This ensures investment in models that offer the best performance for application.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch1 id=\"creating-the-needle-in-a-haystack-internally\"\u003eCreating the \"Needle-in-a-Haystack\" internally\u003c/h1\u003e\u003cp\u003eThe first step in our experiment was to create a detailed labeling instructions set that we could eventually send to LLMs for pre-labeling. It is important to note that we decided to use Text data for our study. Various other asset types such as Video and Image can also emulate a similar test.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1254\" height=\"698\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 1254w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"instructions-overview\"\u003e\u003cstrong\u003eInstructions overview\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe wanted to build a dataset that consisted of conversations between users and a customer support chatbot, focusing on banking and financial transactions. Each conversation would be categorized into specific issues related to accounts, banking services, and transactions.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs a result, our instruction set would include detailed descriptions of each category, example conversations to guide the labeling process, and clear decision-making guidelines to help annotators distinguish between closely-related issues.\u003c/p\u003e\u003ch2 id=\"ontology\"\u003e\u003cstrong\u003eOntology\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe ontology included categories such as:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eACCOUNT_TERMINATION\u003c/li\u003e\u003cli\u003eACCOUNT_RECOVERY\u003c/li\u003e\u003cli\u003eACCOUNT_SECURITY_BREACH\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_UPDATE_DETAILS\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eBANKING_LOAN_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_FEE_DISPUTE\u003c/li\u003e\u003cli\u003eBANKING_OVERDRAFT_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_WIRE_TRANSFER_HELP\u003c/li\u003e\u003cli\u003eBANKING_SAVINGS_PLANS\u003c/li\u003e\u003cli\u003eBANKING_INVESTMENT_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eBANKING_INSURANCE_PRODUCTS\u003c/li\u003e\u003cli\u003eBANKING_MOBILE_APP_SUPPORT\u003c/li\u003e\u003cli\u003eBANKING_DEBIT_CARD_ACTIVATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eTRANSACTION_DISPUTE\u003c/li\u003e\u003cli\u003eTRANSCATION_REFUND\u003c/li\u003e\u003cli\u003eTRANSACTION_VERIFICATION\u003c/li\u003e\u003cli\u003eTRANSACTION_FRAUD_REPORT\u003c/li\u003e\u003cli\u003eTRANSACTION_LIMIT_INCREASE\u003c/li\u003e\u003cli\u003eTRANSACTION_HISTORY_QUERY\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAs seen, we chose closely-correlated categories and provided precise instructions so that while there were many similarities between subcategories, there were slight differences and nuances that our chosen LLM would have to notice and use to drive the decision-making process.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"building-the-dataset\"\u003e\u003cstrong\u003eBuilding the dataset\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCreating the dataset involved curating and structuring the data row to reflect real-world scenarios that modeled the above ontology. This ensured the dataset was comprehensive and challenging for the models.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"dataset-composition\"\u003e\u003cstrong\u003eDataset composition\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData Row Content: \u003c/strong\u003eEach data row represented a conversation between a user and customer support chatbot.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"sample-data-rows\"\u003e\u003cstrong\u003eSample data rows\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eACCOUNT_TERMIATION: User conversations requesting closure of their account\u003c/li\u003e\u003cli\u003eBANKING_LOAN_SERVICES: Inquiries about applying for or managing loans\u003c/li\u003e\u003cli\u003eTRANSACTION_FRAUD_REPORT: Reports of suspected fraudulent activities\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo view our labeling instructions, click the link \u003ca href=\"https://storage.googleapis.com/labelbox-datasets/lb_rahul/pdfs/Customer%20Support%20Ticket%20LLM%20Instructions.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"model-evaluation-using-foundry-llms\"\u003eModel evaluation using Foundry LLMs\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1246\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 1246w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"model-selection\"\u003e\u003cstrong\u003eModel selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe decided to evaluate our dataset on four leading LLMs currently on the market:\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"gemini-15-pro\"\u003e\u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eReleased by Google as part of the Gemini series;\u003c/li\u003e\u003cli\u003eKnown for its strong multimodal capabilities;\u003c/li\u003e\u003cli\u003eDesigned for complex reasoning and task completion.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gpt-4o\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eDeveloped by OpenAI;\u003c/li\u003e\u003cli\u003eAn advanced iteration of the GPT (Generative Pre-trained Transformer) series;\u003c/li\u003e\u003cli\u003eKnown for its strong natural language understanding and generation;\u003c/li\u003e\u003cli\u003eOptimized for faster response times and efficient computational resource usage.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"claude-35-sonnet\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eCreated by Anthropic;\u003c/li\u003e\u003cli\u003ePart of the Claude 3 model family;\u003c/li\u003e\u003cli\u003eKnown for its strong performance in writing and complex tasks;\u003c/li\u003e\u003cli\u003eCapable of engaging in nuanced conversations and providing detailed explanations.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gemini-15-flash\"\u003e\u003cstrong\u003eGemini 1.5 Flash\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eAnother model in the Google Gemini series;\u003c/li\u003e\u003cli\u003eOptimized for speed and efficiency;\u003c/li\u003e\u003cli\u003eDesigned for tasks requiring quick responses;\u003c/li\u003e\u003cli\u003eSuitable for applications where real-time responses are crucial.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"analysis-and-insights\"\u003e\u003cstrong\u003eAnalysis and insights\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1314\" height=\"712\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 1314w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce we had created Model Foundry Predictions on our dataset for all four LLMs, we placed them into a Model Experiment for model evaluation. Creating an experiment allowed us to dive deeply into the intricacies of each model to determine their overall performance on a needle in a haystack application.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFrom the exhibits above, we can see which models performed best from an precision perspective:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eGemini 1.5 Pro (81.55%)\u003c/li\u003e\u003cli\u003eClaude 3.5 Sonnet (80.98%)\u003c/li\u003e\u003cli\u003eGPT-4o (79.02%)\u003c/li\u003e\u003cli\u003eGemini 1.5 Flash (76.96%)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1220\" height=\"772\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 1220w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eConfusion matrices and Precision graphs are also available in Model Experiment, giving us a better understanding of the above precision scores.\u003c/p\u003e\u003cp\u003eFrom the graphs and further analysis, we can see the categories in the ontology that each model struggled with. Note that a struggle indicates a precision score of less than 0.75.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1306\" height=\"734\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 1306w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"gemini-15-pro-1\"\u003e\u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"claude-35-sonnet-1\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gpt-4o-1\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eBANKING_MISC_QUERY\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gemini-15-flash-1\"\u003e\u003cstrong\u003eGemini 1.5 Flash\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_FEE_DISPUTE\u003c/li\u003e\u003cli\u003eBANKING_INSURANCE_PRODUCTS\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eBANKING_MISC_QUERY\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBased on the performance breakdown, we can draw several insights:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTop performers\u003c/strong\u003e: Gemini 1.5 Pro and Claude 3.5 Sonnet emerge as the leading models for this particular needle in a haystack task, with very similar performance profiles.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCommon challenges\u003c/strong\u003e: All models struggled with certain categories, particularly ACCOUNT_ID_CONFIRMATION and BANKING_CREDIT_CARD_ISSUES. This suggests these categories may be inherently more difficult to classify or may require more specific training data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrecision vs. Speed\u003c/strong\u003e: While Gemini 1.5 Pro achieved the highest accuracy, teams should consider their specific needs. If real-time responses are crucial, Gemini 1.5 Flash might be a better choice despite its lower accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRoom for improvement\u003c/strong\u003e: Even the top-performing models have areas where they struggle. This information can be valuable for fine-tuning models or adjusting the labeling instructions for future iterations.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch1 id=\"leveraging-model-foundry-for-decision-making-and-pre-labeling\"\u003eLeveraging Model Foundry for decision making and pre-labeling\u003c/h1\u003e\u003cp\u003eThe experiment demonstrates the power of Quantumworks Lab Model Foundry in facilitating data-driven decision-making for model selection and optimizing the pre-labeling process. By providing comprehensive evaluation tools and visualizations, Model Foundry enables teams to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompare \u003cstrong\u003emultiple\u003c/strong\u003e models \u003cstrong\u003esimultaneously\u003c/strong\u003e\u003c/li\u003e\u003cli\u003eIdentify \u003cstrong\u003especific\u003c/strong\u003e strengths and weaknesses of \u003cstrong\u003eeach\u003c/strong\u003e \u003cstrong\u003emodel\u003c/strong\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eMake informed decisions based on \u003cstrong\u003eprecision\u003c/strong\u003e, \u003cstrong\u003erecall\u003c/strong\u003e, and \u003cstrong\u003eoverall accuracy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePinpoint\u003c/strong\u003e areas for potential model \u003cstrong\u003eimprovement\u003c/strong\u003e or \u003cstrong\u003efine-tuning\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to model evaluation, Model Foundry significantly enhances the pre-labeling workflow:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEfficient pre-labeling\u003c/strong\u003e: Once the best-performing model is identified, it can be seamlessly integrated into the pre-labeling pipeline, significantly reducing manual labeling efforts\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eQuality assurance\u003c/strong\u003e: By understanding model strengths and weaknesses, teams can strategically allocate human resources to review and correct pre-labels in categories where models struggle\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIterative improvement\u003c/strong\u003e: As more data is labeled and models are retained, teams can continuously evaluate and update their pre-labeling model, ensuring ongoing optimization of the labeling process.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost reduction\u003c/strong\u003e: By selecting the most accurate model for pre-labeling, teams can minimize the need for manual corrections, leading to substantial time and cost savings in large-scale labeling projects.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy leveraging Model Foundry for both decision-making and pre-labeling processes, teams can significantly enhance the efficiency and accuracy of their entire data labeling pipeline.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"next-steps\"\u003eNext Steps\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/2024/08/image-1.png 1000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo further improve model performance and decision-making, consider the following steps:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eFine-tune models on challenging categories.\u003c/li\u003e\u003cli\u003eConduct additional experiments with different data types or industry-specific datasets.\u003c/li\u003e\u003cli\u003eImplement regular evaluations and feedback loops to identify areas for improvement and adapt to changing requirements.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy continually refining your approach and leveraging the insights gained from Model Foundry, you can ensure that your team is always using the most effective LLM for your specific needs, driving efficiency and accuracy in your AI-powered workflows.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test, as implemented through Quantumworks Lab Model Foundry, proves to be an effective method for evaluating LLM performance on complex, nuanced tasks. By simulating real-world scenarios and leveraging Model’s advanced evaluation tools, teams can select the most suitable model for their specific pre-labeling needs.\u003c/p\u003e\u003cp\u003eIn our experiment, Gemini 1.5 Pro and Claude 3.5 Sonnet demonstrated superior performance, but the choice between them (or other models) would depend on the specific requirements of the project, including factors like speed, resource efficiency, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs the field of AI continues to evolve rapidly, tools like Quantumworks Lab Model Foundry become increasingly valuable, enabling teams to stay at the forefront of the space by consistently evaluating and selecting the best models for their unique challengers.\u003c/p\u003e","comment_id":"66b28fe6c2887d000107cdde","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-2.55.05-PM.png","featured":false,"visibility":"public","created_at":"2024-08-06T21:04:38.000+00:00","updated_at":"2024-09-03T20:04:14.000+00:00","published_at":"2024-08-06T22:08:47.000+00:00","custom_excerpt":"Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our \"Needle-in-a-Haystack\" experiment. Learn how to enhance accuracy and efficiency in complex data annotations.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"66b29da3c2887d000107ce2e","name":"Raj Jain","slug":"raj","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/raj/"},{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"66b29da3c2887d000107ce2e","name":"Raj Jain","slug":"raj","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/raj/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation/","excerpt":"Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our \"Needle-in-a-Haystack\" experiment. Learn how to enhance accuracy and efficiency in complex data annotations.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66b1410331089400019364ee","uuid":"2207c60e-9f86-4ef2-9613-168f8707aee5","title":"A comprehensive approach to evaluating text-to-video models","slug":"a-comprehensive-approach-to-evaluating-text-to-video-models","html":"\u003cp\u003eThe emergence of text-to-video AI models has marked a significant milestone in artificial intelligence, with models from Runway ML (Gen-3), Luma Labs, and Pika transforming written descriptions into dynamic and lifelike videos. This technology is reshaping industries from video production to digital marketing, democratizing visual storytelling.\u003c/p\u003e\u003cp\u003eHowever, despite their impressive capabilities, these models often fall short of human expectations, producing results that lack prompt adherence, realism, or fidelity to the input text. To accelerate the development of text-to-video models, it is crucial to establish comprehensive evaluation methodologies to pinpoint areas for improvement.\u003c/p\u003e\u003cp\u003eThis article presents a rigorous approach to assessing the strengths and limitations of Runway ML (Gen-3), Luma Labs, and Pika using human preference ratings. Let’s dive into how we systematically analyzed these leading models.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"human-preference-evaluation\"\u003e\u003cstrong\u003eHuman preference evaluation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate video outputs across several key criteria:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1245\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 2246w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eChecklist for evaluating text-to-video models using human preferences.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"prompt-adherence\"\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRaters assessed how well each generated video matched the given text prompt on a scale of high, medium, or low. For example, in the given prompt: “A peaceful Zen garden with carefully raked sand, bonsai trees, and a small koi pond.” Raters looked to see if there was prompt adherence by looking at the presence of key concepts for the prompt.\u003c/p\u003e\u003cul\u003e\u003cli\u003eIs there a garden?\u003c/li\u003e\u003cli\u003eDoes it look peaceful?\u003c/li\u003e\u003cli\u003eIs the sand present, and is it raked?\u003c/li\u003e\u003cli\u003eAre there bonsai trees?\u003c/li\u003e\u003cli\u003eIs the a small koi pond?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: If all or most of the key concepts are present.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e:\u0026nbsp; If half the key concepts are present.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: If less than half of key concepts are present .\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdENKR1QEaokNDikPODa-kTi209qknyV3PBGvEm5xa4QWqugwI5sx0zmlAZOIH2XrpQRjB9xuAAb4gshoWZfSl7mjgNmNDIWcL_bXxK4pqk0TnzP0-Xn7EG0LTznK4sBhXk1ZxuiG4p_WwoCXqx2d3dj5V_?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"509\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 1: \"A romantic Parisian street scene with couples walking and street musicians playing\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"video-realism\"\u003e\u003cstrong\u003eVideo realism\u003c/strong\u003e \u003c/h4\u003e\u003cp\u003eRaters assessed how closely the video resembled reality.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Realistic lighting, textures, and proportions.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Somewhat realistic but with slight issues in shadows or textures.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Animated or artificial appearance.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd4QhDzWX5l35W_ecjUSerUNd0sTSA_DtOLU2DjwPLenslTtGrRyf4tPjhEcwQdIxFw50JqfnPwk86brTWRtowMUFRHbMdG1hr-dqGZ69-nYFebJ9KVMslxPNvHQdjrBWPA6soRH5LZ3uUDZBggfO_tJSFl?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"504\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 2: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"video-resolution\"\u003e\u003cstrong\u003eVideo resolution\u003c/strong\u003e \u003c/h4\u003e\u003cp\u003eThis criterion evaluates the level of detail and overall clarity in the video.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Fine details visible (e.g., individual leaves, fabric textures).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Major elements are clear, but finer details are somewhat lacking.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Overall blurry or lacking significant detail.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdbrYYgTHolz9LOsLQzJHJVpzFx7TWbyIXrutMv52jbQl6nu_qHjrEV5kFXbCH01iYVLRTgoJV3BDjOIdZKU8TOod8uA_Ev-5QemuPmYTYiymsr2XN7nMs6F2AbWoVph_NnkZ54BRHe2Ldq8-IJK2J1ggNj?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"502\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 3: \"An ancient temple in the jungle with hidden traps and treasures waiting to be discovered\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"artifacts\"\u003e\u003cstrong\u003eArtifacts \u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRaters identified any visible artifacts, distortions, or errors, such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnnatural distortions in objects or backgrounds\u003c/li\u003e\u003cli\u003eMisplaced or floating elements\u003c/li\u003e\u003cli\u003eInconsistent lighting or shadows\u003c/li\u003e\u003cli\u003eUnnatural repeating patterns\u003c/li\u003e\u003cli\u003eUnnatural movements\u003c/li\u003e\u003cli\u003eBlurred or pixelated areas\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: If all or 5 of the errors are present.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: If 2 or 3 errors are present.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: \u0026nbsp;If 1 or 0 errors are present.\u0026nbsp;\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcOz4iP5QSqnQJpBB5F4neKuaS-jElkj40E21sIahJZmgm2qEE2oBxWOhDBTriFa26xOPwHbpQUGPjzUMFIfIjzWxl8I8vI8Z904UjiO-Jut14igYMNzk55VBU0U82is1tavV3FJA7uSU1kXtjCEuAmhB6I?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"502\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 4: \"Cat following a mouse\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"results\"\u003e\u003cstrong\u003eResults\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOur evaluation of 25 diverse set of complex prompts, generated by GPT-4, was stress-tested and provided valuable insights into the capabilities of Runway ML, Luma Labs and Pika. Each prompt was assessed by three different raters to ensure more accurate and diverse perspectives. This rigorous stress testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of the top models: Runway ML (Gen-3), Luma Labs, and Pika.\u003c/p\u003e\u003ch3 id=\"overall-ranking-for-human-preference-evaluations\"\u003e\u003cstrong\u003eOverall ranking for human-preference evaluations\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1\u003c/strong\u003e: Runway ML (Gen-3) ranked 1st in 65.22% of cases, Luma Labs in 18.84% and Pika in 15.94%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2\u003c/strong\u003e: Luma Labs ranked 2nd in 59.42% of cases, Runway ML (Gen-3) in 21.74% and Pika in 18.84%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3\u003c/strong\u003e: Pika ranked 3rd in 65.22% of cases, Luma Labs in 21.74% and\u0026nbsp; Runway ML in 13.04%\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1248\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRank 1 standings: Runway ML (Gen-3), Luma Labs, Pika\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"model-specific-performance-results\"\u003e\u003cstrong\u003eModel-Specific Performance results\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1248\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch4 id=\"runway-ml-gen-3\"\u003e\u003cstrong\u003eRunway ML (Gen-3)\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 59.42% of cases. This model excels at accurately reflecting the input prompts, making it a reliable choice for generating intended content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 17.39% of cases. Although it performs well relative to other models, it still has room for improvement in minimizing artifacts and errors.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 46.38% of cases. Runway ML generates realistic videos nearly half the time, indicating strong capabilities in producing lifelike content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 56.52% of cases. This model is proficient in delivering high-resolution videos, enhancing the viewing experience.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uyngtw0382\" title=\"RUNWAYML_____Gen-3 Alpha 2891026367, A grand fantasy cast Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"576\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRunway ML (Gen-3): \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"luma-labs\"\u003e\u003cstrong\u003eLuma Labs\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 37.68% of cases. While not as consistent as Runway ML, it still performs reasonably well in adhering to prompts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 17.39% of cases. Similar to Runway ML, it needs improvements to reduce visual defects.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 20.29% of cases. This model struggles more with realism, making it less suitable for applications requiring lifelike video content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 30.43% of cases. Luma Labs offers moderate video resolution quality but lags behind Runway ML.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/pyz5tqupa7\" title=\"LUMA_______A_grand_fantasy_castle_surrounded_by_lush_landscapes_and_mythical_creatures__85043b Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"530\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLuma Labs: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"pika\"\u003e\u003cstrong\u003ePika\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 36.23% of cases. Comparable to Luma Labs, Pika maintains a fair level of consistency with input prompts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 43.48% of cases. Pika has the highest occurrence of artifacts and errors, indicating significant areas for enhancement.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 20.29% of cases. Like Luma Labs, Pika also faces challenges in producing realistic videos.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 23.19% of cases. Pika offers the least in terms of video resolution among the three models, suggesting a need for improvement in this aspect.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/k1kvephgc9\" title=\"PIKA____A_grand_fantasy_castle_surrounded_by_lush_landscapes_and_mythical_creatures._seed3125151661828847 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePika: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIt's worth noting that the scope of this study was constrained by two key factors: \u003c/p\u003e\u003cul\u003e\u003cli\u003eThe absence of a public API for large-scale video generation from prompts; \u003c/li\u003e\u003cli\u003eOur deliberate use of a diverse prompt dataset. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis dataset encompassed a wide range of complexity, from simple to intricate descriptions. Additionally, we attempted to use automatic evaluations, such as assessing video quality based on all video frames and evaluations by large language models (LLMs) that support video. However, due to conflicting results, these methods were omitted from the blog post.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation of state-of-the-art text-to-video models reveals a clear preference hierarchy among Runway ML (Gen-3), Luma Labs, and Pika. Runway ML (Gen-3) emerges as the top performer, securing the first rank in 65.22% of cases, thanks to its high prompt adherence and superior video resolution. However, it still exhibits a notable occurrence of artifacts and errors, suggesting room for enhancement.\u003c/p\u003e\u003cp\u003eLuma Labs, while trailing behind Runway ML, demonstrates moderate performance, particularly in maintaining prompt consistency and video resolution. Its primary weakness lies in generating realistic videos, which is crucial for lifelike content applications. On the other hand, Pika, ranking third, shows the highest need for improvement, especially in minimizing artifacts and enhancing video resolution.\u003c/p\u003e\u003cp\u003eWhile each model has its strengths and weaknesses, Runway ML (Gen-3) stands out for its robust performance across most evaluation criteria, making it the preferred choice for generating high-quality, realistic videos. As the field of text-to-video generation continues to evolve, addressing the identified shortcomings will be key to advancing the capabilities of these models.\u003c/p\u003e\u003cp\u003eBy targeting these key areas, we can drive the next wave of innovations in text-to-video technology, creating more sophisticated and versatile text-to-video systems that cater to a broader range of applications and user needs.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-video models presented here represents a significant advance in assessing AI-generated videos. When combined with Quantumworks Lab's platform, AI teams can accelerate the development and refinement of sophisticated, domain-specific video generation models with greater efficiency and quality through our dataset curation, automated evaluation techniques, and human-in-the-loop QA.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-video model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more. \u003c/p\u003e\u003cp\u003eWe'd love to hear from you and discuss how we can support your AI evaluation needs.\u003c/p\u003e","comment_id":"66b1410331089400019364ee","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.13.57-AM.png","featured":false,"visibility":"public","created_at":"2024-08-05T21:15:47.000+00:00","updated_at":"2024-09-03T20:08:05.000+00:00","published_at":"2024-08-05T22:14:00.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/a-comprehensive-approach-to-evaluating-text-to-video-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66a978d931089400019364a5","uuid":"bbe7d436-1e06-4439-bce3-780f72151bdf","title":"A comprehensive approach to evaluating text-to-image models","slug":"a-comprehensive-approach-to-evaluating-text-to-image-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in image generation evaluation, visit\u0026nbsp;\u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAs text-to-image AI models continue to evolve, it's become increasingly important to develop robust evaluation methods that can assess their performance across multiple dimensions. In this post, we'll explore a comprehensive approach to evaluating 3 leading text-to-image models - \u003cem\u003eDALL-E 3, Stable Diffusion, and Imagen 2 \u003c/em\u003e- using both human preference ratings and automated evaluation techniques.\u003c/p\u003e\u003ch2 id=\"the-rise-of-text-to-image-models\"\u003eThe rise of text-to-image models\u003c/h2\u003e\u003cp\u003eText-to-image generation has seen remarkable progress in recent years. Models like \u003cem\u003eDALL-E 3, Stable Diffusion, and Imagen 2\u003c/em\u003e can now produce strikingly realistic and creative images from natural language descriptions. This technology has many useful applications, from graphic design and content creation to scientific visualization and beyond.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe5C5TuOA7TCKRT_7O_PQ5hnqXPPAlM4jOO4KvRk2DR0y0k9OWNYJzNhGIL5rqanTMlYK9reVzCb_pwx__rvW6rTmRkRljBX6aIjnA3DuEH1L_-ahgR0MnJU9JD-vWdQnDi8pZeoRIvpXD0kskzRf3WdSxH?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"723\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemOVe_M8fg0ddRqLYwN1MaKDcuJtPGMXgkP1hlaXtbjU7ZKsbunWQZMVM34ttGlsa8ulDjWoCxW-KVagWUiNG4xdUc3yCYdWMcEKsC1Pl8da6kyk0UgjjL66-qxaua9sYL4IUNTOSNsyggkztxBhXbIur6?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"733\"\u003e\u003c/figure\u003e\u003cp\u003eAs these models become more advanced, having reliable ways to compare their performance and identify areas for improvement are paramount. Let’s next dive into how we developed a two-fold evaluation approach for getting more granularity into their performance.\u003c/p\u003e\u003ch2 id=\"1-human-preference-evaluation\"\u003e[1] Human preference evaluation\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfTKedTbSWhlq5swtSUlaHwVE--84lNrAPDAGpko3p_AnEz0jooz4-lin5Mpg-SwHG3CJktHiRqSHTCJcuavjLCJOau4-GxczB2Odq4mQOXSaH7ijz1wCWZic_o0B_npX0x5qgxAqiHULmNouzCOv2l8nMy?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"902\"\u003e\u003c/figure\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human labelers per data row, allowing us to tap into a network of expert raters to evaluate image outputs across several key criteria:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAlignment with prompt: \u003c/strong\u003eRaters assessed how well each generated image matched the given text prompt on a scale of high, medium, or low. For example, for the prompt \"A red apple on a wooden table\":\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Image shows a clear, realistic red apple on a wooden table\u003c/li\u003e\u003cli\u003eMedium: Image shows an apple, but it's green or the table isn't clearly wooden\u003c/li\u003e\u003cli\u003eLow: Image shows an unrelated scene\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePhotorealism: \u003c/strong\u003eThis criterion evaluates how closely the image resembled a real photograph:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Realistic lighting, textures, and proportions\u003c/li\u003e\u003cli\u003eMedium: Somewhat realistic but with slight issues in shadows or textures\u003c/li\u003e\u003cli\u003eLow: Cartoonish or artificial appearance\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eDetail: \u003c/strong\u003eRaters then determined the level of detail and overall clarity:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Fine details visible (e.g., individual leaves, fabric textures)\u003c/li\u003e\u003cli\u003eMedium: Major elements clear, but finer details somewhat lacking\u003c/li\u003e\u003cli\u003eLow: Overall blurry or lacking significant detail\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eArtifacts: \u003c/strong\u003eFinally, raters identified any visible artifacts, distortions, or errors, such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnnatural distortions in objects or backgrounds\u003c/li\u003e\u003cli\u003eMisplaced or floating elements\u003c/li\u003e\u003cli\u003eInconsistent lighting or shadows \u003c/li\u003e\u003cli\u003eUnnatural repeating patterns\u003c/li\u003e\u003cli\u003eBlurred or pixelated areas\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"2-automated-evaluations\"\u003e[2] Automated evaluations\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXduxNvNrJswCQvT7DGZfSyEoIGUOLdlCVN1c3UTP_Bbz-s-sL-P246muageVr1aPVyH7DlaXtqkmGS6Lf9fm_MuztOdJAddLr2muQLG7MzHCC4BqHfBLlN9YAO7teoaN1Qma8wTq8kRv53xfh-V1iGRVMqO?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"907\"\u003e\u003c/figure\u003e\u003cp\u003eTo complement human ratings, we implemented several automated evaluation techniques. Here’s an example for one \u003ca href=\"https://storage.googleapis.com/text_image_eval/gen_images/014ac7aa527c953fd0a7aeb08e238dea/results.json?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eImage Quality Score: \u003c/strong\u003eWe calculated an objective image quality score based on several key metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSharpness: Using Laplacian variance to assess image clarity.\u003c/li\u003e\u003cli\u003eContrast: Evaluating the range between minimum and maximum pixel values.\u003c/li\u003e\u003cli\u003eNoise Estimation: Employing a filter-based approach to quantify image noise.\u003c/li\u003e\u003cli\u003eStructural Similarity Index (SSIM): Comparing the image to a slightly blurred version to assess structural integrity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese metrics were combined into a comprehensive quality score in order to provide an objective measure of the image's technical attributes.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePrompt adherence: \u003c/strong\u003eWe utilized the CLIP (Contrastive Language-Image Pre-training) model to measure similarity between the text prompt and the generated image in a shared embedding space. This approach provides an automated assessment of how well the image aligns with the given prompt, offering insights into the model's ability to accurately interpret and visualize textual descriptions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDetailed scoring: \u003c/strong\u003eWe employed Claude, an advanced AI model, to provide detailed scoring and analysis of the generated images. This multifaceted evaluation includes several key components.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cu\u003eElement accuracy scores\u003c/u\u003e: For each key element in the prompt, Claude assesses its presence, provides a description, and assigns an accuracy score out of 10 reflecting how well these elements matched the prompt.\u003c/li\u003e\u003cli\u003e\u003cu\u003eCategory scores\u003c/u\u003e: Claude evaluates images across various categories such as objects, colors, spatial relations, activities, and materials. Each category receives a score out of 10, providing a comprehensive view of the image's content accuracy.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eClaude prompt adherence\u003c/u\u003e: Claude assigns an overall similarity score, expressed as a percentage, indicating how closely the entire image matches the given prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eUnexpected elements and inconsistencies\u003c/u\u003e: Claude identifies any unexpected elements or inconsistencies in the image that do not align with the intended prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eOverall impression:\u003c/u\u003e Claude provides an overall impression of the image, summarizing how well it captures the essence of the prompt. This includes a qualitative assessment of the image's strengths and areas for improvement.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"results\"\u003eResults\u0026nbsp;\u003c/h2\u003e\u003cp\u003eOur evaluation of 100 images across a diverse set of complex prompts, generated by GPT-4, stress-tested and provided valuable insights into the capabilities of Stable Diffusion, DALL-E, and Imagen 2.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHere’s a summary of the human-preference evaluations:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eModel rankings and initial findings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eStable Diffusion ranked first in 50.7% of cases, DALL-E in 38%, and Imagen 2 in 11.3%.\u003c/li\u003e\u003cli\u003eStable Diffusion ranked second (37%), followed by DALL-E (33%) and Imagen 2 (30%)\u003c/li\u003e\u003cli\u003eImagen 2 was ranked third (59%), while DALL-E and Stable Diffusion were ranked last less often (29% and 12% respectively\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ePerformance metrics (as percentages of maximum possible scores):\u003c/p\u003e\u003cul\u003e\u003cli\u003eStable Diffusion: 84.3% prompt alignment, 85.3% photorealism, 91.7% detail/clarity.\u003c/li\u003e\u003cli\u003eDALL-E: 84.3% prompt alignment, 58.3% photorealism, 83.7% detail/clarity.\u003c/li\u003e\u003cli\u003eImagen 2: 61.3% prompt alignment, 74.7% photorealism, 71.3% detail/clarity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eHere’s a summary of the detailed auto evaluation metrics:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eStable Diffusion emerged as a consistent performer across various metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003eClaude’s prompt adherence score: 82.27%. More specifically, attributed to accurate/realistic colors (89.40%) and depicting objects (89.30%)\u003c/li\u003e\u003cli\u003eHowever, image quality (34.98%) was relatively low\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDALL-E excelled in prompt interpretation and visualization:\u003c/p\u003e\u003cul\u003e\u003cli\u003eClaude’s prompt adherence score: 87.04%. More specifically, attributed to depicting objects well (91.60%) and displaying moving activities accurately (81.10%)\u003c/li\u003e\u003cli\u003eStrongest in translating textual descriptions into visual elements\u003c/li\u003e\u003cli\u003eHowever, image quality (30.11%) was lowest\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eImagen 2 performed the worst, but had a higher technical quality for images:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLowest Claude prompt adherence score (76.86%) and aspect accuracy (70.92%)\u003c/li\u003e\u003cli\u003eMuch weaker in moving activities (72.20%) and detailed attributes (77.70%)\u003c/li\u003e\u003cli\u003eHigher image quality (55.55%) than the other two models\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eAnalysis and insights:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"831\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eComparing the auto evaluation metrics to the human preference evaluations reveals some additional interesting findings:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStable Diffusion's balanced performance:\u003c/strong\u003e Stable Diffusion emerged as the top performer overall in human evaluations, ranking first in 50.7% of cases. It showed consistent high scores across human-evaluated metrics, particularly excelling in detail/clarity (91.7%) and photorealism (85.3%). However, the auto evaluation revealed a relatively low image quality score (34.98%), suggesting that technical image quality doesn't always correlate with human perception of quality.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDALL-E's strengths and weaknesses:\u003c/strong\u003e While DALL-E ranked first in 38% of human evaluations, it showed a significant weakness in human-perceived photorealism (58.3%). Interestingly, it had the highest Claude prompt adherence score (87.04%) in the auto evaluation, which aligns with its strong performance in human-evaluated prompt alignment (84.3%). This suggests DALL-E excels at interpreting and executing prompts, but may struggle with realistic rendering.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImagen 2's technical quality vs. human preference:\u003c/strong\u003e Imagen 2 consistently ranked lower in human preferences, struggling particularly with prompt alignment (61.3%). However, it had the highest technical image quality score (55.55%) in the auto evaluation. This discrepancy highlights that technical image quality doesn't necessarily translate to human preference or perceived prompt adherence.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrompt alignment discrepancies\u003c/strong\u003e: While human evaluations showed Stable Diffusion and DALL-E tied in prompt alignment (84.3% each), the auto evaluation gave DALL-E a higher score (87.04%) compared to Stable Diffusion (82.27%). This suggests that human and AI perceptions of prompt adherence may differ slightly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePhotorealism and image quality\u003c/strong\u003e: The human-evaluated photorealism scores don't align with the auto-evaluated image quality scores. Stable Diffusion led in human-perceived photorealism (85.3%) but had low technical image quality (34.98%). Conversely, Imagen 2 had the highest technical image quality (55.55%) but ranked second in human-perceived photorealism (74.7%).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDetail and clarity vs. technical metrics\u003c/strong\u003e: Stable Diffusion stood out in human-evaluated detail and clarity (91.7%), which aligns with its high auto-evaluated scores in depicting objects (89.30%) and accurate colors (89.40%). This suggests a correlation between these technical aspects and human perception of detail and clarity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIt's also important to note that this study didn't include Midjourney due to its Discord-only integration, which made it challenging to implement into our evaluation study. While Midjourney is recognized for its high-quality output, its unconventional access method can be a barrier for users seeking traditional API or web-based interactions. Additionally, Google’s Imagen 2 implements strict safety and content filters across a wide range of topics, which did limit versatility and required additional pre-processing. Such factors, alongside the technical and human-based perceptual metrics evaluated in our study, also influence the overall usability and adoption of AI image generation models in real-world scenarios.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eOur comprehensive evaluation of leading text-to-image models, combining human preference ratings with automated metrics, reveals intriguing contrasts between quantitative performance and human perception. Stable Diffusion emerged as the overall top performer in human evaluations, excelling in detail/clarity and photorealism despite a lower technical image quality score. This underscores the complex relationship between technical metrics and human perception of quality. DALL-E demonstrated strength in prompt interpretation and adherence across both human and automated evaluations, although it showed weakness in human-perceived photorealism. Imagen 2, while scoring highest in technical image quality, consistently ranked lower in human preferences, particularly struggling with prompt alignment.\u003c/p\u003e\u003cp\u003eAs these technologies continue to evolve, our results indicate that each model has distinct strengths and areas for improvement. Stable Diffusion offers balanced performance across various criteria, making it suitable for a wide range of applications. DALL-E excels in prompt interpretation and execution, making it ideal for tasks requiring precise visualization of detailed descriptions. Imagen 2's high technical quality suggests it could be particularly useful in applications where image fidelity is prioritized, although improvements in prompt adherence would enhance its overall performance. Future research should focus on bridging the gap between technical metrics and human perception, as well as addressing specific weaknesses identified in each model, such as DALL-E’s photorealism or Imagen 2’s prompt alignment. By refining these aspects, we can push the boundaries of AI-generated imagery and develop more versatile and capable text-to-image systems that better meet the needs of various applications and user preferences.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003eGet started today\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-image models presented here represents a significant advance in assessing AI-generated images. When combined with Quantumworks Lab's platform, AI teams can accelerate the development and refinement of sophisticated, domain-specific image generation models with greater efficiency and quality through our dataset curation, automated evaluation techniques, and human-in-the-loop QA.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-image model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more. We'd love to hear from you and discuss how we can support your AI evaluation needs.\u003c/p\u003e","comment_id":"66a978d931089400019364a5","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-4.35.13-PM.png","featured":false,"visibility":"public","created_at":"2024-07-30T23:35:53.000+00:00","updated_at":"2024-11-26T00:11:57.000+00:00","published_at":"2024-07-31T00:02:09.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"66a9822f31089400019364c9","name":"Krish Maniar","slug":"krish","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/krish/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"66a9822f31089400019364c9","name":"Krish Maniar","slug":"krish","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/krish/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/a-comprehensive-approach-to-evaluating-text-to-image-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"669a9314e017de000190bda5","uuid":"88ed362b-5dd4-48eb-950c-0e2798e34a3f","title":"Using Quantumworks Lab to improve data quality via AutoQA \u0026 advanced labeler review","slug":"using-labelbox-to-improve-data-quality-via-autoqa-advanced-labeler-review","html":"\u003cp\u003eWorking with leading generative AI teams who are building frontier models and developing task-specific generative AI products, we've seen first hand the importance of data quality and robust QA processes in order to deliver performant models. The availability of human-evaluated data sets the companies apart in their AI offerings. In this solution accelerator, we will guide you through the different workflows and demonstrate how Quantumworks Lab can expedite the quality review process for creating better data for generative AI use cases.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dkbjlz0nnn\" title=\"Kushal AudoQA - 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe generative AI use cases we’ll focus on for this guide is multi-turn conversations, similar to what you’d find interacting with an LLM. As an introduction, Quantumworks Lab offers multimodal chat for two main options:\u003c/p\u003e\u003ch3 id=\"1-live-online-based-evaluation\"\u003e\u003cstrong\u003e1. Live online-based evaluation\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn this scenario, Quantumworks Lab provides an experience similar to a \u003ca href=\"https://labelbox.com/blog/announcing-multimodal-chat-for-genai-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003echatbot arena\u003c/a\u003e where multiple models or multiple versions of the same model are compared against each other. A labeler then ranks the responses to determine which one is better. For example, you can input a prompt and receive three different responses from different versions of a model, such as Gemini, GPT, or Claude. It's important to note that the labelers don’t know which response corresponds to which model to prevent bias.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExample:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eInput a prompt.\u003c/li\u003e\u003cli\u003eReceive three responses from different model versions.\u003c/li\u003e\u003cli\u003eThe labeler then ranks the responses based on quality (defined as per your business-specific needs).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"2-offline-multimodal-chat\"\u003e\u003cstrong\u003e2. Offline multimodal chat\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThis option allows you to upload existing multi-turn conversations to Quantumworks Lab for evaluation (e.g. accuracy, relevance, tone, etc.).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExample:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eUpload a multi-turn conversation between a human and a chatbot.\u003c/li\u003e\u003cli\u003eEvaluate and label the conversation across different axes, such as relevance, factuality, and fluency.\u003c/li\u003e\u003cli\u003eThis can be done on a per-message level or for the entire conversation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy leveraging these workflows, teams can effectively utilize Quantumworks Lab for various chatbot-based use cases.\u003c/p\u003e\u003ch2 id=\"prompt-and-response-generation-walkthrough\"\u003e\u003cstrong\u003ePrompt and response generation walkthrough\u003c/strong\u003e\u003cbr\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mfgloie4dw\" title=\"Kushal AutoQA - 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a first step, let’s walk through a common workflow in Quantumworks Lab involving prompt and response generation, which is crucial for training a model. This process involves creating question and answer pairs and offers three main options: workforce-generated prompts and responses, guided prompt and response creation, and responding to uploaded prompts.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor workforce-generated prompts, labelers input questions and corresponding responses, selecting the appropriate category for each pair.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXemODUkHK4G4d1-GDQpXuY-GpXvdZKokbuOZ7pqhwH4YtEqYP-c2UNwakRhkJKp_jja7mEgD-pvdSUAe0F4QAVu5DX6cAB3pqCww2cWOGqc3fG9nMwl99D5VqUJnPyoh2nzblS1VQo3EKAvIJ0t4690fUET?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"823\"\u003e\u003c/figure\u003e\u003cp\u003eAs shown above, the guided prompt and response creation option uses an image, code snippet, video, or text as a basis for labelers to generate relevant question and answer pairs.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rk8jhk0ro6\" title=\"Kushal AutoQA - 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLastly, if you have existing prompts, labelers can evaluate these prompts on multi-turn conversations and provide suitable responses. This flexible workflow supports various input types, enabling efficient model training through comprehensive prompt and response generation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXfp0NYehNcvRTlOFRnRnvl_c88o9_XZO4MQcTGAhlxO0zFJiKJhOxoSC-SnfAb2rPOQAovFmjRKWEW422hl6gqrONAYrDTo_dasu3GxgQ71K3_bVCYrJay_cdRd3fEyQz-89-puoPPKlghrbm1l49dhsSxV?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"819\"\u003e\u003c/figure\u003e\u003cp\u003eAfter identifying the data to be labeled and setting up the labeling schema, which includes fields for the prompt, response, and category, you can attach detailed instructions via PDF. These instructions are accessible to labelers during the labeling process and labelers can start labeling from scratch by generating prompt-response combinations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXf-pM7Cev56U0tGeAKUQ2qsmDjGPHfwGn_sLk8yb_7sKCuzuWhystmTA2--SOTMp5C9uPmIvxzr1KnkMmbAfuD6YG6GzDmfU_tVf6l7cJ3TU1BoJfLNqyAILHuD7q-QFx4CMYBInrek29GFll6Q3lupNyYW?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"885\"\u003e\u003c/figure\u003e\u003ch2 id=\"ai-assisted-alignment-ai-assisted-labeling\"\u003e\u003cstrong\u003eAI-assisted alignment (AI-assisted labeling)\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dxv4cpcs3z\" title=\"Kushal AutoQA - 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLabelbox allows you to use a large language model or a custom model for pre-labeling by using our model-assisted labeling option.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can select any available model such as \u003cem\u003eGPT, Gemini, Claude\u003c/em\u003e, etc or bring in your own custom one. By generating a preview, you receive pre-labels that labelers can then modify and correct without having to create labels from scratch. While this speeds up the labeling process, it can introduce bias.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf your task requires unbiased labeling, you can enforce labelers to create labels from scratch without any model assistance. The platform offers flexibility, enabling you to choose between speed and accuracy based on your task requirements.\u003c/p\u003e\u003ch2 id=\"labelbox-ai-assisted-alignment-autoqa-aka-ai-critic\"\u003e\u003cstrong\u003eLabelbox AI-assisted alignment (AutoQA aka AI critic)\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ugdbth629w\" title=\"Kushal AutoQA - 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a next step, let’s walk through how you can leverage Quantumworks Lab’s platform to utilize an LLM to act as an AI critic or judge to help auto QA your data during the review process. Quantumworks Lab employs an LLM to review prompt and response pairs, providing scores and feedback and critique on the quality of labels and why things were good or bad. This feedback helps identify insights and scores to figure out which labels require further review.\u003c/p\u003e\u003ch3 id=\"scoring-and-feedback\"\u003e\u003cstrong\u003eScoring and feedback\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXd49bah2g-Y-dJi7rv07qxkQxH3QVspBNwHJMYT06TKhZPS61KEjDIKdfGsMAWaMLXwVFHHXSAgnfePp1TjzyBtG_15Xmz91syjfZN2XDU34jovG1Akyao2_7o_h1LvX-CBG6oJnJWuYDOJ05rvpGL4-sK-?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"829\"\u003e\u003c/figure\u003e\u003cp\u003eAs an example shown above, you can see that for this specific data row, the labeling score is 4.75,\u0026nbsp; and includes ideas for improving the reasoning. With this view, you can go through all of your different data rows and filter the data based on scores to identify labels that need additional scrutiny. For example, you might filter for scores below 4 and set a range, and move these labels to a custom queue for further review. Creating custom queues for specific score ranges, like between 3 and 4, allows for a more organized review and QA process.\u003c/p\u003e\u003cp\u003eReviewers in the loop can view label instructions, scores, and improvement ideas for each label. This information is crucial for understanding the quality of your labels and making necessary adjustments. You can filter and select labels based on their scores and move them to appropriate queues for further action. As a best practice, the Quantumworks Lab team works closely with our customers to come up with the best scoring for your review and evaluation needs.\u003c/p\u003e\u003cp\u003eFurthermore, Quantumworks Lab allows you to create custom review workflows tailored to your specific needs. As shown below, you can define different metrics and criteria for scoring, such as \u003cem\u003eBLEU\u003c/em\u003e scores, which is common specifically for Gen AI use cases around free-form text.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXdDrCCs3pFr5vZc1XiH4NKSRy10Z52AwFMxspY8rlrM8VwWSD97jtCucEWQl9y6siSKXRVDwnrZBP78CPRobkNjdp2MhwpGUt9PrZGbmPdFlTo0vkvDOD5XnRx1DL9vKuYXJ5FKFvv4gx3AIdgU9eNOpiKs?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1096\"\u003e\u003c/figure\u003e\u003cp\u003eThis flexibility ensures that the review process aligns with all of your project requirements and quality standards.\u003c/p\u003e\u003ch3 id=\"exporting-your-data\"\u003e\u003cstrong\u003eExporting your data\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXe3mrwO8jvCOqnuLQNzGBZcY_h4iVwJygVYyRXE0JUDEk4yijWUglS6tjJI29fGsZESN1kSyiIqbFoBcH4LqQdUHaf72pb5Az2PMV0SAYzNQE4JaXaJ6PncP_wGBU17MHmu7pHCpuvioc-OXqB9VvN2yj38?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"786\"\u003e\u003c/figure\u003e\u003cp\u003eOnce labels are finished with the review process, labels are moved to the done stage and are ready for export out of the Quantumworks Lab platform. You can export the data by selecting it and triggering the export JSON. This process ensures that you have high-quality data ready.\u003c/p\u003e\u003cp\u003eBecause Quantumworks Lab tracks in-depth metrics around labeling operations (average label time, average review time, etc.) disaggregated by labeling member, you can very easily calculate additional inter-annotator agreement metrics, such as Krippendorff’s Alpha Score and Cohen’s Cappa for qualitative metrics (e.g. likert scales), beyond the off-the-shelf benchmark and consensus metrics that Quantumworks Lab already calculates.\u003c/p\u003e\u003ch3 id=\"providing-feedback-to-labelers-and-ensuring-quality\"\u003e\u003cstrong\u003eProviding feedback to labelers and ensuring quality\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXdNzGDU92vOA2K8yP34ISKLkN1FD-qRMenZxYSsJZFRfI3mdgVMAWymkkZV4eukzSUuxyt1nEobCbJJoVPlXIrACbfz6c6-HCDCjf9KqIlAkIVw2D_ivX6SVpfzJMeUc2MwOfW4YYhrpBwuriI-2aSEkJkc?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"817\"\u003e\u003c/figure\u003e\u003cp\u003eCustomers and reviewers can also easily collaborate and provide real-time feedback to labelers by raising any issues or instructions, and labelers can be notified to make necessary changes. As an example, you can instruct your labelers to “incorporate the feedback from the LLM to make this response better.”\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcD95jtRW_w_sxoXYDqzq5_U2KUBcaMwhKlfyFedseFMCRNbUjTEm0RNeb4TqIghead-cgTt1AosJ-yELJpndZfwL25GZOjIFJPPX_lDNF0gMhRMnmeiTg2jVCmjMBUiJl0lILmbeytnwpi3Y3bcZa5HaRI?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"862\"\u003e\u003c/figure\u003e\u003cp\u003eThe review and feedback loop helps maintain high-quality data and can be tailored to include as many review steps as needed to meet your quality control standards, allowing you to select how much data you want auto-QA’d, how much data you want Quantumworks Lab to internally review amongst themselves or whether you want to review it on your own. Quantumworks Lab provides full control and typically works with customers to determine how many review steps you want and what are the different scores you want to evaluate against.\u003c/p\u003e\u003cp\u003eThis ongoing process ensures a consistent and efficient labeling engine; as new data comes in, it gets quickly labeled and if it meets the criteria, it moves to an appropriate queue which then gets reviewed by subject matter experts, and finally gets you the batch of high-quality labels delivered.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003e\u003c/h2\u003e\u003cp\u003eWe hope that this walkthrough gives you a better understanding of how Quantumworks Lab makes it easier than ever before to iterate quickly with real-time, granular visibility into labels and implement autoQA workflows for data quality. In addition, AI labs and generative AI companies can benefit from tapping into \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ediverse pools\u003c/a\u003e of expertise in order to improve the underlying data and model performance with Labelbox.\u0026nbsp;\u003c/p\u003e\u003cp\u003eReady to improve the data quality for your generative AI initiatives? \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eContact us\u003c/u\u003e\u003c/a\u003e to learn more and we’d love to hear from you.\u0026nbsp;\u003c/p\u003e","comment_id":"669a9314e017de000190bda5","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-19-at-9.36.05-AM.png","featured":false,"visibility":"public","created_at":"2024-07-19T16:23:48.000+00:00","updated_at":"2024-07-19T18:03:53.000+00:00","published_at":"2024-07-19T16:32:08.000+00:00","custom_excerpt":"Learn how you can use Quantumworks Lab to accelerate the quality review process for creating better data for generative AI use cases using autoQA and advanced labeler reviews.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/using-labelbox-to-improve-data-quality-via-autoqa-advanced-labeler-review/","excerpt":"Learn how you can use Quantumworks Lab to accelerate the quality review process for creating better data for generative AI use cases using autoQA and advanced labeler reviews.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"668c2f59878a56000112a4e0","uuid":"93014cdf-3555-4d2b-b480-2b9db686e2db","title":"Using multimodal chat to enhance a customer’s online support experience","slug":"using-multimodal-chat-to-enhance-a-customers-online-support-experience","html":"\u003cp\u003eCustomer service systems are prime targets for multimodal chat as this new technology leverages an intuitive interface for answering user questions\u0026nbsp; while enabling companies to unlock previously hidden insights about customer journeys and sentiment.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can streamline the process of labeling and managing data for various use cases, including intent classification, dialogue retrieval, state tracking, and model evaluation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. We’ll illustrate the various labeling tasks that Quantumworks Lab can accomplish to ensure the chatbot is intelligent and effective, providing customers with relevant and timely answers.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://files.readme.io/ce6e056-Labelbox_2024-06-04_10-37-061.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"678\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eLabelbox offers a \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/docs/live-multimodal-chat-evaluation?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003enumber of editor types\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, including the live multimodal chat editor.\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"labeling-tasks\"\u003e\u003cstrong\u003eLabeling Tasks\u003c/strong\u003e\u003c/h1\u003e\u003ch2 id=\"multimodal-intent-classification-dialogue-retrieval-and-state-tracking\"\u003e\u003cstrong\u003eMultimodal Intent Classification, Dialogue Retrieval and State Tracking\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xs53lmro56\" title=\"Customer Query Resolution Training - Part 1 - Training Data Collection + Using Classifications Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 1: Generate prompt-response pairs for supervised fine-tuning of agentic workflows.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eLabel the customer’s intent and sentiments in a given turn of the chat, so you can identify the appropriate next steps accordingly.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1427\" height=\"1333\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/first-one.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/first-one.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png 1427w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe customer appears frustrated, so the sentiment is labeled as “frustrated.”\u003c/p\u003e\u003cp\u003eFurthermore, the customer appears to be reporting their order was damaged and is requesting a new one, so both of those requests are labeled using checklist classifications.\u003c/p\u003e\u003cp\u003eWith these labels, you know the customer’s intent (get a new piece) and can also record the current state of the conversation (damage reported but no action taken yet). As the conversation proceeds, you can update the state based on what actions are taken and keep checking the intent and tone of the customer to ensure they are satisfied.\u003c/p\u003e\u003cp\u003eLabelbox allows you to label every aspect of every turn-by-turn conversation, enabling you to develop fine-grained data sets of annotated real-world conversations.\u003c/p\u003e\u003cp\u003eYou can use these labels to train your agent to retrieve similar past interactions and follow similar response patterns or simply be inspired by them.\u003c/p\u003e\u003ch2 id=\"model-evaluation-ranking-and-selection\"\u003e\u003cstrong\u003eModel evaluation, ranking and selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eYou trained a few models offline with the previously generated data using supervised fine-tuning or RLHF. How do you know which of the models is the best performing, and how often do they make mistakes?\u003c/p\u003e\u003cp\u003eLabelbox allows you to connect these models hosted behind your custom endpoints. Then, with the Live Multimodal Chat editor, labelers can interact with all of them simultaneously by feeding prompts, comparing and contrasting responses, and even rewriting the best ones to be even better.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1vpskixih4\" title=\"Customer Query Resolution Training - Part 2 - LLM Development \u0026amp; Evaluation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 2: Evaluate a single model, generate multiple responses and write your own gold standard response.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eWhen evaluating a single model, Quantumworks Lab allows you to generate multiple responses and also add your own response. This is very useful for use cases where all of the model’s responses are lacking in quality and a human-written response provides the gold standard for further downstream training. Furthermore, in the course of the labeling conversation, you can choose one of these responses to be passed in as prior context into later turns, allowing you to choose the path of the conversation in real-time for better training data generation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uzesocscxk\" title=\"Customer Query Resolution Training - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 3: Evaluate multiple models at once in “chat arena” style.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/secone-one-1-1-1-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"450\" height=\"640\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox also supports evaluating multiple models in a “chat arena” style setup.\u003c/p\u003e\u003cp\u003eSay you have trained candidate models and now want to compare them against each other and also foundation models like GPT 4o. Connect your own models with Quantumworks Lab as \u003cem\u003ecustom\u003c/em\u003e models, and easily choose any latest and greatest foundation model from Foundry within Labelbox. The chat arena excludes information about which response was generated from which model so as not to bias the labeling process.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWith Quantumworks Lab's multimodal chat functionalities, you can generate high-quality datasets to train, evaluate, and test your custom LLMs or pit off-the-shelf LLMs like GPT4o and Gemini 1.5 against each other in a Chatbot Arena-style competition to see which one suits your needs best.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/b3f423vxob\" title=\"Customer Query Resolution Training - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 4: Survey of all LLM capabilities in Quantumworks Lab\u003c/em\u003e\u003c/p\u003e\u003cp\u003eAs the landscape of LLMs evolves rapidly, Quantumworks Lab will continue to build functionality to ensure our customers can align models to their use cases quickly and reliably. With differentiated data, streamlined operations, and transparent orchestration, you can build and refine foundation models that meet your specific needs.\u003c/p\u003e\u003ch2 id=\"evaluate-multimodal-chat-data-today-for-free\"\u003eEvaluate multimodal chat data today for free\u003c/h2\u003e\u003cp\u003eReady to deliver on the next-generation of generative AI?\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eSign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more and we’d love to hear from you.\u003c/p\u003e","comment_id":"668c2f59878a56000112a4e0","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/mmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp","featured":false,"visibility":"public","created_at":"2024-07-08T18:26:33.000+00:00","updated_at":"2024-07-10T16:09:14.000+00:00","published_at":"2024-07-09T16:06:15.000+00:00","custom_excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-multimodal-chat-to-enhance-a-customers-online-support-experience/","excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"662804d1d733f0000145b1c4","uuid":"3db0aafe-d6b7-470a-8ff0-37835d066fe4","title":"How to harness AI for efficient video labeling","slug":"harnessing-ai-for-efficient-video-labeling","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eWorking in collaboration with numerous leading companies in artificial intelligence, we're observing a surge in enthusiasm for using advanced models to initially label data, integrating human expertise later to refine and tailor previously labor-intensive and time-consuming tasks.\u003c/p\u003e\u003cp\u003eThese AI models are transforming one of the most daunting tasks in machine learning—the creation of high-quality video datasets. Utilizing such models allows machine learning teams to leverage automated tools to pre-label or enrich data, facilitating a range of applications from monitoring driver behavior to detecting objects in manufacturing environments.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis blog post will explore how models like Gemini 1.5 Pro, Grounding DINO, and SAM are redefining the video labeling landscape, while boosting efficiency and speed. \u003c/p\u003e\u003cp\u003eBy automating the labor-intensive labeling tasks, these models not only accelerate the workflow, but also liberate time for users and decrease labeling costs.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"steps\"\u003eSteps\u003c/h2\u003e\u003ch3 id=\"step-1-select-video\"\u003eStep 1: Select video\u0026nbsp;\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eTo select a video: \u003c/p\u003e\u003cul\u003e\u003cli\u003eNarrow in on a subset of data. Users can use Quantumworks Lab Catalog filters, including media attributes, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eUsers can click “Predict with Foundry” once the data of interest is selected.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-choose-a-model-of-interest\"\u003eStep 2: Choose a model of interest\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eTo select a model: \u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, users will be prompted to choose a model of interest for a model run.\u003c/li\u003e\u003cli\u003eThen, select a model from the ‘model gallery’ based on the type of task - such as video classification, video object detection, and video segmentation.\u003c/li\u003e\u003cli\u003eTo locate a specific model, users can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine-learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-configure-model-settings-and-submit-a-model-run\"\u003eStep 3: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eOnce the model of interest is selected, users can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of settings, which can be found in the Advanced model setting.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can generate preview predictions on up to five data rows to understand how the current model settings affect the final predictions.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows users to confirm the configuration settings confidently:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf users are unhappy with the generated preview predictions, they can edit the model settings and continue to generate preview predictions until they're satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eAfter users are satisfied with the preview predictions, a model run can be submitted.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-send-the-images-to-annotate\"\u003eStep 4: Send the images to Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eUsers can transfer the results to a labeling project using the UI via the \"Send to Annotate\" feature. Labelers can then quickly review labels for accuracy.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"pre-labeling-use-cases\"\u003ePre-labeling Use Cases\u003c/h2\u003e\u003ch3 id=\"example-1-segmentation-mask-using-grounding-dino-sam\"\u003eExample 1: Segmentation mask using Grounding DINO + SAM\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eSegmentation masks\u003c/a\u003e are used for autonomous vehicles, medical imagery, retail applications, face recognition and analysis, video surveillance, satellite image analysis, etc. Masks are some of the most time-consuming annotations to make for video. Below, we see an example of how this can be automated with Grounding DINO + SAM so the reviewers can make small edits if needed instead of starting from scratch.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7k184d0h65\" title=\"Pre-labeling Use Case: Segmentation mask using Grounding DINO + SAM Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"example-2-bounding-box-using-grounding-dino\"\u003eExample 2: Bounding box using Grounding DINO\u003c/h3\u003e\u003cp\u003eBounding boxes are utilized in similar scenarios as segmentation masks, but these scenarios demand less precision than those requiring pixel-level (masks) detail. Bounding boxes can be automated using Grounding DINO, as illustrated below with detection of a person in video.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ie9s2r3zff\" title=\"Pre-labeling Use Case: Bounding Box using Grounding DINO Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"example-3-global-classification-using-gemini-15-pro\"\u003eExample 3: Global classification using Gemini 1.5 Pro\u003c/h3\u003e\u003cp\u003eGlobal classification for video is used when the overall classification for video is required like when a driver safety system needs to detect if a driver is distracted. Gemini 1.5 Pro can analyze an hour long video and provide answers about events that took place in the video. This automation reduces the need for human intervention, allowing personnel to focus on reviewing videos only when they are flagged with specific classifications. \u003c/p\u003e\u003ch3 id=\"example-4-frame-based-classification-using-gemini-15-pro\"\u003eExample 4: Frame based classification using Gemini 1.5 Pro\u003c/h3\u003e\u003cp\u003eFrame-based classification is utilized in scenarios similar to segmentation masks. Gemini 1.5 Pro can analyze an hour-long video and identify the specific timestamps for a particular event. \u003c/p\u003e\u003ch2 id=\"additional-considerations\"\u003e\u003cstrong\u003eAdditional considerations\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAdditional considerations as users incorporate Foundry labels into their projects and workflows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo produce frame based binary classifications in Gemini 1.5 Pro, users are recommended to experiment with the prompt and provide as much context as possible to get the best results. \u003cul\u003e\u003cli\u003eFor example, the following yields better results:\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003c!--kg-card-begin: html--\u003e\n“For the given video, what timestamps have a banana and be as thorough as possible about checking each second for a banana. Make sure there is no overlap in timestamps. Return the result as a dictionary without any new lines. {“banana”: “\u003ctimestamps-with-banana\u003e“, “no_banana”:  “\u003ctimestamps-without-banana\u003e“}” than using “For the given video, what timestamps have a banana. Return the result as a dictionary without any new lines. {“banana”: “\u003ctimestamps-with-banana\u003e” }”\n\u003c!--kg-card-end: html--\u003e\n\u003cul\u003e\u003cli\u003eUsers can also \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA/B test different models \u003c/u\u003e\u003c/a\u003efrom Foundry to find the model that best fits the use case using Quantumworks Lab Model.\u003c/li\u003e\u003cli\u003eIf we do not support your use case or any questions arise, feel free to contact our \u003ca href=\"https://docs.labelbox.com/docs/contacting-customer-support?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esupport team\u003c/u\u003e\u003c/a\u003e, as we would love to hear your feedback to improve Foundry.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAnnotating video data has traditionally been a tedious and time-consuming task. The integration of advanced AI models from Quantumworks Lab Foundry into the video labeling process marks a significant transformation in how video data is annotated. By leveraging Foundry's capabilities, users can drastically speed up their video labeling projects. This acceleration not only diminishes the time required to bring products to market but also substantially reduces the costs involved in model development.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our additional resources on how to utilize state-of-the-art AI models in Foundry, including using \u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003emodel distillation\u003c/a\u003e and fine-tuning to leverage the power of foundation models:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-accelerate-labeling-projects/?ref=labelbox-guides.ghost.io#conclusion\"\u003e\u003cu\u003eHow to accelerate labeling projects using GPT–4V in Foundry\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io#what-are-the-benefits-of-using-image-segmentation-for-my-machine-learning-model\"\u003e\u003cu\u003eHow to create high-quality image segmentation masks quickly and easily\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://event.on24.com/wcc/r/4460171/5D5357846BEAB1FD1E12A5ED146DE6E1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomated Labeling With Any Foundation Model\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFine tune LLMs\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"662804d1d733f0000145b1c4","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/image.png","featured":false,"visibility":"public","created_at":"2024-04-23T18:58:25.000+00:00","updated_at":"2024-11-22T23:53:12.000+00:00","published_at":"2024-04-23T23:40:19.000+00:00","custom_excerpt":"Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/harnessing-ai-for-efficient-video-labeling/","excerpt":"Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66183ccc8d5c4a00014061cc","uuid":"2afa1d4e-85b9-48d3-ae01-81c57db2916b","title":"How to use AI to improve website search relevance","slug":"how-to-improve-search-relevance","html":"\u003cp\u003eWith the latest advances in foundation models, organizations can now enhance search relevance for websites by better matching between user intent with product listings. While companies now have access to a wealth of search queries, sifting through all of these search results can be incredibly time-consuming and resource-intensive. By leveraging AI, teams can now analyze search queries and feedback at scale, to gain insights into common topics or customer sentiment. This allows businesses to identify common themes and pinpoint areas of improvement to enhance\u0026nbsp;their overall website experience to maximize for key metrics such as user retention, conversion and revenue.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for search relevance. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eImproving search relevance requires a vast amount of data in the form of search queries and accurate product descriptions. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape: \u003c/strong\u003eThe changing nature and format of customer review data from multiple sources (e.g webpages, apps, social media, etc.) poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their website search relevance for product descriptions and listings. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s assisted data enrichment and flexible training frameworks to quickly build AI systems that uncover actionable insights from customer searches. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-full\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1746\" height=\"952\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1746w\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to dramatically improve search relevance for any website or app. Specifically, this guide will walk through how you can explore and better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-use-ai-to-improve-search-relevance-for-your-website\"\u003eSee it in action: How to use AI to improve search relevance for your website\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below in either: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1-kTALPqchHXGogUdRVnya7GklKHrsWVL?ref=labelbox-guides.ghost.io#scrollTo=Vw_nNPl8baNJ\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1-kTALPqchHXGogUdRVnya7GklKHrsWVL?ref=labelbox-guides.ghost.io#scrollTo=Vw_nNPl8baNJ\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs customer queries and product descriptions across channels proliferate, brands want to learn from customer feedback to build the most user-friendly experience on their website or app. For this use case, we’ll be working with a dataset of e-commerce website queries – with the goal of analyzing the queries to demonstrate how a company could gain insight into how their customers search for products and how to optimize for relevance.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/vpl1wf0vui\" title=\"Search relevance 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003eFor the purpose of this tutorial, we’ve provided a sample open-source \u003ca href=\"https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe?ref=labelbox-guides.ghost.io\"\u003eKaggle dataset\u003c/a\u003e that can be downloaded.  \u003c/p\u003e\u003cp\u003ePlease \u003ca href=\"https://drive.google.com/file/d/1hh2MYzol6-4PSsqX7mbX2YSLLGA4jAYZ/view?usp=drivesdk\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edownload the dataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your text data as a CSV. \u003c/li\u003e\u003cli\u003eIf your text snippets sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your text data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data-by-clustering\"\u003eSearch and curate data by clustering\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"708\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 1538w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Smart select to cluster data and focus your model improvement on specific data rows\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExplore topics of interest\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith your data in Quantumworks Lab, you can begin to leverage Catalog to uncover interesting topics to get a sense of what customers are searching for. \u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize your data –\u0026nbsp; you can click through individual data rows to get a sense for which queries are the most popular.\u003c/li\u003e\u003cli\u003eDrill into specific topics of interest\u003cstrong\u003e \u003c/strong\u003e– leverage natural language search, for example searching “beds, mirrors, etc,” to bring up all related queries related to that topic. You can adjust the confidence threshold of your searches accordingly which can be helpful in gauging the volume of data related to the topic of interest.\u0026nbsp;\u003c/li\u003e\u003cli\u003eEasily find all instances of similar examples to data of interest.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-2-create-an-initial-model-run-of-search-relevance-assessments\"\u003ePart 2: Create an initial model run of search relevance assessments \u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/t3er59wcfk\" title=\"Search relevance 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can proceed to using Quantumworks Lab's Foundry product to model run an initial model run to accelerate search relevance assessments.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, you'll need to set up your ontology for search relevance assessment based on your project's requirements.\u003c/li\u003e\u003cli\u003eAfterwards, you can define the criteria for rating the relevance of search results to each type of query.\u003c/li\u003e\u003cli\u003eNext, you can communicate the business definition of relevance to the models directly into the prompt. You can use Foundry to add context to the prompt, allowing it to rank results as if it were part of your respective business. In this example, we'll include the prompts for what \"good relevance\", \"excellent relevance\", etc and help the model predict what would fit under this criteria. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAs an illustrative example, you can set up \"excellent relevance\" as a result that perfectly matches the search query, including all specific attributes (category, material, color, purpose, etc). This indicates that the term is exactly what the user is searching for. For the query, \"kitchen blender stainless steel\", a result for \"stainless steel countertop blender\" is highly relevant, matching the user's intended category.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"673\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eGenerate an initial preview to assess how well the adjusted prompt performs and you can save the adjusted prompt as an app, including data type (text), ontology, and the original prompt. This allows for easy re-use and the ability to build upon the saved app for future assessments of search relevance criteria. \u003c/p\u003e\u003cp\u003eAfter this has been set up, you can now generate the next preview to ensure quality before submitting the model run for assessments.\u003c/p\u003e\u003ch3 id=\"view-search-relevance-assessments-results\"\u003eView search relevance assessments results \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gq8nnv2ykw\" title=\"Search relevance 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce your model inferencing job has been completed, you can then navigate to the model tab and locate a variety of foundation models (e.g., Claude 3 will be used in this tutorial) to view the completed model run with your rankings.\u003c/li\u003e\u003cli\u003eOptionally, you can add an explanation classification or review the results of the ranking, which includes all 560 items/data rows.\u003c/li\u003e\u003cli\u003eBy adding the results to your project, you can next perform further analytics such as analyzing the distributions of predictions from the Metrics view.\u003c/li\u003e\u003cli\u003eNext, select all items and you can send them to Annotate, and choose \"search relevance assessment\", where you'll then be able to have humans review as an additional quality check.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"further-analyze-and-optimize-your-search-relevance-assessments-results\"\u003eFurther analyze and optimize your search relevance assessments results \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kw7in8bosa\" title=\"Search relevance 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe last part of the walkthrough is to analyze your distribution of relevance categories within your project, noting varying levels of relevance and to review the query class for all 560 data rows to identify trends in relevance. You can do this by using automated approaches to understand query types and relevance patterns, as we show in the video above.\u003c/li\u003e\u003cli\u003eBy filtering your dataset by the search relevance assessment project, you can navigate to the Analytics view to identify trends and examples of excellent relevance and poor relevance within specific query classes (as shown below).\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"883\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample analytics distribution showing excellent relevance on terms like kids wall decor, sectionals and area rugs.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"838\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample analytics distribution showing poor relevance for beds, furniture cushions, mirrors\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAs this time, you can consider adjusting prompts to accurately reflect relevance criteria, or use metadata fields, such as query class, to further analyze relevance.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo further evaluate and enrich the data, teams can also explore incorporating human supervision in the labeling process, with a hybrid or combination approaches: fully automated, half human in the loop, half automated, or all human-in-the-loop.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can improve your data further in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eAlignerr\u003c/a\u003e: Leverage our global network of\u0026nbsp;specialized labelers for a variety of tasks.\u0026nbsp;This community of subject matter experts from several disciplines align AI models by creating high-quality data in their field of expertise. The community spans nearly every major discipline of sciences, industries and languages, worldwide.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eBy tapping into the most recent developments in foundation models, businesses can transform the effectiveness of website searches by refining the alignment between user intent and product offerings. Given the abundance of search queries that a prospective customer may use, the process of sorting through them manually is labor-intensive and time-consuming. \u003c/p\u003e\u003cp\u003eBy harnessing the power of AI, organizations can efficiently examine search queries and feedback on a large scale, uncovering recurring themes and gauging customer sentiment. \u003c/p\u003e\u003cp\u003eThis enables enterprises to detect prevalent trends and target areas for enhancement, allowing them to optimizing the overall website experience to drive key metrics like user retention, conversion rates, and revenue. Remember to optimize the \u003ca href=\"https://www.web4business.com.au/portfolio-item/the-most-important-24-pages-to-include-on-website/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ewebsite content\u003c/a\u003e as well to ensure it's meeting your end user's goals. Give the walkthrough a try and we also recommend checking out our other solution accelerators such as \u003ca href=\"https://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003epersonalized experiences\u003c/a\u003e for retail to improve customer experiences.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful search relevance websites. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"66183ccc8d5c4a00014061cc","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/thumbnail--6-.png","featured":false,"visibility":"public","created_at":"2024-04-11T19:41:00.000+00:00","updated_at":"2024-09-12T23:45:53.000+00:00","published_at":"2024-04-12T17:02:16.000+00:00","custom_excerpt":"Learn how you can dramatically improve search relevance for any website or app by using AI and foundation models to better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-improve-search-relevance/","excerpt":"Learn how you can dramatically improve search relevance for any website or app by using AI and foundation models to better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6601bf732822410001bac2ba","uuid":"f740e28a-2932-4536-b087-575a1edd5fde","title":"How to improve your task-specific chatbot for better safety, relevancy, and user feedback","slug":"how-to-improve-your-task-specific-chatbot-for-safety-relevancy-and-user-feedback","html":"\u003cp\u003eBuilding task-specific chatbots requires a structured approach when it comes to improving their everyday usefulness, specifically for better safety, relevancy, and user feedback. Given the highly subjective tasks LLM-powered chat applications are expected to perform, a common denominator for how well they do in real-world settings depend on the availability of reliable high-quality training data and how closely aligned they are to human preferences. Working hand-in-hand with leading AI teams, we've observed a set of best practices that we wanted to share in order to help you improve the performance of your task-specific chatbots.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo ensure high levels of trust \u0026amp; safety, an LLM-powered chatbot should be able to detect intentions, entities, and topics when interacting with a user. With quality labeled examples, a task driven application can steer away from conversations that are not relevant to its intended task, ensuring a safe and smooth user experience.\u003c/li\u003e\u003cli\u003eThe large language model (LLM) at the heart of the chat application must be fine tuned with relevant responses or enhanced with human feedback from RAG techniques (e.g. \u003ca href=\"https://labelbox.com/guides/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eReranking\u003c/a\u003e) in order to ensure that the user receives the most relevant information. Examples of \u003ca href=\"https://www.ssw.com.au/rules/train-gpt/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLLMs that can be used, include GPT\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eTo prevent a chatbot from drifting or getting stale with outdated responses, ML teams must continuously monitor and evaluate model performance using ground-truth responses and / or human feedback.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this tutorial guide, we'll walk through some of these top considerations and how Quantumworks Lab can be used as a platform to help accelerate chatbot development.\u003c/p\u003e\u003ch3 id=\"part-1-trust-and-safety-%E2%80%94-understanding-intentions\"\u003e\u003cstrong\u003ePart 1: Trust and Safety — Understanding Intentions\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/u8nd7zpy0o\" title=\"Chat Pt.1 Safety Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo ensure the best user experience, an LLM-based chatbot must be properly scoped to deliver on its intended area of expertise. For example, a chatbot application for an airline company should not be responding to off topic questions, such as politics. Therefore, a chatbot that understands user intent can steer the user towards its intended areas of expertise and away from potentially harmful or unrelated conversations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/image--10-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1720\" height=\"792\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/image--10-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/image--10-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/image--10-.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/image--10-.png 1720w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAn example visual of an LLM-based chatbot which without guardrails, would go off and answer questions that are not pertinent to their intended area of expertise.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn this section, we'll leverage Quantumworks Lab to classify the intent of historical conversations as on-topic (coffee / tea) or off-topic (politics). To start off, let’s load a subset of the \u003ca href=\"https://github.com/thunlp/UltraChat?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eUltrachat\u003c/u\u003e\u003c/a\u003e dataset to Quantumworks Lab Catalog. Ultrachat is an open-source dialogue dataset powered by Turbo APIs to train powerful language models with general conversational capability.\u003c/p\u003e\u003cp\u003eTo being, let's first identify political conversations that your chatbot shouldn’t be able to answer.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing the \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eCatalog\u003c/a\u003e filters, we can cast an initial wide net of examples.\u003cul\u003e\u003cli\u003eSemantic Search — utilize the underlying vector embeddings to return relevant prompts (Politics and opinionated text)\u003c/li\u003e\u003cli\u003eOther functions (e.g. find text) — this will be used to identify targeted keywords or phrases\u003c/li\u003e\u003cli\u003eSave the filters as a \"Slice\" for further reference\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/dHdLvbrx9IA9Uxtlz5Ka2lcRVwT-U6PqOr3Dn7rJ8RtNjv5UQIys4KXULZiOl-8O80BV7jqvifcV4c7evkfG-zIi4RGiI034Ou6_iC9NixZjw90SIxjXbzFS93YzT6PJT323LhJRuORDokUxr_df2rU\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"294\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Quantumworks Lab Catalog filters to help you identify target keywords\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can see that using semantic search and labeling functions have produced promising initial results. As a next step, let’s validate your results further with foundation models.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eWith Quantumworks Lab, you can use a pre-trained foundation model to verify the results from semantic search and other functions.\u003c/li\u003e\u003cli\u003eBy selecting the targeted Data Rows within the slice, you can apply Google’s Gemini Pro to check for political and opinionated classification.\u003c/li\u003e\u003cli\u003eThe results will be returned to you as pre-labels.\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/U-RLJxDtPSYJ99SfjhdUoAv4Nhi6B9O0eaaQMtBtrhesVDrNHvcfUYLym-q_xzn8jZPdpvunuAVrP5VY_fWOaigJH1vIMCg4vxi17LLJ_Rs82Mz759wDEGTss97OxRH_xBJh8Vkdfg4TFZpNUoQlag4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"758\"\u003e\u003c/figure\u003e\u003cp\u003eTo ensure completeness, you can next leverage a human-in-the-loop (HITL) approach\u0026nbsp;to conduct a final review for intent.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eAnnotate\u003c/a\u003e, you create an Annotation Project for Text.\u003c/li\u003e\u003cli\u003eYou can now send the Prompts and responses (as pre-labels) to your Annotation Project.\u003c/li\u003e\u003cli\u003eA human-in-the-loop (HITL) approach ensures that the prompts are checked for complex nuances by skilled professionals that the models may have missed.\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/ES6aNgATSLyk3CKSOqxwuDHPqLq0KsG7K8QeH9MGZKEFwcPqk-CmR5pNKgVfUyJ6TL0LeLSJx-YH-HMEYo-lmAILem_GiFLmUgNI36ntbiPSPdndCgq7cc2fz2aNEoS3pmRF3-h_JPCiyD7Yu36i8BM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"529\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eUtilizing the \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox SDK\u003c/a\u003e, these prompts, along with their labeled classifications, can now be seamlessly integrated with trust and safety frameworks, such as\u003ca href=\"https://github.com/NVIDIA/NeMo-Guardrails?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003eNeMo Guardrails\u003c/u\u003e\u003c/a\u003e, an open source toolkit for controlling the output of a large language model.\u003c/li\u003e\u003cli\u003eBased on the prompts and classifications labeled in the previous steps, we can feed these examples into NeMo to ensure that the application does not respond to potentially sensitive political topics.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Q00lf_YPoPlL_11VaJyfLtcYK9e7VBgb55ZvOv0SmvhWAq5McNJ4wjyului_alK0F0M4rWoUHLXpETRZAlE_zUbTg7_WrDvuptD_WQWExyiMR-EusJL58Fs8YYtoqudO0KGN6mCKcgk1M_oxDnuu-ow\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"300\"\u003e\u003c/figure\u003e\u003ch3 id=\"part-2-generating-quality-responses-for-your-llm-based-chatbot\"\u003e\u003cstrong\u003ePart 2: Generating Quality Responses for your LLM-based Chatbot \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y8zb2m8mt1\" title=\"Chatbot Pt2. Fine Tuning Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWhen your intelligent chat application is powered by an underlying large language model, you can customize these LLMs to a defined task with 2 key approaches: Fine-tuning or Retrieval Augmented Generation (RAG).\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor this exercise, you will fine tune a task-specific LLM, and you can follow along via text below or from the video above. It’s worth noting that Quantumworks Lab can also be used for optimizing RAG based systems with techniques such as \u003ca href=\"https://labelbox.com/guides/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eReranking\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo start off, let's first generate quality responses to selected prompts. Ideally, we’ll want the chatbot to replicate the responses provided by your annotators.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab Catalog, you'll identify the relevant prompts related to coffee, and you can combine filters such as:\u003cul\u003e\u003cli\u003eSemantic search with input prompt\u003c/li\u003e\u003cli\u003eSimilarity search with ideal example data rows\u003c/li\u003e\u003cli\u003eKeywords matching\u003c/li\u003e\u003cli\u003eExclude already classified prompts that were shown in Part 1 of this tutorial\u003c/li\u003e\u003cli\u003eSave your filters as a Slice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/4lzE9AcYtkeXWiz6ZpFvV65fK1hOJbnY4OYDoe-7a5kCzFILWHQ3yH3GXGe7rfqC4pP1QjJ4V44OwPhiSdg3YgCJXBVlwMqmvUqSgkxVYXAl2WfnWpQVsWp9yqz-02wzymgp_2hRFgn4iPuDzzrOTiU\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"703\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce you have the initial set of LLM prompts, you will next create an annotation project for \u003cstrong\u003eHumans Responses to Uploaded Prompts.\u003c/strong\u003e This allows you to quickly generate quality responses to each of the selected prompts.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/6dGzWO9-wbbS2WJnqxtXYsBuOY21s9CJAqsPAks6LH4WvUiy9Ld7aIwTcz8ZQzvGmHTWX9bIaHNq8PA6ZKGmGl2-wVtpuHIHWe3dYry75aXJeJFATMnt5uLxWJgROWD312eqX1Sh-7-FCrOS185jvvs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"776\" height=\"472\"\u003e\u003c/figure\u003e\u003cp\u003eYour team of annotators can now produce specific responses for the LLM to learn from.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eIf you need help in this domain, Quantumworks Lab provides on-demand teams of skilled professional labelers with LLM experience via Quantumworks Lab’s \u003ca href=\"https://labelbox.com/product/boost/workforce/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBoost Workforce\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/W7qsBM44eDsFDtQEHnWHzKt9pACdNp3o5MOrLpCFnVfrXmRiL-ZTk3yao-WHkSPO6SocQ02waG2WkbYVJNo-BWuPbjSzgzZNmqr0xWmSoBIXIlasNWi-pQnWRXeGWFALhB9czIz7U4OXZDhNhlbMQbQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"654\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce your dataset is generated, you can use the Python SDK to export the labels and convert it to a format for model fine-tuning (e.g. JSONL format for GCP Vertex AI)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy identifying relevant prompts and generating quality responses, you can now train a task specific LLM model and output predictions for any new requests.\u003c/p\u003e\u003ch3 id=\"part-3-model-evaluation-and-deployment\"\u003e\u003cstrong\u003ePart 3: Model Evaluation and Deployment\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9a0o6o5vne\" title=\"Chatbot pt3. Evaluation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"558\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBefore deploying your model into production, let’s evaluate the performance of the responses when compared to your ground-truth data (expected output). Using holdout prompts and responses pairs not used to train the model, you can evaluate the performance of your fine-tuned LLM.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModel\u003c/a\u003e, let’s first create a new experiment as shown below.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/dS7V4NoliqteLrHDgvgOAnvjWmyl5BvWfBFXpQpI0JfwAbnHPdtSj4riVFvYjGgSqwIJur5n8cW2POzvJWDnkNi_MBXsmsw73gRLvwFabtjJIJKT5p69clEPPP1SiPY67SqwYN1kO1AqDVZosSS7Sso\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"759\"\u003e\u003c/figure\u003e\u003cp\u003eAfterwards, you can create a model run to add the holdout dataset containing the ground-truth responses.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/PYteE32jXIijEUpH5HqvAk8I__xzRMkv9d30sqZU0ztL1lbdWBQp6GwJenkqDmcN3LkuJh-QlbQB4wV-0eisZOIUN9j7FIz4s1ODAf7zkh-vhm1h2WGjFPbOZlqt72GZEmc-4EgWo8Jo2EYU0mcW1SM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"557\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eUsing the Python SDK, you can next export the prompts to generate responses from your fine-tuned model from Google's Vertex AI.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eUsing 3rd-party libraries, you can calculate custom metrics for evaluating NLP tasks, such as the BLEU score\u003c/li\u003e\u003cli\u003eThe predictions and custom metrics can now be uploaded back into your model run\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/9DYocSmDHD410N5L4APC2cQ_LoWrzK380GO8LpvaLysfihLwLse2S7sTkFNQiui5RfRYTr7Uy5oCIL9dqAZ7PW3KTScFZ_WbLbR8rq7iu5384DxzF3932GBLTQuaiPkboL6D8hjpPHMQIP5RblomLXw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"588\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWith the metrics and predictions uploaded, you can filter by various metrics to identify the highest and lowest performing prompts, along with overall model performance.\u003c/li\u003e\u003cli\u003eYou can then select these data rows (such as this high performing prompt below) to find similar prompts within the catalog, which will be used to train the next iteration of your chat application.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/yZHhwpGt5t1TM9EoNUufy7G239bLWVWYpWMcX8m5nfhIcRTE_XZ3FRdq5tiCCpM-T9WrsDQd6Qo7XVQgduIc-r2_n1_JN0nk13UJ7XfFwpgYFzdE5_RX_tqWXsSnhBAM-nzVssr_ZQ4xoKG6H5u2qaE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"938\"\u003e\u003c/figure\u003e\u003ch3 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn this guide, we highlighted a few best practices for ensuring better safety, relevancy, and user feedback when building a task-specific chatbot. Regardless of the techniques used and the models chosen, it is crucial to generate and classify data to the highest quality for optimizing chatbot outputs. \u003c/p\u003e\u003cp\u003eBy incorporating semantic search, labeling functions, foundation models, and a human-in-the-loop approach, you will be able to generate and classify data  in higher qualities consistently. Combined with an SDK-driven approach, you can more easily train models and enhance LLM performance through faster iterations. Give the tutorial a try and we'd love to hear your feedback or ideas on how we can help you improve your LLM-based chatbot applications.\u003c/p\u003e","comment_id":"6601bf732822410001bac2ba","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/03/thumbnail--3-.png","featured":false,"visibility":"public","created_at":"2024-03-25T18:16:19.000+00:00","updated_at":"2024-05-28T17:01:29.000+00:00","published_at":"2024-03-25T19:16:18.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab for optimizing your task-specific LLM chatbot for better safety, relevancy, and user feedback.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/how-to-improve-your-task-specific-chatbot-for-safety-relevancy-and-user-feedback/","excerpt":"Learn how to leverage Quantumworks Lab for optimizing your task-specific LLM chatbot for better safety, relevancy, and user feedback.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"tag":{"slug":"label-data-for-ai","id":"653aa53f375d13000123d7ec","name":"Label data for AI","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","count":{"posts":48},"url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},"slug":"label-data-for-ai","currentPage":1},"__N_SSG":true},"page":"/guides/tag/[id]","query":{"id":"label-data-for-ai"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/tag/label-data-for-ai/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 11:32:16 GMT -->
</html>