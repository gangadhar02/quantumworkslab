<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/tag/label-data-for-ai/page/3/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:51:17 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../../../static/scripts/munchkin.js"></script><script src="../../../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../../../_next/static/chunks/pages/guides/tag/%5bid%5d/page/%5bpagenum%5d-da4e9ee1c105845a.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../../../index.html"><img width="106" height="24" alt="logo" src="../../../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-12 md:py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../../../index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Latest</a><a href="../../../build-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Build AI</a><a href="../../../use-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Use AI</a><a href="../../../explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="../../index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Label data for AI</a><a href="../../../train-fine-tune-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../../../mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../guide-to-using-model-foundry/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index0b48.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F12%2FWelcome-to-the-Foundry-1--2-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../guide-to-using-model-foundry/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to use the model Foundry for automated data labeling and enrichment</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-build-a-powerful-product-recommendation-system-for-retail/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexe6a1.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-build-a-powerful-product-recommendation-system-for-retail/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to build a powerful product recommendation system for retail</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to leverage Quantumworks Lab&#x27;s data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../a-guide-to-the-data-i-o-process-in-labelbox/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index9d23.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F08%2FGroup-3078.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../a-guide-to-the-data-i-o-process-in-labelbox/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">A guide to the Data I/O process in Quantumworks Lab</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index38a9.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F06%2FGroup-3074.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Meta&#x27;s Segment Anything (SAM) model on video with Quantumworks Lab&#x27;s model-assisted labeling</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexc8e8.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3062.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-accelerate-image-text-pair-generation-with-blip-2/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexf5d3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGuide_BLIP-2.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-accelerate-image-text-pair-generation-with-blip-2/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to accelerate image-text pair generation with BLIP-2</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../automatically-label-text-with-96-accuracy-using-foundation-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexf3e3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F05%2FGroup-3060--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../automatically-label-text-with-96-accuracy-using-foundation-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Automatically label text with 96%+ accuracy using foundation models</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab&#x27;s search capabilities, bulk classification, and foundation models.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-create-and-label-text-layers-from-pdf-documents-for-ai/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index408d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2FScreen-Shot-2023-04-24-at-9.16.14-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-create-and-label-text-layers-from-pdf-documents-for-ai/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to create and label text layers from PDF documents for AI</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn about the different types of PDF layers and how to import annotations to build robust AI models with contextual information from PDF documents.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/index3e9d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Quantumworks Lab and Weights &amp; Biases to fine tune your computer vision projects</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can use Quantumworks Lab and Weights &amp; Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../../../how-to-kickstart-and-scale-your-data-labeling-efforts/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../../../_next/image/indexc47c.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../../../how-to-kickstart-and-scale-your-data-labeling-efforts/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to kickstart and scale your data labeling efforts</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. </p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8"><a class="mr-9 text-neutral-700 mb-1" href="../2/index.html">&lt;</a>Page 3 of 5<a class="ml-9 text-neutral-700 mb-1" href="../4/index.html">&gt;</a></div></div></div></div></div></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"65300bc0a6fedb0001417480","uuid":"59641c97-b4ef-4ade-8e83-52f3887883d0","title":"How to use the model Foundry for automated data labeling and enrichment","slug":"guide-to-using-model-foundry","html":"\u003cp\u003eFoundation models, such as GPT-4, are ushering in a new era of AI by outperforming humans on numerous tasks across modalities including images and language. With the introduction of\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e \u003c/a\u003e\u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ethe Foundry add-on for Quantumworks Lab Model\u003c/u\u003e\u003c/a\u003e, we’re bringing the power of foundation models into the Quantumworks Lab platform.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eMeet your new AI co-pilot: Foundry\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLabelbox Foundry enables AI teams to use world-class foundation models to enrich datasets and automate tasks. In just a few clicks, AI builders can explore, test and integrate powerful models to build vital workflows for pre-labeling, or other specific data tasks. Kickstart your AI efforts with this AI copilot to build intelligent applications faster than ever.\u003c/p\u003e\u003cp\u003eEarly tests with Fortune 500 companies show up to 88% reductions in human labeling time, with complex tasks going from days to hours. Foundry integrates these state-of-the-art models into every step of the workflow, unlocking the next evolution of AI development. Kickstart your AI and data labeling efforts with this co-pilot to build intelligent applications faster than ever.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re excited to announce that Foundry is available as an add-on for Quantumworks Lab Model!\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith Foundry, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAccess the world’s most cutting-edge models and combine them with human-in-the-loop systems to accelerate model development.\u003c/li\u003e\u003cli\u003ePre-label data in a few clicks to reduce labeling costs by up to 90%.\u003c/li\u003e\u003cli\u003eTailor intelligence to your needs using Quantumworks Lab’s comprehensive end-to-end platform and workflow in Foundry. From fine-tuning to model distillation, you can customize intelligence beyond pre-built AI.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFoundry works across Quantumworks Lab's products — Catalog, Annotate, and Model — to supercharge your labeling and model development.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"how-to-access-foundry\"\u003eHow to access Foundry:\u003c/h2\u003e\u003ch3 id=\"free-tier-organizations\"\u003eFree Tier organizations\u003c/h3\u003e\u003cp\u003eFoundry is only available to our Starter and Enterprise plans, to access Foundry you will need to:\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Upgrade your account to our \u003cstrong\u003eStarter plan\u003c/strong\u003e:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn the \u003ca href=\"https://app.labelbox.com/workspace-settings/billing?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBilling tab\u003c/u\u003e\u003c/a\u003e, locate “Starter” in the All Plans list and select “Switch to Plan.” \u003cem\u003eThe credit card on file will only be charged when you exceed your existing free 10,000 LBU.\u0026nbsp;\u003c/em\u003e\u003c/li\u003e\u003cli\u003eUpgrades take effect immediately so you'll have access to Foundry right away on the Starter plan. After upgrading, you’ll see the option to activate Foundry for your organization.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-04_16-22-54--1--min-1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1904\" height=\"932\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/2023-12-04_16-22-54--1--min-1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/2023-12-04_16-22-54--1--min-1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/2023-12-04_16-22-54--1--min-1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-04_16-22-54--1--min-1.gif 1904w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e2) After upgrading to our Starter plan, please follow the instructions under the Starter plan below to activate Foundry as an add-on for your organization.\u003c/p\u003e\u003ch3 id=\"self-serve-organizations-on-a-starter-or-standard-plan\"\u003eSelf-serve organizations on a Starter or Standard plan\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs part of the self-serve Starter or Standard plan, you automatically have the ability to opt-in to using Foundry.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eTo generate model predictions, just submit a model run and you’ll be guided to activate Foundry along the way:\u0026nbsp;\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Generate predictions:\u003c/strong\u003e Start by selecting data in Catalog and click “Predict with Foundry.” You’ll be able to select a foundation model and submit a model run to generate predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Activate Foundry:\u003c/strong\u003e The first time you submit a model run, you’ll see the option to activate Foundry for your organization’s \u003ca href=\"https://docs.labelbox.com/docs/members-group-management?ref=labelbox-guides.ghost.io#member-roles\"\u003e\u003cu\u003eAdmins\u003c/u\u003e\u003c/a\u003e. You’ll need to agree to Quantumworks Lab’s Model Foundry add-on service terms and confirm you understand the associated compute fees.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Payment method:\u003c/strong\u003e With Foundry, you will be charged for the Quantumworks Lab units (LBUs) your account consumes, plus any\u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003epass-through compute costs\u003c/u\u003e\u003c/a\u003e for the models you use.\u003c/p\u003e\u003cp\u003eFor self-serve Starter and Standard tier to enable billing for these model compute charges, you'll be asked to confirm the credit card on file for your Quantumworks Lab account. Alternatively, you may add another card if you prefer to keep the Foundry charges separate. By confirming your payment method, you agree to let Quantumworks Lab to bill your card as Foundry model compute fees accrue based on your usage.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/OLBO-X7WF_y9EXTcmQ5nM1-6W75tJyYWojkmQp_IH-bdx2R_i985qA00Tbf3nMqHLGzUIkKo-0MES9-jfKUlv86kFjlmASbzfCo_CZTAau-38nUtH22IYHxUJ4mHgYi_VgfTdf9CaBsT3PK-46FeAF8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"305\"\u003e\u003c/figure\u003e\u003ch3 id=\"enterprise-organizations\"\u003eEnterprise organizations\u003c/h3\u003e\u003cp\u003eAs an organization on our Enterprise plan, you automatically have the ability to opt-in to using Foundry.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eTo generate model predictions, just submit a model run and you’ll be guided to activate Foundry along the way:\u0026nbsp;\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Generate predictions:\u003c/strong\u003e Start by selecting data in Catalog and click “Predict with Foundry.” You’ll be able to select a foundation model and submit a model run to generate predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Activate Foundry:\u003c/strong\u003e The first time you submit a model run, you’ll see the option to activate Foundry for your organization’s \u003ca href=\"https://docs.labelbox.com/docs/members-group-management?ref=labelbox-guides.ghost.io#member-roles\"\u003e\u003cu\u003eAdmins\u003c/u\u003e\u003c/a\u003e. You’ll need to agree to Quantumworks Lab’s Model Foundry add-on service terms and confirm you understand the associated compute fees.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Payment method:\u003c/strong\u003e With Foundry, you will be charged for the Quantumworks Lab units (LBUs) your account consumes, plus any\u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003epass-through compute costs\u003c/u\u003e\u003c/a\u003e for the models you use.\u003c/p\u003e\u003cp\u003eIf you are on an annual contract plan, you may add a credit card on file to pay for the associated compute costs of a model or choose to receive a monthly invoice at the end of each month based on your organization’s Foundry usage.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-06_12-19-47.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1016\" height=\"738\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/2023-12-06_12-19-47.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/2023-12-06_12-19-47.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-06_12-19-47.png 1016w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"see-it-in-action-how-foundry-works\"\u003eSee it in action: How Foundry works\u003c/h2\u003e\u003cp\u003eIn the below interactive demos, you'll learn how to leverage foundation models to generate model predictions, send predictions as pre-labels to a labeling project in Annotate, and verify the predictions with human-in-the-loop review:\u003c/p\u003e\u003ch3 id=\"step-1-how-to-generate-model-predictions-with-foundry\"\u003eStep 1: How to generate model predictions with Foundry\u003c/h3\u003e\u003cp\u003eAccess a vast range of ready-to-use foundation models that embed advanced AI into your data tasks with ease. Quickly generate predictions to pre-label datasets or to enrich your existing data to extract better insights that boost productivity and increase time-savings.\u003c/p\u003e\u003ch3 id=\"step-2-how-to-send-foundry-predictions-as-pre-labels-to-a-labeling-project\"\u003e\u003cstrong\u003eStep 2: How to send Foundry predictions as pre-labels to a labeling project\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRather than labeling from scratch, combine the power of foundation models with human-in-the-loop review to accelerate your labeling operations.\u003c/p\u003e\u003ch3 id=\"step-3-how-to-verify-pre-labels-with-human-in-the-loop-review\"\u003eStep 3: How to verify pre-labels with human-in-the-loop review\u003c/h3\u003e\u003cp\u003eFocus human intelligence on critical quality assurance instead of on initial labeling efforts. Seamlessly validate model-generated pre-labels in Annotate – approving accurate predictions with a click and easily editing or sending incorrect labels to be corrected.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out the below tutorials to learn how Foundry can accelerate your pre-labeling and data enrichment workflows in Labelbox.\u0026nbsp;\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003cdiv style=\"position: relative; padding-bottom: calc(48.601036269430054% + 41px); height: 0;\"\u003e\u003ciframe src=\"https://demo.arcade.software/HnecUTysIiTR66fMWJ1C?embed\" title=\"Object Detection - Foundry Arcade \" frameborder=\"0\" loading=\"lazy\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;color-scheme: light;\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\u003c!--kg-card-end: html--\u003e\n\n\u003c!--kg-card-begin: html--\u003e\n\u003cdiv style=\"position: relative; padding-bottom: calc(48.57586742620404% + 41px); height: 0;\"\u003e\u003ciframe src=\"https://demo.arcade.software/PqkNFZRaNrGWBPjlotve?embed\" title=\"Named Entity Recognition - Foundry Arcade\" frameborder=\"0\" loading=\"lazy\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;color-scheme: light;\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003ch2 id=\"what-use-cases-is-foundry-available-for\"\u003eWhat use cases is Foundry available for?\u003c/h2\u003e\u003cp\u003eFoundry currently supports a variety of tasks for computer vision and natural language processing. This includes:\u003c/p\u003e\u003ch3 id=\"computer-vision\"\u003eComputer Vision\u003c/h3\u003e\u003cul\u003e\u003cli\u003eObject detection\u003c/li\u003e\u003cli\u003eImage classification\u003c/li\u003e\u003cli\u003eImage segmentation \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWatch the below demo to see how to use Foundry for an object detection use case with Amazon Rekognition and Grounding Dino:\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/78pt3397io\" title=\"Model Foundry - Image Demo with Amazon Rekognition and Grounding Dino Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThis demo uses Amazon Rekognition and Grounding Dino, but you can leverage any other \u003ca href=\"https://labelbox.com/model-foundry/models/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision model available in Foundry\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"natural-language-processing\"\u003eNatural Language Processing\u003c/h3\u003e\u003cul\u003e\u003cli\u003eText generation\u003c/li\u003e\u003cli\u003eTranslation\u003c/li\u003e\u003cli\u003eQuestion answering\u003c/li\u003e\u003cli\u003eZero-shot classification\u003c/li\u003e\u003cli\u003eSummarization\u003c/li\u003e\u003cli\u003eConversational\u003c/li\u003e\u003cli\u003eText classification\u003c/li\u003e\u003cli\u003eNamed entity recognition\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWatch the below demo to see how to use Foundry for a text classification use case with GPT-4:\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jf0g7ou9bk\" title=\"Model Foundry - Text Demo with GPT-4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThis demo uses GPT-4, but you can leverage any other \u003ca href=\"https://labelbox.com/model-foundry/models/?ref=labelbox-guides.ghost.io\"\u003elarge language model available in Foundry\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"how-is-foundry-priced\"\u003e\u003cstrong\u003eHow is Foundry priced?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFoundry pricing will be calculated and billed monthly based on the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInference cost\u003c/strong\u003e – Quantumworks Lab will charge customers for inference costs for all models hosted by Labelbox. Inference costs will be bespoke to each model available in Foundry. The inference price is determined based on vendors or our compute costs – these are published publicly on \u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour website\u003c/u\u003e\u003c/a\u003e as well as inside the product.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLabelbox's platform cost\u003c/strong\u003e – each asset with predictions generated by Foundry will accrue LBUs.\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eModel LBU\u003c/strong\u003e is consumed for every data row that goes through Foundry. \u003cstrong\u003eAnnotate LBU\u003c/strong\u003e is consumed for every data row that has a prediction (from Foundry) submitted as a label.\u003c/p\u003e\u003cp\u003eThe unit compute costs are listed on the model card for each model found in the Quantumworks Lab app.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Iqv7Lg9_XlQFIdXK_NVWx-Az4ig-zIlTZ2H2ZFiCsb-BQ-U94-VdccpvJ17SpayYK4VkS-GbAnGBWu8qpvfiARtORoPagIpr4wcqUSHRWq9LcmLBJ5eNOGCLMpQ-d-_c1_Xo9IrT1iZ9nAmigLk0A_A\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"220\"\u003e\u003c/figure\u003e\u003cp\u003eLearn more about the pricing of the Foundry add-on for Quantumworks Lab Model on our \u003ca href=\"https://labelbox.com/pricing/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003epricing page\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"where-can-i-provide-feedback-or-ask-questions\"\u003e\u003cstrong\u003eWhere can I provide feedback or ask questions?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf you have questions or feedback related to the Foundry workflow, please\u003ca href=\"https://labelbox.atlassian.net/servicedesk/customer/portal/2/group/3/create/214?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003esubmit a ticket here\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"additional-resources\"\u003eAdditional Resources\u0026nbsp;\u003c/h2\u003e\u003cp\u003eTo learn more about Foundry and familiarize yourself with the workflow, we recommend checking out the below resources:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox/docs/foundry?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eFoundry Documentation\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eDemo videos\u0026nbsp;\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.wistia.com/medias/78pt3397io?ref=labelbox-guides.ghost.io\"\u003eImage \u003c/a\u003e- Amazon Rekognition and Grounding DINO Demo\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.wistia.com/medias/jf0g7ou9bk?ref=labelbox-guides.ghost.io\"\u003eText \u003c/a\u003e- GPT-4 Demo\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModels currently available in Foundry\u003c/a\u003e\u003c/li\u003e\u003cli\u003eRecent blog posts exploring the power of foundation models:\u003cul\u003e\u003cli\u003eExplore six of the most powerful foundation models available to AI builders, the use cases and applications they are best suited for. \u003ca href=\"https://labelbox.com/blog/6-cutting-edge-foundation-models-for-computer-vision-and-how-to-use-them/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eGPT-4 offers a versatile toolkit to help enterprises kickstart labeling. Learn how to leverage GPT-4 for your machine learning or specific business use case. \u003ca href=\"https://labelbox.com/blog/how-to-unlock-the-full-power-of-gpt-4-for-enterprise-ai/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eMany businesses have more specialized and domain-specific use cases. Discover the benefits of incorporating foundation models into your specialized AI application development workflow. \u003ca href=\"https://labelbox.com/blog/why-you-should-leverage-foundation-models-for-specialized-ai-applications/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eRemoving personally identifiable information from datasets is crucial for teams building AI. Explore how you can extract PII from your datasets with higher accuracy and speed. \u003ca href=\"https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"65300bc0a6fedb0001417480","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/12/Welcome-to-the-Foundry-1--2-.png","featured":false,"visibility":"public","created_at":"2023-10-18T16:45:52.000+00:00","updated_at":"2023-12-12T15:05:47.000+00:00","published_at":"2023-10-18T17:21:36.000+00:00","custom_excerpt":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/guide-to-using-model-foundry","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/guide-to-using-model-foundry/","excerpt":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":"How to use the model Foundry for automated data labeling and enrichment","og_description":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to use the model Foundry for automated data labeling and enrichment","meta_description":"Introducing the model Foundry - Enrich data and automate tasks using foundation models","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64e7a2c3b0c09f000179dbed","uuid":"2b64de59-c4f8-4bf5-aa4b-07d1aabcb933","title":"How to build a powerful product recommendation system for retail","slug":"how-to-build-a-powerful-product-recommendation-system-for-retail","html":"\u003cp\u003ePersonalized experiences are at the heart of customer satisfaction and are key to long-term brand loyalty and success. Amidst the abundance of choices available to the modern consumer, businesses must find innovative ways to stand out and forge meaningful relationships with their audience. \u003c/p\u003e\u003cp\u003eThe rise of AI has enabled companies to craft personalized experiences at an unprecedented scale. Organizations can now rely on algorithms taught to recognize customer preferences, behavior, and provide recommendations based on purchase history, and more. With a powerful product recommendation system, retailers can create individualized customer interactions and foster stronger connections to boost customer loyalty and increase key metrics such as conversion rate, average order value, and repeat purchase rates. \u003c/p\u003e\u003cp\u003eHowever, building a robust and effective AI-powered product recommendation system can be challenging for many teams. Some key challenges include: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eBuilding a strong recommendation system that makes accurate predictions requires a vast amount of high-quality data. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability: \u003c/strong\u003eAs a business grows and their product catalog expands, the recommendation system should be able to handle new and incoming data. Ensuring scalability and maintaining model performance with new data can be particularly challenging for teams relying on in-house solutions or disparate ML tools.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrivacy and Security: \u003c/strong\u003eWhen it comes to customer data and specific product information, ensuring user privacy and safeguarding against potential security violations is critical to maintain trust with customers and build a successful recommendation system. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform to help build the best personalized product recommendation engine. Rather than spending valuable time building in-house or relying on disparate systems and applications, teams can leverage Quantumworks Lab’s platform to seamlessly build an end-to-end workflow that integrates with your existing tech stack and helps teams build AI systems faster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1425\" height=\"635\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/image-3.png 1425w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build a powerful recommendation system, ensuring your customers embark on a seamless and delightful shopping journey that keeps them coming back for more.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"see-it-in-action-how-to-build-a-powerful-product-recommendation-system-in-labelbox\"\u003eSee it in action: How to build a powerful product recommendation system in Quantumworks Lab\u003c/h2\u003e\u003cp\u003e\u003cem\u003eThe  walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eCatalog\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eAnnotate\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eModel\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003ecreate a Quantumworks Lab account\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1: \u003c/strong\u003eExplore and enhance your data (\u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003eGoogle Colab Notebook\u003c/a\u003e)\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Prepare data and evaluate model performance: (\u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\"\u003eGoogle Colab Notebook\u003c/a\u003e)\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u003c/strong\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\" class=\"kg-btn kg-btn-accent\"\u003ePart 1: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"painlessly-consolidate-all-your-product-data\"\u003ePainlessly consolidate all your product data\u003c/h3\u003e\u003cp\u003eBuilding a recommendation engine requires consolidating data of different types from various sources. Such data can include product, business, and customer information that might be siloed or stored in different databases. To holistically browse and visualize your entire product catalog, leverage Quantumworks Lab Catalog to bring and view all of your data in a single place.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rdlcin4zh8\" title=\"[Personalized Experiences Demo] Data ingestion Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ve provided a sample retail dataset for you to view in your Quantumworks Lab app: \u003c/p\u003e\u003cp\u003e1) Input your Quantumworks Lab API key into the provided \u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003eGoogle Colab notebook\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e2) Specify an amount of data that you wish to ingest — we’ve provided up to 44,000 data rows for ingestion, but keep in mind that this will accrue \u003ca href=\"https://docs.labelbox.com/docs/billing?ref=labelbox-guides.ghost.io#labelbox-units-lbus\"\u003eLBUs\u003c/a\u003e in your account. \u003cem\u003eIf you are using Quantumworks Lab for free, we suggest that you ingest around 5,000 data rows.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e3) Select ‘Runtime’ in the navigation bar and hit ‘Run all’ to bring the selected amount of data rows into your \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Catalog \u003c/a\u003e— where you can browse, explore, and curate the data for insights and model development.\u003c/p\u003e\u003ch3 id=\"accelerate-product-discovery-across-your-entire-catalog\"\u003eAccelerate product discovery across your entire catalog\u003c/h3\u003e\u003cp\u003eAn effective product recommendation relies on training a model with a thorough understanding of your product data, encompassing product tags, categories, and more. However, retailers often have an ever-growing product list with hundreds or thousands of products. Dealing with this volume of data at scale and effectively searching, organizing, and managing data for machine learning tasks can be a challenge.\u003c/p\u003e\u003cp\u003eYou can leverage Quantumworks Lab Catalog to visualize, browse, and curate your product listings.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ehcuz2b4rk\" title=\"[Personalized Experiences Demo] Search and curate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSearch and curate data\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eYou’ll now be able to see the sample retail dataset in your Quantumworks Lab Catalog. Try searching across key product-specific metadata such as category, the year the item was released, season, gender, and more. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1226\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 2304w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003eSearch across datasets\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003efind similar data\u003c/a\u003e in seconds with off-the-shelf embeddings \u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003enatural language\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003elayer structured and unstructured filters\u003c/a\u003e for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"categorize-and-curate-product-listings-faster\"\u003eCategorize and curate product listings faster\u003c/h3\u003e\u003cp\u003eAdvanced ML teams often adopt partially automated labeling workflows to mitigate costs and accelerate model development. Product recommendation models require a vast amount of accurately labeled data with a wide array of features. Manually labeling this data can not only be time consuming, but can also get exponentially expensive. Scaling data curation and enrichment effectively is key to quickly creating a powerful ML solution. \u003c/p\u003e\u003cp\u003eOne simple way to achieve this is by leveraging bulk classification and using human-in-the-loop review for quality assurance. Some AI teams using this technique have cut labeling costs by nearly 90%.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/epl8hstb7c\" title=\"[Personalized Experiences Demo] Streamlined labeling automation through bulk classification Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eStreamline labeling automation through bulk classification\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCreate a new labeling project\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"905\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve narrowed in on a specific slice of data that you’d like to take action on, you can send them to a labeling project of interest in just a few clicks.\u003c/p\u003e\u003cp\u003e1) Create a new image project in \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e2) Configure the editor and create a new ontology. For the ontology, create a new classification called ‘Occasions’ with options such as ‘Casual’, ‘Sports’, ‘Formal’, etc. feel free to add any other classifications of interest.\u003c/p\u003e\u003cp\u003e3) Save your labeling project.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSend a subset of data to the labeling project\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"739\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eReturn to Catalog and surface a specific subset of data that you want to bulk classify. For example, you can surface all instances of ‘Sporty apparel’ clothing with a natural language search.\u003c/p\u003e\u003cp\u003e1) Highlight any data rows of interest and select ‘Manage selection’ \u0026gt; ‘Add classifications’.\u003c/p\u003e\u003cp\u003e2) Select the labeling project that you made in the previous step and determine a step of the project’s review workflow that you would like to send the classifications to. In the above demo, we are sending these to the ‘Done’ stage because we have verified that these images fall under the ‘Sports’ category and want to automatically create ground truth labels.\u003c/p\u003e\u003cp\u003e3) Pick ‘Sports’ under the Classification section and you can submit the classification batch. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSave high-impact searches\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1029\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can save any combination of searches as a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e in Catalog. For example, saving the natural language search of ‘Sporty apparel’ as a slice called ‘Sporty apparel’, creates a dynamic slice of data that can easily be revisited or edited as project needs evolve. Any future data that gets uploaded to Quantumworks Lab will automatically populate in any relevant slices based on its filters, creating an automatic data curation pipeline as your product catalog grows.\u003c/p\u003e\u003ch2 id=\"part-2-prepare-data-and-evaluate-model-performance\"\u003ePart 2: Prepare data and evaluate model performance\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eFollow along with the below with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u003c/strong\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\" class=\"kg-btn kg-btn-accent\"\u003ePart 2: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"prepare-a-training-dataset-for-model-diagnosis\"\u003ePrepare a training dataset for model diagnosis\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eModel diagnosis can play a pivotal role when training a model for personalized shopping experiences. The success of personalized recommendation systems hinge on the accuracy of a model’s understanding of a retailer’s product catalog or individual customer preferences. A properly curated and organized training dataset serves as the foundation for accurate model performance evaluation and fine-tuning.\u003c/p\u003e\u003cp\u003eSend the curated training dataset from Quantumworks Lab Annotate to Model in a few clicks to efficiently diagnose model performance of the training dataset.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate a training dataset for evaluation\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/zo945lphbi\" title=\"[Personalized Experiences Demo] Prepare a training dataset for model diagnosis Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eCreating a new model\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1866\" height=\"1408\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 1866w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Navigate to the Model tab, select ‘Experiments’, and create a new model.\u003c/p\u003e\u003cp\u003e2) Name your model (e.g ‘Occasions’) and select a model thumbnail.\u003c/p\u003e\u003cp\u003e3) Select the same ontology that was used to bulk classify the data rows in the previous step.\u003c/p\u003e\u003cp\u003e4) Select the project that the classifications were sent to in the previous step. After selecting both the correct ontology and project, you should see the number of data rows that were bulk classified and ready to be added to the new model.\u003c/p\u003e\u003cp\u003e5) Hit ‘Create model’.\u003c/p\u003e\u003cp\u003e\u003cem\u003eCreating a new model run\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"938\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 2306w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Once you’ve created your model, you can navigate back to the ‘Experiments’ tab and find the newly created model.\u003c/p\u003e\u003cp\u003e2) Create a ‘new model run.’ \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003eA model run\u003c/a\u003e is a model training experiment within a model, providing a versioned data snapshot of all data rows, annotations, and data splits for that model run.\u003c/p\u003e\u003cp\u003e3) To create a model run, you’ll need to give it a name (e.g ‘dataset version 1’) and can adjust the balance of the data split. For this demo, we will leave them in the default setting (80% train, 10% validate, 10% test).\u003c/p\u003e\u003cp\u003e4) After creating the model run, you’ll be able to see the populated training data classifications.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTrain a model on a provided training dataset\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ab1phltynu\" title=\"[Personalized Experiences] Train a model on the provided training dataset Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eWe’ll be using the \u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\"\u003eColab notebook\u003c/a\u003e to train a model on the training dataset and bring back inferences from the trained model for evaluation and diagnosis.\u003c/p\u003e\u003cp\u003eFor this step, you will need:\u003c/p\u003e\u003cul\u003e\u003cli\u003eYour Ontology ID — found in the Settings tab on the model run page\u003c/li\u003e\u003cli\u003eYour Model Run ID — found in the gear icon on the top-right of the model run page\u003c/li\u003e\u003cli\u003eYour API key\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e1) Enter your API key, Ontology ID, and Model Run ID in the Colab Notebook.\u003c/p\u003e\u003cp\u003e2)  Once those are inputted, you can select ‘Runtime’ in the navigation bar and hit ‘Run all’ – this will take the classifications from your model run and train a provided image classification model. After training, the notebook will also take the trained model and use it to run inference on the data.\u003c/p\u003e\u003cp\u003e3)  If you want to adjust your data splits, you can leverage search filters in Model to surface any data rows and move them to the train, test, and validation splits.\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness-for-retail\"\u003eEvaluate and diagnose model effectiveness for retail\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eA well-performing model can accurately predict consumer behavior or recommend products based on past preferences. Effective model diagnosis helps fine-tune recommendation algorithms, resulting in more accurate and appealing product suggestions. \u003c/p\u003e\u003cp\u003eModel diagnosis and evaluation are not one-time tasks. By leveraging effective diagnostic tools and an active learning workflow, retailers can continuously identify areas of improvement and adapt to changing customer behavior or an evolving product catalog. This iterative approach keeps personalized experiences relevant and effective for driving business outcomes over time.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qwqdx21ml2\" title=\"[Personalized Experiences Demo] Diagnose model performance and fix model errors Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eDiagnose model performance with model metrics\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"692\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 2330w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) After running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/p\u003e\u003cp\u003e2) Use the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/p\u003e\u003cp\u003e3) Model metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/p\u003e\u003cp\u003e4) Detect and visualize corner-cases where the model is underperforming. For example, in the demo above, we notice that the model is classifying this type of white shoe as a ‘Sports’ shoe when in fact it is a ‘Casual’ shoe.\u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1172\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 2324w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Select any corner-cases and select ‘Find similar in Catalog’ from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows to the selected example.\u003c/p\u003e\u003cp\u003e2) In addition to the similarity search, you can filter on ‘Annotation’ \u0026gt; ‘is none’ to surface only unlabeled data rows that you can retrain the model on to boost model performance.\u003c/p\u003e\u003cp\u003e3) Select any relevant examples and ‘Manage selection’ \u0026gt; ‘Add classifications’. In this case, we’d want to bulk classify these examples to reinforce to the model that these images are ‘casual’ shoes.\u003c/p\u003e\u003cp\u003e4) This step is similar to the bulk classification step in part 1. Select the labeling project that you made in the previous step and determine a step of the project’s review workflow that you would like to send the classifications to. We can send these to the ‘Done’ stage because we want to tell the model these white shoes fall under the ‘Casual’ category and want to automatically create ground truth labels.\u003c/p\u003e\u003cp\u003e5) Create a new model run (within the same model) and have the newly added classifications as a part of the training dataset.\u003c/p\u003e\u003cp\u003e6) Run the notebook again to train the model on this new training dataset and evaluate model performance with model metrics. You can compare results from the initial model run with the new model run to evaluate how the adjustment influenced model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAs consumer businesses strive to distinguish themselves in a competitive market, the power of AI-driven product recommendation systems cannot be underestimated. Companies can tap into their vast data stores and harness the capabilities of advanced algorithms to forge deeper connections with their customers.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..\u0026ref=labelbox-guides.ghost.io\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e. \u003c/p\u003e","comment_id":"64e7a2c3b0c09f000179dbed","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078--1-.png","featured":false,"visibility":"public","created_at":"2023-08-24T18:34:43.000+00:00","updated_at":"2023-11-20T19:39:23.000+00:00","published_at":"2023-08-24T19:15:09.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-build-a-powerful-product-recommendation-system-for-retail/","excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","reading_time":10,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078--1--2.png","og_title":"How to build a powerful product recommendation system for retail","og_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078--1--1.png","twitter_title":"How to build a powerful product recommendation system for retail","twitter_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","meta_title":"How to build a powerful product recommendation system for retail","meta_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64cab907306044000155752f","uuid":"baf1efed-584a-42c0-975b-78b8368fad3d","title":"A guide to the Data I/O process in Quantumworks Lab","slug":"a-guide-to-the-data-i-o-process-in-labelbox","html":"\u003cp\u003eAs you navigate the world of intelligent application creation, one element remains pivotal - your data. At Quantumworks Lab, we recognize the value of your data and its role in driving your operations, which is why we focus on simplifying the Data In and Out (I/O) process. In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with our platform.\u003c/p\u003e\u003cp\u003eWhether your goal is to store data in a cloud-hosted table, a ML training pipeline, a database, or even a production environment, our aim is to equip you with the knowledge and tools needed for a flexible, robust, and effective data management system.\u003c/p\u003e\u003ch2 id=\"understanding-data-io\"\u003eUnderstanding Data I/O\u003c/h2\u003e\u003cp\u003eAt its core, Data I/O refers to the import and export of data in your Quantumworks Lab workflow. As simple as it sounds, the process can be quite intricate, given the variety of data formats and storage locations. To manage data effectively, Quantumworks Lab uses a structured approach that tackles each data type - \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io\"\u003edata rows\u003c/a\u003e, \u003ca href=\"https://docs.labelbox.com/docs/import-metadata?ref=labelbox-guides.ghost.io\"\u003emetadata\u003c/a\u003e (including embeddings), \u003ca href=\"https://docs.labelbox.com/reference/attachments?ref=labelbox-guides.ghost.io\"\u003eattachments\u003c/a\u003e, and \u003ca href=\"https://docs.labelbox.com/reference/feature?ref=labelbox-guides.ghost.io\"\u003eannotations\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"data-in\"\u003eData In\u003c/h2\u003e\u003ch3 id=\"data-rows\"\u003eData Rows\u003c/h3\u003e\u003cp\u003eData rows typically exist in cloud storage like Amazon Web Services (AWS), Google Cloud Storage (GCS), or Microsoft Azure (Azure). However, data rows can also exist as local files, offering you flexibility in how you access and utilize your data.\u003c/p\u003e\u003ch3 id=\"setting-up-data-in\"\u003eSetting Up Data In\u003c/h3\u003e\u003cp\u003eFor those using cloud storage, setting up \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eDelegated Access\u003c/a\u003e is the first step. This involves granting Quantumworks Lab permission to securely gain read access to your unlabeled data as hosted in your preferred cloud storage provider while providing Quantumworks Lab with the limited access necessary to display and label your data. Once access is granted, you need to identify where your metadata (including embeddings) and attachments are stored.\u003c/p\u003e\u003cp\u003eRefer to the below links to learn more about setting up Delegated Access with the below cloud storage providers:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/import-aws-s3-data?ref=labelbox-guides.ghost.io\"\u003eAmazon S3\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/using-google-cloud-storage?ref=labelbox-guides.ghost.io\"\u003eGoogle Cloud\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/microsoft-azure-blob-storage?ref=labelbox-guides.ghost.io\"\u003eMicrosoft Azure\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"attachments-and-metadata\"\u003eAttachments and Metadata\u003c/h3\u003e\u003cp\u003eAttachments and metadata traditionally exist in a table (Databricks, Excel, BigQuery, CSV, etc.) alongside or separately from the data rows. In an attempt to maintain consistency and streamline the process, Quantumworks Lab encourages users to upload metadata and attachments directly with the data rows.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Metadata\u003c/strong\u003e is any information known about an asset pre-Labelbox that could be useful in data filtering and selection Metadata also includes embeddings - these are representations of your data in a vector space. These vectors capture the essential features of your data and represent them in a form that can be processed by machine learning algorithms.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/metadata?ref=labelbox-guides.ghost.io\"\u003eDeveloper guide on metadata\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2) Attachments\u003c/strong\u003e on the other hand could be any additional files or information that supplement your data and assist in the creation of high quality human labels. This could include anything from text documents with descriptive data, additional images, audio files, or any other data type that provides more context to your main data row.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/attachments?ref=labelbox-guides.ghost.io\"\u003eDeveloper guide on attachments\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Embeddings \u003c/strong\u003ecan improve your data exploration and allow you to make use of similarity search within Catalog. Lablebox computes off-the-shelf embeddings using neural networks trained on publicly available data. Off-the-shelf embeddings provide a useful starting point to explore your data, but to get the most out of similarity search you’ll want to experiment with different embeddings to power your selection based on your particular data. \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/custom-embeddings?ref=labelbox-guides.ghost.io\"\u003eDeveloper guide on embeddings\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHere are a few pointers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf your data is in a BigQuery table, you can refer to this \u003ca href=\"https://github.com/Quantumworks Lab/labelbox-bigquery/tree/main?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eIf your data is in a Databricks table, check out this \u003ca href=\"https://github.com/Quantumworks Lab/labelspark?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eFor data in a CSV format, use this \u003ca href=\"https://github.com/Quantumworks Lab/labelpandas?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eFor unique data formats, our comprehensive Quantumworks Lab documentation will be your guide\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"annotations\"\u003eAnnotations\u003c/h3\u003e\u003cp\u003eAs an output from machine learning models, annotations typically exist in JSON files and can be stored either locally or in cloud storage. The beauty of annotations lies in their versatility; they come in various forms including bounding box, mask, radio classification, and others, giving you the freedom to choose what best suits your application.\u003c/p\u003e\u003cp\u003eWhen it comes to Quantumworks Lab, if you want to upload pre-labels or submitted labels (labels made elsewhere), the process involves a few crucial steps:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Setting up a Quantumworks Lab Project and Ontology:\u003c/strong\u003e An ontology serves as a blueprint for your labeling project. It defines the labels and the structure for the annotation data you are handling. Setting up an ontology in Quantumworks Lab that aligns with your annotation data is an essential step to ensure that your pre-labels or submitted labels can be properly read and processed by the system. You can learn more about how to set up ontologies in our \u003ca href=\"https://chat.openai.com/c/link?ref=labelbox-guides.ghost.io\"\u003edeveloper guide on ontologies\u003c/a\u003e.\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Understanding the Annotation Data Format:\u003c/strong\u003e Quantumworks Lab supports a wide range of data formats including JSON, CSV, and others. It is important to understand the format of your annotation data to ensure compatibility with the Quantumworks Lab platform.\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Identifying the Annotation Types:\u003c/strong\u003e The type of annotation used is dependent on your use case. It could be a bounding box annotation for object detection tasks, a mask annotation for semantic segmentation tasks, or a radio classification for multi-choice tasks. By identifying the annotation types that your use case requires, you can ensure that your data is appropriately annotated for your model.\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4) Determining the Annotation Format:\u003c/strong\u003e Annotation formats can either be customer-specific or adhere to industry standards like COCO. Understanding this will help you prepare your data in a way that fits the requirements of Quantumworks Lab and aids in efficient data processing.\u003c/p\u003e\u003cp\u003eBy paying close attention to these steps, you can maximize the utility of your annotations, thereby boosting the effectiveness of your labeling projects and the performance of your machine learning models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5) Converting Annotations to Quantumworks Lab Format: \u003c/strong\u003eLabelbox supports two formats for importing annotations: NDJSON and Python annotation type. How these annotations are uploaded depends on your media type, see the links below for further information.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-image-annotations?ref=labelbox-guides.ghost.io\"\u003eImport image annotations \u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-video-annotations?ref=labelbox-guides.ghost.io\"\u003eImport video annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-text-annotations?ref=labelbox-guides.ghost.io\"\u003eImport text annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-geospatial-annotations?ref=labelbox-guides.ghost.io\"\u003eImport geospaital annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-document-annotations?ref=labelbox-guides.ghost.io\"\u003eImport document annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-conversational-text-annotations?ref=labelbox-guides.ghost.io\"\u003eImport conversational text annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-audio-annotations?ref=labelbox-guides.ghost.io\"\u003eImport audio conversations\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy paying close attention to these steps, you can maximize the utility of your annotations, thereby boosting the effectiveness of your labeling projects and the performance of your machine learning models.\u003c/p\u003e\u003ch2 id=\"data-out\"\u003e\u003cbr\u003eData Out\u003c/h2\u003e\u003cp\u003eContrary to data in, the data out process focuses on how data is extracted from Labelbox. Extracting data from Quantumworks Lab involves exporting labeled data rows, along with their associated metadata, attachments, and annotations. Data exported from Quantumworks Lab can be used for a variety of purposes, such as model training or data enrichment. Given that every model has unique input requirements and organizations have unique data storage formats, the export process is often more unique than the import.\u003c/p\u003e\u003ch3 id=\"streamlining-data-out\"\u003eStreamlining Data Out\u003c/h3\u003e\u003cp\u003eThe Quantumworks Lab platform supports a variety of data formats and storage solutions to ensure that your data is exported in a format that suits your needs and is compatible with your storage system. Whether you're using BigQuery, Databricks, CSV, or other formats, Quantumworks Lab has a solution for you. Here's how you can navigate the process:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Choose an Export Format:\u003c/strong\u003e Determine the export format that suits your needs and is compatible with your storage system. Quantumworks Lab supports a wide variety of formats including JSON, CSV, and others. This gives you the flexibility to choose a format that best aligns with your downstream workflows.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Select a Connector:\u003c/strong\u003e Just as with data in, Quantumworks Lab offers connectors to aid in exporting data. These connectors are designed to seamlessly bridge the gap between Quantumworks Lab and your storage system, making the data out process efficient and hassle-free.  Quantumworks Lab offers connectors to aid in exporting data:\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor BigQuery users, this \u003ca href=\"https://github.com/Quantumworks Lab/labelbox-bigquery/tree/main?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e will come in handy\u003c/li\u003e\u003cli\u003eDatabricks users can refer to this \u003ca href=\"https://github.com/Quantumworks Lab/labelspark?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eFor CSV-formatted data, this \u003ca href=\"https://github.com/Quantumworks Lab/labelpandas?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e is useful\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Export your Data:\u003c/strong\u003e Initiate the export process via your chosen connector. During the export, Quantumworks Lab compiles your labeled data rows, associated metadata, attachments, and annotations, and organizes them in your chosen export format.\u003c/p\u003e\u003cp\u003eFor a deeper understanding of the process, our \u003ca href=\"https://docs.labelbox.com/reference/export-v2-glossary?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e provides a wealth of information.\u003c/p\u003e\u003cp\u003eIn essence, the Data I/O process is a crucial aspect of your interaction with Quantumworks Lab, designed to make your data management effortless. As you become more familiar with the process, you'll find it an essential tool in creating intelligent applications with Labelbox.\u003c/p\u003e\u003ch3 id=\"utilizing-your-exported-data\"\u003eUtilizing Your Exported Data\u003c/h3\u003e\u003cp\u003eOnce your data is exported, it is ready to be utilized for a variety of purposes:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Model Training:\u003c/strong\u003e The primary use case of exported data is to feed it into your machine learning models. The labeled data serves as the training data, guiding your models to recognize patterns and make predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Data Analysis \u0026amp; Enrichment:\u003c/strong\u003e The exported data, especially metadata and annotations, can provide valuable insights when analyzed. Additionally, it can enrich your existing data sets, enhancing the accuracy and detail of your data and leading to more effective models and analytics. This could guide decision-making processes and strategies within your organization.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Iterative Improvements:\u003c/strong\u003e In some cases, exported data can also be fed back into your annotation process for iterative improvements, creating a feedback loop that continually enhances your data quality and model performance.\u003c/p\u003e\u003cp\u003eIn conclusion, the Data I/O process in Quantumworks Lab is an integral component to successful application creation. This guide provides an in-depth understanding of the process, allowing you to accurately import and export data, whether that be data rows, metadata, attachments, or annotations. The process ensures compatibility with various data formats and storage systems, while facilitating efficient data management for your machine learning projects. \u003c/p\u003e\u003cp\u003eFor detailed instructions or further understanding, our comprehensive \u003ca href=\"https://docs.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e is always available.\u003c/p\u003e","comment_id":"64cab907306044000155752f","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078.png","featured":false,"visibility":"public","created_at":"2023-08-02T20:13:59.000+00:00","updated_at":"2023-10-27T17:13:05.000+00:00","published_at":"2023-08-03T13:44:19.000+00:00","custom_excerpt":"In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guide/a-guide-to-the-data-i-o-process-in-labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/a-guide-to-the-data-i-o-process-in-labelbox/","excerpt":"In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"A guide to the Data I/O process in Quantumworks Lab","meta_description":"In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6478fbf1f06ecd0001d70bc5","uuid":"506ee668-bb83-4bc8-80b1-8c9b4c76e759","title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling","slug":"using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWhile Yolov8 is no longer supported on Quantumworks Lab, this blog remains relevant if you are working with other object detection models. Alternatives such as OWL-ViT, Rekognition, GroundingDINO, and GroundingDINO + SAM can still be found and used on Quantumworks Lab’s platform.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this guide, we will demonstrate the application of foundation models, such as Meta’s Segment Anything and YOLOv8, to automatically detect, classify and draw masks on objects of interest in a video. This is a follow-up to earlier guide: \u003ca href=\"https://labelbox.com/guides/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/?ref=labelbox-guides.ghost.io\"\u003eUsing Meta’s Segment Anything with YOLOv8 to Automatically Classify Masks\u003c/a\u003e. In this guide, we’ll automatically detect and segment objects in a video.\u003c/p\u003e\u003cp\u003eVideos have many frames and are tedious to label. Segmentation masks are even more time consuming to label as they vary ever so slightly frame-by-frame, requiring manual fine-tuning each time. With foundation models, you can automate and significantly speed up the labeling process to label more video data, in less time. This allows you to focus valuable time on review, simply correcting the AI models’ output.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"498\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we will be walking through a simple semantic segmentation task: drawing masks around a person as they skateboard.\u003c/p\u003e\u003cp\u003eHere’s a high-level summary of the process that we will be walking through step-by-step below, with code:\u003c/p\u003e\u003cp\u003e1) Load YOLOv8, SAM and Quantumworks Lab Python SDK\u003c/p\u003e\u003cp\u003e2) For each frame of the video:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun an object detector to generate bounding boxes with classifications for specified classes\u003c/li\u003e\u003cli\u003eFeed the bounding boxes as inputs to Meta’s Segment Anything model which will produce segmentation masks\u003c/li\u003e\u003cli\u003ePrepare mask predictions in a format that Quantumworks Lab Python SDK expects\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e3) Upload all frames at once to Quantumworks Lab via prediction import\u003c/p\u003e\u003cp\u003e4) Open up video editor and review or modify the pre-labels as you usually do\u003c/p\u003e\u003cp\u003eYou can run all of the above out-of-the-box on your video(s) using our \u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/sam/meta_sam_labelbox_video.ipynb?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e. Simply load in your video and get automatically segmented masks, with classes in Quantumworks Lab, in minutes!\u003c/p\u003e\u003cp\u003eFor this guide, we will use the following video:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-orig.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-orig.gif 600w\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-load-yolov8\"\u003eStep 1: Load YOLOv8\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.ultralytics.com/?ref=labelbox-guides.ghost.io\"\u003eYOLOv8\u003c/a\u003e is a state-of-the-art object detector that produces bounding boxes and classes around common objects. It's the latest iteration of the YOLO (You Only Look Once) family of models, and it boasts some impressive features. YOLOv8 is known for its speed and accuracy, making it an invaluable tool for a wide range of applications. Here, we use YOLOv8 to automatically detect and localize the person skateboarding in the video.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport ultralytics\nultralytics.checks()\nfrom ultralytics import YOLO\nmodel = YOLO(f'{HOME}/yolov8n.pt')\n\n# each class id is assigned a different color\ncolors = np.random.randint(0, 256, size=(len(model.names), 3))\nprint(model.names)\n\n# Specify which classes you care about. The rest of classes will be filtered out.\nchosen_class_ids = [0] # 0 refers to person, as per model.names\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-2-load-sam\"\u003eStep 2: Load SAM\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eMeta's SAM model\u003c/a\u003e is a state-of-the-art computer vision model that is designed to accurately segment images and videos into distinct objects. Using advanced deep learning techniques, Segment Anything is able to identify and segment objects in images, making it a powerful tool for a wide range of applications. The SAM model is able to generate segmentation masks based on prompts, including bounding box prompts, which we will use in the code below.\u003c/p\u003e\u003cp\u003e\u003cbr\u003eFor an in-editor experience of SAM, please read our other blog post \u003ca href=\"https://labelbox.com/blog/coming-soon-auto-segment-powered-by-sam/?ref=labelbox-guides.ghost.io\"\u003eAuto-Segment 2.0 powered by Meta’s Segment Anything Model\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport torch\nimport matplotlib.pyplot as plt\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nsam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\nmask_predictor = SamPredictor(sam)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-3-load-labelboxs-python-sdk\"\u003eStep 3: Load Quantumworks Lab's Python SDK\u003c/h2\u003e\u003cp\u003eLabelbox’s Python SDK gives you easy methods to create ontologies, projects and datasets, and upload masks to a video.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab as lb\nimport labelbox.types as lb_types\n\n# Create a Quantumworks Lab API key for your account by following the instructions here:\n# https://docs.labelbox.com/reference/create-api-key\n# Then, fill it in here\nAPI_KEY = \"\"\nclient = lb.Client(API_KEY)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-4-run-yolov8-and-sam-per-frame\"\u003eStep 4: Run YOLOv8 and SAM per-frame\u003c/h2\u003e\u003cp\u003eHere we run the models on each frame and generate masks automatically.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003ecap = cv2.VideoCapture(VIDEO_PATH)\n\n# This will contain the resulting mask predictions for upload to Quantumworks Lab\nmask_frames = []\n\nframe_num = 1\nwhile cap.isOpened():\n  ret, frame = cap.read()\n  if not ret:\n    break\n\n  # Run frame through YOLOv8 to get detections\n  detections = model.predict(frame, conf=0.7)\n \n  # Run frame and detections through SAM to get masks\n  transformed_boxes = mask_predictor.transform.apply_boxes_torch(detections[0].boxes.xyxy, list(get_video_dimensions(cap)))\n  mask_predictor.set_image(frame)\n  masks, scores, logits = mask_predictor.predict_torch(\n    boxes = transformed_boxes,\n    multimask_output=False,\n    point_coords=None,\n    point_labels=None\n  )\n\n  # Combine mask predictions into a single mask, each with a different color\n  class_ids = detections[0].boxes.cpu().cls\n  merged_with_colors = add_color_to_mask(masks[0][0], colors[int(class_ids[0])]).astype(np.uint8)\n  for i in range(1, len(masks)):\n    curr_mask_with_colors = add_color_to_mask(masks[i][0], colors[int(class_ids[i])])\n    merged_with_colors = np.bitwise_or(merged_with_colors, curr_mask_with_colors)\n\n  # Upload multi-colored combined mask to temp location\n  # to get temp instance uri\n  instance_uri = get_instance_uri(client, global_key, merged_colored_mask)\n\n  # Create MaskFrame object to be uploaded to Quantumworks Lab\n  mask_frame = lb_types.MaskFrame(index=frame_num, instance_uri=instance_uri)\n  mask_frames.append(mask_frame)\n\n  frame_num += 1\n\ncap.release()\n\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-bboxes.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-bboxes.gif 600w\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-masks.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-masks.gif 600w\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-5-upload-the-predicted-masks-as-pre-labels-onto-labelbox\"\u003eStep 5: Upload the predicted masks as pre-labels onto Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThe predicted masks can be easily and seamlessly integrated into Quantumworks Lab via our SDK.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Create MaskInstance per unique class predicted / chosen\ninstances = []\n for cid in chosen_class_ids:\n   color = get_color(colors[int(cid)])\n   name = model.names[int(cid)]\n   instances.append(lb_types.MaskInstance(color_rgb=color, name=name))\n\n# Create list of VideoMaskAnnotation objects, one for each unique class\nannotations = []\nfor instance in instances:\n  video_mask_annotation = lb_types.VideoMaskAnnotation(\n       frames=mask_frames,\n       instances=[instance]\n   )\n  annotations.append(video_mask_annotation)\n\n# Create Label object\nlabels = [\nlb_types.Label(data=lb_types.VideoData(global_key=global_key),\n                  annotations=annotations))\n]\n\n# Run import job\nupload_job = lb.MALPredictionImport.create_from_objects(\n   client=client,\n   project_id=project.uid,\n   name=\"mal_import_job\" + str(uuid.uuid4()),\n   predictions=labels\n)\nupload_job.wait_until_done()\n\nprint(f\"Errors: {upload_job.errors}\", )\nprint(f\"Status of uploads: {upload_job.statuses}\")\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-LB.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"374\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-LB.gif 600w\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eCreating segmentation masks on video data can be tedious and time-consuming. Using the power of foundation models in Quantumworks Lab, you can easily generate masks with classifications in a matter of minutes. Rather than spending hours labeling video data, you now have a way to accelerate video labeling and not only reduce time to market, but also the cost of developing your models.\u003c/p\u003e","comment_id":"6478fbf1f06ecd0001d70bc5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074.png","featured":false,"visibility":"public","created_at":"2023-06-01T20:13:37.000+00:00","updated_at":"2024-11-25T21:18:27.000+00:00","published_at":"2023-06-01T22:14:48.000+00:00","custom_excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/","excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074-1.png","og_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","og_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074-2.png","twitter_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","twitter_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","meta_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","meta_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"632ccf4016e912003d39b2a7","uuid":"fc620d97-ea51-468e-ae87-a99b87ca45b8","title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","slug":"using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWhile YOLOv8 is no longer supported on Quantumworks Lab, this blog remains relevant if you are working with other object detection models. Alternatives such as OWL-ViT, Rekognition, GroundingDINO, and GroundingDINO + SAM are fully supported on the Quantumworks Lab platform.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this guide, you will learn how to chain \u003ca href=\"https://labelbox.com/solutions/computer-vision/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision\u003c/a\u003e foundation models together to automatically populate pre-labels with class in Quantumworks Lab very quickly. We will be walking through a simple semantic segmentation task: drawing masks around all objects of a particular class in an image.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1086\" height=\"332\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png 1086w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeveraging chained foundation models in Quantumworks Lab will greatly reduce the time it takes you or your team to draw segmentation masks; by augmenting the recently-released \u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eSAM\u003c/a\u003e model with classifications, you will automate the task of assigning classes. Rather than performing a tedious labeling effort, you can focus your valuable efforts reviewing, verifying, and correcting labels drawn by AI models.\u003c/p\u003e\u003cp\u003eHere’s a high-level summary of the process that we will be walk through step-by-step below:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun an object detector on the image to generate bounding boxes with classifications for specified classes\u003c/li\u003e\u003cli\u003eFeed the bounding boxes as inputs to Meta’s Segment Anything model which will produce segmentation masks for each one\u003c/li\u003e\u003cli\u003eUpload the mask predictions onto Quantumworks Lab as pre-labels\u003c/li\u003e\u003cli\u003eOpen up image editor and review or modify the pre-labels as you usually do\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can run all of the above out-of-the-box on your image(s) using our \u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/develop/examples/integrations/sam/meta_sam_labelbox.ipynb?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e. Simply load the images and automatically get segmented masks, with classes, in just a few minutes. \u003c/p\u003e\u003cp\u003eFor this guide, we will use the following image of a lot of colorful chairs: \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"896\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-run-yolov8-on-the-image\"\u003eStep 1: Run YOLOv8 on the image\u003c/h2\u003e\u003cp\u003eThe latest iteration of the YOLO (You Only Look Once) family of models, \u003ca href=\"https://docs.ultralytics.com/?ref=labelbox-guides.ghost.io\"\u003eYOLOv8\u003c/a\u003e is an object detector that produces bounding boxes and classes around common objects. Known for its speed and accuracy, YOLOv8 boasts some impressive features – making it an invaluable tool for a wide range of applications. Here, we use YOLOv8 to automatically detect and localize all the chairs in the image.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# load the YOLOv8 model\nmodel = YOLO(f'{HOME}/yolov8n.pt')\n\n# run the model on the image\nresults = model.predict(source='chairs.jpg', conf=0.25)\npredicted_boxes = results[0].boxes.xyxy\n\n\n# read in the image for visualization\nimage_bgr = cv2.imread(IMAGE_PATH, cv2.IMREAD_COLOR)\n\n# use cv2 to visualize the bounding boxes on the image\nfor box in predicted_boxes:\n cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\ncv2.imshow(\"YOLOv8 predictions\", image_bgr)\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"896\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-2-feed-bounding-boxes-as-inputs-to-meta%E2%80%99s-sam-model\"\u003eStep 2: Feed bounding boxes as inputs to Meta’s SAM model\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eSAM (Segment Anything Model)\u003c/a\u003e – recently released by Meta AI, is an advanced computer vision model designed to accurately segment images and videos into distinct objects. Using advanced deep learning techniques, SAM is able to identify and segment objects in images, making it a powerful tool for a wide range of applications. The SAM model is able to generate segmentation masks based on prompts, including bounding box prompts, which we will use in the code below.\u003c/p\u003e\u003cp\u003eTo see an in-editor experience of SAM, please check out our blog post \u003ca href=\"https://labelbox.com/blog/coming-soon-auto-segment-powered-by-sam/?ref=labelbox-guides.ghost.io\"\u003eAuto-Segment 2.0 powered by Meta’s Segment Anything Model\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# load the SAM model\nsam = sam_model_registry[\"vit_h\"](checkpoint=\"/sam_vit_h_4b8939.pth\n\").to(device=torch.device('cuda:0'))\n\nmask_predictor = SamPredictor(sam)\n\n# transform the YOLOv8 predicted boxes to match input format expected by SAM model\ntransformed_boxes = mask_predictor.transform.apply_boxes_torch(predicted_boxes, image_bgr.shape[:2])\n\n\n# run SAM model on all the boxes\nmask_predictor.set_image(image_bgr)\nmasks, scores, logits = mask_predictor.predict_torch(\n   boxes = transformed_boxes,\n   multimask_output=False,\n   point_coords=None,\n   point_labels=None\n)\n\n# combine all masks into one for easy visualization\nfinal_mask = None\nfor i in range(len(masks) - 1):\n  if final_mask is None:\n    final_mask = np.bitwise_or(masks[i][0], masks[i+1][0])\n  else:\n    final_mask = np.bitwise_or(final_mask, masks[i+1][0])\n\n# visualize the predicted masks\nplt.figure(figsize=(10, 10))\nplt.imshow(image_rgb)\nplt.imshow(final_mask, cmap='gray', alpha=0.7)\nplt.show()\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1daa38c9-1485-4ca8-8c03-57f6a87b9cc7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"794\" height=\"454\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-1daa38c9-1485-4ca8-8c03-57f6a87b9cc7.png 600w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1daa38c9-1485-4ca8-8c03-57f6a87b9cc7.png 794w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-3-upload-the-predicted-masks-as-pre-labels-onto-labelbox\"\u003eStep 3: Upload the predicted masks as pre-labels onto Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThe predicted masks can be easily and seamlessly integrated into Quantumworks Lab via our SDK. The upload is just a few lines of code that run in less than a minute.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass_names = []\nfor c in results[0].boxes.cls:\n class_names.append(model.names[int(c)])\n\nannotations = []\nfor mask in masks:\n  # convert a 2D array to 3D array\n  mask_data = lb_types.MaskData.from_2D_arr(np.asarray(mask[0], dtype=\"uint8\"))\n  mask_annotation = lb_types.ObjectAnnotation(\n    name = class_names[idx], # assign class from Step 1\n    value=lb_types.Mask(mask=mask_data, color=color),\n  )\n  annotations.append(mask_annotation)\n\nlabels = [\nlb_types.Label(data=lb_types.ImageData(global_key=\"image_name\"),annotations=annotations)\n]\nupload_job = lb.MALPredictionImport.create_from_objects(\n   client=client,\n   project_id=project.uid,\n   name=\"mal_job\" + str(uuid.uuid4()),\n   predictions=labels\n)\nupload_job.wait_until_done()\n\nprint(f\"Errors: {upload_job.errors}\", )\nprint(f\"Status of uploads: {upload_job.statuses}\")\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"815\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"final-thoughts-on-using-meta%E2%80%99s-sam-model-with-yolov8-to-automatically-classify-masks\"\u003eFinal thoughts on using Meta’s SAM model with YOLOv8 to automatically classify masks\u003c/h2\u003e\u003cp\u003eWhile Meta’s AI's SAM is really powerful at segmentation, it leaves out the crucial task of classification. In this guide, we demonstrated how you can use YOLOv8 (or another object detector) to generate bounding boxes with classes and then automatically apply those classes to the masks generated by SAM. We also showed how this seamlessly integrates with the Quantumworks Lab Model Assisted Labeling SDK.\u003c/p\u003e\u003cp\u003eIf you are interested in applying SAM on images through our image editor, you can \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003esign up\u003c/a\u003e for a Quantumworks Lab account and give it a try today. \u003c/p\u003e","comment_id":"632ccf4016e912003d39b2a7","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3062.png","featured":false,"visibility":"public","created_at":"2022-09-22T21:10:24.000+00:00","updated_at":"2024-11-26T18:38:02.000+00:00","published_at":"2023-05-09T18:26:38.000+00:00","custom_excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/","excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","og_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3062-2.png","twitter_title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","twitter_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","meta_title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","meta_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"645a7494225a9a00012fcfae","uuid":"e07975bd-27c6-4c00-bf8c-bc96c772ce56","title":"How to accelerate image-text pair generation with BLIP-2","slug":"how-to-accelerate-image-text-pair-generation-with-blip-2","html":"\u003cp\u003e\u003ca href=\"https://labelbox.com/solutions/generative-ai/?ref=labelbox-guides.ghost.io\"\u003eGenerative AI\u003c/a\u003e has taken the world by storm, opening doors to a plethora of applications, from creating realistic images and videos to generating novel text and music. The success of these applications often hinges on the \u003ca href=\"https://labelbox.com/blog/data-quality-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003equality and quantity of data used to train the underlying machine learning models\u003c/a\u003e, the production of which is often time consuming and costly. As a result, leading AI teams have been innovating on ways to streamline the caption creation process and empower human annotators to work more efficiently without sacrificing quality.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2301.12597?ref=labelbox-guides.ghost.io\"\u003eBLIP-2\u003c/a\u003e (Bootstrapping Language-Image Pre-training) is an AI model that can perform various multi-modal tasks like visual question answering, image-text retrieval (image-text matching) and image captioning. It can analyze an image, understand its content, and generate a relevant and concise caption. BLIP-2 helps language models understand images without changing their original structure. It does this by using querying transformer (q-former) that acts as a bridge between the image and the language model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"527\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSee the original flowchart as published in the \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2301.12597.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBLIP-2 research paper\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBLIP-2 achieves state-of-the-art performance on various vision-language tasks while being more compute efficient than existing methods. Powered by \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox-guides.ghost.io\"\u003eLarge Language Models (LLMs)\u003c/a\u003e, it can perform \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot image-to-text generation\u003c/a\u003e based on natural language instructions, enabling capabilities like visual knowledge reasoning and visual conversation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"898\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExamples of images and their BLIP 2 captions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, we'll explore how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions. Additionally, you can use any model to make pre-labels in Quantumworks Lab as shown \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e. Quantumworks Lab customers using \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003emodel-assisted labeling\u003c/a\u003e have seen 50-70% reductions in labeling costs driven by dramatic reductions in labeling time and complexity. Therefore, using a model like BLIP-2 will further reduce labeling time.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/BLIP-2-lucidchart--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1340\" height=\"420\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/BLIP-2-lucidchart--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/BLIP-2-lucidchart--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/BLIP-2-lucidchart--1-.png 1340w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHow to use BLIP-2 with Quantumworks Lab\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"step-1-create-a-project-and-attach-an-ontology\"\u003eStep 1: Create a project and attach an ontology.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eproject = client.create_project(name = \"BLIP project\", media_type=labelbox.MediaType.Image)\nproject.setup_editor(ontology)\nontology_from_project = labelbox.OntologyBuilder.from_project(project)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-initialize-and-load-a-pre-trained-blip-2-model\"\u003eStep 2: Initialize and load a pre-trained BLIP-2 model.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003edevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n   \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n)\nmodel.to(device)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-3-collect-inferences-to-be-used-as-pre-labels\"\u003eStep 3: Collect inferences to be used as pre-labels.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003equeued_data_rows = project.export_queued_data_rows()\nground_truth_list = list()\n\n\nfor data_row in queued_data_rows:\n url = data_row[\"rowData\"]\n image = Image.open(requests.get(url, stream=True).raw)\n inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n generated_ids = model.generate(**inputs, max_new_tokens=30)\n generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n print(generated_text) \n  text_annotation = labelbox.data.annotation_types.ClassificationAnnotation(\n     name=\"BLIP model prediction\",\n     value=labelbox.data.annotation_types.Text(answer = generated_text)\n   )\n  ground_truth_list.append(Label(\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-4-upload-pre-labels-to-your-project\"\u003eStep 4: Upload pre-labels to your project.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eupload_task = labelbox.MALPredictionImport.create_from_objects(client, project.uid, str(uuid.uuid4()), ground_truth_list)\nupload_task.wait_until_done()\nprint(upload_task.errors)\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1132\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of asset with a pre-label and no human-generated caption\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/unknown.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"893\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/unknown.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/unknown.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/unknown.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of an asset with a pre-label and a human-generated caption\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"review-labels\"\u003eReview labels\u003c/h2\u003e\u003cp\u003eAfter the labels have been annotated, you can use \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e to create highly customizable, multi-step review pipelines, making your review process more efficient and automated. Workflows offer granular control over how your data rows get reviewed, saving you both time and resources. You can create tasks that enable you to filter based on who created the label, what annotations exist and when the label was created.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/unknown--1--2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"804\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/unknown--1--2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/unknown--1--2.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/unknown--1--2.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLabelbox lets you customize labeling and review workflows to your exact requirements.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"export-labels\"\u003eExport labels\u003c/h2\u003e\u003cp\u003eAfter you are done reviewing the labels, you can easily export the annotations as show \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-JSON\"\u003e         \"annotations\": {\n             \"objects\": [],\n             \"classifications\": [\n               {\n                 \"feature_id\": \"clhdn79ae0ent076c4h579rxu\",\n                 \"name\": \"BLIP model prediction\",\n                 \"text_answer\": {\n                   \"content\": \"a yellow flower with a green background\"\n                 }\n               }\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eBy using captions generated by BLIP-2 or inferences by AI models as pre-labels, AI teams can significantly reduce labeling time and costs. To learn more, explore this \u003ca href=\"https://colab.research.google.com/drive/1vnD4gVBu8uAE3dn44h8qIXx3R_DYv5E2?ref=labelbox-guides.ghost.io#scrollTo=PcfsMaUu1GrV\"\u003efull script\u003c/a\u003e for using the BLIP-2 model to generate pre-labels. These captions can then be amended or approved in Quantumworks Lab by labelers. You can also learn more about the BLIP-2 model \u003ca href=\"https://huggingface.co/docs/transformers/main/model_doc/blip-2?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e and model-assisted labeling \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"645a7494225a9a00012fcfae","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Guide_BLIP-2.png","featured":false,"visibility":"public","created_at":"2023-05-09T16:28:04.000+00:00","updated_at":"2023-10-27T17:11:22.000+00:00","published_at":"2023-05-09T17:09:47.000+00:00","custom_excerpt":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-accelerate-image-text-pair-generation-with-blip-2/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-accelerate-image-text-pair-generation-with-blip-2/","excerpt":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"How to accelerate image-text pair generation with BLIP-2","og_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","twitter_image":null,"twitter_title":"How to accelerate image-text pair generation with BLIP-2","twitter_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","meta_title":"How to accelerate image-text pair generation with BLIP-2 | Quantumworks Lab","meta_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so a specialized workforce can further improve the image captions.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64590b32638f810001454877","uuid":"25f18d3c-fa4c-4206-a4c2-db2f9c6ba390","title":"Automatically label text with 96%+ accuracy using foundation models","slug":"automatically-label-text-with-96-accuracy-using-foundation-models","html":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, you’ll learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab and foundation models. We will be walking through a \u003ca href=\"https://labelbox.com/product/annotate/text/?ref=labelbox-guides.ghost.io\"\u003etext classification\u003c/a\u003e task: identifying news articles that talk about sports. \u003c/p\u003e\u003cp\u003eHere's a high-level summary of the process:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConnect your data to Quantumworks Lab with just a few lines of code.\u003c/li\u003e\u003cli\u003eLabelbox leverages foundation models to automatically enrich your data.\u003c/li\u003e\u003cli\u003eUse the powerful search capabilities of Quantumworks Lab to quickly find articles with similar traits and classify them in one click. With the help of foundation models, you can instantaneously label large amounts of data. \u003cem\u003ePro tip: Combine a variety of search techniques, such as a similarity search, natural language search, keyword search, and investigate clusters of similar data, to boost your results.\u003c/em\u003e\u003c/li\u003e\u003cli\u003eWhile foundation models are a helpful starting point, they might not always correctly classify data, especially on challenging or rare data points. In this case, utilize human-in-the-loop labeling and QA by pre-labeling data using foundation models and sending it for your internal or external labeling team to review.\u003c/li\u003e\u003cli\u003eAutomatically apply these rules to all new incoming data by creating a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e in Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, let’s take a look at how we can do the above in Labelbox. As a sneak peek into the process, by leveraging foundation models, we managed to \u003cstrong\u003eclassify 88% of our news articles in minutes with a 96.5% accuracy rate\u003c/strong\u003e. An additional 15% of our news articles were successfully pre-labeled using foundation models, with 85% accuracy, and sent for human review. This left us with only 493 data points that were missed by foundation models – a massive efficiency gain for any labeling team.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"step-1-connect-your-data-to-labelbox-with-a-few-lines-of-code\"\u003eStep 1: Connect your data to Quantumworks Lab with a few lines of code\u003c/h2\u003e\u003cp\u003eSince this is a classification task, our goal is to correctly have the model identify news articles about sports. We will be using the following Hugging Face dataset: \u003ca href=\"https://huggingface.co/datasets/ag_news?ref=labelbox-guides.ghost.io\"\u003eag_news\u003c/a\u003e which contains 120,000 articles, including 30,000 about sports, for our analysis.\u003c/p\u003e\u003cp\u003eTo begin, let's connect this data to Labelbox. Simply retrieve the dataset from Hugging Face and integrate it with Quantumworks Lab in just a few lines of code.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom datasets import load_dataset\ndataset = load_dataset(\"ag_news\",split='train')\n\n# iterate over the data\npayload_imgs = []\ncounter = 0\n\n\n# iterate over the data\npayloads = []\nglobal_keys = []\ncounter = 0\n\nfor data in dataset:\n\n  text = data['text']\n  label = data['label']\n  global_key = \"ag_news_\" + str(counter)\n  global_keys.append(global_key)\n\n  # create payload for texts\n  payloads.append({\n    \"row_data\": text, \n    \"global_key\": global_key,\n  })\n\n  counter += 1\n\n\n# create dataset in Quantumworks Lab\nlb_dataset = client.create_dataset(name=\"ag_news\") \n\n# add data in Quantumworks Lab\ntask = lb_dataset.create_data_rows(payloads)  \ntask.wait_till_done() # async\nprint(task.errors) # check errors \u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-leverage-foundation-models-to-instantly-enhance-your-data\"\u003eStep 2: Leverage foundation models to instantly enhance your data\u003c/h2\u003e\u003cp\u003eLabelbox will automatically compute and store MPNet embeddings for your data. We are using \u003ca href=\"https://huggingface.co/sentence-transformers/all-mpnet-base-v2?ref=labelbox-guides.ghost.io\"\u003ethis implementation\u003c/a\u003e available through Hugging Face. \u003c/p\u003e\u003cp\u003eOnce your data has been uploaded, watch as Quantumworks Lab enriches your data with foundation model embeddings. These embeddings are powerful in that they can be harnessed to automatically label, or pre-label, your data.  \u003c/p\u003e\u003cp\u003eIf you don’t want to use the default embeddings by Quantumworks Lab, you can also \u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003eupload custom embeddings\u003c/a\u003e from any other foundation model, with up to 100 custom embeddings for each data point. \u003c/p\u003e\u003cp\u003eWhether you’re using the default or custom embeddings, embeddings are helpful in curating and finding subsets of data that share similar characteristics. For instance, embeddings power \u003ca href=\"https://labelbox.com/blog/how-vector-similarity-search-works/?ref=labelbox-guides.ghost.io\"\u003eLabelbox’s similarity search\u003c/a\u003e, natural language search, and 2D projector view. You can search and explore all of your data with tools that help you powerfully surface specific subsets of similar texts.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"927\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-3-use-powerful-search-capabilities-to-quickly-find-data\"\u003eStep 3: Use powerful search capabilities to quickly find data\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities in Quantumworks Lab, you can easily find and classify data that share similar characteristics. This is a special case of \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot and few-shot learning\u003c/a\u003e: the challenge is to find all examples of sports articles, based on zero (or a few) examples. With the help of foundation models, and minimal human signal, you can quickly label a lot of data in just a few clicks. The following are tools in Quantumworks Lab that help provide labeling signal to make it easy to automatically classify your data:\u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-projector-view-for-classification\"\u003eZero-shot Labeling: Projector View for Classification\u003c/h3\u003e\u003cp\u003eLabelbox allows you to visualize data clusters in 2D. For this example, we can see distinct clusters. By inspecting a few examples, we discover that some of the data clusters correspond to sports news articles. We manually select each cluster and tag it with \"UMAP: sports. We intentionally leave out data points situated between clusters, as these represent challenging data points. This is expected since each labeling function won’t be perfect in isolation, and some data points are difficult and challenging.\u003c/p\u003e\u003cp\u003eWe then repeat the process with t-SNE instead of UMAP and tag each sports cluster with \"t-SNE: sports\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"569\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUMAP: a cluster of news articles about sports\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"705\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003et-SNE: a subcluster of news articles about basketball\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"zero-shot-labeling-keyword-search\"\u003eZero-shot Labeling: Keyword search\u003c/h3\u003e\u003cp\u003eLabelbox enables you to search all data points that contain some keywords. We filter all data points that contain the following keywords\u003cem\u003e: sport, sports, basketball, baseball, soccer, football, tennis, hockey\u003c/em\u003e. And tag these 5,990 texts as “Keyword search: sports”. \u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-natural-language-search-for-classification\"\u003eZero-shot Labeling: Natural language search for classification\u003c/h3\u003e\u003cp\u003eLabelbox enables you to conduct natural language searches on text. For example you can type in “news articles about sports” to surface all pieces of text about sports. Adjusting the similarity threshold will narrow the search to only relevant articles. For this use case, we filter for a similarity score higher than 0.85 and tag all of the 6,468 texts as “Natural language search: sports”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"922\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA natural language search will surface thousands of news articles about sports. By adjusting the similarity score, we can keep the most confident zero-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"few-shot-labeling-similarity-search-for-classification\"\u003eFew-shot Labeling: Similarity search for classification\u003c/h3\u003e\u003cp\u003eLabelbox also streamlines few-shot labeling. Quickly browse all your data in Catalog to surface 5 news articles about sports. For each of them, run a similarity search and tag the top results (e.g with a similarity score higher than 0.85) as “Similarity search: sports”. This provides us with 5 new labeling functions that surface sports news articles.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"967\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA similarity search example with an anchor article about college basketball. We can filter to keep the most confident few-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"combining-different-sources-of-signal-weak-labeling\"\u003eCombining different sources of signal: weak labeling\u003c/h3\u003e\u003cp\u003eWhile each of these labeling signals is powerful on its own, you can combine multiple sources in Labelbox. This allows you to apply simple rules in a weak supervision fashion to further enhance your results. Integrate different labeling signals, such as similarity searches, natural language searches, and data clusters, to boost your outcomes. You can \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003ecombine various filters\u003c/a\u003e by using the AND and OR functions.\u003c/p\u003e\u003ch2 id=\"step-4-automatically-classify-data-with-foundation-models-and-use-human-in-the-loop-qa-for-challenging-cases\"\u003eStep 4: Automatically classify data with foundation models and use human-in-the-loop QA for challenging cases\u003c/h2\u003e\u003ch3 id=\"high-confidence-data-points-direct-classification\"\u003eHigh confidence data points: direct classification\u003c/h3\u003e\u003cp\u003eFoundation models are highly confident about most data points. So much so that we can directly classify data points leveraging Quantumworks Lab’s \u003ca href=\"https://labelbox.com/blog/pre-label-and-enrich-your-data-with-bulk-classifications/?ref=labelbox-guides.ghost.io\"\u003ebulk classification feature\u003c/a\u003e. With this new feature, you can specify and send your texts to a specific step of the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox.ghost.io#how-it-works\"\u003elabeling and review workflow\u003c/a\u003e. We can directly move these high-confidence data points straight to the “Done” task.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe classify thousands of texts in bulk, and send them to the “Done” task of our labeling project, in just a click, since foundation models are confident on those.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these high-confident data points are those that belong to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe sports clusters, both with UMAP and t-SNE. But just how accurate are these predictions? To answer this question, we looked at the Hugging Face ground truths. 704 out of the 21,560 sports predictions are incorrect.\u003c/li\u003e\u003cli\u003eOr, the similarity search score to two or more anchors is higher than 0.85. This results in 3,548 sports classifications, all of which are accurate except 185.\u003c/li\u003e\u003cli\u003eThe sports cluster in UMAP or t-SNE and a natural language search higher than 0.85. This results in 1,219 sports classifications, all of which are accurate except 58. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis method of surfacing high-confident data points enables us to directly classify 26,427 pieces of text - with only 947 errors - achieving an \u003cstrong\u003eaccuracy of 96.5%.\u003c/strong\u003e Since 26,427 out of 30,000 sports articles have been classified directly by foundation models, the \u003cstrong\u003ecoverage is 88%.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhat about the 947 errors? Upon closer inspection of these errors, it turns out that they are all related to sports, but in the context of News, World, or Science, and hence have been labeled on Hugging Face according to those categories instead of Sports.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"952\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFoundation models failed on 947 news articles. It turns out that these articles are all related to sports, but are classified on HuggingFace as World news, or Business news, or Science \u0026amp; Tech news.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eNow, let’s move on to classify the remaining 12% of data rows, on which foundation models are less confident.\u003c/p\u003e\u003ch3 id=\"low-confidence-data-points-human-in-the-loop-labeling\"\u003eLow confidence data points: Human-in-the-Loop labeling\u003c/h3\u003e\u003cp\u003eFor some pieces of text, foundation models exhibit low confidence. We can bulk classify these data points in Quantumworks Lab, but move them to the “To Review” task. This will ensure a human is looped in and will review the classifications coming from foundation models.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe pre-label thousands of data points in bulk, and send them to “Review” in our labeling project, in just a click, since foundation models are moderately confident on these pieces of text.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these data points are those that belong to the sports cluster, with UMAP or t-SNE, and that haven’t been classified yet.\u003c/p\u003e\u003cp\u003eUsing this approach, we managed to classify 4,723 additional data rows, with an \u003cstrong\u003eaccuracy of 85%\u003c/strong\u003e (696 errors). We can send these low-confident data rows for Human-in-the-Loop review. \u003c/p\u003e\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eDirect classification with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eHuman in the loop with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eSports articles missed by foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of data rows classified\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e26,427\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e4,723\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e493\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of errors\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e947\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e696\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e-\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAccuracy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e96.5%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e85%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e-\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eFraction of sports articles\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e88%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e15%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e1.8%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 26,427 pieces of text (88%) in minutes, with \u003cstrong\u003e96.5% accuracy\u003c/strong\u003e thanks to foundation models. An additional 4,723 data points (13.5%) have been pre-labeled with foundation models, with \u003cstrong\u003e85% accuracy\u003c/strong\u003e, and sent for human review. This leaves only 493 data points talking about sports, missed by foundation models.\u003c/p\u003e\u003ch2 id=\"step-5-set-it-and-forget-it-%E2%80%93-automatically-apply-these-rules-to-fresh-incoming-data\"\u003eStep 5: Set it and forget it – automatically apply these rules to fresh, incoming data\u003c/h2\u003e\u003cp\u003eWith Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e, we automatically classify fresh, incoming news articles about sports. \u003c/p\u003e\u003cp\u003eFor example, we can set up a slice that automatically surfaces all new pieces of text, that have been connected to Quantumworks Lab in the past week, that haven’t been classified yet. We can set the slice's criteria to include only text data rows where the natural language search for the prompt is \"news articles about sports\" and is higher than 0.85 (since we know that these data rows are very likely to be on sports). \u003c/p\u003e\u003cp\u003eWith slices, you can easily surface and inspect any new, high-impact data that gets added to your data lake.\u003c/p\u003e\u003cp\u003eFrom there, it only takes one click to classify all of these text data rows as sports articles. \u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 26,427 pieces of text (88%) in minutes, with \u003cstrong\u003e96.5% accuracy\u003c/strong\u003e thanks to foundation models. An additional 4,723 data points (15%) have been pre-labeled with foundation models, with \u003cstrong\u003e85% accuracy\u003c/strong\u003e, and sent for human review. This leaves only 493 data points talking about sports, missed by foundation models.\u003c/p\u003e\u003chr\u003e\u003cp\u003eIf you’re a current Quantumworks Lab user who wants to leverage any foundation model to supercharge your data labeling process in just a few clicks,\u003ca href=\"https://app.labelbox.com/catalog?ref=labelbox-guides.ghost.io\"\u003e try our bulk classification feature today\u003c/a\u003e or \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started with a free Quantumworks Lab account\u003c/a\u003e.\u003cbr\u003eIf you’re interested in seeing how quickly you can label images leveraging foundation models, check out our guide on \u003ca href=\"https://labelbox.com/guides/automatically-label-images-with-99-accuracy-using-foundation-models/?ref=labelbox-guides.ghost.io\"\u003ehow to automatically label images with 99% accuracy.\u003c/a\u003e\u003c/p\u003e","comment_id":"64590b32638f810001454877","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1-.png","featured":false,"visibility":"public","created_at":"2023-05-08T14:46:10.000+00:00","updated_at":"2023-10-27T17:11:48.000+00:00","published_at":"2023-05-08T20:43:24.000+00:00","custom_excerpt":"Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/automatically-label-text-with-96-accuracy-using-foundation-models","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/automatically-label-text-with-96-accuracy-using-foundation-models/","excerpt":"Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","reading_time":9,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1--2.png","og_title":"Automatically label text with 96%+ accuracy using foundation models","og_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1--1.png","twitter_title":"Automatically label text with 96%+ accuracy using foundation models","twitter_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","meta_title":"Automatically label text with 96%+ accuracy using foundation models","meta_description":"Automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","email_subject":null,"frontmatter":null,"feature_image_alt":"Automatically label text with 96%+ accuracy using foundation models","feature_image_caption":null},{"id":"6446b44974911d003db0cad0","uuid":"1b76d2d0-679c-4198-a77b-ab4acdb50498","title":"How to create and label text layers from PDF documents for AI","slug":"how-to-create-and-label-text-layers-from-pdf-documents-for-ai","html":"\u003cp\u003ePDF documents are one of the toughest data types to handle when it comes to building AI models. While most other data types usually contain information in one format, PDFs can comprise multiple types of data within them, such as photos, graphs and charts, text formatted in paragraphs, outlines, lists, links, and more. To add to the complexity, PDF documents can be hundreds of pages long, so a single data point can amount to hundreds of hours of work to label for training a model.\u003c/p\u003e\u003cp\u003eThat's why many AI teams working with this data type use PDF layers to separate and organize different information formats within a document, making them easier to view, edit, and enrich with labels. As most PDF documents contain primarily text, creating a text layer is often essential to the process of building AI on PDF data.\u003c/p\u003e\u003cp\u003eIn this post, we’ll take a look at the different types of PDF layers and discuss various methods for creating PDF text layers, such as using PDF creation software, OCR software, or Python scripts with libraries like reportlab or PyPDF2. Additionally, we will highlight the usefulness of PDF text layers in applications such as Quantumworks Lab, where they can be imported for annotation and used to build robust AI models with contextual information from PDF documents.\u003c/p\u003e\u003ch3 id=\"what-are-pdf-layers\"\u003eWhat are PDF layers?\u003c/h3\u003e\u003cp\u003eA PDF layer comprises a specific type of data contained within a PDF document. Separating a document into layers enables AI builders to process the different types of information within it. \u003c/p\u003e\u003cp\u003eTypes of layers include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eText layer: Refers to the selectable and searchable text contained within a PDF document. The text layer is an essential component of the PDF format, as it enables users to interact with the document's content by copying, searching, and editing text.\u003c/li\u003e\u003cli\u003eImage and graphics layers: Contain images, illustrations, and graphical elements.\u003c/li\u003e\u003cli\u003eBackground layer: Contains the background color or images, usually placed behind the main content.\u003c/li\u003e\u003cli\u003eAnnotations and comments layer: Contains any added annotations, comments, or interactive elements like form fields.\u003c/li\u003e\u003cli\u003eWatermarks or overlays: Contains watermarks, stamps, or other overlay elements that appear over the main content.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"how-to-create-a-pdf-text-layer\"\u003eHow to create a PDF text layer\u003c/h3\u003e\u003cp\u003ePDF text layers can be created in several ways, depending on the source of the document and the tools available. Here are a few methods for creating a PDF text layer:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eUsing PDF creation software.\u003c/strong\u003e There are dedicated PDF creation tools, such as Adobe Acrobat, that allow you to create a PDF document from various file formats, including images, Word files, and other document types. These tools often have built-in OCR (Optical Character Recognition) capabilities that can detect and create text layers from scanned or image-based documents.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOCR software.\u003c/strong\u003e If you have a scanned or image-based document without a text layer, you can use OCR software to recognize the text and create a new PDF with a text layer. There are many OCR tools available, such as Adobe Acrobat, AWS Textract, and Tesseract.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePython script.\u003c/strong\u003e You can separate a document into layers using libraries like reportlab or PyPDF2.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"why-is-a-pdf-text-layer-useful\"\u003eWhy is a PDF text layer useful?\u003c/h3\u003e\u003cp\u003ePDF text layers enable AI teams to annotate text data on its own, and then later add context to that information by combining it with the other data types within the document. Quantumworks Lab enables you to import your PDF and text layer so that you can annotate on the text layer and then export the labeled text with photos, graphics, and other information from the document to build robust AI models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to convert your PDF into Quantumworks Lab PDF text layer format\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eYou can convert your PDF into the Quantumworks Lab PDF text layer format using the CLI shown \u003ca href=\"https://github.com/Quantumworks Lab/PDF-OCR-Transform-CLI?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eFirst, you will need to have AWS CLI installed globally and configured for your AWS user with permissions to S3 and Textract. The CLI will upload your PDF to S3 and save the Quantumworks Lab formatted PDF text layer JSON file in the specified folder.\u003c/p\u003e\u003cp\u003eThere is a configuration file named config.json at the root level of the directory that must be updated before first running the CLI.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-CLI\"\u003e{\n  // The name of the bucket in your cloud provider that pdfs will be uploaded to\n  \"bucketName\": \"\u0026lt;name_of_s3_bucket\u0026gt;\"\n}\nconvert - Run OCR on all pdfs contained in the input folder and convert the result into Quantumworks Lab's text layer JSON.\n\nconvert --inputFolder \u0026lt;input_folder_containing_pdfs\u0026gt; --format \u0026lt;aws-textract\u0026gt; --outputFolder \u0026lt;output_folder\u0026gt; --concurrency 10\n\n--inputFolder The input folder containing the pdfs\n\n--format The OCR format to use (aws-textract, google-cloud-vision)\n\n--outputFolder The output folder to place the generated text layer json files\n\n--concurrency How many pdfs to process at the same time. CAUTION: Setting this value too high can result in rate limits being reached.\n\nExample (Mac)\n\n./textlayer-macos convert --inputFolder input --format aws-textract --outputFolder output --concurrency 10\nvalidate - Validates the provided text layer json\n\nvalidate --textLayerFilepath \u0026lt;text_layer_filepath\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOnce you have the PDF text layer in the Quantumworks Lab format, you can upload it and the PDF file to Quantumworks Lab as shown \u003ca href=\"https://docs.labelbox.com/reference/documents?ref=labelbox-guides.ghost.io#import-format\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eYou can also convert GCP OCR JSON or Adobe OCR JSON formats into Quantumworks Lab's format as shown \u003ca href=\"https://github.com/Quantumworks Lab/PDF-OCR-Transform-CLI?ref=labelbox-guides.ghost.io#ad-hoc-transform-scripts\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"view-and-label-pdf-text-layers\"\u003e\u003cbr\u003eView and label PDF text layers\u003c/h3\u003e\u003cp\u003eSee how you can visualize and annotate PDF text layers for your AI projects using Quantumworks Lab by selecting \"Show text layer\" (illustrated below).\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/80w2yj6yqi\" title=\"text layer Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eShow text layers within Quantumworks Lab\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"627\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample view of a text layer within the Quantumworks Lab UI\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe hope what we covered in this post helps inspire you to better work with the different ways to create PDF text layers, from using software to OCR or Python scripts. These text layers can come in handy when building AI models with PDF context. Feel free to give it a try for yourself and we'd love to hear your feedback and what else you'd like to see when it comes to creating and labeling text layers for your ML use cases.\u003c/p\u003e\u003cp\u003eLearn more about:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHow to \u003ca href=\"https://docs.labelbox.com/reference/documents?ref=labelbox-guides.ghost.io#import-format\"\u003eupload\u003c/a\u003e text layers to Quantumworks Lab\u003c/li\u003e\u003cli\u003eHow to natively \u003ca href=\"https://labelbox.com/guides/how-to-natively-annotate-a-pdf-document/?ref=labelbox-guides.ghost.io\"\u003eannotate\u003c/a\u003e a PDF document in Quantumworks Lab\u003c/li\u003e\u003cli\u003eWhat types of \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#import-document-data\"\u003eDocument\u003c/a\u003e annotation tasks can be used\u003c/li\u003e\u003c/ul\u003e","comment_id":"6446b44974911d003db0cad0","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/04/Screen-Shot-2023-04-24-at-9.16.14-AM.png","featured":false,"visibility":"public","created_at":"2023-04-24T16:54:33.000+00:00","updated_at":"2023-10-26T18:10:32.000+00:00","published_at":"2023-04-20T16:54:00.000+00:00","custom_excerpt":"Learn about the different types of PDF layers and how to import annotations to build robust AI models with contextual information from PDF documents.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa608375d13000123d7fa","name":"Industry: Finance \u0026 insurance","slug":"industry-finance-insurance","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-finance-insurance/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-create-and-label-text-layers-from-pdf-documents-for-ai/","excerpt":"Learn about the different types of PDF layers and how to import annotations to build robust AI models with contextual information from PDF documents.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63ff856122e0a7003dacb795","uuid":"6f3a4aaf-3582-413a-857c-b7686a5bdb48","title":"Using Quantumworks Lab and Weights \u0026 Biases to fine tune your computer vision projects","slug":"using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects","html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\u003cp\u003eRepeatable, scalable, and diagnosable production artificial intelligence (AI), requires a sophisticated machine learning operations (MLOps) ecosystem. MLOps is the backbone of machine learning (ML) engineering, focused on streamlining development of AI/ML models and deploying, and monitoring those models in production. \u003c/p\u003e\u003cp\u003eIn this post, we’ll explore an MLOps architecture that uses both Quantumworks Lab and \u003ca href=\"https://wandb.ai/site?ref=labelbox-guides.ghost.io\"\u003eWeights \u0026amp; Biases\u003c/a\u003e to develop a computer vision model for a manufacturing focused use case. The goal of the model is to reduce defects on a production manufacturing line using automated visual inspection and the model requires human judgment to curate (supervise) the training data for model development.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/rx0-34Eba9ac5-xI9vIRIt8eZW7uCrVspPzHvmlLuXsF18qJCo-lWBZinRHHwm48UXGvnS3pdYgXOkEBqaeIkH5hk6epd9RfIHPtOEeVgZ1S2p8q_yMVOkOhXIypGKnl8zrwT4JpkzKXRW0IT70pBb6sDnYnl3pG1jDBCMPfbGFPbcnC9M6ai5gWBcwDhw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"465\" height=\"346\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample image of reducing defects on a production manufacturing line via the Quantumworks Lab interface.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eKey Components of AI Development\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eDeveloping a production caliber model is an extremely iterative process. Rarely, if ever, is a model trained once and ready for production. Typically, there are a series of experiments that need to be run across several combinations of datasets and models, followed by expert analysis to determine which one yields the best results. Each experiment must be meticulously tracked and analyzed as ML teams cycle through the development process. \u003c/p\u003e\u003cp\u003eIt’s not uncommon for data science teams to jump into model training without a clear understanding of their data. This can greatly increase the number of model training iterations needed to achieve the desired result, drive up development cost and increase the risks of achieving shipping ML-powered products on time.  \u003c/p\u003e\u003cp\u003eAn efficient, data centric model development approach is valuable, and consists of two key ML Ops components, a data-centric AI platform (Quantumworks Lab) and a model diagnostics platform (Weights \u0026amp; Biases).\u003c/p\u003e\u003ch3 id=\"improving-your-data-and-models\"\u003eImproving your data and models\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eWhen your model is being developed/trained or when it is in production, there will be indicators that your model is either drifting or performing poorly. This will be noticed in the different statistics your model outputs - confidence, accuracy, loss, a fluctuation in the number of detections or the classes that are being detected, all of which will be tracked and triggered in a model diagnostics platform such as Weights \u0026amp; Biases. \u003c/p\u003e\u003cp\u003eBut what about improving the actual dataset? Or understanding which specific data my model is performing poorly on and determining which new data will most improve my model? How do I quickly get that data labeled and back into model training? When transitioning from model diagnostics to data diagnostics, we recommend leveraging tools to quickly understand the data that your model is performing poorly on, find more data similar to that, and curate subsequent datasets in order to iterate through the model development process faster. \u003c/p\u003e\u003cp\u003eThe bottom line is that model diagnostic tools and AI platforms supplement each other and should be used together. Let's dive deeper into what this looks like in practice.\u003c/p\u003e\u003ch2 id=\"how-it-looks-in-practice\"\u003eHow it looks in practice\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eOnce Quantumworks Lab and Weights \u0026amp; Biases have been installed and are set up in your pipeline, what does it look like to embark on the model development journey for the first time (or subsequent times)? Let’s look at an example workflow, most of which can be performed in a seamless process by leveraging both the \u003ca href=\"https://labelbox-python.readthedocs.io/en/latest/index.html?ref=labelbox-guides.ghost.io\"\u003eLabelbox\u003c/a\u003e and \u003ca href=\"https://docs.wandb.ai/quickstart?ref=labelbox-guides.ghost.io\"\u003eW\u0026amp;B\u003c/a\u003e SDKs:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1300\" height=\"706\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 1300w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003col\u003e\u003cli\u003eBegin by first \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003econnecting your data source(s)\u003c/a\u003e in Quantumworks Lab Catalog for easy data visualization and curation.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/View-your-dataset-in-Catalog--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"995\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/View-your-dataset-in-Catalog--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/View-your-dataset-in-Catalog--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/View-your-dataset-in-Catalog--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/03/View-your-dataset-in-Catalog--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eQuickly search, explore and manage your data in one place. View raw data, metadata, and ground truth labels as shown in the mdetal cast defect example above.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e2. Label and review your data in Quantumworks Lab Annotate\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/MTDzpAqLb7Fnqe5_3GbURP2RlB56Dp4lGfprQf_difu_gRWtUxUWw3Ftkn-nhEjC25ZC66rxrZuoIc0WN7U66lBBVXL4z-p1iU1WjlFvAdiSHjOHygivHBtyVI5z2LN1IUpwBVF-FONNhYSfTUlPEvoY8RVKUp91u31dNmeDqANDdvIQShZCOBJpKgEcNQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"339\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEasily set up a consistent ontology for your defect detection use case.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/ezgif.com-optimize--6-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1491\" height=\"976\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/ezgif.com-optimize--6-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/ezgif.com-optimize--6-.gif 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/ezgif.com-optimize--6-.gif 1491w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLeverage auto-annotation to speed up your manual labeling tasks.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e3. Easily split data into \u003ca href=\"https://docs.labelbox.com/docs/curate-data-splits?ref=labelbox-guides.ghost.io\"\u003etrain, test, and validate\u003c/a\u003e sets in Quantumworks Lab Model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Model_Run_Comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"880\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Model_Run_Comparison--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Model_Run_Comparison--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/Model_Run_Comparison--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/03/Model_Run_Comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eYou can use SDK to customize how you want to split your data rows for a given model run.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003e4. Send your data to Weights \u0026amp; Biases from Quantumworks Lab for handling model training and hyperparameter search and tuning.\u003c/p\u003e\u003cp\u003e5. Weights \u0026amp; Biases will handle model training and do a hyperparameters search to run a series of model experiments to be compared with each other. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/StJ0uM0O4-VRXym-dfdNG4JF2ArLtF3qHyoed2yp-F6fYE7dB7ztXzVn5u6AtaPlwp-eSu9fO6VugHVYneYoyOmfZSxk8cCjoEq9YdIkjJ43g7vbZg5B7U49VpraoHWt-uY2WHbDyb4EgTxtxTylEnw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"327\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eTo run a Hyperparameter search using Weights \u0026amp; Biases Sweeps you can \u003c/span\u003e\u003ca href=\"https://colab.research.google.com/drive/13eKhoSbn13kHRRQexbXEfQPz8OwjjcEh?ref=labelbox-guides.ghost.io#scrollTo=EhOKTaHcg4Xl\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003erun this colab notebook here\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e6. Visualize your results in W\u0026amp;B dashboards to quickly diagnose your model performance and create reports in order to share with colleagues and streamline communication. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/ugAyjHH-6i8xJCCdujBBbmC8L7NKrivhJ2j7_UW6Fd5rMf0S7EFPRT8r_v9x9yVrqxRxjqFqYiaikYIrzhdOY_IOjpLt_Qf9hYzBCsMVVxS-SUbKbsSQSSJYKEvuGf1qI9trJwCy_Y4HLDuh6Hgc7JE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"313\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHere we can see the results of a search over 40 experiments to find the best training settings. \u003c/span\u003e\u003ca href=\"https://wandb.ai/morgan/industrial-images/sweeps/t88dzqwe?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eClick here to see this live Weights \u0026amp; Biases dashboard\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e7. Quickly visualize your results in Quantumworks Lab Model to quickly diagnose the data that is related to your model, rapidly query all of your data sources to find data similar to where your model performs poorly (edge cases), and seamlessly queue that data for labeling.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/01/Model-metrics-view--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1753\" height=\"959\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003e8. Rapidly repeat this iterative model development process until you have a production-worthy model, deploy that model, and continuously monitor\u003c/p\u003e\u003ch2 id=\"the-labelbox-ai-platform\"\u003eThe Quantumworks Lab AI platform\u003c/h2\u003e\u003cp\u003eOne of the biggest mistakes made when creating an AI ecosystem is not integrating a data-centric approach into your MLOps pipeline and serves as a foundational tool for data curation and fast iterative improvement through the model development lifecycle. \u003c/p\u003e\u003cp\u003eUsing Quantumworks Lab, ML Teams can easily connect to their sources (i.e. file systems, data lake platforms etc.) of unstructured data and quickly begin exploring and prioritizing their data curation efforts using Quantumworks Lab \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003eOnce data has been assessed and prioritized for curation \u0026amp; labeling, a combination of weak supervision and human supervised labeling campaigns are supported with the use of Quantumworks Lab \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003eFinally, the training data is curated into data splits for test, train and validation and can be easily integrated with a model diagnostics solution (Weights and Biases) to efficiently manage the first of many model training experiments and iterations. This integration is accomplished with the Quantumworks Lab \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eMode\u003c/a\u003el product.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/03/Screen-Shot-2023-03-01-at-8.26.03-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1568\" height=\"898\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe Quantumworks Lab platform allows teams to more quickly go from data curation, annotation and fast iterative improvement through the model development lifecycle.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"weights-biases%E2%80%99-model-diagnostics-platform\"\u003eWeights \u0026amp; Biases’ Model Diagnostics Platform\u003c/h2\u003e\u003cp\u003eA model diagnostics platform should offer quick iteration, easy collaboration, and a centralized system of record for ML teams. Because ML development is often closer to a science than traditional software engineering, experimentation is at its core, and tracking the progress of these experiments is critical.\u003c/p\u003e\u003cp\u003eUsing Weights \u0026amp; Biases’ experiment tracking, ML teams can easily log their work in a standardized way, knowing that they can return to the results of their experiments days, months, or years later. \u003c/p\u003e\u003cp\u003eEase of collaboration is critical for ML teams so that they can move quickly, and Weights \u0026amp; Biases’ Reports enables colleagues to share quick notes, training journals, and polished analysis to teammates and managers to unlock decision making and keep the team moving forward.\u003c/p\u003e\u003cp\u003eFinally, knowing that your multiple model checkpoints are securely stored gives you the full picture of which model is best to select for deployment and to send to the W\u0026amp;B Model Registry, where your MLOps or DevOps team can pull it down for deployment.\u003cbr\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/4PUY0WUteKeig2EtGDEgFP_VVBSQSWkEV3ILX0b_rPUIFMDC2LsEdajU6cykdxVjjinMfRd9dEzEijMLgj0MkaS78BN4g_-yiPwbh3deCfVAWW7IEML2h_dI-YUUwCfUYG4LwErUmaIouAyL-ZqLR0TDH_l_cjnu4_ZhaXNiEzQ8Ds5We-caYw7dQdwc8A\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"144\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003eAI model development is an iterative process, which can be tedious and time consuming without a reliable MLOps pipeline. An efficient model development environment can be broken into two parts: a data-centric workflow for data ingestion, exploration, understanding, and preparation, and a model diagnostics platform for model training, tracking, evaluation, and versioning. Quantumworks Lab and Weights \u0026amp; Biases are leading platforms that were designed to complement this iterative process in mind, and when combined, can scale and help AI development teams build better models, faster than ever before.\u003c/p\u003e\u003cp\u003eWe'll be releasing more technical guides in the coming weeks on how to best utilize Quantumworks Lab and Weights \u0026amp; Biases so stay tuned! \u003cbr\u003e\u003c/p\u003e","comment_id":"63ff856122e0a7003dacb795","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-8.57.36-AM.png","featured":false,"visibility":"public","created_at":"2023-03-01T17:03:29.000+00:00","updated_at":"2023-12-18T22:47:42.000+00:00","published_at":"2023-03-01T18:56:05.000+00:00","custom_excerpt":"Learn how you can use Quantumworks Lab and Weights \u0026 Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},"url":"https://labelbox-guides.ghost.io/using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/","excerpt":"Learn how you can use Quantumworks Lab and Weights \u0026 Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Using Quantumworks Lab and Weights \u0026 Biases | Quantumworks Lab","meta_description":"Use Quantumworks Lab and Weights \u0026 Biases to build better computer vision models with step-by-step workflows of data curation, annotation, \u0026 diagnostics","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63bdef75be228d003d1e9ce3","uuid":"abe288c8-c70e-407b-b727-da6a90605cb3","title":"How to kickstart and scale your data labeling efforts","slug":"how-to-kickstart-and-scale-your-data-labeling-efforts","html":"\u003cp\u003eYour model performance will only ever be as strong as the quality of your training data. A common bottleneck for many AI teams is how to obtain vast amounts of high-quality training data for their use case at scale in the most time efficient and cost-effective way possible. \u003c/p\u003e\u003cp\u003eWhen it comes to deciding how to label your data, you might consider one of the following options:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompletely outsource this task to a labeling service — these external teams often receive training on the specific labeling tasks required and quickly proceed to label large datasets\u003c/li\u003e\u003cli\u003eLeverage AI-powered solutions from a labeling platform to speed up the labeling process\u003c/li\u003e\u003cli\u003eManage homegrown or open source tools and rely on your own internal team of labelers to label your dataset\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOn the surface, the above options may seem sufficient, but they have their disadvantages. The labeling process itself is opaque, so by relying on completely outsourcing your task, you risk having little insight into metrics such as labeling quality, throughput, and efficiency. If you’re working with sensitive data, outsourcing labeling can be a greater challenge regarding security concerns. Many service providers also don’t provide access to a labeling platform, hindering AI teams from experimenting within the labeling process and taking advantage of techniques like automation and active learning. In addition, utilizing in-house or open source tools can quickly become hard to manage, resulting in an exorbitant amount of time and resources in maintenance and scale. This can lead to delays from quality management and labeling iteration, poor ontology creation and management, miscommunication between stakeholders, SMEs, labelers, and more.\u003c/p\u003e\u003cp\u003eTo appropriately scale and maintain the quality required for your production use case, you’ll need to leverage a \u003ca href=\"https://labelbox.com/learn/library/complete-guide-data-engines-for-ai/?ref=labelbox-guides.ghost.io\"\u003edata engine\u003c/a\u003e. An effective data engine combines data management, quality and performance monitoring, and advanced techniques and labeling services to help improve the speed and efficiency of your labeling operations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Frame_2963.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1368\" height=\"1010\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Frame_2963.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Frame_2963.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/Frame_2963.png 1368w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eLabelbox provides \u003c/em\u003e\u003c/i\u003e\u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003elabeling services\u003c/em\u003e\u003c/i\u003e\u003c/a\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e and AI expertise, on-demand. You can outsource labeling work and partner with ML experts to fine-tune the above workflows to ensure clarity on tasks and achieve your quality targets.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRegardless of your use case, if you’re working with an external labeling team or partnering with a service provider, you’ll want to make sure that you’re set up for success. Carefully outlining your labeling project and task, defining your project’s success criteria, measuring and maintaining quality, scaling your labeling operations, and evaluating your project’s results are all key steps to ensuring that you are producing high-quality training data.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-define-a-task-for-your-labeling-project\"\u003eStep 1: Define a task for your labeling project\u003c/h2\u003e\u003col\u003e\u003cli\u003eAlign on the key components of your labeling task so that it can be effectively communicated to your labeling team\u003c/li\u003e\u003cli\u003eDetermine the \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#supported-data-types\"\u003edata type\u003c/a\u003e or industry vertical — this allows your Labeling Team Manager to appropriately match you with a team of labelers well-suited for your task\u003c/li\u003e\u003cli\u003eOutline any specific labeling or compliance requirements for this task — this will often require a specialized workforce that is trained in your specific industry or task\u003c/li\u003e\u003cli\u003eDefine your data volume and agree on a project timeline — this will help allocate resources for your project and set expectations upon project start\u003c/li\u003e\u003cli\u003eCreate \u003ca href=\"https://labelbox.com/guides/how-to-create-and-manage-ontologies/?ref=labelbox-guides.ghost.io\"\u003ean ontology\u003c/a\u003e with the goals of proper labeling, efficiency, and reusability in mind\u003c/li\u003e\u003cli\u003eProvide labeling instructions for the labeling team to use — instructions should provide context to the task, explain what the task entails, describe the labeling steps, and be treated as a “living document”\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003eHow to define a task for your data labeling project\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"step-2-define-your-labeling-project%E2%80%99s-success-criteria\"\u003e\u003cbr\u003eStep 2: Define your labeling project’s success criteria \u003c/h2\u003e\u003col\u003e\u003cli\u003eUnderstand your project’s timeline and scope — this includes any deadlines, projected data volume, and the  average time per label\u003c/li\u003e\u003cli\u003eSelect the grading requirements for your project — this will help determine what is a “good” or “bad” quality label.\u003c/li\u003e\u003cli\u003eDecide if you want to implement a \u003ca href=\"https://labelbox.com/guides/how-to-define-your-data-labeling-projects-success-criteria/?ref=labelbox-guides.ghost.io#slas-quality-speed-throughput\"\u003equality SLA\u003c/a\u003e with your labeling team — this is a bidirectional commitment with your labeling team that is built on throughput and quality calculations \u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide,\u003cstrong\u003e \u003c/strong\u003e\u003ca href=\"https://labelbox.com/guides/how-to-define-your-data-labeling-projects-success-criteria/?ref=labelbox-guides.ghost.io\"\u003eHow to define your data labeling project’s success criteria.\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"step-3-create-a-quality-strategy-for-your-labeling-project\"\u003eStep 3: Create a quality strategy for your labeling project\u003c/h2\u003e\u003cp\u003eAfter defining and setting expectations on how quality is defined, you'll want to spend some time developing a quality strategy.\u003c/p\u003e\u003col\u003e\u003cli\u003eMake sure you have quality monitoring tools in place — Quantumworks Lab’s \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003ebenchmark\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003econsensus\u003c/a\u003e tools help measure labeling accuracy and labeling consistency so you can gauge your project’s labeling efficiency\u003c/li\u003e\u003cli\u003eIncorporate manual review and feedback throughout your projects’ duration, as labeling data is a collaborative process. Quantumworks Lab’s \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003eworkflow feature\u003c/a\u003e allows you to set up customized review steps based on your quality strategy\u003c/li\u003e\u003cli\u003eIf you have a quality SLA, regularly monitor and review your data to determine whether the SLA has been met\u003c/li\u003e\u003cli\u003eEnsure that there is an open two-way communication channel between your labeling team and key stakeholders — this can resemble Quantumworks Lab platform features such as \u003ca href=\"https://docs.labelbox.com/docs/issues-comments?ref=labelbox-guides.ghost.io\"\u003eissues \u0026amp; comments\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/boost-workforce-notifications?ref=labelbox-guides.ghost.io\"\u003eupdates\u003c/a\u003e, Slack, Google Docs, etc.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-create-a-quality-strategy-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003eHow to create a quality strategy for your data labeling project\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"step-4-scale-your-labeling-operations-while-maintaining-quality\"\u003e\u003cbr\u003e\u003cstrong\u003eStep 4:\u003c/strong\u003e \u003cstrong\u003eScale your labeling operations while maintaining quality\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eOnce a desired quality strategy has been implemented, a key question becomes how to maintain consistency and quality as team size or data volume grows.\u003c/p\u003e\u003col\u003e\u003cli\u003eManage your labeling workflow by making use of iteration with small batches and an initial calibration phase\u003c/li\u003e\u003cli\u003eThe \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io#calibration-phase\"\u003ecalibration phase\u003c/a\u003e is often a smaller subset of your task — it is used to train the labeling team on labeling instructions, the ontology, and to help them become familiar with the data in the project\u003c/li\u003e\u003cli\u003eProvide feedback and work with your labeling team to iterate on the data until the desired quality threshold is reached\u003c/li\u003e\u003cli\u003eMonitor overall quality and speed of your labeling operations as you enter the \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io#production-phase\"\u003eproduction phase\u003c/a\u003e of your project\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003eHow to scale up your labeling operations while maintaining quality\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"step-5-evaluate-and-optimize-your-labeling-project%E2%80%99s-results\"\u003eStep 5: Evaluate and optimize your labeling project’s results\u003c/h2\u003e\u003cp\u003eAfter a task is completed and you have entered the production phase, it’s important to evaluate and consider factors that can guide you toward greater optimization of future batches.\u003c/p\u003e\u003cp\u003eAfter a task is completed and you have entered the production phase, it’s important to evaluate and consider factors that can guide you toward greater optimization of future batches. \u003c/p\u003e\u003col\u003e\u003cli\u003eCrowdsource feedback from your labelers — understanding their challenges with the given task can help clarify future labeling instructions, discover edge cases, and suggest ways to improve efficiency\u003c/li\u003e\u003cli\u003eReview project results and labeler performance against your existing ontology and labeling instructions — see if project results reveal an opportunity to improve ontology structure or guidance\u003c/li\u003e\u003cli\u003eSave labeling time and cost by leveraging active learning techniques to prioritize high-impact data — Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e can help you quickly identify label and model errors, find all instances of similar data to edge cases or mislabeled data rows, and more\u003c/li\u003e\u003cli\u003eDetermine how well your project’s results aligned with your quality strategy outlined in step 3 — see if you notice areas for improvement or if further customization to improve review efficiency is needed with \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eworkflows\u003c/a\u003e\u003c/li\u003e\u003cli\u003eEvaluate whether the labeling team size and skillset was appropriate for your use case and the desired production capability \u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-and-optimize-your-data-labeling-projects-results/?ref=labelbox-guides.ghost.io\"\u003eHow to evaluate and optimize your data labeling project’s results\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003ePowered by Quantumworks Lab’s data engine, experience the next level of data labeling service with direct access to curated data labeling teams for your projects in any expert domain or popular languages. Set new standards in quality and throughput at half the cost.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003eContact us\u003c/a\u003e today to access the best data labeling services with specialized labeling teams that match your use case. You can also sign up and \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started with Quantumworks Lab for free\u003c/a\u003e. \u003c/p\u003e","comment_id":"63bdef75be228d003d1e9ce3","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2973--1-.png","featured":false,"visibility":"public","created_at":"2023-01-10T23:06:29.000+00:00","updated_at":"2024-09-12T23:47:29.000+00:00","published_at":"2023-01-12T21:31:12.000+00:00","custom_excerpt":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-kickstart-and-scale-your-data-labeling-efforts","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-kickstart-and-scale-your-data-labeling-efforts/","excerpt":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2973--1--3.png","og_title":"How to kickstart and scale your data labeling efforts","og_description":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2973--1--1.png","twitter_title":"How to kickstart and scale your data labeling efforts","twitter_description":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","meta_title":"How to kickstart and scale your data labeling efforts","meta_description":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":48,"tag":{"slug":"label-data-for-ai","id":"653aa53f375d13000123d7ec","name":"Label data for AI","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","count":{"posts":48},"url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},"slug":"label-data-for-ai","currentPage":"3"},"__N_SSG":true},"page":"/guides/tag/[id]/page/[pagenum]","query":{"id":"label-data-for-ai","pagenum":"3"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/tag/label-data-for-ai/page/3/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:51:20 GMT -->
</html>