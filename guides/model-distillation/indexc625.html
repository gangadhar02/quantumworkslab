<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/model-distillation/?ref=labelbox.ghost.io by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:54:10 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">What is Model Distillation?</title><meta name="description" content="Model distillation compresses large models for deployment, transferring knowledge from a teacher to a smaller student model efficiently." data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="What is Model Distillation?" data-next-head=""/><meta property="og:description" content="Model distillation compresses large models for deployment, transferring knowledge from a teacher to a smaller student model efficiently." data-next-head=""/><meta property="og:url" content="https://labelbox-guides.ghost.io/model-distillation/" data-next-head=""/><meta property="og:image" content="https://labelbox-guides.ghost.io/content/images/2024/04/Model-Distillation.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="What is Model Distillation?" data-next-head=""/><meta name="twitter:description" content="Model distillation compresses large models for deployment, transferring knowledge from a teacher to a smaller student model efficiently." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox-guides.ghost.io/model-distillation/" data-next-head=""/><meta property="twitter:image" content="https://labelbox-guides.ghost.io/content/images/2024/04/Model-Distillation.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g34[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.giShFC .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.giShFC .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.giShFC .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:2px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h2{padding-top:10px;}}/*!sc*/
.giShFC .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h3{padding-top:10px;}}/*!sc*/
.giShFC .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.giShFC .content a:hover{color:#1e40af;}/*!sc*/
.giShFC .content li{margin-bottom:20px;}/*!sc*/
.giShFC .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.giShFC .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.giShFC .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.giShFC .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}/*!sc*/
.giShFC .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.giShFC .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100% !important;height:100% !important;margin:0 auto;}/*!sc*/
.giShFC .content .kg-button-card{margin-bottom:20px;height:auto;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn{-webkit-transition-property:all;transition-property:all;-webkit-transition-timing-function:cubic-bezier(.4,0,.2,1);transition-timing-function:cubic-bezier(.4,0,.2,1);-webkit-transition-duration:.15s;transition-duration:.15s;background-color:#2563eb;padding:0.75rem;font-size:1rem;line-height:1.5rem;font-weight:500;border-radius:0.5rem;color:#fafafa;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn:hover{background-color:#1d4ed8;}/*!sc*/
.giShFC .content .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 0 0 0.75em;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;}/*!sc*/
data-styled.g35[id="id__PostContentWrapper-sc-1ct5gml-0"]{content:"giShFC,"}/*!sc*/
.kUXEED #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.kUXEED .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.kUXEED .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.kUXEED #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.kUXEED #image-viewer .close:hover,.kUXEED #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.kUXEED .modal-content{width:100%;}}/*!sc*/
data-styled.g36[id="id__ImageModal-sc-1ct5gml-1"]{content:"kUXEED,"}/*!sc*/
@media (max-width:1026px){.cwKcJJ.toc-container{display:none;}}/*!sc*/
.cwKcJJ.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g37[id="id__TocContainer-sc-1ct5gml-2"]{content:"cwKcJJ,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/guides/%5bid%5d-78cf43cbe169ea75.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="id__ImageModal-sc-1ct5gml-1 kUXEED"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All guides</a><main class="id__TocContainer-sc-1ct5gml-2 cwKcJJ toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">What is Model Distillation?</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox-guides.ghost.io/content/images/2024/04/Model-Distillation.png"/></div><main class="id__PostContentWrapper-sc-1ct5gml-0 giShFC md:px-24"><div class="content js-toc-content"><p>AI models are increasingly getting bigger with the increase in training data and the number of parameters. For instance, the latest <a href="../../product/model/foundry-models/gpt4/index.html"><u>Open AI’s GPT-4 </u></a>model is estimated to have about 1.76 trillion parameters and terabytes of training corpus. Whether training <a href="../../blog/6-key-llms-to-power-your-text-based-ai-applications/index.html"><u>large language models (LLMs)</u></a> or neural networks, the main goal remains: to train using as much data as possible.&nbsp;</p><p>While training from diverse data and increasing the number of parameters produces powerful models, real-world application becomes challenging. Deploying larger and larger models to edge devices like mobile phones and smart devices becomes difficult because of their intensive computational requirements and costs.&nbsp;</p><p>To solve this challenge, model distillation comes in handy. Model distillation, also known as knowledge distillation, is a machine learning technique that involves transferring knowledge from a large model to a smaller one that can be deployed to production. It bridges the gap between computational demand and the cost of enormous models in the training lab and the real-world application of these models while maintaining performance.&nbsp;</p><h1 id="why-model-distillation"><strong>Why Model Distillation?&nbsp;</strong></h1><p>Before diving into the steps taken to achieve model distillation, let’s discuss why it’s needed and the technical challenges it addresses in model training and deployment. Significant differences between requirements for model training and model inference stem from the infrastructure differences in training and deployment environments.&nbsp;</p><p>While LLMs like <u>Llama</u> and GPT-4 have incredible power, their applications suffer drawbacks caused by hardware requirements, speed, and cost. Hosting these models or directly accessing them through APIs would be expensive. The infrastructure cost and carbon footprints would be astronomical, even when run in the cloud. Large models also tend to be slow, which affects performance.&nbsp;</p><p>With model distillation, the deployment and real-world application of AI models can be realized. An inference-optimized model that keeps all the qualities and behaviors of the larger training model can be achieved. Using the teacher-student architecture, this supervised learning approach ensures knowledge transfer, enabling AI teams to develop secondary models that are responsive and cheaper to host and run.&nbsp;</p><h1 id="how-does-the-model-distillation-process-work"><strong>How Does the Model Distillation Process Work?</strong></h1><p>The model distillation process can be complex, depending on the complexity of the base and target models. The key proponents are the teacher model, the knowledge, the student model, and the distillation algorithm. This model compression technique starts with the teacher model and ends with the optimization of the resultant model using the distillation loss function, as described in the steps below.</p><h2 id="teacher-model-training"><strong>Teacher Model Training</strong></h2><p>The model distillation process starts with a pre-trained model, like an LLM trained on a large corpus. This phase involves selecting, fine-tuning, and/or training an expensive AI model with billions of parameters and gigabytes of data in a lab environment. The teacher model would be the base model in this case, serving as the knowledgeable expert system from which the knowledge is distilled. The student model would then inherit the behaviors and functionalities of this teacher model during knowledge transfer.&nbsp;&nbsp;</p><p>Soft targets are also generated during the training of the teacher model. Unlike hard labels, which are binary encoded, soft targets provide a more precise classification of the data points during the teacher training phase. The soft target generated offers more information on the teacher model’s decision-making, which will guide the student model's behavior during distillation.&nbsp;&nbsp;&nbsp;&nbsp;</p><h2 id="student-model-initialization"><strong>Student Model Initialization&nbsp;</strong></h2><p>In this phase, a smaller and lightweight student model is introduced. The model is initialized with random weights and set ready for knowledge transfer, targeting deployability on edge devices. With a simpler architecture and shrank computational demands, this student model is trained to replicate the teacher model’s output probabilities.&nbsp;&nbsp;</p><h2 id="knowledge-transfer"><strong>Knowledge Transfer</strong></h2><p>After the student model is initialized, it is trained using the outputs of the teacher model in a process known as knowledge transfer; this is the distillation phase. Soft targets generated by the teacher model are combined with the original training dataset to train the student model. With the aim of matching the student and teacher’s predictions, the student model is steered towards mimicking the feature representation of the teacher model’s intermediate layers. In doing so, the student model learns the pairwise relation between data points captured by the base model. A distillation algorithm is applied in the knowledge transfer phase to ensure student models can acquire knowledge from the teacher model efficiently. Such algorithms include <strong>adversarial distillation, multi-teacher distillation, graph-based, and cross-modal distillation. </strong>The choice of these algorithms depends on the work at hand, the model’s features, and data points. Either way, applying a distillation knowledge algorithm is a highly encouraged practice during model distillation as it reduces complexity and improves the performance of the student model.&nbsp; &nbsp; <strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></p><h2 id="optimization-with-distillation-loss-function"><strong>Optimization with Distillation Loss Function</strong></h2><p>As already highlighted, the learning curve is often steep for the student model, considering its minimal computation power and performance expectations. As a result, it might sometimes drift from the training domain, resulting in distillation loss. To address this challenge, a distillation loss function is applied to guide the knowledge transfer process. This loss function helps the student model to steadily acquire knowledge by quantifying the discrepancies between the teacher model’s soft targets and the student model’s predictions. These discrepancies provide a blueprint for minimizing feature activation differences between the teacher and student models, helping the student model to adapt gradually.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><h2 id="fine-tuning-the-student-model"><strong>Fine-Tuning the Student Model&nbsp;</strong></h2><p>As much as the knowledge transfer process is expected to be seamless and the resultant model a replica of the teacher model, the student model might not achieve perfection. Therefore, it is a good practice to fine-tune the student model further on the original dataset after the model distillation process. This optional process employs supervised learning methodologies to improve the performance and accuracy of the student model.&nbsp;&nbsp;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_FOKDBjmmxNhpoXMbwUjZFJYJQAR76SF2mN4tXpYdKSYg_1sWTSkWktbgIuKFZsTAb5X9qZXRMAFxyE2-b8lJh42WrIBRXOKsMO_Ig7UAjJMnHDJ1ppcfOW8COFmxglz0Q5Agm3HtS9iX1YmQINYj66icWivaaIBKjmXdgGgCsiznFvsTkaJ98ElTfPfM6vryEKrcQ7atnMjK-EEqfBMTssgLNlCPZlmfOFDkBkBsrn2mM1LbE9pQ37yvkWVTKXZXWN9Bfln_fL7z5-UCHK82RD-5jaalewkFnA5JPX_N8PJ49FxLkyvVdPf2_sAZUkS5WfF0niqVo7gsRiIdNey8y75ZzSiUDjYcpjcJZfL2hd0l0PGo0gwpsVJoYu4EvOS7geWFM7QyGbE5J2uHv2Xvw8qTEEf2rLVK8pND2lqckPPWbEcKPsErmyilB8gWugJNqIjslCUOSaO3dH8JXDCh8d012C0b7OQPw3nkltUTaagYvSe9v_sgjjw9GXEUOAVDPieGMD3I0Q=s1600" class="kg-image" alt="" loading="lazy" width="850" height="316"><figcaption><span style="white-space: pre-wrap;">Model distillation lifecycle. Image from</span><a href="https://www.researchgate.net/figure/Fig-2-Generic-architecture-of-knowledge-distillation-using-a-teacher-student-model_fig2_355180688"><u><span class="underline" style="white-space: pre-wrap;"> ResearchGate</span></u></a></figcaption></figure><h1 id="what-are-the-different-types-of-model-distillation"><strong>What are the different types of Model Distillation?</strong></h1><p>Model distillation can be classified into three types:<strong> response-based, feature-based, and relation-based</strong>. Each type has a unique approach to knowledge transfer from the teacher model to the student model, as discussed below.&nbsp;&nbsp;&nbsp;&nbsp;</p><h2 id="response-based-model-distillation"><strong>Response-based Model Distillation</strong></h2><p>This is the most common and easiest-to-implement type of model distillation that relies on the teacher-model’s outputs. Instead of making primary predictions, the student model is trained to mimic the prediction of the teacher model. During the distillation process, the teacher model is prompted to generate soft labels. An algorithm is then applied to train the student model to predict the same soft labels as the teacher model and minimize the differences in their outputs (also know as distillation loss).&nbsp;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_E1SCDitF8wdjy8fAMoAbMWUzgyeUciTTrhEvM4XplG1Q6PmbPUusTkNoVr-1Sy9dB24e0kgSrjNhgGwHDF7kCDEMTHi_R90CN4hfHgNrbzaUHE5BrmTR6Sb8R7aOHd3lPKC1uB6jeWX1M5NmkQChBOxqeoVsY-N0YMoSuGVmZqhoVV9em7ekR0vmo1vWQFnMY9jZIYDo6nt9fW4kEEbBhdr1fLAyOBKqkbtQxr7sCRdVvrcUnTVMpWwKJLKEwsSo0TaTk6BDQOo-W3D99bKoAl03WAHjwsKZoO2GtW5y3aDiEhCn7LPuIwIYPEhdn2RO5Q3mZxlA-UTrQ80CgiGSGW37oyGsDNFs_tKXzNsS_zdPQiERJkXGy7HY3acaoocio7BsNDjQpPJqd_KaeTB5pC6TjnIqhP60BjW0hTeDlFHgOQPhdw1-3uvnmt4GTl0IK3GM1DZAlrR5WCtX2IU_GRzEGQcXHGlXJjrRAH479Aoj7rvaplsLGdUxfQGaSI81TYBLDnr7xn=s1600" class="kg-image" alt="" loading="lazy" width="850" height="317"><figcaption><span style="white-space: pre-wrap;">Response-based distillation process. Image from </span><a href="https://www.researchgate.net/figure/Response-based-knowledge-distillation_fig3_356869657"><u><span class="underline" style="white-space: pre-wrap;">ResearchGate</span></u></a></figcaption></figure><h2 id="feature-based-model-distillation"><strong>Feature-based Model Distillation</strong></h2><p>As the name suggests, feature-based distillation involves the student model learning from the teacher model’s internal features. In this type of model distillation, the teacher model is trained on task-specific features. The intermediate layers of these features are then extracted and used as targets when training student models. The student model is also trained to incrementally minimize the difference between the features it learns and those learned by the teacher model.&nbsp;&nbsp;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_EEeYnnB4X-n3xsiUGqa41_sKR-CdpxtXDayrXBe_ns2w0zraayCq5vHht_s1LS0lBRdiR99M4kCXrzQJRJ9qvS-zS2KOCI1Q2qtyetD58jNJJEvV7y8CQFn6Jx0d8ddVzYCZ7zeJId6HPYCo11PPFJIrz1MdsU_58AlFH3c_j9gQVLE5tWp82u7EDZFRfjLOdoaJ_1QhmPF6S4zE1FhB9b3wlsbjmmxJqT4SqcNmCa-G-NIJglTlDX2DUVqM7v2UnyHmL7CZJkRUqVwyR5BROJV4Dwl0R4o-S702iE5DZuhPdU2gL-kQB-BEN4e8sO1XffH6Z8lmbvtcmOEWUaP1x6C9El3Eo9JZaZ0LjyhgsdSt6Q1UfLtpjbZFZZ2AC4IiDfTk9hzOikBTWfZ2gqUclmhOauGHnZq8Y72rqRTRLvhI3YjwAwk6xdFCXSEikDMyZ1pGF9q657TMfHyqtsulw7CN04rP4FUuiWuhmE4Lpn3XZJVU6hbInq4LkT0sYLAVWQqPa6-Bo1=s1600" class="kg-image" alt="" loading="lazy" width="753" height="290"><figcaption><span style="white-space: pre-wrap;">Knowledge transfer over feature layers in feature-based distillation. Image from </span><a href="https://www.researchgate.net/figure/Feature-based-knowledge-distillation_fig5_371616469"><u><span class="underline" style="white-space: pre-wrap;">ResearchGate</span></u></a></figcaption></figure><h2 id="relation-based-model-distillation"><strong>Relation-based Model Distillation</strong></h2><p>This is the most sophisticated yet robust type of model distillation. It focuses on transferring the underlying relationships between the inputs and outputs from the teacher model to the student model. The teacher model generates sets of relationship matrices for the input examples and output labels. While the goal is to minimize the loss function, the student model is trained on these relationship matrices progressively.&nbsp;&nbsp;&nbsp;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_EYS-w4izBqtz_keh26XjxYNCXh91mnB4DcO0zUHuFnjn4kXeiYchXlkmQ7EUefZRpZyyaA4wabkyMLeHggAthvJcUkD3ZMfYFcd0FV8Cz9Qs1LCHHa-aWL_Yz96b3sQMwnZR4rdEIjgihQVB2sgPwwBCJLuuZpy10pZVi3W3qtUTjdGyYZ7zV-Av_mvgIgooU1CFUNK-yBQvjN_I43nk4fO0WbUv6T4Nx--qgb0woi0RyCGken8CltKkcs9lwS2xLbIMAXBr20nYJYXCD0wNDSDdlfIFQcahOzZ8iWAPFGyeh3Jt0ldCyXdDeEo6sUldPfRq5aU3-Jku_EZEsCWNc8lyBioFjSKp2ZucCR3v19sI2njTdMspG35ud-pjRwOC-LaF161esf-NJ8kz04bifPu0zgPJkLZAmNaEt-HcmVXH6bLUv-_HsZSZY5V5EthvtYVNRyxZ4rTMUJgxYOjusO2wvEBoZ0RyC20qWgKRwBd7dYnbs7dbWZ21XJq054R_cezJNK8RAV=s1600" class="kg-image" alt="" loading="lazy" width="715" height="351"><figcaption><span style="white-space: pre-wrap;">Diagrammatic representation of relation-based model/ knowledge distillation. Image from </span><a href="https://www.researchgate.net/figure/The-generic-instance-relation-based-knowledge-distillation_fig10_342094012"><u><span class="underline" style="white-space: pre-wrap;">ResearchGate</span></u></a></figcaption></figure><h1 id="model-distillation-schemes"><strong>Model Distillation Schemes</strong></h1><p>Aside from the model distillation process and types, the technique applied during the student model training is also crucial. Depending on whether the teacher model is modified concurrently with the student model, there exist three primary model distillation schemes: <strong>offline, online, and self-distillation&nbsp;.&nbsp;</strong></p><h2 id="offline-distillation"><strong>Offline Distillation</strong></h2><p>This is the most popular model distillation method, which involves pre-training a teacher model, freezing it, and later using it to train a student model. During the model distillation process, the training focuses on the student model as the teacher model remains unmodified.&nbsp;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_EvSsPfZcZbDq82oEhQpACl1WUZ1duR60RdP2pw3eWb5WWgnsGqVNVTKOAMgkm4rgPRT7l4q7FLtIBsbWuYNOC7QKXXYcXAJYvo29dKm9NYoMmFTt6h4mDpewm9_To2VaajZKpdxCNCaUdwKuX0dmE2Iuq0KuEuvcq0DSyMzDzVBwTP4QBxYMN1_-rQ7Mdu1WUtibcD1-XgMF4HmDBTdIytdTHXg4kr9Y5lZhZQZQJ9BmV5p05bh5LpApw-qBJl1OT5Sn27f7FLYBBsIOT95dDA1-90oXbTR2bOXlre1eDZpUmQhZVZCjfRPAFiHdaFDArr7nfUJWyX6tmmg_ZZTX_fnrFd7--eFEyPO30GHS1rJXCu7s16DhpaXuK6hCVkxEJzETq9ahPfLtlwWFMdU40PqhDYiSphlHwhypXL9FsttUFP1jQrwWzHKvd5zV80sjhqwPq5bpEy1QZGhwQmr9khwRHeg5ga534fO6cdYf1y5_wXr-GYPdJf8mec6-zN3Q0L_yhPg4yo=s1600" class="kg-image" alt="" loading="lazy" width="352" height="143"><figcaption><span style="white-space: pre-wrap;">Offline Distillation. Image from </span><a href="http://arxiv.org/pdf/2206.12005"><u><span class="underline" style="white-space: pre-wrap;">arXiv</span></u></a></figcaption></figure><h2 id="online-distillation"><strong>Online Distillation</strong></h2><p>Online distillation is an end-to-end technique in which the student and teacher models are trained simultaneously. In other words, the teacher model is continuously updated with new datasets, and the student model is trained to reflect these changes.&nbsp;&nbsp;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_FU8VJx5Iwvcud3TiIZAkE-OGUgh5sAlzkvwYN_MbeQ09bw0Q7ByY4vDLUi_yjGg_f7bY1ORMNdFYMJfP8ep3QiFx15Jh7HTWKzL5uGNqJkipRdHtrljE2Kmz_fBFCTPVlDNaUtM1A02cCq_lWjSXuEgK3c5O-XjRenncAnAJn7AlNpSiyXd_ooC_CMXgTy290IpeiCqIH-4vCkg3rrfOlt8nHogWkEKapY5Yfr3S27i6kwnKlF8-_zbJ-pIZmS50FT2189iQEHtx0bNwfa3eoX2xlIPQznlHvyHfWJh5f7dl6kFwxavq79rNbFRKIWIPx7--s5-liUgRgPUsC-RFwsHPhEN97Xg3JxaXIFjeaaHs5BqoK1c5uXbd-k7CWwoTYp8RgpFfhJZlqmvKzJQysO2YiN6Zuzbcm7wFd0yWK-HdD0nQmfOHU1RB9p4biIVuU4Dv3vlAif5KLiA69FaSc2nrftJvgU2wWuqVPhr5FoolXxsnbm7d_c23NuOYJ2FG_9mCh-FI50=s1600" class="kg-image" alt="" loading="lazy" width="382" height="132"><figcaption><span style="white-space: pre-wrap;">Online Distillation. Image from </span><a href="http://arxiv.org/pdf/2206.12005"><u><span class="underline" style="white-space: pre-wrap;">arXiv</span></u></a></figcaption></figure><h2 id="self-distillation"><strong>Self-Distillation</strong></h2><p>Self-distillation involves using the same networks for the teacher and student models. In this case, a model learns from itself, meaning knowledge from the deeper layers of the model is continuously distilled into its shallow section. This method solves the conventional limitations experienced when either online or offline techniques are used—the discrepancy in accuracy between teacher and student models.&nbsp;&nbsp;&nbsp;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_G4Ku1Pr8buTcXeU-NmlKy20JS_N73cCR9Xn7V11OueziE3kdoDPYKKodYD-IJmFale4mWH8ZEEKQt6OM3hJGh24ns47caIJuYencDvpBt_2mX-z10lfvhNCUr_2mS27MEjJ_LoHFgGz2qmflZyl1-d3KKQLyaOvBQee_vUDNKXY0PV-V2BcPmbjGSK7_GnSH5VnGGIACJe5X3-Iz7RpYZCp0RwkIzCuUjS2k1QgCLQ1cn0jeHjeEbo2Ipik9Lo7qkPhsNEu-sOpWOAsj65aaWuBah7wMsQzwhexHM24HFikh9p-9slbOE6VZg6pg3zFEI67mSmrA7JTahOOUNTpb5sZQ84mvxjdn7eRtVeAO94kTfVl3wBVB1SzvKx6WuADP_gmKRVNSWivmCcL0-bhjjwlPY_PGKbLO5V5aDYILQ0AI3cnEtCkNa4Vued4Ms8wz0b5gRuM_p0dTG4AzHd7cXQj88yPCN2lqETtCtI1sGFTXIejeE5DV-IdNrclA8JfF8vCD81uuFO=s1600" class="kg-image" alt="" loading="lazy" width="264" height="191"><figcaption><span style="white-space: pre-wrap;">Self-Distillation. Image from </span><a href="http://arxiv.org/pdf/2206.12005"><u><span class="underline" style="white-space: pre-wrap;">arXiv</span></u></a></figcaption></figure><h1 id="limitations-of-model-distillation"><strong>Limitations of Model Distillation&nbsp;</strong></h1><p>Having looked at the benefits of model distillation, it’s important to discuss its limitations. While model distillation is a powerful knowledge transfer approach between models in different computational environments, it still suffers from various challenges:</p><ul><li>The technical complexities of the distillation process</li><li>Difficulty in multi-task learning</li><li>The student model is limited by the teacher&nbsp;</li><li>Potential loss of information during the distillation process</li><li>Limited applicability on existing proprietary models</li></ul><h1 id="final-thoughts-on-model-distillation"><strong>Final Thoughts on Model Distillation</strong></h1><p>The AI models currently being rolled out are cumbersome and certainly not ideal for deployment straight from the lab. Model distillation provides an alternative model compression mechanism to train lightweight models with comparably lesser computational demands from these heavyweight teacher models. In the realm of artificial intelligence, this model training approach has proven helpful in bridging the gap between limited resources and high-performance capabilities of various AI models. <a href="../end-to-end-workflow-with-model-distillation-for-computer-vision/index.html" rel="noreferrer">Model distillation has limitless potential ahead as AI model training advances.</a></p><p>Quantumworks Lab aids in model distillation by providing a robust platform for generating high-quality labeled datasets, which are essential for both training the teacher model and fine-tuning the student model. With support for various data types and comprehensive management features, Quantumworks Lab also helps organize and version-control datasets, streamlining the process of integrating labeled data into the distillation pipeline. <a href="https://app.labelbox.com/signup"><u>Try Quantumworks Lab for free</u></a> to enhance the efficiency and effectiveness of the distillation process.</p></div></main></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"model-distillation","id":"6628249bd733f0000145b209","uuid":"016e4a90-2a89-43a0-97fb-16a7d6cfd91c","title":"What is Model Distillation?","html":"\u003cp\u003eAI models are increasingly getting bigger with the increase in training data and the number of parameters. For instance, the latest \u003ca href=\"https://labelbox.com/product/model/foundry-models/gpt4/\"\u003e\u003cu\u003eOpen AI’s GPT-4 \u003c/u\u003e\u003c/a\u003emodel is estimated to have about 1.76 trillion parameters and terabytes of training corpus. Whether training \u003ca href=\"https://labelbox.com/blog/6-key-llms-to-power-your-text-based-ai-applications/\"\u003e\u003cu\u003elarge language models (LLMs)\u003c/u\u003e\u003c/a\u003e or neural networks, the main goal remains: to train using as much data as possible.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile training from diverse data and increasing the number of parameters produces powerful models, real-world application becomes challenging. Deploying larger and larger models to edge devices like mobile phones and smart devices becomes difficult because of their intensive computational requirements and costs.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo solve this challenge, model distillation comes in handy. Model distillation, also known as knowledge distillation, is a machine learning technique that involves transferring knowledge from a large model to a smaller one that can be deployed to production. It bridges the gap between computational demand and the cost of enormous models in the training lab and the real-world application of these models while maintaining performance.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"why-model-distillation\"\u003e\u003cstrong\u003eWhy Model Distillation?\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eBefore diving into the steps taken to achieve model distillation, let’s discuss why it’s needed and the technical challenges it addresses in model training and deployment. Significant differences between requirements for model training and model inference stem from the infrastructure differences in training and deployment environments.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile LLMs like \u003cu\u003eLlama\u003c/u\u003e and GPT-4 have incredible power, their applications suffer drawbacks caused by hardware requirements, speed, and cost. Hosting these models or directly accessing them through APIs would be expensive. The infrastructure cost and carbon footprints would be astronomical, even when run in the cloud. Large models also tend to be slow, which affects performance.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith model distillation, the deployment and real-world application of AI models can be realized. An inference-optimized model that keeps all the qualities and behaviors of the larger training model can be achieved. Using the teacher-student architecture, this supervised learning approach ensures knowledge transfer, enabling AI teams to develop secondary models that are responsive and cheaper to host and run.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"how-does-the-model-distillation-process-work\"\u003e\u003cstrong\u003eHow Does the Model Distillation Process Work?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe model distillation process can be complex, depending on the complexity of the base and target models. The key proponents are the teacher model, the knowledge, the student model, and the distillation algorithm. This model compression technique starts with the teacher model and ends with the optimization of the resultant model using the distillation loss function, as described in the steps below.\u003c/p\u003e\u003ch2 id=\"teacher-model-training\"\u003e\u003cstrong\u003eTeacher Model Training\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe model distillation process starts with a pre-trained model, like an LLM trained on a large corpus. This phase involves selecting, fine-tuning, and/or training an expensive AI model with billions of parameters and gigabytes of data in a lab environment. The teacher model would be the base model in this case, serving as the knowledgeable expert system from which the knowledge is distilled. The student model would then inherit the behaviors and functionalities of this teacher model during knowledge transfer.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eSoft targets are also generated during the training of the teacher model. Unlike hard labels, which are binary encoded, soft targets provide a more precise classification of the data points during the teacher training phase. The soft target generated offers more information on the teacher model’s decision-making, which will guide the student model's behavior during distillation.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"student-model-initialization\"\u003e\u003cstrong\u003eStudent Model Initialization\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIn this phase, a smaller and lightweight student model is introduced. The model is initialized with random weights and set ready for knowledge transfer, targeting deployability on edge devices. With a simpler architecture and shrank computational demands, this student model is trained to replicate the teacher model’s output probabilities.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"knowledge-transfer\"\u003e\u003cstrong\u003eKnowledge Transfer\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAfter the student model is initialized, it is trained using the outputs of the teacher model in a process known as knowledge transfer; this is the distillation phase. Soft targets generated by the teacher model are combined with the original training dataset to train the student model. With the aim of matching the student and teacher’s predictions, the student model is steered towards mimicking the feature representation of the teacher model’s intermediate layers. In doing so, the student model learns the pairwise relation between data points captured by the base model. A distillation algorithm is applied in the knowledge transfer phase to ensure student models can acquire knowledge from the teacher model efficiently. Such algorithms include \u003cstrong\u003eadversarial distillation, multi-teacher distillation, graph-based, and cross-modal distillation. \u003c/strong\u003eThe choice of these algorithms depends on the work at hand, the model’s features, and data points. Either way, applying a distillation knowledge algorithm is a highly encouraged practice during model distillation as it reduces complexity and improves the performance of the student model.\u0026nbsp; \u0026nbsp; \u003cstrong\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch2 id=\"optimization-with-distillation-loss-function\"\u003e\u003cstrong\u003eOptimization with Distillation Loss Function\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAs already highlighted, the learning curve is often steep for the student model, considering its minimal computation power and performance expectations. As a result, it might sometimes drift from the training domain, resulting in distillation loss. To address this challenge, a distillation loss function is applied to guide the knowledge transfer process. This loss function helps the student model to steadily acquire knowledge by quantifying the discrepancies between the teacher model’s soft targets and the student model’s predictions. These discrepancies provide a blueprint for minimizing feature activation differences between the teacher and student models, helping the student model to adapt gradually.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"fine-tuning-the-student-model\"\u003e\u003cstrong\u003eFine-Tuning the Student Model\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAs much as the knowledge transfer process is expected to be seamless and the resultant model a replica of the teacher model, the student model might not achieve perfection. Therefore, it is a good practice to fine-tune the student model further on the original dataset after the model distillation process. This optional process employs supervised learning methodologies to improve the performance and accuracy of the student model.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/gRn1yl70cPIULEAEZvNYJ2V4z3ScNA7IfLRzz3OiPde7DGuvKRaywdOY3ke44dNW7s3eDeRvq5BwZrGT0qgFQN8vApHRZWqbnvbIEGudIQbKAm6uWlFLpswl-04ATweV2StKuSgqtOm7M3BSZs00Wr0\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"850\" height=\"316\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eModel distillation lifecycle. Image from\u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Fig-2-Generic-architecture-of-knowledge-distillation-using-a-teacher-student-model_fig2_355180688\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003e ResearchGate\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"what-are-the-different-types-of-model-distillation\"\u003e\u003cstrong\u003eWhat are the different types of Model Distillation?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eModel distillation can be classified into three types:\u003cstrong\u003e response-based, feature-based, and relation-based\u003c/strong\u003e. Each type has a unique approach to knowledge transfer from the teacher model to the student model, as discussed below.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"response-based-model-distillation\"\u003e\u003cstrong\u003eResponse-based Model Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThis is the most common and easiest-to-implement type of model distillation that relies on the teacher-model’s outputs. Instead of making primary predictions, the student model is trained to mimic the prediction of the teacher model. During the distillation process, the teacher model is prompted to generate soft labels. An algorithm is then applied to train the student model to predict the same soft labels as the teacher model and minimize the differences in their outputs (also know as distillation loss).\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/k7jfzbrR_B2AruVBbtCBOLK5wARtxwmcMyCrhk_O-feR1TONPKtWDHzd7kZ6VZ0GXkkOo_IJsLxYj472bx_fJy4hXI-y60aP4FJ4N4JjK36lxftY20L15XZWKr0EYpuvkjYui7GSfuHyFRoxDvYXJxc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"850\" height=\"317\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eResponse-based distillation process. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Response-based-knowledge-distillation_fig3_356869657\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eResearchGate\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"feature-based-model-distillation\"\u003e\u003cstrong\u003eFeature-based Model Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAs the name suggests, feature-based distillation involves the student model learning from the teacher model’s internal features. In this type of model distillation, the teacher model is trained on task-specific features. The intermediate layers of these features are then extracted and used as targets when training student models. The student model is also trained to incrementally minimize the difference between the features it learns and those learned by the teacher model.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J0szaw1p4EGVavZkPaiQQZYPZancpacn4WZTcparpZx7JFR8Lzv4Mh0rpmSjGS0XvZkq8W4UWPAgnLmMuy_RFzP2tYQyhyhkJwA5k-w5iWNvsj8Ju7p6AdIqA_P8mOP1Vwih606Vnz8GlBNrOh9f8hI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"753\" height=\"290\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eKnowledge transfer over feature layers in feature-based distillation. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Feature-based-knowledge-distillation_fig5_371616469\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eResearchGate\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"relation-based-model-distillation\"\u003e\u003cstrong\u003eRelation-based Model Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThis is the most sophisticated yet robust type of model distillation. It focuses on transferring the underlying relationships between the inputs and outputs from the teacher model to the student model. The teacher model generates sets of relationship matrices for the input examples and output labels. While the goal is to minimize the loss function, the student model is trained on these relationship matrices progressively.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/BGscvBD1uJJN9F1Fs7nkbZsx48IQQd0hcxOh7OZD4hRtJUW4UHgwpzP7AE0EP8F3VgCiD9unM6IM-4i-lx6regEDqFZOatI0fyLW7h_k6CULklRDuulT6La2X2p8R2lkVaYSLJomy2bw5cwrCZ97Ix4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"715\" height=\"351\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eDiagrammatic representation of relation-based model/ knowledge distillation. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-generic-instance-relation-based-knowledge-distillation_fig10_342094012\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eResearchGate\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"model-distillation-schemes\"\u003e\u003cstrong\u003eModel Distillation Schemes\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAside from the model distillation process and types, the technique applied during the student model training is also crucial. Depending on whether the teacher model is modified concurrently with the student model, there exist three primary model distillation schemes: \u003cstrong\u003eoffline, online, and self-distillation\u0026nbsp;.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch2 id=\"offline-distillation\"\u003e\u003cstrong\u003eOffline Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThis is the most popular model distillation method, which involves pre-training a teacher model, freezing it, and later using it to train a student model. During the model distillation process, the training focuses on the student model as the teacher model remains unmodified.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/lZglb4gNoQCFjcAtPDgBvL0KiCfEZooH84oYVvYFNLZWZdOJi7qV12m1e8tevOTG8Hlmij6GNIflCOO4TTHa2SF6ejlKdCvZvDjV66JRXnpfl3MNXiTB99rSUDbCD_2La2fXslrWyJRbv8xBj-YNca8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"352\" height=\"143\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOffline Distillation. Image from \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2206.12005.pdf\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003earXiv\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"online-distillation\"\u003e\u003cstrong\u003eOnline Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eOnline distillation is an end-to-end technique in which the student and teacher models are trained simultaneously. In other words, the teacher model is continuously updated with new datasets, and the student model is trained to reflect these changes.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/7frHJRuPtc3-RaxfVjJ9xbcwQYKJU4cQPfxy5bVy-HAtRD41bnGLVyU7Lavq2w-zf-q5DnXUmG9AJYE5C9bJHIqxb7VyalJSSGs53kRn3OCzyC4v0GswSqPRbFCBuVcMEw0w2s5gENLvfXjaAWAIJ5w\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"382\" height=\"132\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOnline Distillation. Image from \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2206.12005.pdf\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003earXiv\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"self-distillation\"\u003e\u003cstrong\u003eSelf-Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eSelf-distillation involves using the same networks for the teacher and student models. In this case, a model learns from itself, meaning knowledge from the deeper layers of the model is continuously distilled into its shallow section. This method solves the conventional limitations experienced when either online or offline techniques are used—the discrepancy in accuracy between teacher and student models.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Rh58RzWLGycQSarK3JqZjpQJ15vIqOVTf-MnpG4bd7b3ISaoQwIQZIntnkQss7FpRt-hqdVpovwD-GkpPaNwgMcMmMIxOBW3i5dUUxLDZZVa6xibarSOUA2d8LIUFnCKjl-ceq94w7DOTpNXUgfm-ec\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"264\" height=\"191\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSelf-Distillation. Image from \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2206.12005.pdf\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003earXiv\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"limitations-of-model-distillation\"\u003e\u003cstrong\u003eLimitations of Model Distillation\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eHaving looked at the benefits of model distillation, it’s important to discuss its limitations. While model distillation is a powerful knowledge transfer approach between models in different computational environments, it still suffers from various challenges:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe technical complexities of the distillation process\u003c/li\u003e\u003cli\u003eDifficulty in multi-task learning\u003c/li\u003e\u003cli\u003eThe student model is limited by the teacher\u0026nbsp;\u003c/li\u003e\u003cli\u003ePotential loss of information during the distillation process\u003c/li\u003e\u003cli\u003eLimited applicability on existing proprietary models\u003c/li\u003e\u003c/ul\u003e\u003ch1 id=\"final-thoughts-on-model-distillation\"\u003e\u003cstrong\u003eFinal Thoughts on Model Distillation\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe AI models currently being rolled out are cumbersome and certainly not ideal for deployment straight from the lab. Model distillation provides an alternative model compression mechanism to train lightweight models with comparably lesser computational demands from these heavyweight teacher models. In the realm of artificial intelligence, this model training approach has proven helpful in bridging the gap between limited resources and high-performance capabilities of various AI models. \u003ca href=\"https://labelbox.com/guides/end-to-end-workflow-with-model-distillation-for-computer-vision/\" rel=\"noreferrer\"\u003eModel distillation has limitless potential ahead as AI model training advances.\u003c/a\u003e\u003c/p\u003e\u003cp\u003eLabelbox aids in model distillation by providing a robust platform for generating high-quality labeled datasets, which are essential for both training the teacher model and fine-tuning the student model. With support for various data types and comprehensive management features, Quantumworks Lab also helps organize and version-control datasets, streamlining the process of integrating labeled data into the distillation pipeline. \u003ca href=\"https://app.labelbox.com/signup\"\u003e\u003cu\u003eTry Quantumworks Lab for free\u003c/u\u003e\u003c/a\u003e to enhance the efficiency and effectiveness of the distillation process.\u003c/p\u003e","comment_id":"6628249bd733f0000145b209","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/Model-Distillation.png","featured":false,"status":"published","visibility":"public","created_at":"2024-04-23T21:14:03.000Z","updated_at":"2024-10-02T23:00:01.000Z","published_at":"2024-01-15T21:15:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[],"authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","email":"ldimyadi@labelbox.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":"{\"whatsNew\":{\"lastSeenDate\":\"2025-04-28T15:29:08.000+00:00\"}}","status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2025-05-02T19:01:22.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-29T22:58:55.000Z","updated_at":"2025-05-02T19:01:37.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","email":"ldimyadi@labelbox.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":"{\"whatsNew\":{\"lastSeenDate\":\"2025-04-28T15:29:08.000+00:00\"}}","status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2025-05-02T19:01:22.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-29T22:58:55.000Z","updated_at":"2025-05-02T19:01:37.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":null,"email_segment":"all","url":"https://labelbox-guides.ghost.io/model-distillation/","excerpt":"AI models are increasingly getting bigger with the increase in training data and the number of parameters. For instance, the latest Open AI’s GPT-4 model is estimated to have about 1.76 trillion parameters and terabytes of training corpus. Whether training large language models (LLMs) or neural networks, the main goal remains: to train using as much data as possible. \n\nWhile training from diverse data and increasing the number of parameters produces powerful models, real-world application become","reading_time":8,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"What is Model Distillation?","meta_description":"Model distillation compresses large models for deployment, transferring knowledge from a teacher to a smaller student model efficiently.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":false},"recommended":[]},"__N_SSG":true},"page":"/guides/[id]","query":{"id":"model-distillation"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/model-distillation/?ref=labelbox.ghost.io by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:54:19 GMT -->
</html>