<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:52:37 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">How to Implement Reinforcement Learning from Human Feedback (RLHF)</title><meta name="description" content="RLHF allows users to interactively provide model feedback with corrections, ratings, and preferences. Learn how to implement it with this guide." data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="How to Implement Reinforcement Learning from Human Feedback (RLHF)" data-next-head=""/><meta property="og:description" content="RLHF allows users to interactively provide model feedback with corrections, ratings, and preferences. Learn how to implement it with this guide." data-next-head=""/><meta property="og:url" content="https://labelbox-guides.ghost.io/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/" data-next-head=""/><meta property="og:image" content="https://labelbox-guides.ghost.io/content/images/2024/04/Frame-3697.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="How to Implement Reinforcement Learning from Human Feedback (RLHF)" data-next-head=""/><meta name="twitter:description" content="RLHF allows users to interactively provide model feedback with corrections, ratings, and preferences. Learn how to implement it with this guide." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox-guides.ghost.io/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/" data-next-head=""/><meta property="twitter:image" content="https://labelbox-guides.ghost.io/content/images/2024/04/Frame-3697.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g34[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.giShFC .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.giShFC .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.giShFC .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:2px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h2{padding-top:10px;}}/*!sc*/
.giShFC .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h3{padding-top:10px;}}/*!sc*/
.giShFC .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.giShFC .content a:hover{color:#1e40af;}/*!sc*/
.giShFC .content li{margin-bottom:20px;}/*!sc*/
.giShFC .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.giShFC .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.giShFC .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.giShFC .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}/*!sc*/
.giShFC .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.giShFC .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100% !important;height:100% !important;margin:0 auto;}/*!sc*/
.giShFC .content .kg-button-card{margin-bottom:20px;height:auto;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn{-webkit-transition-property:all;transition-property:all;-webkit-transition-timing-function:cubic-bezier(.4,0,.2,1);transition-timing-function:cubic-bezier(.4,0,.2,1);-webkit-transition-duration:.15s;transition-duration:.15s;background-color:#2563eb;padding:0.75rem;font-size:1rem;line-height:1.5rem;font-weight:500;border-radius:0.5rem;color:#fafafa;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn:hover{background-color:#1d4ed8;}/*!sc*/
.giShFC .content .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 0 0 0.75em;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;}/*!sc*/
data-styled.g35[id="id__PostContentWrapper-sc-1ct5gml-0"]{content:"giShFC,"}/*!sc*/
.kUXEED #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.kUXEED .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.kUXEED .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.kUXEED #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.kUXEED #image-viewer .close:hover,.kUXEED #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.kUXEED .modal-content{width:100%;}}/*!sc*/
data-styled.g36[id="id__ImageModal-sc-1ct5gml-1"]{content:"kUXEED,"}/*!sc*/
@media (max-width:1026px){.cwKcJJ.toc-container{display:none;}}/*!sc*/
.cwKcJJ.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g37[id="id__TocContainer-sc-1ct5gml-2"]{content:"cwKcJJ,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/guides/%5bid%5d-78cf43cbe169ea75.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="id__ImageModal-sc-1ct5gml-1 kUXEED"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All guides</a><main class="id__TocContainer-sc-1ct5gml-2 cwKcJJ toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">How to Implement Reinforcement Learning from Human Feedback (RLHF)</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox-guides.ghost.io/content/images/2024/04/Frame-3697.png"/></div><main class="id__PostContentWrapper-sc-1ct5gml-0 giShFC md:px-24"><div class="content js-toc-content"><p>The Artificial Intelligence (AI) revolution has been brought to reality with the development of systems and solutions that align with human values and preferences. Reinforcement Learning from Human Feedback (RLHF) is one such example of a system that has transformed model training and improved the accuracy and applicability of AI applications.&nbsp;</p><p>Implementing RLHF presents a promising avenue for enhancing AI systems with human guidance. RLHF has been used to develop impressive, human-like conversational bots, such as OpenAI’s ChatGPT. While this model training technique is still under development, its application is widespread, and is the cornerstone of large language models (LLMs).&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>RLHF is an extension of <a href="../../blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/index.html"><u>Reinforcement Learning (RL)</u></a>, a reward and punishment-based training technique for AI models. The only difference from the other RL techniques lies in the introduction of human feedback to ensure that the resultant models behave in safe, ethical, and desirable ways. Instead of relying on predefined rewards, RLHF allows human users to interactively provide feedback to the model in the form of corrections, ratings, and preferences. The feedback is taken to train a reward model, which is then used to fine-tune the target model using a reinforcement learning algorithm.&nbsp;</p><h1 id="human-in-the-loop-as-rlhf-backbone"><strong>Human-in-the-Loop as RLHF Backbone&nbsp;</strong></h1><p>Human feedback is fundamental to RLHF and distinguishes RLHF from other supervised RL techniques. Since most LLMs are trained on a large corpus with diverse contexts and domains, their applicability to individual users are limited. To make these models helpful, harmless, and context-specific, a supervised machine learning technique called human-in-the-loop (HITL) is applied. HITL involves introducing human evaluators in the model-training lifecycle to show the system how to generate human-preferred content. It is considered the backbone of RLHF as it creates a continuous feedback loop where human input is integrated into the AI system to improve its usability in various contexts. The goal of human feedback is to augment a reward system for fine-tuning LLMs beyond sub-optimal generalized performance. This feedback data aligns LLMs with complex human values and preferences.&nbsp;&nbsp;</p><p>Although RLHF is a powerful model training technique, it can be slow and costly to implement and maintain because of its reliance on human feedback. Without the right tools and systems in-place, collecting and annotating human feedback for pre-training and fine-tuning datasets can be an expensive and time-consuming proposition.&nbsp;</p><p>Let’s dive into the current approach being used by AI leaders developing new LLMs, leveraging RLHF to incrementally fine-tune base LLMs using human-provided reward signals and automated inputs.</p><h1 id="the-rlhf-process-a-step-by-step-guide"><strong>The RLHF Process: A Step-by-Step Guide</strong></h1><p>While RLHF is a complex concept coupled with multiple model-training processes and tools, its implementation can be broken down into four straightforward steps:</p><ul><li>Pre-training a language model</li><li>Supervised Fine-tuning</li><li>Training a reward model using Human Feedback</li><li>Fine-tuning the RL policy with the Reward Model&nbsp;&nbsp;</li></ul><h2 id="step-1-pre-training-a-language-model"><strong>Step 1: Pre-training a Language Model&nbsp;</strong></h2><p>Pre-training a language model is the foundation of the RLHF process. It involves coming up with a base model through an end-to-end training or simply selecting a pre-trained language model to begin with. Depending on the approach taken, pretraining is the most tedious, time-consuming, and resource-intensive phase of RLHF. Since training a language model from scratch complicates the RLHF process even more, choosing one of the many pre-trained models is recommended.</p><p>Simply put, RLHF can be seen as a way of unlocking the capabilities of the existing pre-trained models. For example, a <a href="../how-to-train-a-chatbot/index.html"><u>conversational AI chatbot</u></a> can be developed from existing mid-sized models like<a href="../../product/model/foundry-models/llama-3-1-405b/index.html" rel="noreferrer"> <u>LLaMA</u></a>, which has 65 billion parameters, or the extensive <a href="../../product/model/foundry-models/gpt4/index.html"><u>GPT-4 model</u></a>, which has 1.76 trillion parameters. Selecting or pre-training the right base model depends on the available resources and task at hand, as there is no universally best model to kickstart RLHF training.</p><figure class="kg-card kg-image-card"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_FdH1_QOUk8bMYBBwMEoo4ciujjOe4PMT6g6-pHogKAWdEXZl8yNENJRzznXf2VmM7dJhz07fNgv8xcXeZJo_PKTNr-EbPIpVPH35WmNAn1NEpWt4P0oqt3r-ju2F5TfHskuNndxXYgs_j2p6QHEU8zI21i8FVJCQ7rvVpHa9t9AfESPzpFbyxyGVMJvDz9TLYg3TvrxWzWkjIcR5p-ua1_pC6MVSZKvMPaxWC1VXd8Hh5XUXtNPXNp58FWbfAZCf4TR8TQQOmf4VbevuU1L8OHtutekgoKEtybwRuMlJ2fVjL5akVZwbwCvd8Fu0I2nPy-Y669c3b_1XYDbhvLAtJtX6QYenSC2sp5x02Lejrp2smfHF6ahYwhDLWSJ4OzDI5yhqntCgW8jbA1ADcxv2a2ZsNDruTiAmVMj0ap6z0CMEwR9m8ZpOYhOmJaL3oOzk3cRjalFuSDqbJbcDddrXXPen2gTGoT9VbucGat7Qk2Yem9S5GYJPb2W4JGyhP-OKKXVRXT25Qe=s1600" class="kg-image" alt="" loading="lazy" width="1400" height="1046"></figure><p><em>Pretrained LM as a starting point: Image from </em><a href="https://huggingface.co/blog/rlhf"><em><u>Huggingface</u></em></a></p><h2 id="step-2-supervised-fine-tuning"><strong>Step 2: Supervised Fine-tuning</strong></h2><p>The base model pre-trained or selected in step 1 above has the responses that users may want, but lacks the context and capability to generate them in formats expected by users. Therefore, before reinforcement learning, supervised fine-tuning (SFT) is applied on the pre-trained model. The goal of SFT is to prime the model to respond appropriately to different user prompts. It uses <a href="../../solutions/supervised-fine-tuning/index.html" rel="noreferrer"><u>supervised learning</u></a> in which human annotators point the existing model to specific desired patterns through prompting. It is a significant starting point for RLHF implementation. Prompting guides the model towards the desired output per training data. It is important to note that the SFT phase focuses on optimizing the base model’s parameters by distilling the original language model on context clues for the target criteria. For that reason, having a model that responds well to diverse instructions is foundational to the RLHF process.&nbsp;</p><p>The goal of the SFT phase of the RLHF process is to prime the base model to understand user goals, language patterns, and contexts. It exposes the model to diverse linguistic patterns that enable it to generate coherent and contextually appropriate text. The human trainer guides the base model to iterate numerous examples of human-preferred outputs. Throughout this process, the model learns various relationships between words and concepts and their appropriate usage. This text-based machine learning approach is the building block of <a href="../../usecases/natural-language-processing/index.html"><u>Natural Language Processing (NLP)</u></a>. However, at this point, the model still lacks the human touch and preferences. Additional data is needed to bring this human-like feel to the model. This is where human feedback comes in.&nbsp;</p><p>In the next phase of RLHF implementation, a reward model is developed from the pre-trained language model blueprint to integrate human preferences into the system.&nbsp;</p><h2 id="step-3-training-a-reward-model-using-human-feedback"><strong>Step 3: Training a Reward Model Using Human Feedback</strong></h2><p>A reward model (RM) is key for implementing RLHF. In an ideal scenario, we could just fine-tune the base model using RLHF techniques until we achieve a domain-specific model. However, this approach would need large chunks of training samples directly fed into the base model by the human annotator. As a result, this could be slow, expensive, and counterproductive. The best way to overcome these shortcomings is by training a reward model and introducing it in the RL loop.</p><p>A reward model’s precept is to map the input text with a scalar reward value in ways humans would. It is an alignment tool that evaluates the base model’s output and returns a reward signal, which is then used by the main LLM to optimize its parameters.&nbsp;</p><p>Human annotators do the heavy lifting in this phase of RLHF implementation. They generate the training dataset (prompt answer pairs) and rank them according to their preferences before feeding them to the model. The reward model then has to align its ‘rewarding’ system with the patterns of such samples. Nonetheless, this process is subjective since the human perception reinforced by annotators could be biased. As such, there is a need for diversity when creating prompt and reward pairs.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>In practice, annotation prediction would be the most straightforward way to build a reward model. The model could be curated to provide a rating score and determine which output aligns more closely with human preferences. It then rewards the more appropriate output. The reward and prompt pairs are then used to train the reward model to associate specific outputs with reward values.&nbsp;</p><p>Human feedback is then used to refine the reward model as it matures. For instance, users can rank the AI output with either a thumbs-up or thumbs-down feedback. This feedback data gives the reward model insights into human preference. It can then automatically rank the RL agent’s output without human intervention while iteratively learning from such feedback to better imitate humans.&nbsp;</p><figure class="kg-card kg-image-card"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_EmVOm1qQZXu7bBvAce4bo-w765PELcw_Y2Y1dD0hqZnGVdUqY2L-QhsSSki73bCNVW1iEuszYf5CM0bwW_yqEyolocTsLaEVICXNB0qkz5-l-6uXD851h116bF3b2peAOrjxQIrdGdW-cKovjsJ8Idmo42CF_9E0GJ-y7yuQCv_ZKIAkcsVF6yT-fY9i0tzipjfO6NX914Z7_c2NaWRPBlvOwgxz_oRRvtCm1WnHZih-T4Zo2CEEELbNdoyhWsvVXFQfxWTONLicqyXFjQ7yhVEWp-2rHK7nHkDD5dC_cAsibMz0nIK-j6EwCBWi1W7TifrgGNJhd-k2QrDvPbNIuQVAm6NTh2MEHyEGF4nMXyiYCaYKoJ2pkTS5wMzGKi6dvwBZtDke9GcQ2SfPK7QSpXc22UhZNLzes1-n0DrmAT0KiMlYFiYkrZjjL7dqwFcz-Ux2e6gjAGX3tsxmkYURdOEWYH-85bJILzXJkolmVCMoCWRTltshE18uTQ9MbzFHj_7Y-kOwl9=s1600" class="kg-image" alt="" loading="lazy" width="1600" height="1000"></figure><p><em>Reward model training lifecycle </em><a href="https://bdtechtalks.com/2023/01/16/what-is-rlhf/"><em><u>TechTalks</u></em></a></p><h2 id="step-4-fine-tuning-the-rl-policy-with-the-reward-model"><strong>Step 4: Fine-Tuning the RL Policy with the Reward Model&nbsp;</strong></h2><p><a href="../../blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/index.html"><u>Fine-tuning</u></a> is one of the ways to unlock the potential of LLMs. It involves upskilling the base model for specific tasks and adapting it to more specialized domains. This is the last phase of RLHF implementation. It involves creating a feedback loop to train and fine-tune the RL policy (a copy of the original LLM) with the reward model trained in step 2 above.&nbsp;</p><p>The RL policy interacts with the reward model by taking reward signals to adjust its behavior. It then sends its output to the reward model for evaluation. The output is evaluated, and a reward score is sent back to the RL policy. Through the RM’s reward score, the RL policy can generate responses it deems human-preferable.</p><p>A policy-gradient RL algorithm called <a href="https://arxiv.org/abs/1707.06347"><u>Proximal Policy Optimization (PPO)</u></a> and the <a href="../../../hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf"><u>Kullback-Leibler (KL) Divergence</u></a> are the basis of this RLHF phase. The RL policy is optimized using PPO, which balances exploitation and exploration during training. At this point, some base LLM parameters are frozen because fine-tuning, let’s say, 65 billion parameters would be practically slow.&nbsp;</p><p>PPO fine-tuning improves training stability by limiting changes made to the policy at each training epoch. However, given the chance, the PPO algorithm might exploit the imperfections of a reward model and generate hogwash output. To counter such exploits, the KL penalty is introduced.&nbsp;</p><p>In RLHF, Kullback-Leibler (KL) Divergence measures the difference between a reference distribution representing the most human-aligned response and the RL policy’s current responses. Simply put, it penalizes the RL policy from substantially veering off the base model with each training batch.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><figure class="kg-card kg-image-card"><img src="https://work.fife.usercontent.google.com/fife/ALs6j_GgVRfZa102sjThKGbNtktgg6vLPk06M5o4fLVhlq_q2hhvy_9KeGSW4uKOELmjQlai52JOlAFxHW13iZGOomEJJws07BPuQpN7ANMOMqxuYrljCS1-YN1vWjfzYBvUfpVdhl8JLzyk7MY83dJzAgh2g8uS8McAelTnTWdrvSaA8wtw1qDpe5Xqvmrqc1HFQCAgct6XmuiMh2EDkqIObaXK50G8jJc7Dzk6UhZ-2UBYBT81CijfdwaISx9LXN5wvO-Q7hVft6OX6xPaS8eUJttUPJbs45Wydbuzf3qNiAGhokPo0pa6HEVTALLncJQpUzuJ-d84Pc8Ym0iJxpbgIIc8mpBXHzxWnizYrrCuFaz3DWk7EffgorRqSo9yNsAoSCOySijymAzVcQcX1Nuuhu8mWYoSQxOlcJBK0x9TsRN8yV6HSziLtR1l4Eih_wtbJduBjKp9dK7Q1y9V4vQO3cV88XeJbx4rt6csI58pUI-2d4c61T_ah5Bty9gYmH8Xm_xt9wKnzgm_=s1600" class="kg-image" alt="" loading="lazy" width="1400" height="695"></figure><p><em>RL policy fine-tuning using the reward model and the PPO from </em><a href="https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093?permalink_comment_id=4519136"><em><u>GitHub Gist</u></em></a></p><p>Fine-tuning with the reward model discourages inappropriate responses by punishing those with low rewards. Since such low-reward outputs are unlikely to be repeated, the language model iteratively learns and produces outputs that align closely with human expectations– this is the prime of RLHF.&nbsp;&nbsp;&nbsp;</p><h1 id="rlhf-use-cases"><strong>RLHF Use Cases</strong></h1><p>RLHF has proven useful in developing models used in healthcare, technology, banking, and finance, among other fields, from pre-trained models like GPT-4 and LLaMA. OpenAI’s InstructGPT, Anthropic’s Claude and Google’s Gemini are some of the successful applications of RLHF.&nbsp;&nbsp;</p><ul><li><strong>OpenAI’s InstructGPT –</strong> In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. <a href="https://openai.com/research/instruction-following"><u>OpenAI used the prompts submitted by its customers to the API as human feedback</u></a>. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF.&nbsp;</li><li><strong>Anthropic’s Claude –</strong> RLHF has also been applied in training<a href="https://www.anthropic.com/claude" rel="noreferrer"><u> Claude,</u></a> Anthropic’s next-gen AI, to be more helpful. The AI assistant depends on human feedback and AI Constitution to align its responses with human values and preferences.&nbsp;&nbsp;&nbsp;&nbsp;</li><li><strong>Google’s Gemini –</strong> Google Deepmind introduced Gemini Ultra, a model enhanced through RLHF. This powerful model <a href="https://deepmind.google/technologies/gemini/#gemini-1.0"><u>outperformed GPT-4 on several benchmarks, including reasoning and math</u></a>, because it was primed to generate helpful and harmless responses using RLHF.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li></ul><h1 id="challenges-of-rlhf"><strong>Challenges of RLHF&nbsp;</strong></h1><p>While RLHF emerged as a groundbreaking AI model training technique, its implementation is not always a straightforward process. Some of the limitations of RLHF are:</p><ul><li><strong>Shortcomings of the human agents –</strong> Introducing human agents into the training cycle comes with issues of reliance, scalability, and bias (divergence from the expected outcome). Ineffective human feedback may lead to suboptimal performance and create biases, leading to skewed learning.&nbsp;</li><li><strong>Time and resource complexities –</strong> Scaling the models to handle more complex tasks could also be time-consuming and resource-intensive with the introduction of human agents in the training cycle.</li></ul><p>However, the benefits outweigh the setup and maintenance costs of implementing RHF. The challenges can be mitigated by balancing feedback, diversifying the perspectives of human annotators, and evaluating performance of the model from time to time. Another method used by companies like Anthropic and Google to mitigate the cost and timeframe of gathering the necessary feedback is RLAIF, or reinforcement learning from AI feedback. By using another LLM to augment or replace the volume of human feedback needed during the supervised fine-tuning stage, teams can move faster and even choose to focus human efforts on more expert-level, domain-specific evaluation tasks.</p><h1 id="final-thoughts-on-reinforcement-learning-from-human-feedback-rlhf"><strong>Final Thoughts on Reinforcement Learning from Human Feedback (RLHF)</strong></h1><p>As AI advances, RLHF ensures that the LLMs’ capabilities are aligned with complex human preferences, goals, and environments. RLHF has significantly revolutionized the subfield of NLP, specifically downstream LLM applications.&nbsp;</p><p>It has pioneered the humanization of AI solutions, following feedback from users and the alignment of human annotators. We have seen that implementing RLHF takes a three-step process that starts with a pre-trained model and ends with fine-tuning the base model with a reward model trained from human feedback.&nbsp;</p><p>Introducing humans in the training loop is the cornerstone of RLHF, although a balance is needed as slight inefficiencies could lead to biases and skew the model learning process. It is important to note that RLHF performance is only as good as the quality of human annotators and the human-generated text used for fine-tuning.&nbsp;</p><h1 id="get-started-with-rlhf-today"><strong>Get started with RLHF today </strong></h1><p>Quantumworks Lab is a complete solution combining the best tools and fully managed services for RLHF and LLM evaluation. We ensure high quality, trustworthy, safe outputs with highly accurate datasets for instruction tuning, RLHF, and supervised fine-tuning. <br><br>Ready to experiment with implementing humans-in-the loop for your generative AI projects? Try out our <a href="../../services/labeling/index.html" rel="noreferrer">on-demand expert labeling services</a> with SMEs for your projects in any expert domain or popular languages.&nbsp;<br><br> Or want to directly find the right experts yourself? Quickly search and select your own team of expert AI labelers with <a href="../../services/alignerr-connect/index.html" rel="noreferrer">Quantumworks Lab's Alignerr Connect</a>. </p></div></main></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="Footer__StyledFooter-sc-u68pnv-0 eJChXt"><div class="undefined lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="py-24"><div class=" w-full h-[1px] bg-neutral-200"></div></div><div class="hidden md:block"><img src="../../static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36"/></div><section class="hidden md:grid footer-grid"></section><section class="social-media"></section><div class="text-center "><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">© Quantumworks Lab, Inc <br/>We enable breakthroughs</p><div class="flex flex-row flex-wrap justify-content-center gap-4 mt-4"><a href="https://docs.labelbox.com/page/terms-of-service" class=" " target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Terms of Service</p></a><div class="mx-1 border"></div><a href="https://docs.labelbox.com/page/privacy-notice" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Privacy Notice</p></a><div class="mx-1 border hidden sm:block"></div><a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Copyright Dispute Policy</p></a></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"how-to-implement-reinforcement-learning-from-human-feedback-rlhf","id":"661577554f0096000190d872","uuid":"724b64ec-ebdb-4c3f-a6a3-dc485e867fa2","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","html":"\u003cp\u003eThe Artificial Intelligence (AI) revolution has been brought to reality with the development of systems and solutions that align with human values and preferences. Reinforcement Learning from Human Feedback (RLHF) is one such example of a system that has transformed model training and improved the accuracy and applicability of AI applications.\u0026nbsp;\u003c/p\u003e\u003cp\u003eImplementing RLHF presents a promising avenue for enhancing AI systems with human guidance. RLHF has been used to develop impressive, human-like conversational bots, such as OpenAI’s ChatGPT. While this model training technique is still under development, its application is widespread, and is the cornerstone of large language models (LLMs).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eRLHF is an extension of \u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/\"\u003e\u003cu\u003eReinforcement Learning (RL)\u003c/u\u003e\u003c/a\u003e, a reward and punishment-based training technique for AI models. The only difference from the other RL techniques lies in the introduction of human feedback to ensure that the resultant models behave in safe, ethical, and desirable ways. Instead of relying on predefined rewards, RLHF allows human users to interactively provide feedback to the model in the form of corrections, ratings, and preferences. The feedback is taken to train a reward model, which is then used to fine-tune the target model using a reinforcement learning algorithm.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"human-in-the-loop-as-rlhf-backbone\"\u003e\u003cstrong\u003eHuman-in-the-Loop as RLHF Backbone\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eHuman feedback is fundamental to RLHF and distinguishes RLHF from other supervised RL techniques. Since most LLMs are trained on a large corpus with diverse contexts and domains, their applicability to individual users are limited. To make these models helpful, harmless, and context-specific, a supervised machine learning technique called human-in-the-loop (HITL) is applied. HITL involves introducing human evaluators in the model-training lifecycle to show the system how to generate human-preferred content. It is considered the backbone of RLHF as it creates a continuous feedback loop where human input is integrated into the AI system to improve its usability in various contexts. The goal of human feedback is to augment a reward system for fine-tuning LLMs beyond sub-optimal generalized performance. This feedback data aligns LLMs with complex human values and preferences.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eAlthough RLHF is a powerful model training technique, it can be slow and costly to implement and maintain because of its reliance on human feedback. Without the right tools and systems in-place, collecting and annotating human feedback for pre-training and fine-tuning datasets can be an expensive and time-consuming proposition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLet’s dive into the current approach being used by AI leaders developing new LLMs, leveraging RLHF to incrementally fine-tune base LLMs using human-provided reward signals and automated inputs.\u003c/p\u003e\u003ch1 id=\"the-rlhf-process-a-step-by-step-guide\"\u003e\u003cstrong\u003eThe RLHF Process: A Step-by-Step Guide\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWhile RLHF is a complex concept coupled with multiple model-training processes and tools, its implementation can be broken down into four straightforward steps:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePre-training a language model\u003c/li\u003e\u003cli\u003eSupervised Fine-tuning\u003c/li\u003e\u003cli\u003eTraining a reward model using Human Feedback\u003c/li\u003e\u003cli\u003eFine-tuning the RL policy with the Reward Model\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"step-1-pre-training-a-language-model\"\u003e\u003cstrong\u003eStep 1: Pre-training a Language Model\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003ePre-training a language model is the foundation of the RLHF process. It involves coming up with a base model through an end-to-end training or simply selecting a pre-trained language model to begin with. Depending on the approach taken, pretraining is the most tedious, time-consuming, and resource-intensive phase of RLHF. Since training a language model from scratch complicates the RLHF process even more, choosing one of the many pre-trained models is recommended.\u003c/p\u003e\u003cp\u003eSimply put, RLHF can be seen as a way of unlocking the capabilities of the existing pre-trained models. For example, a \u003ca href=\"https://labelbox.com/guides/how-to-train-a-chatbot/\"\u003e\u003cu\u003econversational AI chatbot\u003c/u\u003e\u003c/a\u003e can be developed from existing mid-sized models like\u003ca href=\"https://labelbox.com/product/model/foundry-models/llama-3-1-405b/\" rel=\"noreferrer\"\u003e \u003cu\u003eLLaMA\u003c/u\u003e\u003c/a\u003e, which has 65 billion parameters, or the extensive \u003ca href=\"https://labelbox.com/product/model/foundry-models/gpt4/\"\u003e\u003cu\u003eGPT-4 model\u003c/u\u003e\u003c/a\u003e, which has 1.76 trillion parameters. Selecting or pre-training the right base model depends on the available resources and task at hand, as there is no universally best model to kickstart RLHF training.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/ohLISJBxZIz4eLtcCos26LxM9P_IQY8X0gWQJWUbvu9PQkBpIBULSsGTNaZ5YYt6zwePlXcVCHODTiUKDh2A1bBj1TVI_Mi_8QRMUDhmy_CQYEHAjWheRnoqENVkIvdtPRqiR06-lwUuZdwYmygyr7o\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1400\" height=\"1046\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePretrained LM as a starting point: Image from \u003c/em\u003e\u003ca href=\"https://huggingface.co/blog/rlhf\"\u003e\u003cem\u003e\u003cu\u003eHuggingface\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"step-2-supervised-fine-tuning\"\u003e\u003cstrong\u003eStep 2: Supervised Fine-tuning\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe base model pre-trained or selected in step 1 above has the responses that users may want, but lacks the context and capability to generate them in formats expected by users. Therefore, before reinforcement learning, supervised fine-tuning (SFT) is applied on the pre-trained model. The goal of SFT is to prime the model to respond appropriately to different user prompts. It uses \u003ca href=\"https://labelbox.com/solutions/supervised-fine-tuning/\" rel=\"noreferrer\"\u003e\u003cu\u003esupervised learning\u003c/u\u003e\u003c/a\u003e in which human annotators point the existing model to specific desired patterns through prompting. It is a significant starting point for RLHF implementation. Prompting guides the model towards the desired output per training data. It is important to note that the SFT phase focuses on optimizing the base model’s parameters by distilling the original language model on context clues for the target criteria. For that reason, having a model that responds well to diverse instructions is foundational to the RLHF process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe goal of the SFT phase of the RLHF process is to prime the base model to understand user goals, language patterns, and contexts. It exposes the model to diverse linguistic patterns that enable it to generate coherent and contextually appropriate text. The human trainer guides the base model to iterate numerous examples of human-preferred outputs. Throughout this process, the model learns various relationships between words and concepts and their appropriate usage. This text-based machine learning approach is the building block of \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/\"\u003e\u003cu\u003eNatural Language Processing (NLP)\u003c/u\u003e\u003c/a\u003e. However, at this point, the model still lacks the human touch and preferences. Additional data is needed to bring this human-like feel to the model. This is where human feedback comes in.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn the next phase of RLHF implementation, a reward model is developed from the pre-trained language model blueprint to integrate human preferences into the system.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"step-3-training-a-reward-model-using-human-feedback\"\u003e\u003cstrong\u003eStep 3: Training a Reward Model Using Human Feedback\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eA reward model (RM) is key for implementing RLHF. In an ideal scenario, we could just fine-tune the base model using RLHF techniques until we achieve a domain-specific model. However, this approach would need large chunks of training samples directly fed into the base model by the human annotator. As a result, this could be slow, expensive, and counterproductive. The best way to overcome these shortcomings is by training a reward model and introducing it in the RL loop.\u003c/p\u003e\u003cp\u003eA reward model’s precept is to map the input text with a scalar reward value in ways humans would. It is an alignment tool that evaluates the base model’s output and returns a reward signal, which is then used by the main LLM to optimize its parameters.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHuman annotators do the heavy lifting in this phase of RLHF implementation. They generate the training dataset (prompt answer pairs) and rank them according to their preferences before feeding them to the model. The reward model then has to align its ‘rewarding’ system with the patterns of such samples. Nonetheless, this process is subjective since the human perception reinforced by annotators could be biased. As such, there is a need for diversity when creating prompt and reward pairs.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn practice, annotation prediction would be the most straightforward way to build a reward model. The model could be curated to provide a rating score and determine which output aligns more closely with human preferences. It then rewards the more appropriate output. The reward and prompt pairs are then used to train the reward model to associate specific outputs with reward values.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHuman feedback is then used to refine the reward model as it matures. For instance, users can rank the AI output with either a thumbs-up or thumbs-down feedback. This feedback data gives the reward model insights into human preference. It can then automatically rank the RL agent’s output without human intervention while iteratively learning from such feedback to better imitate humans.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Jn7XN-gMuM5s2KrzOOTokotEXcNXArdG48Lk1N_f6_4OWQ5EKyK_Dn1WtPcIxX7e3ybQFe5xlgMPmlG48G5o8WmUyH4fB9t4lsDgk2OplIPlnFqsdqPXyCn1AeufgT35-oZUxgl3jTcSCwEpPmXiQD0\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1000\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eReward model training lifecycle \u003c/em\u003e\u003ca href=\"https://bdtechtalks.com/2023/01/16/what-is-rlhf/\"\u003e\u003cem\u003e\u003cu\u003eTechTalks\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"step-4-fine-tuning-the-rl-policy-with-the-reward-model\"\u003e\u003cstrong\u003eStep 4: Fine-Tuning the RL Policy with the Reward Model\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/\"\u003e\u003cu\u003eFine-tuning\u003c/u\u003e\u003c/a\u003e is one of the ways to unlock the potential of LLMs. It involves upskilling the base model for specific tasks and adapting it to more specialized domains. This is the last phase of RLHF implementation. It involves creating a feedback loop to train and fine-tune the RL policy (a copy of the original LLM) with the reward model trained in step 2 above.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe RL policy interacts with the reward model by taking reward signals to adjust its behavior. It then sends its output to the reward model for evaluation. The output is evaluated, and a reward score is sent back to the RL policy. Through the RM’s reward score, the RL policy can generate responses it deems human-preferable.\u003c/p\u003e\u003cp\u003eA policy-gradient RL algorithm called \u003ca href=\"https://arxiv.org/abs/1707.06347\"\u003e\u003cu\u003eProximal Policy Optimization (PPO)\u003c/u\u003e\u003c/a\u003e and the \u003ca href=\"https://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf\"\u003e\u003cu\u003eKullback-Leibler (KL) Divergence\u003c/u\u003e\u003c/a\u003e are the basis of this RLHF phase. The RL policy is optimized using PPO, which balances exploitation and exploration during training. At this point, some base LLM parameters are frozen because fine-tuning, let’s say, 65 billion parameters would be practically slow.\u0026nbsp;\u003c/p\u003e\u003cp\u003ePPO fine-tuning improves training stability by limiting changes made to the policy at each training epoch. However, given the chance, the PPO algorithm might exploit the imperfections of a reward model and generate hogwash output. To counter such exploits, the KL penalty is introduced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn RLHF, Kullback-Leibler (KL) Divergence measures the difference between a reference distribution representing the most human-aligned response and the RL policy’s current responses. Simply put, it penalizes the RL policy from substantially veering off the base model with each training batch.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/5hUDv7nrLlXTVmj5JzKfa3BS-kXNq6_IG2hg1i2MCeUxcKlqLAvTAjNpE1x63KoFIZHT81J14XVeyNgfS7QgAfimPPF9ZTKgAHd7t2UogMUogdG93_OJUm--3XOb4IN3rOor1_12Bz7qczWUwPYtCLE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1400\" height=\"695\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eRL policy fine-tuning using the reward model and the PPO from \u003c/em\u003e\u003ca href=\"https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093?permalink_comment_id=4519136\"\u003e\u003cem\u003e\u003cu\u003eGitHub Gist\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003eFine-tuning with the reward model discourages inappropriate responses by punishing those with low rewards. Since such low-reward outputs are unlikely to be repeated, the language model iteratively learns and produces outputs that align closely with human expectations– this is the prime of RLHF.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"rlhf-use-cases\"\u003e\u003cstrong\u003eRLHF Use Cases\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eRLHF has proven useful in developing models used in healthcare, technology, banking, and finance, among other fields, from pre-trained models like GPT-4 and LLaMA. OpenAI’s InstructGPT, Anthropic’s Claude and Google’s Gemini are some of the successful applications of RLHF.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI’s InstructGPT –\u003c/strong\u003e In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. \u003ca href=\"https://openai.com/research/instruction-following\"\u003e\u003cu\u003eOpenAI used the prompts submitted by its customers to the API as human feedback\u003c/u\u003e\u003c/a\u003e. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAnthropic’s Claude –\u003c/strong\u003e RLHF has also been applied in training\u003ca href=\"https://www.anthropic.com/claude\" rel=\"noreferrer\"\u003e\u003cu\u003e Claude,\u003c/u\u003e\u003c/a\u003e Anthropic’s next-gen AI, to be more helpful. The AI assistant depends on human feedback and AI Constitution to align its responses with human values and preferences.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle’s Gemini –\u003c/strong\u003e Google Deepmind introduced Gemini Ultra, a model enhanced through RLHF. This powerful model \u003ca href=\"https://deepmind.google/technologies/gemini/#gemini-1.0\"\u003e\u003cu\u003eoutperformed GPT-4 on several benchmarks, including reasoning and math\u003c/u\u003e\u003c/a\u003e, because it was primed to generate helpful and harmless responses using RLHF.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch1 id=\"challenges-of-rlhf\"\u003e\u003cstrong\u003eChallenges of RLHF\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWhile RLHF emerged as a groundbreaking AI model training technique, its implementation is not always a straightforward process. Some of the limitations of RLHF are:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eShortcomings of the human agents –\u003c/strong\u003e Introducing human agents into the training cycle comes with issues of reliance, scalability, and bias (divergence from the expected outcome). Ineffective human feedback may lead to suboptimal performance and create biases, leading to skewed learning.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTime and resource complexities –\u003c/strong\u003e Scaling the models to handle more complex tasks could also be time-consuming and resource-intensive with the introduction of human agents in the training cycle.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHowever, the benefits outweigh the setup and maintenance costs of implementing RHF. The challenges can be mitigated by balancing feedback, diversifying the perspectives of human annotators, and evaluating performance of the model from time to time. Another method used by companies like Anthropic and Google to mitigate the cost and timeframe of gathering the necessary feedback is RLAIF, or reinforcement learning from AI feedback. By using another LLM to augment or replace the volume of human feedback needed during the supervised fine-tuning stage, teams can move faster and even choose to focus human efforts on more expert-level, domain-specific evaluation tasks.\u003c/p\u003e\u003ch1 id=\"final-thoughts-on-reinforcement-learning-from-human-feedback-rlhf\"\u003e\u003cstrong\u003eFinal Thoughts on Reinforcement Learning from Human Feedback (RLHF)\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAs AI advances, RLHF ensures that the LLMs’ capabilities are aligned with complex human preferences, goals, and environments. RLHF has significantly revolutionized the subfield of NLP, specifically downstream LLM applications.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIt has pioneered the humanization of AI solutions, following feedback from users and the alignment of human annotators. We have seen that implementing RLHF takes a three-step process that starts with a pre-trained model and ends with fine-tuning the base model with a reward model trained from human feedback.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIntroducing humans in the training loop is the cornerstone of RLHF, although a balance is needed as slight inefficiencies could lead to biases and skew the model learning process. It is important to note that RLHF performance is only as good as the quality of human annotators and the human-generated text used for fine-tuning.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"get-started-with-rlhf-today\"\u003e\u003cstrong\u003eGet started with RLHF today \u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eLabelbox is a complete solution combining the best tools and fully managed services for RLHF and LLM evaluation. We ensure high quality, trustworthy, safe outputs with highly accurate datasets for instruction tuning, RLHF, and supervised fine-tuning. \u003cbr\u003e\u003cbr\u003eReady to experiment with implementing humans-in-the loop for your generative AI projects? Try out our \u003ca href=\"https://labelbox.com/services/labeling/\" rel=\"noreferrer\"\u003eon-demand expert labeling services\u003c/a\u003e with SMEs for your projects in any expert domain or popular languages.\u0026nbsp;\u003cbr\u003e\u003cbr\u003e Or want to directly find the right experts yourself? Quickly search and select your own team of expert AI labelers with \u003ca href=\"https://labelbox.com/services/alignerr-connect/\" rel=\"noreferrer\"\u003eLabelbox's Alignerr Connect\u003c/a\u003e. \u003c/p\u003e","comment_id":"661577554f0096000190d872","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/Frame-3697.png","featured":false,"status":"published","visibility":"public","created_at":"2024-04-09T17:13:57.000Z","updated_at":"2024-11-20T23:41:32.000Z","published_at":"2024-01-08T17:33:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/","tags":[],"authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","email":"ldimyadi@labelbox.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":"{\"whatsNew\":{\"lastSeenDate\":\"2025-04-28T15:29:08.000+00:00\"}}","status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2025-05-02T19:01:22.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-29T22:58:55.000Z","updated_at":"2025-05-02T19:01:37.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","email":"ldimyadi@labelbox.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":"{\"whatsNew\":{\"lastSeenDate\":\"2025-04-28T15:29:08.000+00:00\"}}","status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2025-05-02T19:01:22.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-29T22:58:55.000Z","updated_at":"2025-05-02T19:01:37.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":null,"email_segment":"all","url":"https://labelbox-guides.ghost.io/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/","excerpt":"The Artificial Intelligence (AI) revolution has been brought to reality with the development of systems and solutions that align with human values and preferences. Reinforcement Learning from Human Feedback (RLHF) is one such example of a system that has transformed model training and improved the accuracy and applicability of AI applications. \n\nImplementing RLHF presents a promising avenue for enhancing AI systems with human guidance. RLHF has been used to develop impressive, human-like convers","reading_time":9,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to Implement Reinforcement Learning from Human Feedback","meta_description":"RLHF allows users to interactively provide model feedback with corrections, ratings, and preferences. Learn how to implement it with this guide.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":false},"recommended":[]},"__N_SSG":true},"page":"/guides/[id]","query":{"id":"how-to-implement-reinforcement-learning-from-human-feedback-rlhf"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:53:13 GMT -->
</html>