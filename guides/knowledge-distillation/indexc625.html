<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/knowledge-distillation/?ref=labelbox.ghost.io by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:54:19 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">How to do knowledge distillation</title><meta name="description" content="Knowledge distillation compresses large, powerful AI models into smaller, faster versions without losing performance. Learn how it works." data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="How to do knowledge distillation" data-next-head=""/><meta property="og:description" content="Knowledge distillation compresses large, powerful AI models into smaller, faster versions without losing performance. Learn how it works." data-next-head=""/><meta property="og:url" content="https://labelbox-guides.ghost.io/knowledge-distillation/" data-next-head=""/><meta property="og:image" content="https://labelbox-guides.ghost.io/content/images/2024/07/knowledge-distillation.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="How to do knowledge distillation" data-next-head=""/><meta name="twitter:description" content="Knowledge distillation compresses large, powerful AI models into smaller, faster versions without losing performance. Learn how it works." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox-guides.ghost.io/knowledge-distillation/" data-next-head=""/><meta property="twitter:image" content="https://labelbox-guides.ghost.io/content/images/2024/07/knowledge-distillation.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g34[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.giShFC .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.giShFC .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.giShFC .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:2px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h2{padding-top:10px;}}/*!sc*/
.giShFC .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h3{padding-top:10px;}}/*!sc*/
.giShFC .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.giShFC .content a:hover{color:#1e40af;}/*!sc*/
.giShFC .content li{margin-bottom:20px;}/*!sc*/
.giShFC .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.giShFC .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.giShFC .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.giShFC .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}/*!sc*/
.giShFC .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.giShFC .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100% !important;height:100% !important;margin:0 auto;}/*!sc*/
.giShFC .content .kg-button-card{margin-bottom:20px;height:auto;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn{-webkit-transition-property:all;transition-property:all;-webkit-transition-timing-function:cubic-bezier(.4,0,.2,1);transition-timing-function:cubic-bezier(.4,0,.2,1);-webkit-transition-duration:.15s;transition-duration:.15s;background-color:#2563eb;padding:0.75rem;font-size:1rem;line-height:1.5rem;font-weight:500;border-radius:0.5rem;color:#fafafa;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn:hover{background-color:#1d4ed8;}/*!sc*/
.giShFC .content .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 0 0 0.75em;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;}/*!sc*/
data-styled.g35[id="id__PostContentWrapper-sc-1ct5gml-0"]{content:"giShFC,"}/*!sc*/
.kUXEED #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.kUXEED .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.kUXEED .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.kUXEED #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.kUXEED #image-viewer .close:hover,.kUXEED #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.kUXEED .modal-content{width:100%;}}/*!sc*/
data-styled.g36[id="id__ImageModal-sc-1ct5gml-1"]{content:"kUXEED,"}/*!sc*/
@media (max-width:1026px){.cwKcJJ.toc-container{display:none;}}/*!sc*/
.cwKcJJ.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g37[id="id__TocContainer-sc-1ct5gml-2"]{content:"cwKcJJ,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/guides/%5bid%5d-78cf43cbe169ea75.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="id__ImageModal-sc-1ct5gml-1 kUXEED"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All guides</a><main class="id__TocContainer-sc-1ct5gml-2 cwKcJJ toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">How to do knowledge distillation</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox-guides.ghost.io/content/images/2024/07/knowledge-distillation.png"/></div><main class="id__PostContentWrapper-sc-1ct5gml-0 giShFC md:px-24"><div class="content js-toc-content"><p>Knowledge Distillation, which compresses large, powerful AI models into smaller, faster versions without losing performance, vital for efficient deployment on less powerful devices, has become an important technique for AI development and streamline the process of building intelligent applications.&nbsp;</p><p>As advancements in artificial intelligence continue, <a href="../../usecases/large-language-models/index.html"><u>large language models (LLMs)</u></a> and deep neural networks (DNNs) are becoming increasingly capable. The latest iterations outperform their predecessors, partly due to <a href="https://deepchecks.com/question/how-does-the-size-of-the-training-data-affect-the-accuracy/#:~:text=A%20machine%20learning%20model's%20accuracy,and%20are%20expensive%20to%20store."><u>expanded datasets used during training</u></a>.&nbsp;</p><p>These sophisticated models are invaluable across various sectors including marketing, cybersecurity, logistics, and medical diagnostics. However, their deployment is often limited by the significant computing resources they require.</p><p>As these models grow in complexity with increased data and parameters, they also become larger and slower, which complicates deployment on less powerful user devices such as office computers, embedded systems, and mobile devices. Knowledge distillation is the ideal solution to this problem, allowing models to maintain similar accuracy and performance in a much smaller, more deployable format.</p><p>In this article, we will learn how knowledge is distilled from large language models to smaller models that can be deployed in downstream edge devices and user systems.</p><h1 id="what-is-knowledge-distillation"><strong>What is knowledge distillation?</strong></h1><p>Knowledge distillation is a technique where a smaller, simpler model—referred to as the "student"—learns from a larger, more complex model, known as the "teacher." This process goes beyond just mimicking the final decision outputs (hard targets) of the teacher; it crucially involves the student model learning from the soft output distributions (soft labels) provided by the teacher.&nbsp;</p><p>These soft labels represent the probabilities the teacher model assigns to each class, conveying not just the decision but also the confidence levels across potential outcomes. The goal of this method is to transfer the comprehensive knowledge of the teacher model to a student model that retains much of the teacher’s accuracy and performance but with significantly fewer parameters.&nbsp;</p><p>This allows the student model to be deployed on user devices where computational resources are more limited. Knowledge distillation thus offers a strategic trade-off between the robust training capabilities of large models and the deployment needs of smaller, more efficient ones. The outcome is a compact model that meets the latency, throughput, and performance benchmarks of its larger counterpart but is more suitable for environments with resource constraints.&nbsp;</p><h1 id="what-is-the-knowledge-distillation-process"><strong>What is the knowledge distillation process</strong>?</h1><p>The student-teacher architecture is the basis for knowledge distillation. It ensures that the teacher model can be compressed into a simpler student model deployed on low-grade devices. The student model can learn as much as possible from the teacher model through this architecture, capturing all the knowledge with minimal computing resources. This model compression technique banks on three processes: pre-training the teacher model, knowledge transfer, and refining the student model.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../lh7-us.googleusercontent.com/docsz/AD_4nXcZthO0ui4PEtOFEY3UBxRpbNdJxRicYT-CXmt9FiV1CiDJVWqF7a1Mfogq3_jqxeRt9-KKxL725-BJnw9Ag5xgbQem_rPBX4jZaFZff7SJ5f6xCMYnwuBNG_2-qjTdK3mC1DbB28COgRJSRWm3MAjmDmAt.a507cf7?key=DqY3bu6KydZu1cDgHMknJQ" class="kg-image" alt="" loading="lazy" width="600" height="298"><figcaption><span style="white-space: pre-wrap;">A generic representation of the Knowledge Distillation process. Image from </span><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285901"><u><span class="underline" style="white-space: pre-wrap;">Plos One</span></u></a></figcaption></figure><p>The <strong>teacher model pre-training</strong> phase builds the knowledge to be transferred during the distillation process. In this step of the knowledge distillation process, a complex model is trained on large datasets using standard machine learning procedures.&nbsp;</p><p>The pre-training phase is the most expensive and resource-intensive, as the teacher model training requires extensive datasets and computing resources. The teacher model can take the form of existing legacy models like GPT, Llama, or BERT, with billions of parameters and gigabytes of data.</p><p>Once trained, the teacher model generates soft labels (logits) for the training data, which are later used to supervise the student model. In simple terms, these soft labels are the output probability distributions provided by the teacher model for training the student model.&nbsp;</p><p>Unlike hard (binary) labels, soft labels provide more information on the prediction probabilities over the classes. The teacher model pre-training must be thorough enough to capture all the nuances of prediction or the input-output correlations to be transferred to the student model.</p><p>Next, the <strong>student model is trained</strong> on the teacher model-generated soft labels (output), features, or relations, depending on the chosen type of distillation. <strong>Knowledge transfer</strong> happens in this phase of knowledge distillation. The training of the student model aims to distill the knowledge of the teacher model by matching its soft targets to the student model. In doing so, the difference between the student model predictions and the teacher model's soft labels is minimized.</p><p>Besides soft labels, the student model is also trained to learn the teacher model's pairwise relation between data points. It is also primed to mimic the feature representation of the teacher model's layers. These processes steer the student model to capture almost all the knowledge of the teacher model and obtain similar or higher accuracy while using minimal computing resources.&nbsp;</p><p>To enhance the performance of the student model, it is refined through further training. Once the knowledge has been distilled from the teacher to the student model, the <strong>student model may undergo additional training</strong> with the original dataset and hyperparameter tuning. Refining the student model augments the knowledge distillation process, ensuring we achieve an optimal model.</p><h1 id="knowledge-categories"><strong>Knowledge categories</strong></h1><p>As discussed in the knowledge distillation process above, the student model can learn from different knowledge categories of the teacher model. This knowledge includes soft labels, intermediate layer features, and the relationship between various layers and data points. Various types of knowledge distillation emerge from these knowledge categories. As a result, we have three known knowledge distillation types:</p><ul><li><strong>Response-based knowledge</strong>: The student model learns from the soft outputs of the teacher model.</li><li><strong>Feature-based knowledge</strong>: The student model mimics intermediate feature representations from the teacher.</li><li><strong>Relation-based knowledge</strong>: Focuses on the relationships between different layers and data points in the teacher model.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../lh7-us.googleusercontent.com/docsz/AD_4nXecgNwozETH0rMVZLeQ6e-k9K6_Rt2PXv7owTcjrhRYOsF78p9SLKKr92XfkN-nLlP3UmdUgg08P5R-VKO6sT5Ps7b1MjS3pfSBVGFCn6tQ_0UYoPydWdlmT-mt3kblmW458puG1WFm6q2Blgb-Q4-Hrz42.a517cf7?key=DqY3bu6KydZu1cDgHMknJQ" class="kg-image" alt="" loading="lazy" width="685" height="491"><figcaption><span style="white-space: pre-wrap;">A representation of the different knowledge categories in the knowledge distillation process for a neural network. Image from </span><a href="https://link.springer.com/chapter/10.1007/978-3-031-32095-8_1"><u><span class="underline" style="white-space: pre-wrap;">Springer Link</span></u></a></figcaption></figure><h2 id="response-based-knowledge"><strong>Response-based knowledge</strong></h2><p>Response-based distillation systems capture the knowledge from the teacher model's output layer and transfer it to the student model. The student model is trained to directly mimic the teacher model's output probabilities (soft targets). However, not all knowledge will be distilled to the student model as certain underlying factors might result in divergence. In this case, the Kullback-Leibler Divergence is used to compute the divergence metrics between the teacher and student predictions and minimize the loss function.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../lh7-us.googleusercontent.com/docsz/AD_4nXcYOX2dO4c72xjisaADT2FpoDMYu74qlf924WrxJtZZUx8vFuWaA_UusjEPZYUqRfMZt-OPgrCAn6fNLB6fsl8i2hqVmWehdr6qBMZUBtoIYHl1sx0tjeL3CFPt7WhlJzSKJ4L3UeGx3n-on0LqHxFBMFcr.a527cf7?key=DqY3bu6KydZu1cDgHMknJQ" class="kg-image" alt="" loading="lazy" width="753" height="263"><figcaption><span style="white-space: pre-wrap;">A representation of the Response-based Distillation process. Image by </span><a href="https://www.researchgate.net/figure/Response-based-knowledge-distillation_fig4_371616469"><u><span class="underline" style="white-space: pre-wrap;">Abdelaziz Abohamama</span></u></a></figcaption></figure><h2 id="feature-based-knowledge"><strong>Feature-based knowledge</strong></h2><p>Feature-based knowledge represents the intermediate levels of feature representations of the teacher model. When distilling knowledge based on the features, the intermediate layers of the teacher model that contain feature activations are transferred to the student model. Instead of relying solely on the teacher model's output, this approach goes a step higher by training the student model to mimic the feature maps of the various layers of the teacher model.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../lh7-us.googleusercontent.com/docsz/AD_4nXfkuU4ha3M5L-8rMMfS4N8N6I3nq1lUhnKCr8ZiGLl5O0n5LYgutD9jOTup4CqUBiFfBWzXFDuREN4Gv3oWTTQgYQgrxBQqH_Uu97aQZxvnFdTg12lcNKii65GlW5yOHt-O0eBFSwEejHmLpSQOXhqffZQ.a537cf7.d?key=DqY3bu6KydZu1cDgHMknJQ" class="kg-image" alt="" loading="lazy" width="722" height="320"><figcaption><span style="white-space: pre-wrap;">Feature-based Knowledge Distillation lifecycle. Image by </span><a href="https://www.researchgate.net/figure/The-generic-feature-based-knowledge-distillation_fig9_342094012"><u><span class="underline" style="white-space: pre-wrap;">Jianping Gou</span></u></a></figcaption></figure><h2 id="relation-based-knowledge"><strong>Relation-based knowledge</strong></h2><p>Relation-based knowledge goes beyond the output of the teacher model. It covers the relationships between the various layers from which the output is drawn. Relation-based knowledge also includes the data samples learned by the teacher model. In relation-based distillation, we focus on distilling the relationship between the data sample and different layers of the teacher model into the student model.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../labelbox-guides.ghost.io/content/images/2024/07/The-generic-instance-relation-based-knowledge-distillation.png" class="kg-image" alt="" loading="lazy" width="715" height="351" srcset="https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/The-generic-instance-relation-based-knowledge-distillation.png 600w, https://labelbox-guides.ghost.io/content/images/2024/07/The-generic-instance-relation-based-knowledge-distillation.png 715w"><figcaption><span style="white-space: pre-wrap;">Relation-based Knowledge Distillation lifecycle. Image by </span><a href="https://www.researchgate.net/figure/The-generic-feature-based-knowledge-distillation_fig9_342094012"><u><span class="underline" style="white-space: pre-wrap;">Jianping Gou</span></u></a></figcaption></figure><h1 id="knowledge-distillation-algorithms"><strong>Knowledge distillation algorithms</strong></h1><p>Different algorithms are employed to facilitate the transfer of knowledge, including:</p><ul><li><strong>Adversarial learning</strong>: The student model learns to perform tasks that the teacher model finds challenging, improving robustness.</li><li><strong>Cross-modal distillation</strong>: Knowledge is transferred between different modalities, such as from text to images.</li><li><strong>Multi-teacher distillation</strong>: Knowledge from multiple teacher models is distilled into a single student model.</li><li><strong>Graph-based distillation</strong>: Uses graphs to map and transfer intra-data relationships, enriching the student model's learning process.</li></ul><h2 id="adversarial-learning-distillation-algorithm"><strong>Adversarial learning distillation algorithm</strong></h2><p>The adversarial learning algorithm primes the student model to mimic the teacher model's output but generates samples that the teacher model cannot classify correctly. It is inspired by <a href="https://arxiv.org/abs/1406.2661"><u>Generative Adversarial Networks (GANs)</u></a> as it generates synthetic (adversarial) data, which it uses to train the student model alongside the training set. In doing so, the algorithm gives the student model a better understanding of the true data distribution.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../lh7-us.googleusercontent.com/docsz/AD_4nXeuGBfS-LsgI8theaNCxu_b_G_sATswJlU4sJDNcLTS42TyLEBN6p-cJSwlwGcidm3kiF1M3v8wruFClMUAkWY-U1c4uMaAWRuD_LBeMMm94SgXtEP-5-qbAS-UxdTt2I0a1pX_XG6qdeApBjy_9dgRvCZB.a557cf7?key=DqY3bu6KydZu1cDgHMknJQ" class="kg-image" alt="" loading="lazy" width="550" height="336"><figcaption><span style="white-space: pre-wrap;">A generic architecture of the Adversarial Learning Algorithm. Image from </span><a href="https://www.mdpi.com/1999-4893/15/8/283"><u><span class="underline" style="white-space: pre-wrap;">MDPI</span></u></a></figcaption></figure><h2 id="cross-modal-distillation-algorithm"><strong>Cross-modal distillation algorithm</strong></h2><p>Cross-modal algorithms facilitate knowledge transfer between different modalities. Sometimes, data or labels are available in one modality but not another. So, we invoke the cross-modal distillation algorithm to transfer this data from this modality to the missing one during distillation. The algorithm is sequential; the teacher model is trained on the source modality, and then the student model is trained on the target modality. The knowledge transfer is, therefore, achieved across different modalities of the teacher and the student models.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../lh7-us.googleusercontent.com/docsz/AD_4nXcU2Of9W8ndKErF3ARaGX8Qe1LmsG3ALNQnetN3zhYuYirw_0_LKE7fhwUJ_2mZlIFENyUj7357w92nQap2yaQPrPkisGpHEs34Ep-iWR-zz6PxP6FwyQC_GGDUTaJbcpraulkBC3Ox25zCvftzG6hT84P-.a567cf7?key=DqY3bu6KydZu1cDgHMknJQ" class="kg-image" alt="" loading="lazy" width="850" height="387"><figcaption><span style="white-space: pre-wrap;">A representation of Cross-Modal Distillation Algorithm. Image from </span><a href="https://www.researchgate.net/figure/The-generic-framework-for-cross-modal-distillation-For-simplicity-only-two-modalities_fig4_350293589"><u><span class="underline" style="white-space: pre-wrap;">International Journal of Computer Vision</span></u></a></figcaption></figure><h2 id="multi-teacher-distillation-algorithm"><strong>Multi-teacher distillation algorithm&nbsp;</strong></h2><p>The multi-teacher algorithm transfers knowledge from multiple teacher models to a single student model during the distillation process. The best way to achieve this knowledge transfer is by averaging all the teacher models' soft label outputs and then distilling the averaged output into the student model.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../lh7-us.googleusercontent.com/docsz/AD_4nXdV42MlSL5yT5TtP2dRznLC06R01BwSjvT6OudzK8LLduw7vkMMqI43Y8MglPa6OENJFERPqlaGlpgw5jSSTuig3viAIDDdynCYqz1otboynJyaDtt9wiOwqCvXLpB-5dBlJvWuSAK5Pw4git3a4hYm-qDN.a577cf7?key=DqY3bu6KydZu1cDgHMknJQ" class="kg-image" alt="" loading="lazy" width="850" height="419"><figcaption><span style="white-space: pre-wrap;">Basic framework for the Multi-teacher Distillation Algorithm. Image from </span><a href="https://www.researchgate.net/figure/The-generic-framework-for-multi-teacher-distillation_fig3_350293589"><u><span class="underline" style="white-space: pre-wrap;">International Journal of Computer Vision</span></u></a></figcaption></figure><h2 id="graph-based-distillation-algorithm"><strong>Graph-based distillation algorithm</strong></h2><p>While most distillation algorithms focus on transferring individual knowledge instances from the teacher to the student model, graph-based algorithms use graphs to map intra-data relationships instead. These graphs carry the teacher's knowledge and transfer several instances of this knowledge to the student model.&nbsp;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../../../lh7-us.googleusercontent.com/docsz/AD_4nXfquhhA_EB9hn2JD_QsdyMV1asev_Wyg74xXYzDWbr47SqpDKgcVRqUtA3g9mnJQmL6f8oSDbn7LdkGEoY5ZTBK50-zvubLf7IRZQvhyrS-ZeNq8iWKaTLBCT66-VGOD2yDGrLLmyM2IBPC3UTiDUm-plEu.a587cf7?key=DqY3bu6KydZu1cDgHMknJQ" class="kg-image" alt="" loading="lazy" width="774" height="327"><figcaption><span style="white-space: pre-wrap;">A generic framework for Graph-based Distillation Algorithm. Image from </span><a href="https://www.researchgate.net/figure/The-framework-of-Graph-based-Knowledge-Distillation-for-Graph-Neural-Networks-GKD_fig2_368877191"><u><span class="underline" style="white-space: pre-wrap;">ResearchGate</span></u></a></figcaption></figure><h1 id="a-recap-of-knowledge-distillation"><strong>A recap of knowledge distillation</strong></h1><p>Knowledge distillation is a highly effective technique for compressing large, resource-intensive AI models like LLMs and DNNs, enabling their deployment on edge devices with limited computational resources. This process begins with the comprehensive training of a large-scale "teacher" model, followed by the distillation of its essential knowledge into a more compact "student" model. The student model can then be efficiently deployed on a variety of downstream devices and computing systems.</p><p>Knowledge distillation encompasses various methodologies depending on the type of knowledge transferred, including response-based, feature-based, and relation-based distillation. Each type employs specific algorithms tailored to optimize the knowledge transfer, ensuring that the student model achieves comparable performance to the teacher model but with far fewer parameters.</p><p>While the computational demands of deploying full-scale LLMs and DNNs are beyond the reach of many devices, knowledge distillation allows us to create smaller, efficient replicas that perform similarly. This technique bridges the gap between the substantial resource requirements of advanced AI models and the performance expectations of user devices. Critical to the success of knowledge distillation is the quality of the teacher model's training, which heavily depends on the precision of data annotations. </p><p>Quantumworks Lab addresses this need by providing powerful, customizable tools for creating high-quality annotated datasets, thereby enhancing both the efficiency and effectiveness of the distillation process. By improving data annotation, Quantumworks Lab not only boosts the learning efficiency of the student model but also streamlines the overall approach to model training. Experience the benefits firsthand by <a href="https://app.labelbox.com/signup?_r=https://www.google.com/?utm_keyword=Quantumworks Lab&amp;utm_content=model_product_page&amp;utm_campaign=modelfoundry&amp;utm_source=linkedin&amp;utm_medium=organic_social&amp;&amp;landingPageAnonymousId=%22583ce60e-4c83-4821-ada7-bfdc420a7a2b%22&amp;referrer_url=https://www.google.com/"><u>trying Quantumworks Lab for free</u></a> and enhance your model training initiatives.</p></div></main></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"knowledge-distillation","id":"66985287e017de000190bd62","uuid":"f1ed906d-cefe-436a-a545-ef4047f0c14f","title":"How to do knowledge distillation","html":"\u003cp\u003eKnowledge Distillation, which compresses large, powerful AI models into smaller, faster versions without losing performance, vital for efficient deployment on less powerful devices, has become an important technique for AI development and streamline the process of building intelligent applications.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs advancements in artificial intelligence continue, \u003ca href=\"https://labelbox.com/usecases/large-language-models/\"\u003e\u003cu\u003elarge language models (LLMs)\u003c/u\u003e\u003c/a\u003e and deep neural networks (DNNs) are becoming increasingly capable. The latest iterations outperform their predecessors, partly due to \u003ca href=\"https://deepchecks.com/question/how-does-the-size-of-the-training-data-affect-the-accuracy/#:~:text=A%20machine%20learning%20model's%20accuracy,and%20are%20expensive%20to%20store.\"\u003e\u003cu\u003eexpanded datasets used during training\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese sophisticated models are invaluable across various sectors including marketing, cybersecurity, logistics, and medical diagnostics. However, their deployment is often limited by the significant computing resources they require.\u003c/p\u003e\u003cp\u003eAs these models grow in complexity with increased data and parameters, they also become larger and slower, which complicates deployment on less powerful user devices such as office computers, embedded systems, and mobile devices. Knowledge distillation is the ideal solution to this problem, allowing models to maintain similar accuracy and performance in a much smaller, more deployable format.\u003c/p\u003e\u003cp\u003eIn this article, we will learn how knowledge is distilled from large language models to smaller models that can be deployed in downstream edge devices and user systems.\u003c/p\u003e\u003ch1 id=\"what-is-knowledge-distillation\"\u003e\u003cstrong\u003eWhat is knowledge distillation?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eKnowledge distillation is a technique where a smaller, simpler model—referred to as the \"student\"—learns from a larger, more complex model, known as the \"teacher.\" This process goes beyond just mimicking the final decision outputs (hard targets) of the teacher; it crucially involves the student model learning from the soft output distributions (soft labels) provided by the teacher.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese soft labels represent the probabilities the teacher model assigns to each class, conveying not just the decision but also the confidence levels across potential outcomes. The goal of this method is to transfer the comprehensive knowledge of the teacher model to a student model that retains much of the teacher’s accuracy and performance but with significantly fewer parameters.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis allows the student model to be deployed on user devices where computational resources are more limited. Knowledge distillation thus offers a strategic trade-off between the robust training capabilities of large models and the deployment needs of smaller, more efficient ones. The outcome is a compact model that meets the latency, throughput, and performance benchmarks of its larger counterpart but is more suitable for environments with resource constraints.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"what-is-the-knowledge-distillation-process\"\u003e\u003cstrong\u003eWhat is the knowledge distillation process\u003c/strong\u003e?\u003c/h1\u003e\u003cp\u003eThe student-teacher architecture is the basis for knowledge distillation. It ensures that the teacher model can be compressed into a simpler student model deployed on low-grade devices. The student model can learn as much as possible from the teacher model through this architecture, capturing all the knowledge with minimal computing resources. This model compression technique banks on three processes: pre-training the teacher model, knowledge transfer, and refining the student model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcZthO0ui4PEtOFEY3UBxRpbNdJxRicYT-CXmt9FiV1CiDJVWqF7a1Mfogq3_jqxeRt9-KKxL725-BJnw9Ag5xgbQem_rPBX4jZaFZff7SJ5f6xCMYnwuBNG_2-qjTdK3mC1DbB28COgRJSRWm3MAjmDmAt?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"298\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA generic representation of the Knowledge Distillation process. Image from \u003c/span\u003e\u003ca href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285901\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003ePlos One\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe \u003cstrong\u003eteacher model pre-training\u003c/strong\u003e phase builds the knowledge to be transferred during the distillation process. In this step of the knowledge distillation process, a complex model is trained on large datasets using standard machine learning procedures.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe pre-training phase is the most expensive and resource-intensive, as the teacher model training requires extensive datasets and computing resources. The teacher model can take the form of existing legacy models like GPT, Llama, or BERT, with billions of parameters and gigabytes of data.\u003c/p\u003e\u003cp\u003eOnce trained, the teacher model generates soft labels (logits) for the training data, which are later used to supervise the student model. In simple terms, these soft labels are the output probability distributions provided by the teacher model for training the student model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eUnlike hard (binary) labels, soft labels provide more information on the prediction probabilities over the classes. The teacher model pre-training must be thorough enough to capture all the nuances of prediction or the input-output correlations to be transferred to the student model.\u003c/p\u003e\u003cp\u003eNext, the \u003cstrong\u003estudent model is trained\u003c/strong\u003e on the teacher model-generated soft labels (output), features, or relations, depending on the chosen type of distillation. \u003cstrong\u003eKnowledge transfer\u003c/strong\u003e happens in this phase of knowledge distillation. The training of the student model aims to distill the knowledge of the teacher model by matching its soft targets to the student model. In doing so, the difference between the student model predictions and the teacher model's soft labels is minimized.\u003c/p\u003e\u003cp\u003eBesides soft labels, the student model is also trained to learn the teacher model's pairwise relation between data points. It is also primed to mimic the feature representation of the teacher model's layers. These processes steer the student model to capture almost all the knowledge of the teacher model and obtain similar or higher accuracy while using minimal computing resources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo enhance the performance of the student model, it is refined through further training. Once the knowledge has been distilled from the teacher to the student model, the \u003cstrong\u003estudent model may undergo additional training\u003c/strong\u003e with the original dataset and hyperparameter tuning. Refining the student model augments the knowledge distillation process, ensuring we achieve an optimal model.\u003c/p\u003e\u003ch1 id=\"knowledge-categories\"\u003e\u003cstrong\u003eKnowledge categories\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAs discussed in the knowledge distillation process above, the student model can learn from different knowledge categories of the teacher model. This knowledge includes soft labels, intermediate layer features, and the relationship between various layers and data points. Various types of knowledge distillation emerge from these knowledge categories. As a result, we have three known knowledge distillation types:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eResponse-based knowledge\u003c/strong\u003e: The student model learns from the soft outputs of the teacher model.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFeature-based knowledge\u003c/strong\u003e: The student model mimics intermediate feature representations from the teacher.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRelation-based knowledge\u003c/strong\u003e: Focuses on the relationships between different layers and data points in the teacher model.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXecgNwozETH0rMVZLeQ6e-k9K6_Rt2PXv7owTcjrhRYOsF78p9SLKKr92XfkN-nLlP3UmdUgg08P5R-VKO6sT5Ps7b1MjS3pfSBVGFCn6tQ_0UYoPydWdlmT-mt3kblmW458puG1WFm6q2Blgb-Q4-Hrz42?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"685\" height=\"491\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA representation of the different knowledge categories in the knowledge distillation process for a neural network. Image from \u003c/span\u003e\u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-031-32095-8_1\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eSpringer Link\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"response-based-knowledge\"\u003e\u003cstrong\u003eResponse-based knowledge\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eResponse-based distillation systems capture the knowledge from the teacher model's output layer and transfer it to the student model. The student model is trained to directly mimic the teacher model's output probabilities (soft targets). However, not all knowledge will be distilled to the student model as certain underlying factors might result in divergence. In this case, the Kullback-Leibler Divergence is used to compute the divergence metrics between the teacher and student predictions and minimize the loss function.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcYOX2dO4c72xjisaADT2FpoDMYu74qlf924WrxJtZZUx8vFuWaA_UusjEPZYUqRfMZt-OPgrCAn6fNLB6fsl8i2hqVmWehdr6qBMZUBtoIYHl1sx0tjeL3CFPt7WhlJzSKJ4L3UeGx3n-on0LqHxFBMFcr?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"753\" height=\"263\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA representation of the Response-based Distillation process. Image by \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Response-based-knowledge-distillation_fig4_371616469\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eAbdelaziz Abohamama\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"feature-based-knowledge\"\u003e\u003cstrong\u003eFeature-based knowledge\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFeature-based knowledge represents the intermediate levels of feature representations of the teacher model. When distilling knowledge based on the features, the intermediate layers of the teacher model that contain feature activations are transferred to the student model. Instead of relying solely on the teacher model's output, this approach goes a step higher by training the student model to mimic the feature maps of the various layers of the teacher model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXfkuU4ha3M5L-8rMMfS4N8N6I3nq1lUhnKCr8ZiGLl5O0n5LYgutD9jOTup4CqUBiFfBWzXFDuREN4Gv3oWTTQgYQgrxBQqH_Uu97aQZxvnFdTg12lcNKii65GlW5yOHt-O0eBFSwEejHmLpSQOXhqffZQ?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"722\" height=\"320\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFeature-based Knowledge Distillation lifecycle. Image by \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-generic-feature-based-knowledge-distillation_fig9_342094012\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eJianping Gou\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"relation-based-knowledge\"\u003e\u003cstrong\u003eRelation-based knowledge\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eRelation-based knowledge goes beyond the output of the teacher model. It covers the relationships between the various layers from which the output is drawn. Relation-based knowledge also includes the data samples learned by the teacher model. In relation-based distillation, we focus on distilling the relationship between the data sample and different layers of the teacher model into the student model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/The-generic-instance-relation-based-knowledge-distillation.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"715\" height=\"351\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/The-generic-instance-relation-based-knowledge-distillation.png 600w, https://labelbox-guides.ghost.io/content/images/2024/07/The-generic-instance-relation-based-knowledge-distillation.png 715w\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRelation-based Knowledge Distillation lifecycle. Image by \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-generic-feature-based-knowledge-distillation_fig9_342094012\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eJianping Gou\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"knowledge-distillation-algorithms\"\u003e\u003cstrong\u003eKnowledge distillation algorithms\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eDifferent algorithms are employed to facilitate the transfer of knowledge, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAdversarial learning\u003c/strong\u003e: The student model learns to perform tasks that the teacher model finds challenging, improving robustness.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCross-modal distillation\u003c/strong\u003e: Knowledge is transferred between different modalities, such as from text to images.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMulti-teacher distillation\u003c/strong\u003e: Knowledge from multiple teacher models is distilled into a single student model.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGraph-based distillation\u003c/strong\u003e: Uses graphs to map and transfer intra-data relationships, enriching the student model's learning process.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"adversarial-learning-distillation-algorithm\"\u003e\u003cstrong\u003eAdversarial learning distillation algorithm\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe adversarial learning algorithm primes the student model to mimic the teacher model's output but generates samples that the teacher model cannot classify correctly. It is inspired by \u003ca href=\"https://arxiv.org/abs/1406.2661\"\u003e\u003cu\u003eGenerative Adversarial Networks (GANs)\u003c/u\u003e\u003c/a\u003e as it generates synthetic (adversarial) data, which it uses to train the student model alongside the training set. In doing so, the algorithm gives the student model a better understanding of the true data distribution.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXeuGBfS-LsgI8theaNCxu_b_G_sATswJlU4sJDNcLTS42TyLEBN6p-cJSwlwGcidm3kiF1M3v8wruFClMUAkWY-U1c4uMaAWRuD_LBeMMm94SgXtEP-5-qbAS-UxdTt2I0a1pX_XG6qdeApBjy_9dgRvCZB?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"550\" height=\"336\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA generic architecture of the Adversarial Learning Algorithm. Image from \u003c/span\u003e\u003ca href=\"https://www.mdpi.com/1999-4893/15/8/283\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eMDPI\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"cross-modal-distillation-algorithm\"\u003e\u003cstrong\u003eCross-modal distillation algorithm\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCross-modal algorithms facilitate knowledge transfer between different modalities. Sometimes, data or labels are available in one modality but not another. So, we invoke the cross-modal distillation algorithm to transfer this data from this modality to the missing one during distillation. The algorithm is sequential; the teacher model is trained on the source modality, and then the student model is trained on the target modality. The knowledge transfer is, therefore, achieved across different modalities of the teacher and the student models.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcU2Of9W8ndKErF3ARaGX8Qe1LmsG3ALNQnetN3zhYuYirw_0_LKE7fhwUJ_2mZlIFENyUj7357w92nQap2yaQPrPkisGpHEs34Ep-iWR-zz6PxP6FwyQC_GGDUTaJbcpraulkBC3Ox25zCvftzG6hT84P-?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"850\" height=\"387\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA representation of Cross-Modal Distillation Algorithm. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-generic-framework-for-cross-modal-distillation-For-simplicity-only-two-modalities_fig4_350293589\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eInternational Journal of Computer Vision\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"multi-teacher-distillation-algorithm\"\u003e\u003cstrong\u003eMulti-teacher distillation algorithm\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe multi-teacher algorithm transfers knowledge from multiple teacher models to a single student model during the distillation process. The best way to achieve this knowledge transfer is by averaging all the teacher models' soft label outputs and then distilling the averaged output into the student model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXdV42MlSL5yT5TtP2dRznLC06R01BwSjvT6OudzK8LLduw7vkMMqI43Y8MglPa6OENJFERPqlaGlpgw5jSSTuig3viAIDDdynCYqz1otboynJyaDtt9wiOwqCvXLpB-5dBlJvWuSAK5Pw4git3a4hYm-qDN?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"850\" height=\"419\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBasic framework for the Multi-teacher Distillation Algorithm. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-generic-framework-for-multi-teacher-distillation_fig3_350293589\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eInternational Journal of Computer Vision\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"graph-based-distillation-algorithm\"\u003e\u003cstrong\u003eGraph-based distillation algorithm\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhile most distillation algorithms focus on transferring individual knowledge instances from the teacher to the student model, graph-based algorithms use graphs to map intra-data relationships instead. These graphs carry the teacher's knowledge and transfer several instances of this knowledge to the student model.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXfquhhA_EB9hn2JD_QsdyMV1asev_Wyg74xXYzDWbr47SqpDKgcVRqUtA3g9mnJQmL6f8oSDbn7LdkGEoY5ZTBK50-zvubLf7IRZQvhyrS-ZeNq8iWKaTLBCT66-VGOD2yDGrLLmyM2IBPC3UTiDUm-plEu?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"774\" height=\"327\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA generic framework for Graph-based Distillation Algorithm. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-framework-of-Graph-based-Knowledge-Distillation-for-Graph-Neural-Networks-GKD_fig2_368877191\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eResearchGate\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"a-recap-of-knowledge-distillation\"\u003e\u003cstrong\u003eA recap of knowledge distillation\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eKnowledge distillation is a highly effective technique for compressing large, resource-intensive AI models like LLMs and DNNs, enabling their deployment on edge devices with limited computational resources. This process begins with the comprehensive training of a large-scale \"teacher\" model, followed by the distillation of its essential knowledge into a more compact \"student\" model. The student model can then be efficiently deployed on a variety of downstream devices and computing systems.\u003c/p\u003e\u003cp\u003eKnowledge distillation encompasses various methodologies depending on the type of knowledge transferred, including response-based, feature-based, and relation-based distillation. Each type employs specific algorithms tailored to optimize the knowledge transfer, ensuring that the student model achieves comparable performance to the teacher model but with far fewer parameters.\u003c/p\u003e\u003cp\u003eWhile the computational demands of deploying full-scale LLMs and DNNs are beyond the reach of many devices, knowledge distillation allows us to create smaller, efficient replicas that perform similarly. This technique bridges the gap between the substantial resource requirements of advanced AI models and the performance expectations of user devices. Critical to the success of knowledge distillation is the quality of the teacher model's training, which heavily depends on the precision of data annotations. \u003c/p\u003e\u003cp\u003eLabelbox addresses this need by providing powerful, customizable tools for creating high-quality annotated datasets, thereby enhancing both the efficiency and effectiveness of the distillation process. By improving data annotation, Quantumworks Lab not only boosts the learning efficiency of the student model but also streamlines the overall approach to model training. Experience the benefits firsthand by \u003ca href=\"https://app.labelbox.com/signup?_r=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026amp;utm_content=model_product_page\u0026amp;utm_campaign=modelfoundry\u0026amp;utm_source=linkedin\u0026amp;utm_medium=organic_social\u0026amp;\u0026amp;landingPageAnonymousId=%22583ce60e-4c83-4821-ada7-bfdc420a7a2b%22\u0026amp;referrer_url=https://www.google.com/\"\u003e\u003cu\u003etrying Quantumworks Lab for free\u003c/u\u003e\u003c/a\u003e and enhance your model training initiatives.\u003c/p\u003e","comment_id":"66985287e017de000190bd62","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/knowledge-distillation.png","featured":false,"status":"published","visibility":"public","created_at":"2024-07-17T23:23:51.000Z","updated_at":"2024-07-17T23:56:29.000Z","published_at":"2024-01-29T23:27:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/knowledge-distillation/","tags":[],"authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","email":"ldimyadi@labelbox.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":"{\"whatsNew\":{\"lastSeenDate\":\"2025-04-28T15:29:08.000+00:00\"}}","status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2025-05-02T19:01:22.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-29T22:58:55.000Z","updated_at":"2025-05-02T19:01:37.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","email":"ldimyadi@labelbox.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":"{\"whatsNew\":{\"lastSeenDate\":\"2025-04-28T15:29:08.000+00:00\"}}","status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2025-05-02T19:01:22.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-29T22:58:55.000Z","updated_at":"2025-05-02T19:01:37.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":null,"email_segment":"all","url":"https://labelbox-guides.ghost.io/knowledge-distillation/","excerpt":"Knowledge Distillation, which compresses large, powerful AI models into smaller, faster versions without losing performance, vital for efficient deployment on less powerful devices, has become an important technique for AI development and streamline the process of building intelligent applications. \n\nAs advancements in artificial intelligence continue, large language models (LLMs) and deep neural networks (DNNs) are becoming increasingly capable. The latest iterations outperform their predecesso","reading_time":8,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to do knowledge distillation | Quantumworks Lab","meta_description":"Knowledge distillation compresses large, powerful AI models into smaller, faster versions without losing performance. Learn how it works.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":false},"recommended":[]},"__N_SSG":true},"page":"/guides/[id]","query":{"id":"knowledge-distillation"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/knowledge-distillation/?ref=labelbox.ghost.io by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:54:19 GMT -->
</html>