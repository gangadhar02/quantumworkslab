<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:51:04 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">How to enhance RAG chatbot performance by refining a reranking model</title><meta name="description" data-next-head=""/><link rel="preconnect" href="../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="How to enhance RAG chatbot performance by refining a reranking model" data-next-head=""/><meta property="og:description" data-next-head=""/><meta property="og:url" content="https://labelbox-guides.ghost.io/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/" data-next-head=""/><meta property="og:image" content="https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-14-at-11.54.36-AM.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="How to enhance RAG chatbot performance by refining a reranking model" data-next-head=""/><meta name="twitter:description" data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox-guides.ghost.io/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/" data-next-head=""/><meta property="twitter:image" content="https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-14-at-11.54.36-AM.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../static/scripts/munchkin.js"></script><script src="../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g34[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
.giShFC .content p{-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:28px;font-size:19px;margin-bottom:20px;}/*!sc*/
.giShFC .content h1{font-size:34px;line-height:44px;color:#21272c;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
.giShFC .content h2{font-size:30px !important;color:#21272c;line-height:1.3;font-weight:600;padding-top:35px !important;margin-bottom:2px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h2{padding-top:10px;}}/*!sc*/
.giShFC .content h3{font-size:24px;color:#21272c;line-height:1.3;font-weight:600;padding-top:5px;margin:0 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content h3{padding-top:10px;}}/*!sc*/
.giShFC .content a{color:#2563eb;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color linear 0.2s;transition:color linear 0.2s;}/*!sc*/
.giShFC .content a:hover{color:#1e40af;}/*!sc*/
.giShFC .content li{margin-bottom:20px;}/*!sc*/
.giShFC .content ul{list-style:disc;padding-left:20px;}/*!sc*/
.giShFC .content .kg-image-card{padding:20px 0 40px;margin:0 -20px;}/*!sc*/
.giShFC .content .kg-image-card figcaption{text-align:center;-webkit-letter-spacing:0.1px;-moz-letter-spacing:0.1px;-ms-letter-spacing:0.1px;letter-spacing:0.1px;line-height:1.3;font-size:0.75rem;padding:10px 20px 0 20px;}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card figcaption{font-size:0.875rem;padding:15px 0 0 0;}}/*!sc*/
@media only screen and (min-width:48rem){.giShFC .content .kg-image-card{padding:20px 0 50px;margin:0;}}/*!sc*/
.giShFC .content .kg-image{display:block;width:auto;max-width:100%;height:auto;margin:0 auto;cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}/*!sc*/
.giShFC .content .kg-embed-card{margin:50px 0 50px 0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-width:100%;position:relative;padding-top:56.5%;}/*!sc*/
.giShFC .content .kg-embed-card iframe{position:absolute;top:0;left:0;width:100% !important;height:100% !important;margin:0 auto;}/*!sc*/
.giShFC .content .kg-button-card{margin-bottom:20px;height:auto;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn{-webkit-transition-property:all;transition-property:all;-webkit-transition-timing-function:cubic-bezier(.4,0,.2,1);transition-timing-function:cubic-bezier(.4,0,.2,1);-webkit-transition-duration:.15s;transition-duration:.15s;background-color:#2563eb;padding:0.75rem;font-size:1rem;line-height:1.5rem;font-weight:500;border-radius:0.5rem;color:#fafafa;}/*!sc*/
.giShFC .content .kg-button-card .kg-btn:hover{background-color:#1d4ed8;}/*!sc*/
.giShFC .content .kg-gallery-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row .kg-gallery-image{margin:0 0 0 0.75em;}/*!sc*/
.giShFC .content .kg-gallery-container .kg-gallery-row img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;display:block;margin:0;width:100%;height:100%;}/*!sc*/
data-styled.g35[id="id__PostContentWrapper-sc-1ct5gml-0"]{content:"giShFC,"}/*!sc*/
.kUXEED #image-viewer{position:fixed;z-index:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;top:0;height:100vh;width:100%;background-color:rgb(255 255 255);cursor:-webkit-zoom-out;cursor:-moz-zoom-out;cursor:zoom-out;}/*!sc*/
.kUXEED .modal-content{margin:auto;display:block;max-width:1000px;border:none;width:auto;height:auto;padding-top:10px;max-height:70vh;}/*!sc*/
.kUXEED .modal-content{-webkit-animation-name:zoom;animation-name:zoom;-webkit-animation-duration:0.6s;animation-duration:0.6s;}/*!sc*/
@-webkit-keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
@keyframes zoom{from{-webkit-transform:scale(0.6);-ms-transform:scale(0.6);transform:scale(0.6);}to{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}/*!sc*/
.kUXEED #image-viewer .close{position:absolute;top:15px;right:35px;color:#f1f1f1;font-size:40px;font-weight:bold;-webkit-transition:0.3s;transition:0.3s;}/*!sc*/
.kUXEED #image-viewer .close:hover,.kUXEED #image-viewer .close:focus{color:#bbb;-webkit-text-decoration:none;text-decoration:none;cursor:pointer;}/*!sc*/
@media only screen and (max-width:700px){.kUXEED .modal-content{width:100%;}}/*!sc*/
data-styled.g36[id="id__ImageModal-sc-1ct5gml-1"]{content:"kUXEED,"}/*!sc*/
@media (max-width:1026px){.cwKcJJ.toc-container{display:none;}}/*!sc*/
.cwKcJJ.toc-container .js-toc{position:-webkit-sticky;position:sticky;top:148px;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;height:auto;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list{list-style:none;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .is-collapsed{max-height:1000px !important;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list .toc-list-item ol{padding-left:25px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li{margin-bottom:14px;margin-top:14px;line-height:18px;font-size:14px;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a{color:#6a7888;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li a.is-active-link{color:black;}/*!sc*/
.cwKcJJ.toc-container .js-toc .toc-list li .toc-link::before{background-color:none !important;}/*!sc*/
data-styled.g37[id="id__TocContainer-sc-1ct5gml-2"]{content:"cwKcJJ,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../_next/static/chunks/8789-a321e4743358e199.js" defer=""></script><script src="../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../_next/static/chunks/1907-5ca362d03230011c.js" defer=""></script><script src="../../_next/static/chunks/pages/guides/%5bid%5d-78cf43cbe169ea75.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script><style>
        /* Footer styles */
        .footer {
            background-color: #ffffff;
            padding: 100px 0 40px 0;
            border-top: 1px solid #e5e7eb;
        }

        .footer-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 40px;
            margin-bottom: 60px;
        }

        .footer-section h6 {
            font-size: 20px;
            font-weight: 500;
            margin-bottom: 20px;
            color: #1f2937;
        }

        .footer-section ul {
            list-style: none;
        }

        .footer-section ul li {
            margin-bottom: 12px;
        }

        .footer-section ul li a {
            color: #49535F;
            text-decoration: none;
            font-size: 16px;
            transition: color 0.3s ease;
        }

        .footer-section ul li a:hover {
            color: #2876D4;
        }

        .footer-bottom {
            text-align: center;
            padding-top: 40px;
            border-top: 1px solid #e5e7eb;
        }

        .footer-logo {
            margin-bottom: 20px;
        }

        .footer-logo img {
            height: 36px;
            width: auto;
        }

        .footer-copyright {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            color: #1f2937;
            margin-bottom: 20px;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: #49535F;
            text-decoration: none;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 14px;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: #2876D4;
        }

        .footer-divider {
            width: 1px;
            height: 16px;
            background-color: #e5e7eb;
            margin: 0 8px;
        }

        /* Responsive design */
        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: repeat(3, 1fr);
                gap: 30px;
            }
        }

        @media (max-width: 768px) {
            .footer-content {
                grid-template-columns: repeat(2, 1fr);
                gap: 24px;
            }
            
            .footer {
                padding: 60px 0 30px 0;
            }
        }

        @media (max-width: 480px) {
            .footer-content {
                grid-template-columns: 1fr;
                text-align: center;
            }
            
            .footer-links {
                flex-direction: column;
                align-items: center;
                gap: 12px;
            }
            
            .footer-divider {
                display: none;
            }
        }
    </style><link rel="stylesheet" href="/disable-js-footer.css">
<link rel="stylesheet" href="fix-footer-visibility.css">
</head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../index.html"><img width="106" height="24" alt="logo" src="../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><main class="id__ImageModal-sc-1ct5gml-1 kUXEED"><div id="image-viewer"><span class="close">×</span><img class="modal-content" id="full-image"/></div></main><div class="py-12 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3 lg:col-span-2"><div class="sticky top-24"><img src="../../static/images/guide.svg" class="h-10"/><a href="../index.html" class="flex text-md align-items-center mt-6"><img src="../../static/images/leftarrow.svg" class="img-fluid mr-2"/>All guides</a><main class="id__TocContainer-sc-1ct5gml-2 cwKcJJ toc-container py-8"><div class="  js-toc"></div></main></div></div><div class="col-span-12 md:col-span-9 lg:col-span-10"><div class="md:px-24 mb-12"><div class=""><h1 class="md:text-6xl lg:text-7xl font-future text-neutral-900 dark:text-neutral-50 text-2xl md:!text-4xl font-bold max-w-3xl mb-12" style="font-feature-settings:unset">How to enhance RAG chatbot performance by refining a reranking model</h1></div><img class="img-fluid rounded-lg" src="../../../labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-14-at-11.54.36-AM.png"/></div><main class="id__PostContentWrapper-sc-1ct5gml-0 giShFC md:px-24"><div class="content js-toc-content"><p>When building customized chatbots and similar LLM applications, teams may start off with RAG (Retrieval Augmented Generation) approaches. A RAG-centric application relies on context to formulate an appropriate response when given a user query.&nbsp;Because the context is retrieved from internal documents, RAG based approaches are adept at handling organization specific chatbots while minimizing hallucinations. A good example of how organizations are leveraging RAG centric applications is <a href="https://observer.com/2023/06/metamate-foreshadows-ai-workplace-trend/"><u>Metamate</u></a>, an employee agent used at Meta which pulls internal information at request for answering employee questions, producing meeting summaries, writing code and debugging features.</p><p>While easy to prototype, optimizing RAG-based applications for relevant retrieval require complex considerations due to the potential semantic information that is lost when embedding internal documents within vector databases. Therefore, a useful RAG based chatbot requires careful strategies for preprocessing document metadata enrichment (e.g,. metadata filtering, domain specific embedding models, chunking, etc) as well as model inferencing LLM, prompt engineering, and re-ranking retrieved documents.			</p><p>In this guide, we’ll cover how Quantumworks Lab can be a useful tool to help you preprocess documents (e.g., entity detection, document classification, etc) and improve retrieval relevance by fine-tuning a reranking model. Feel free to walk through the full guide or watch the step-by-step video below.</p><figure class="kg-card kg-embed-card"><iframe src="https://fast.wistia.net/embed/iframe/32xu4rxe64" title="RAG Fine Tune Reranking Model Video" allow="autoplay; fullscreen" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" msallowfullscreen="" width="960" height="556"></iframe>
<script src="../../../fast.wistia.net/assets/external/E-v1.js" async=""></script></figure><h3 id="why-reranking"><strong>Why reranking?</strong>			</h3><p>When building RAG-based applications, thereʼs a limit to the amount of text that can be passed to an LLM and therefore the context given to an LLM is limited to the top k documents pulled from the vector space using a similarity search. Because semantic information may be lost during the embedding process, relevant context that may be useful to answering the user query may fall outside of the top k responses.</p><p>With reranking embedded within a RAG application, we now have the ability to consider a larger corpus of retrieved text and pass the most relevant documents into our LLM to generate a better answer.	</p><p>Using the Quantumworks Lab Platform, we will fine tune an reranker model to improve chatbot performance. Holding other considerations consistent (i.e., token limit, embedding model, chunking strategy, LLM choice), we will compare baseline responses without reranking to the output of a fine-tuned reranker workflow.</p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/unnamed-39.png" class="kg-image" alt="" loading="lazy" width="1570" height="1290"></figure><h2 id="part-1-review-initial-baseline-responses"><strong>Part 1. Review initial baseline responses</strong></h2><p>To illustrate this approach, we will attempt to improve responses to the 2022 NFL Rulebook 🏈.</p><p>Note: From the document, the document is divided into different sections. You can leverage the Quantumworks Lab Platform to label metadata for each page (i.e. Contents / Scenarios / Rules) to improve efficient and relevant retrieval.</p><p>The PDF is embedded using the HuggingFace <em>sentence- transformers/distiluse-base-multilingual-cased-v2</em><strong> </strong>model. We will keep the same vector embeddings for both workflows.				</p><p>After extracting the top 2 documents (via similarity search), we will add them as context to the flan-t5-base LLM model to generate initial responses to queries about NFL rules.	</p><p>As you can see from the sample of results below, the initial responses are spotty at best.</p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/unnamed-40.png" class="kg-image" alt="" loading="lazy" width="1600" height="688"></figure><h2 id="part-2-human-in-the-loop-hitl-and-semi-supervised-approaches-to-fine-tune-reranker-model"><strong>Part 2. Human in the Loop (HITL) and Semi-Supervised Approaches to Fine Tune Reranker Model</strong></h2><p>To fine tune our reranker model, let's extract 5 responses for each query and upload the prompt / responses(s) pairs to Quantumworks Lab <a href="../../product/catalog/index.html"><u>Catalog</u></a>.</p><p>Note: We can leverage unsupervised techniques such as semantic search, similarity search, cluster search, etc to enrich data rows with relevant metadata and/or bulk classification. The highlighted cluster shown below contains search queries related to flagrant penalties, which we can use to attach metadata &amp; classifications.</p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/unnamed-41.png" class="kg-image" alt="" loading="lazy" width="1276" height="864"></figure><p>Using <a href="../../product/model/foundry/index.html"><u>Model Foundry</u></a>, we can apply semi-supervised learning with foundation models to generate pre labels. Let’s select GPT4 and attach a customized prompt to generate responses to match the target ontology, in this case, whether the output is relevant or not relevant to the query.		</p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/unnamed-42.png" class="kg-image" alt="" loading="lazy" width="1600" height="774"></figure><p>By sending the query response pairs with pre-labels to our annotation project, human labelers can now leverage pre-labels (and/or metadata &amp; attachments) to speed up the annotation process. Predictions are then converted to ground truth once the human-in-the loop process is completed.			</p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/unnamed-43.png" class="kg-image" alt="" loading="lazy" width="1600" height="639"></figure><p>To track model performance, we can set dataset versions and enable data splits before exporting the data rows via the Quantumworks Lab <a href="https://docs.labelbox.com/reference/install-python-sdk"><u>Python SDK</u></a>.</p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/unnamed-44.png" class="kg-image" alt="" loading="lazy" width="1600" height="794"></figure><h2 id="part-3-fine-tuning-the-reranker-model"><strong>Part 3. Fine Tuning the Reranker Model</strong>			</h2><p>For our workflow, we will fine tune the <em>BAAI/bge-reranker-large</em> open source model on HuggingFace 😊. Companies such as Cohere also provides reranking models as well for fine tuning.	</p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/unnamed-45.png" class="kg-image" alt="" loading="lazy" width="1600" height="217"></figure><p>To fine tune our model, we’ll need to format the ground truth labels into JSON line format.					</p><p>Note: that the model was fine-tuned with AWS Sagemaker <em>ml.g4dn.xlarge </em>machine with T4 GPUs. The Google Colab environment also provides free GPUs for model fine tuning.			</p><p>The model returns relevancy scores for each chunk. We can see that the relative order of the scores and not the absolute value is what matters. Applying this on a data row in the testing set yields the following:</p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/unnamed-46.png" class="kg-image" alt="" loading="lazy" width="1600" height="151"></figure><p>We can see from the histogram distribution of relevancy scores for the testing set that relevant examples tend to have higher scores.				</p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/unnamed-47.png" class="kg-image" alt="" loading="lazy" width="1600" height="818"></figure><p>					</p><h2 id="part-4-inference-with-an-updated-rag-workflow">Part 4. Inference with an updated RAG workflow				</h2><p><br>Using the same document embeddings <em>(sentence-transformers/distiluse- base-multilingual-cased-v2</em>) and LLM (<em>flan-t5-base LLM model</em>), let's retrieve the top 20 document chunks for each query.				</p><p>Using the fine tuned reranker model, let's rerank the top 3 documents which will be sent to the LLM as context.</p><p>Applying inferencing on the testing set, we can now compare the baseline response with the responses with reranking model. From here, we will see some markedly improved responses.</p><figure class="kg-card kg-image-card"><img src="../../../lh7-us.googleusercontent.com/unnamed-48.png" class="kg-image" alt="" loading="lazy" width="1600" height="763"></figure><h2 id="conclusion"><strong>Conclusion</strong><br>		</h2><p>As we’ve shown in this guide, model performance for customized chatbots rely on both the quality and quantity of labeled data. By incorporating Quantumworks Lab Catalog and Foundry to apply pre-labels with unsupervised &amp; semi-supervised techniques, we were able to maximize labeling throughput. Afterwards, by using Quantumworks Lab Annotate, we were able to leverage a human-in-the-loop workflow to ensure that our training labels are of the highest quality. Teams should consider adopting this approach when building RAG-based applications to better preprocess documents and improve retrieval relevance by fine-tuning a reranking model.</p><p>Quantumworks Lab is a data-centric AI platform that empowers teams to iteratively build powerful chatbots that fuel knowledge sharing and foster deeper customer interactions. To get started, <a href="https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab&amp;utm_medium=email&amp;utm_source=house&amp;utm_campaign=modelfoundry&amp;&amp;referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab&amp;utm_medium=email&amp;utm_source=house&amp;utm_campaign=guide103123&amp;&amp;attr=intercom&amp;referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab%2520pricing&amp;utm_source=house&amp;utm_medium=email&amp;utm_campaign=020824&amp;&amp;landingPageAnonymousId=%22b04806c5-ff2b-4a09-9379-a9837b1801a9%22&amp;referrer_url=https://www.google.com/">sign up for a free Quantumworks Lab account</a> or <a href="../../sales/index.html">request a demo</a>.</p></div></main></div></div></div><div class="mt-5 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="my-20 w-full h-[1px] bg-neutral-200"></div><div class=""><div class=""><h2 class="mb-12 text-center text-3xl md:text-4xl font-medium">Continue reading</h2></div><div class="flex flex-wrap justify-content-center"><div class="max-w-md mx-2 "><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/indexa17d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Fguide_LeaderboxHero2.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Quantumworks Lab Leaderboards: Redefining AI evaluations with human-centric assessments</p><p class="text-base max-w-2xl undefined line-clamp-3">Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.</p></a></div></div></div></div></div><div class="max-w-md mx-2 "><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../programmatically-launch-human-data-jobs-for-rlhf-and-evaluation/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/indexbb2a.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-26-at-2.03.08-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../programmatically-launch-human-data-jobs-for-rlhf-and-evaluation/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Programmatically launch human data jobs for RLHF and evaluation</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to harness the SDK to manage human data labeling jobs for RLHF and model evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.</p></a></div></div></div></div></div><div class="max-w-md mx-2 "><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../evaluating-leading-text-to-speech-models/index.html" target="_self" class="relative aspect-video  undefined border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=3840&amp;q=70 3840w" src="../../_next/image/index6ff8.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2FScreenshot-2024-08-17-at-12.12.46-PM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../evaluating-leading-text-to-speech-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Evaluating leading text-to-speech models</p><p class="text-base max-w-2xl undefined line-clamp-3">Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.</p></a></div></div></div></div></div></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer>
        <div class="footer-container">
            <div class="footer-content">
                <!-- Product Section -->
                <div class="footer-section">
                    <h6>Product</h6>
                    <ul>
                        <li><a href="/product/platform/">Platform</a></li>
                        <li><a href="/product/model/foundry-models/">Model Foundry</a></li>
                        <li><a href="/product-demos/">Product Demos</a></li>
                        <li><a href="/recorded-demo/">Recorded Demo</a></li>
                    </ul>
                </div>

                <!-- Solutions Section -->
                <div class="footer-section">
                    <h6>Solutions</h6>
                    <ul>
                        <li><a href="/solutions/computer-vision/">Computer Vision</a></li>
                        <li><a href="/solutions/natural-language-processing/">Natural Language Processing</a></li>
                        <li><a href="/solutions/complex-reasoning/">Complex Reasoning</a></li>
                        <li><a href="/solutions/multimodal-reasoning/">Multimodal Reasoning</a></li>
                        <li><a href="/solutions/coding-tasks/">Coding</a></li>
                        <li><a href="/solutions/multilingual/">Multilingual</a></li>
                        <li><a href="/solutions/text-to-audio/">Audio</a></li>
                    </ul>
                </div>

                <!-- Learn Section (Resources) -->
                <div class="footer-section">
                    <h6>Learn</h6>
                    <ul>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/guides/">Guides</a></li>
                        <li><a href="https://docs.labelbox.com" target="_blank">Docs</a></li>
                        <li><a href="/faqs/">FAQs</a></li>
                        <li><a href="/research/">Research</a></li>
                        <li><a href="/product/model/foundry-models/">Models</a></li>
                        <li><a href="/datasets/">Public datasets</a></li>
                    </ul>
                </div>

                <!-- Company Section -->
                <div class="footer-section">
                    <h6>Company</h6>
                    <ul>
                        <li><a href="/company/about/">About</a></li>
                        <li><a href="/company/security/">Privacy & Security</a></li>
                        <li><a href="https://alignerr.com" target="_blank">Alignerr</a></li>
                    </ul>
                </div>

                <!-- Data Factory Section -->
                <div class="footer-section">
                    <h6>The data factory</h6>
                    <ul>
                        <li><a href="/why-labelbox/">Why Quantumworks Lab</a></li>
                        <li><a href="/services/labeling/">Labeling Services</a></li>
                        <li><a href="/services/alignerr-connect/">Alignerr Connect</a></li>
                        <li><a href="/model-foundry/">Model Foundry</a></li>
                        <li><a href="/leaderboards/">Leaderboards</a></li>
                    </ul>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="footer-logo">
                    <img src="static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36">
                </div>
                <div class="footer-copyright">
                    © Quantumworks Lab, Inc<br>
                    We enable breakthroughs
                </div>
                <div class="footer-links">
                    <a href="https://docs.labelbox.com/page/terms-of-service" target="_blank">Terms of Service</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/privacy-notice" target="_blank">Privacy Notice</a>
                    <div class="footer-divider"></div>
                    <a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank">Copyright Dispute Policy</a>
                </div>
            </div>
        </div>
    </footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models","id":"65cd1695643f030001d2239a","uuid":"ab48a216-971c-438e-bd52-110bab1dfe04","title":"How to enhance RAG chatbot performance by refining a reranking model","html":"\u003cp\u003eWhen building customized chatbots and similar LLM applications, teams may start off with RAG (Retrieval Augmented Generation) approaches. A RAG-centric application relies on context to formulate an appropriate response when given a user query.\u0026nbsp;Because the context is retrieved from internal documents, RAG based approaches are adept at handling organization specific chatbots while minimizing hallucinations. A good example of how organizations are leveraging RAG centric applications is \u003ca href=\"https://observer.com/2023/06/metamate-foreshadows-ai-workplace-trend/\"\u003e\u003cu\u003eMetamate\u003c/u\u003e\u003c/a\u003e, an employee agent used at Meta which pulls internal information at request for answering employee questions, producing meeting summaries, writing code and debugging features.\u003c/p\u003e\u003cp\u003eWhile easy to prototype, optimizing RAG-based applications for relevant retrieval require complex considerations due to the potential semantic information that is lost when embedding internal documents within vector databases. Therefore, a useful RAG based chatbot requires careful strategies for preprocessing document metadata enrichment (e.g,. metadata filtering, domain specific embedding models, chunking, etc) as well as model inferencing LLM, prompt engineering, and re-ranking retrieved documents.\t\t\t\u003c/p\u003e\u003cp\u003eIn this guide, we’ll cover how Quantumworks Lab can be a useful tool to help you preprocess documents (e.g., entity detection, document classification, etc) and improve retrieval relevance by fine-tuning a reranking model. Feel free to walk through the full guide or watch the step-by-step video below.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/32xu4rxe64\" title=\"RAG Fine Tune Reranking Model Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"556\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"why-reranking\"\u003e\u003cstrong\u003eWhy reranking?\u003c/strong\u003e\t\t\t\u003c/h3\u003e\u003cp\u003eWhen building RAG-based applications, thereʼs a limit to the amount of text that can be passed to an LLM and therefore the context given to an LLM is limited to the top k documents pulled from the vector space using a similarity search. Because semantic information may be lost during the embedding process, relevant context that may be useful to answering the user query may fall outside of the top k responses.\u003c/p\u003e\u003cp\u003eWith reranking embedded within a RAG application, we now have the ability to consider a larger corpus of retrieved text and pass the most relevant documents into our LLM to generate a better answer.\t\u003c/p\u003e\u003cp\u003eUsing the Quantumworks Lab Platform, we will fine tune an reranker model to improve chatbot performance. Holding other considerations consistent (i.e., token limit, embedding model, chunking strategy, LLM choice), we will compare baseline responses without reranking to the output of a fine-tuned reranker workflow.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/AjLZJu0SYcbijxfYEg_CdQZeZYWdtqt9AxwwChx9C7hLBPs-h7pv0Zzg7DcmB5yZhSgu9ro9TNBwTnLzrDT8GDc2zUFMseJndTsQJMWBjFWEXYzZ9nLtkHihx3kFoPMIksq-3qH9b96b6Zc9l2gwKc8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1570\" height=\"1290\"\u003e\u003c/figure\u003e\u003ch2 id=\"part-1-review-initial-baseline-responses\"\u003e\u003cstrong\u003ePart 1. Review initial baseline responses\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo illustrate this approach, we will attempt to improve responses to the 2022 NFL Rulebook 🏈.\u003c/p\u003e\u003cp\u003eNote: From the document, the document is divided into different sections. You can leverage the Quantumworks Lab Platform to label metadata for each page (i.e. Contents / Scenarios / Rules) to improve efficient and relevant retrieval.\u003c/p\u003e\u003cp\u003eThe PDF is embedded using the HuggingFace \u003cem\u003esentence- transformers/distiluse-base-multilingual-cased-v2\u003c/em\u003e\u003cstrong\u003e \u003c/strong\u003emodel. We will keep the same vector embeddings for both workflows.\t\t\t\t\u003c/p\u003e\u003cp\u003eAfter extracting the top 2 documents (via similarity search), we will add them as context to the flan-t5-base LLM model to generate initial responses to queries about NFL rules.\t\u003c/p\u003e\u003cp\u003eAs you can see from the sample of results below, the initial responses are spotty at best.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/RZYA6u7ntf_wRWy6bbFQvC-ZydBQ6uHn8tWJF-Jo-R7rCshIEOgR8z051OXL3Hez4NwNLvP5eJjHViBwcYHQDxU9-y214LeD1kLNgULt6TwLL3pleQNCvKAYWuEwRxbL2kzeAki1OJpsgHQVJCq9Zxc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"688\"\u003e\u003c/figure\u003e\u003ch2 id=\"part-2-human-in-the-loop-hitl-and-semi-supervised-approaches-to-fine-tune-reranker-model\"\u003e\u003cstrong\u003ePart 2. Human in the Loop (HITL) and Semi-Supervised Approaches to Fine Tune Reranker Model\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo fine tune our reranker model, let's extract 5 responses for each query and upload the prompt / responses(s) pairs to Quantumworks Lab \u003ca href=\"https://labelbox.com/product/catalog/\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eNote: We can leverage unsupervised techniques such as semantic search, similarity search, cluster search, etc to enrich data rows with relevant metadata and/or bulk classification. The highlighted cluster shown below contains search queries related to flagrant penalties, which we can use to attach metadata \u0026amp; classifications.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/O_bvl7k6r6laf7g4s0cYrXgpzj5CM-_V-ksxn1UyS7-GJN07nzljZ-ql2pFLczwQyM3y3eVyhZYftRbPeDabkopOb3v89KUTWnp91QBw4p5SEOnHDWrKlQTQfB2bB2yWad8k82Z7HJUpb97XkBoI9Fg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1276\" height=\"864\"\u003e\u003c/figure\u003e\u003cp\u003eUsing \u003ca href=\"https://labelbox.com/product/model/foundry/\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, we can apply semi-supervised learning with foundation models to generate pre labels. Let’s select GPT4 and attach a customized prompt to generate responses to match the target ontology, in this case, whether the output is relevant or not relevant to the query.\t\t\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/y0K_1Q3dxs-mPxMFolWTnex_3HcGgXN5fWWBgzKUteoKVWUn6Kjuq20vVxJdC8WjsEYrUnQZGm4_LxM-FiC7n4hG-ommExevr9t3P7s2ofvR5PmkEhgKxx5q9qc4nJgJLH0oW_mqYxfJm2P_mdcV0qs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"774\"\u003e\u003c/figure\u003e\u003cp\u003eBy sending the query response pairs with pre-labels to our annotation project, human labelers can now leverage pre-labels (and/or metadata \u0026amp; attachments) to speed up the annotation process. Predictions are then converted to ground truth once the human-in-the loop process is completed.\t\t\t\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/FdUCET_nQQ4dREWVgomYDI2BQfEBDhZxVytEmCWt7NPJTTiradOcxYgXnZhcuWnxZnrirSR3KLrJQ3kG54Ab_75DAr87x0yovDek6Wa1TAF7X-E229wd32MWsfxBkU2njrQ-dYm1tPXYitJfbv21Lo4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"639\"\u003e\u003c/figure\u003e\u003cp\u003eTo track model performance, we can set dataset versions and enable data splits before exporting the data rows via the Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk\"\u003e\u003cu\u003ePython SDK\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/xZmuBG-L4HutIDp8jJXPFhggWul_1hQxXGfdWPH0MlKRzS2zuMmfuB6QYcIMjW4LeX68fUxTkrsp5Tex0A0PCUj5H955vltUwgVSWElgoQ0zYmKy4FJ7ozcvIm7xiLW8-fpZIxkU1sRooXDeSjzP7Po\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"794\"\u003e\u003c/figure\u003e\u003ch2 id=\"part-3-fine-tuning-the-reranker-model\"\u003e\u003cstrong\u003ePart 3. Fine Tuning the Reranker Model\u003c/strong\u003e\t\t\t\u003c/h2\u003e\u003cp\u003eFor our workflow, we will fine tune the \u003cem\u003eBAAI/bge-reranker-large\u003c/em\u003e open source model on HuggingFace 😊. Companies such as Cohere also provides reranking models as well for fine tuning.\t\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/0sj2Uim6XsXN-4kAAzRAI4vUJrKjfjygn54MiDApgcxa53ENt-4XNgDQVIiCALj-G15SrRNCN-gis-UPLw0p26P7EwU1icADJNz2DqUZ3wfh1LApA_-zGmXjUC2WjuW8EN4pUWne5A7SIsKLKleKUU4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"217\"\u003e\u003c/figure\u003e\u003cp\u003eTo fine tune our model, we’ll need to format the ground truth labels into JSON line format.\t\t\t\t\t\u003c/p\u003e\u003cp\u003eNote: that the model was fine-tuned with AWS Sagemaker \u003cem\u003eml.g4dn.xlarge \u003c/em\u003emachine with T4 GPUs. The Google Colab environment also provides free GPUs for model fine tuning.\t\t\t\u003c/p\u003e\u003cp\u003eThe model returns relevancy scores for each chunk. We can see that the relative order of the scores and not the absolute value is what matters. Applying this on a data row in the testing set yields the following:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/7ETMUv_vDBG2rpsO2YOf8a-aACQQBtrEvickIWyzFrvoRxrWsRNC7pF8_bdCWcMn-6I-pkXAobCt0Me474smxdcctrkUwCqFo1fAWNUjybeQXDWEOKsdq5d2RrvA2-VAKE8WfFbNbaWlfkgwGIvAOaQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"151\"\u003e\u003c/figure\u003e\u003cp\u003eWe can see from the histogram distribution of relevancy scores for the testing set that relevant examples tend to have higher scores.\t\t\t\t\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/DGjI4LQsIC774mWeYpQnCVi-jSOGi8ufd1t1VBwaPquBoCggrNWivQqr0f6t9DAJ-YwwSeweqiJj2ulbGE1MEjf92O7EyoUMsZf4OxxVZKw1eaz4OJn1kI5r-HWBaS4rrvhGJ2jInxi8R3j0e20OAVM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"818\"\u003e\u003c/figure\u003e\u003cp\u003e\t\t\t\t\t\u003c/p\u003e\u003ch2 id=\"part-4-inference-with-an-updated-rag-workflow\"\u003ePart 4. Inference with an updated RAG workflow\t\t\t\t\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eUsing the same document embeddings \u003cem\u003e(sentence-transformers/distiluse- base-multilingual-cased-v2\u003c/em\u003e) and LLM (\u003cem\u003eflan-t5-base LLM model\u003c/em\u003e), let's retrieve the top 20 document chunks for each query.\t\t\t\t\u003c/p\u003e\u003cp\u003eUsing the fine tuned reranker model, let's rerank the top 3 documents which will be sent to the LLM as context.\u003c/p\u003e\u003cp\u003eApplying inferencing on the testing set, we can now compare the baseline response with the responses with reranking model. From here, we will see some markedly improved responses.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/6ViKEgEsjzeiU5zdHBVr7qDUISy-qmnQVNbigQmaUfS_ic41envcosrvtL7VYlomVHE7gC8YMNbDYz_rYVgTpe5XRSbdeq95Oy9o7p7GgI1iXH-4pD59fz6LNSox1Tfbf8NcshrYKnEPFY7HmUh3pTA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"763\"\u003e\u003c/figure\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003cbr\u003e\t\t\u003c/h2\u003e\u003cp\u003eAs we’ve shown in this guide, model performance for customized chatbots rely on both the quality and quantity of labeled data. By incorporating Quantumworks Lab Catalog and Foundry to apply pre-labels with unsupervised \u0026amp; semi-supervised techniques, we were able to maximize labeling throughput. Afterwards, by using Quantumworks Lab Annotate, we were able to leverage a human-in-the-loop workflow to ensure that our training labels are of the highest quality. Teams should consider adopting this approach when building RAG-based applications to better preprocess documents and improve retrieval relevance by fine-tuning a reranking model.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful chatbots that fuel knowledge sharing and foster deeper customer interactions. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026amp;utm_medium=email\u0026amp;utm_source=house\u0026amp;utm_campaign=modelfoundry\u0026amp;\u0026amp;referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026amp;utm_medium=email\u0026amp;utm_source=house\u0026amp;utm_campaign=guide103123\u0026amp;\u0026amp;attr=intercom\u0026amp;referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab%2520pricing\u0026amp;utm_source=house\u0026amp;utm_medium=email\u0026amp;utm_campaign=020824\u0026amp;\u0026amp;landingPageAnonymousId=%22b04806c5-ff2b-4a09-9379-a9837b1801a9%22\u0026amp;referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"65cd1695643f030001d2239a","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-14-at-11.54.36-AM.png","featured":false,"status":"published","visibility":"public","created_at":"2024-02-14T19:37:57.000Z","updated_at":"2024-05-09T18:43:05.000Z","published_at":"2024-02-14T19:55:15.000Z","custom_excerpt":"Learn how to use Quantumworks Lab to preprocess documents (e.g., entity detection, document classification, etc) and improve retrieval relevance by fine-tuning a reranking model. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","email":"product@labelbox.co","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2022-09-19T20:07:21.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-19T20:07:20.000Z","updated_at":"2022-09-19T20:07:21.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","created_at":"2023-10-26T17:40:04.000Z","updated_at":"2023-10-26T17:40:04.000Z","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","created_at":"2023-10-26T17:46:20.000Z","updated_at":"2023-10-26T18:27:09.000Z","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","created_at":"2023-10-26T17:42:30.000Z","updated_at":"2023-10-26T17:42:30.000Z","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","email":"product@labelbox.co","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"active","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2022-09-19T20:07:21.000Z","comment_notifications":true,"free_member_signup_notification":true,"paid_subscription_started_notification":true,"paid_subscription_canceled_notification":false,"created_at":"2022-09-19T20:07:20.000Z","updated_at":"2022-09-19T20:07:21.000Z","mention_notifications":true,"milestone_notifications":true,"donation_notifications":true,"recommendation_notifications":true,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","created_at":"2023-10-26T17:40:04.000Z","updated_at":"2023-10-26T17:40:04.000Z","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},"email_segment":"all","url":"https://labelbox-guides.ghost.io/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/","excerpt":"Learn how to use Quantumworks Lab to preprocess documents (e.g., entity detection, document classification, etc) and improve retrieval relevance by fine-tuning a reranking model. ","reading_time":5,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":false},"recommended":[{"id":"684755a1e82e4e00013fe307","uuid":"fab31394-c337-43cc-bc44-725a7b69cc61","title":"Quantumworks Lab Leaderboards: Redefining AI evaluations with human-centric assessments","slug":"labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments","html":"\u003cp\u003eIn the rapidly evolving landscape of artificial intelligence, traditional benchmarks are no longer sufficient to capture the true capabilities of AI models. At Quantumworks Lab, we're excited to introduce our groundbreaking \u003ca href=\"https://labelbox.com/leaderboards/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e—an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks. \u003c/p\u003e\u003ch2 id=\"the-limitations-of-current-benchmarks-and-leaderboards\"\u003e\u003cstrong\u003eThe limitations of current benchmarks and leaderboards\u003c/strong\u003e\u003c/h2\u003e\u003ch3 id=\"benchmark-contamination\"\u003e\u003cstrong\u003eBenchmark contamination\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOne of the most pressing issues in AI evaluation today is benchmark contamination. As large language models are trained on vast amounts of internet data, they often inadvertently include the very datasets used to evaluate them. This leads to inflated performance metrics that don't accurately reflect real-world capabilities. For example:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe LAMBADA dataset, designed to test language understanding, has been found in the training data of several popular language models, with an \u003ca href=\"https://hitz-zentroa.github.io/lm-contamination/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLM Contamination Index\u003c/u\u003e\u003c/a\u003e of 29.3%.\u003c/li\u003e\u003cli\u003ePortions of the SQuAD question-answering dataset have been discovered in the pretraining corpora of multiple large language models.\u003c/li\u003e\u003cli\u003eEven coding benchmarks like HumanEval have seen their \u003ca href=\"https://arxiv.org/pdf/2407.07565?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esolutions leaked online\u003c/u\u003e\u003c/a\u003e, potentially contaminating future model training.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis contamination makes it increasingly difficult to trust traditional benchmark results, as models may be “cheating” by memorizing test data rather than demonstrating true understanding or capability.\u003c/p\u003e\u003ch3 id=\"existing-leaderboards-a-step-forward-but-not-enough\"\u003e\u003cstrong\u003eExisting leaderboards: A step forward, but not enough\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWhile several leaderboards have emerged to address the limitations of traditional benchmarks, they each come with their own set of challenges.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eLMSYS chatbot arena\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLMSYS Chatbot Arena, despite its broad accessibility, faces notable challenges in providing objective AI evaluations. Its reliance on non-expert assessments and emphasis on chat-based evaluations may introduce personal biases, potentially favoring engaging responses over true intelligence. Researchers worry that this approach could lead companies to prioritize optimizing for superficial metrics rather than genuine real-world performance. Furthermore, LMSYS's commercial ties raise concerns about impartiality and the potential for an uneven evaluation playing field, as usage data may be selectively shared with certain partners.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScale AI's SEAL\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eScale’s Safety, Evaluations, and Alignment Lab (SEAL), released few months ago, offers detailed insights/evaluations for topics such as reasoning, coding, and agentic tool use. However, the infrequent updates and primary focus on language models, while useful, may not capture the full spectrum of rapidly advancing multimodal AI capabilities.\u003c/p\u003e\u003ch2 id=\"challenges-in-ai-evaluation\"\u003e\u003cstrong\u003eChallenges in AI evaluation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThese and other existing leaderboards all run into core challenges with AI evaluations:\u003c/p\u003e\u003cp\u003e1) Data contamination and overfitting to public benchmarks\u003c/p\u003e\u003cp\u003e2) Scalability issues as models improve and more are added\u003c/p\u003e\u003cp\u003e3) Lack of standards for evaluation instructions and criteria\u003c/p\u003e\u003cp\u003e4) Difficulty in linking evaluation results to real-world outcomes\u003c/p\u003e\u003cp\u003e5) Potential bias in human evaluations\u003c/p\u003e\u003ch2 id=\"introducing-the-labelbox-leaderboards-a-comprehensive-approach-to-ai-evaluation\"\u003e\u003cstrong\u003eIntroducing the Quantumworks Lab Leaderboards: A comprehensive approach to AI evaluation\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Quantumworks Lab Leaderboards are the first to tackle these challenges by conducting structured evaluations on subjective AI model outputs using human experts and a scientific process that provides detailed feature-level metrics and multiple ratings. Leaderboards are available for \u003ca href=\"https://labelbox.com/leaderboards/complex-reasoning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eComplex Reasoning\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/multimodal-reasoning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eMultimodal Reasoning\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eImage Generation\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/speech-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSpeech Generation\u003c/u\u003e\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/leaderboards/video-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eVideo Generation\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eOur goal is to go beyond traditional leaderboards and benchmarks by incorporating the following elements:\u003c/p\u003e\u003ch3 id=\"1-multimodal-and-niche-focus\"\u003e\u003cstrong\u003e1. Multimodal and niche focus\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eUnlike leaderboards that primarily focus on text-based large language models, we evaluate a diverse range of AI modalities and specialized applications, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImage generation and analysis\u003c/li\u003e\u003cli\u003eAudio processing and synthesis\u003c/li\u003e\u003cli\u003eVideo creation and manipulation\u003c/li\u003e\u003cli\u003eComplex and multimodal reasoning\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"2-expert-human-evaluation\"\u003e\u003cstrong\u003e2. Expert human evaluation\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFor every evaluation, public or private, it’s critical for the raters to reflect your target audience. We place expert human judgment, using our \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAlignerr\u003c/u\u003e\u003c/a\u003e workforce, at the core of the evaluation process to ensure:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSubjective quality assessment:\u003c/strong\u003e Humans assess aspects like aesthetic appeal, realism, and expressiveness.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContextual understanding:\u003c/strong\u003e Evaluators consider the broader context and intended use.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAlignment with human preferences:\u003c/strong\u003e Raters ensure evaluations reflect criteria that matter to end-users.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResistance to contamination:\u003c/strong\u003e Human evaluations on novel tasks are less prone to data contamination.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"3-reliable-and-transparent-methodology\"\u003e\u003cstrong\u003e3. Reliable and transparent methodology\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWe are committed to performing trustworthy evaluations using a variety of sophisticated metrics. Quantumworks Lab balances privacy with openness by providing detailed feature-level metrics (e.g. prompt alignment, visual appeal, and numerical count for text-image models) and multiple ratings.\u003c/p\u003e\u003cp\u003eIn addition to critical human experts performing the evaluations, our methodology utilizes the \u003ca href=\"https://labelbox.com/product/platform/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLablebox Platform\u003c/a\u003e to generate advanced metrics on both the rater and model performance. We provide the following metrics across our three leaderboards:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eElo rating system:\u003c/strong\u003e Adapted from competitive chess, our Elo system provides a dynamic rating that adjusts based on head-to-head comparisons between models. This allows us to capture relative performance in a way that's responsive to improvements over time.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTrueSkill rating:\u003c/strong\u003e Originally developed for Xbox Live, TrueSkill offers a more nuanced rating that accounts for both a model's performance and the uncertainty in that performance. This is particularly useful for newer models or those with fewer evaluations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank percentages:\u003c/strong\u003e We track how often each model achieves each rank (1st through 5th) in direct comparisons. This provides insight into not just average performance, but consistency of top-tier results.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAverage rating:\u003c/strong\u003e A straightforward metric that gives an overall sense of model performance across all evaluations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to these key metrics, our methodology incorporates the following characteristics to ensure a balanced and fair evaluation:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eExpert evaluators:\u003c/strong\u003e Utilizing skilled professionals from our Alignerr platform to provide nuanced, context-aware assessments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComprehensive and novel datasets:\u003c/strong\u003e Curated to reflect real-world scenarios while minimizing contamination.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTransparent reporting:\u003c/strong\u003e Detailed insights into our methodologies and results without compromising proprietary information.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"4-continuously-updated-evaluations\"\u003e\u003cstrong\u003e4. Continuously updated evaluations\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOur leaderboard isn't static; we plan to regularly update our evaluations to include the latest models and evaluation metrics, ensuring stakeholders have access to current and relevant information.\u003c/p\u003e\u003ch2 id=\"leaderboard-insights-a-glimpse-into-the-image-generation-leaderboard\"\u003e\u003cstrong\u003eLeaderboard insights: A glimpse into the image generation leaderboard\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo illustrate the power of our comprehensive evaluation approach, let's explore the \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eimage generation leaderboard\u003c/a\u003e. For each evaluation of the latest image-generating models, we capture and publish four key pieces of data to help understand capabilities and areas of opportunity for each model.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Elo ratings:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXf_TDIoHT0uGUl1DnLLMI3qkaV4g9M9yrK4GVWRc1Ze495hZYjwXE5Yq0wZefgLQecqsXA8cS7bSmTNz923B8CYgza7d2PkPn25crjiCrd0I3W2MG53hWiTgo-N8BTL8y11b3vuog?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT Image 1 leads with 1069.17, followed by GPT 4.1 at 1039.62 and Recraft v3 at 1039.37\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2) TrueSkill ratings:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdIblItim2rjJ5kSmyMtNqbhzIQZM24C6TwogiUeuNFCwBNyUPydDpZ3vOlYPSimzksITCDgQ_ej4Czavte2eI8aT0OEjCB0FwDDagBBP-yQLgt2T_FV8dTy0DdFqWUfk4cbMSYcg?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT Image 1 again leads with 982.86, with GPT 4.1 following at 979.89\u003c/li\u003e\u003cli\u003eThis indicates high expected performance for GPT Image 1with relatively low uncertainty\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Rank percentages:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe8jjQCJegixK47fNhDT67NDHe8fhN6U0Kwm-mbkrIa9HWWFcwKbIubLDO7uy0_KtVVu1gHOcIk0Zh9VOHGgV6zrBBkGxbaCHW9a5Uu48lSo8L-J_48nAq6Q-bFqD9k-aYwPvY3?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT 4.1 achieves the top rank 60.14% of the time, followed by GPT Image 1 at 59.31%\u003c/li\u003e\u003cli\u003eThis shows GPT 4.1 consistency in achieving top results, but also highlights GPT Image 1 and DALL-E 3’s strong performance\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4) Average rank (lower better):\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemzdbe3m9BuWRyjcZTnpDJMSAn8a4UuMJTaepEJHsQwJ_JocoixeC4UQcftoenMWuZ3y9tToZU7UtSwxRMNCVdLwytjw72Tq_8bMC5MAJOxB0VYMpE4P4-EcVxs-uqZsEPlM6K?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eNote: Lower score is better here so GPT 4.1 leads in average rank.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT 4.1 slightly edges out GPT Image 1 with an average rating of 1.4 vs 1.41 (lower is better)\u003c/li\u003e\u003cli\u003eThis suggests GPT 4.1 performs well in direct comparisons despite lower Elo and TrueSkill ratings\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese metrics provide a multi-faceted view of model performance, allowing users to understand not just which model is \"best\" overall, but which might be most suitable for their specific use case. For instance, while GPT Image 1 and GPT 4.1 leads in most metrics, DALL-E 3’s and Imagen 3’s strong average rating suggests it is a reliable choice for consistent performance across a range of tasks.\u003c/p\u003e\u003ch2 id=\"join-the-revolution-beyond-the-benchmark\"\u003e\u003cstrong\u003eJoin the revolution: Beyond the benchmark\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Quantumworks Lab Leaderboards represent a significant advance in AI evaluation, pushing past traditional leaderboards by incorporating expert human evaluations for subjective generative AI models using comprehensive metrics. We are uniquely able to achieve this thanks to our modern AI data factory that combines human experts and our scalable platform with years of operational excellence evaluating AI models.\u003c/p\u003e\u003cp\u003eWe invite you to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCheck out the \u003ca href=\"http://labelbox.com/leaderboards?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e to explore our latest evaluations across various AI modalities and niche applications.\u003c/li\u003e\u003cli\u003e\u003cu\u003eLet us know\u003c/u\u003e if you have suggestions or want a specific model included in future assessments.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eContact us\u003c/u\u003e\u003c/a\u003e to learn more about how we can help you evaluate and improve your AI models across all modalities.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eReady to go beyond the benchmark? Let's redefine AI evaluation — together—and drive the field toward more meaningful, human-aligned progress that truly captures the capabilities of next-generation AI models.\u003c/p\u003e","comment_id":"684755a1e82e4e00013fe307","feature_image":"https://labelbox-guides.ghost.io/content/images/2025/06/guide_LeaderboxHero2.png","featured":false,"visibility":"public","created_at":"2025-06-09T21:44:01.000+00:00","updated_at":"2025-06-11T17:30:22.000+00:00","published_at":"2025-06-10T23:15:53.000+00:00","custom_excerpt":"Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"authors":[{"id":"664e43b39132db0001f20548","name":"Michael Haag","slug":"michael","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/michael/"}],"primary_author":{"id":"664e43b39132db0001f20548","name":"Michael Haag","slug":"michael","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/michael/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/","excerpt":"Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Quantumworks Lab Leaderboards: Redefining AI Evaluation with Human Experts","meta_description":"Introducing our groundbreaking Quantumworks Lab leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66cceba90693fd000117e1ca","uuid":"baef90d7-868a-4181-b171-0c1bfec81d5c","title":"Programmatically launch human data jobs for RLHF and evaluation","slug":"programmatically-launch-human-data-jobs-for-rlhf-and-evaluation","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eLabelbox’s Python SDK provides AI teams with a powerful approach to orchestrate human data labeling projects. In this guide, we’ll walk through how to harness the Python SDK to manage human data labeling jobs for RLHF and evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.\u003c/p\u003e\u003ch2 id=\"getting-started-set-up-the-labelbox-python-sdk\"\u003eGetting started: Set up the Quantumworks Lab Python SDK\u003c/h2\u003e\u003cp\u003eLet's begin by first setting up the Quantumworks Lab Python SDK in four simple steps:\u003c/p\u003e\u003cp\u003e1) \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCreate an API key\u003c/u\u003e\u003c/a\u003e to start using Quantumworks Lab Python SDK\u003c/p\u003e\u003cp\u003e2) pip install \"Quantumworks Lab[data]\" in terminal or !pip install \"Quantumworks Lab[data]\" in your notebook\u003c/p\u003e\u003cp\u003e3) Authentication can be done by saving your key to an environment variable:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003euser@machine:~$ export LABELBOX_API_KEY=\"\u0026lt;your_api_key\u0026gt;\"\nuser@machine:~$ python3\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e4) Then, import and initialize the API Client. \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab as lb \nclient = lb.Client()\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"importing-your-data-into-labelbox-methods-and-supported-formats\"\u003eImporting your data into Quantumworks Lab: Methods and supported formats\u003c/h2\u003e\u003cp\u003eNow that the SDK has been set up,\u0026nbsp; let's look at an example of uploading LLM response evaluation data for RLHF:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003e# Create a dataset\ndataset = client.create_dataset(\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"RLHF asset upload example\"+str(uuid.uuid4()),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;iam_integration=None\n)\n# Upload assets\ntask = dataset.create_data_rows([\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_1.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;},\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_2.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;},\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_3.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;}\n\u0026nbsp;\u0026nbsp;])\ntask.wait_till_done()\nprint(\"Errors:\",task.errors)\nprint(\"Failed data rows:\", task.failed_data_rows)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLearn more about all supported data types and editors \u003ca href=\"https://docs.labelbox.com/docs/label-data?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003ch2 id=\"creating-an-ontology-using-the-sdk\"\u003eCreating an ontology using the SDK\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWith the data imported, the next step is to create your ontology for the project. The ontology defines the structure and relationships within the data for your labeling process. Below is an example of how to create an ontology using the Quantumworks Lab Python SDK:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003eimport Quantumworks Lab as lb\nontology_builder = lb.OntologyBuilder(\n\u0026nbsp;\u0026nbsp;classifications=[\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.TEXT,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Free form text example\"),\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.CHECKLIST,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Checklist example\",\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"first_checklist_answer\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"second_checklist_answer\")\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;),\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.RADIO,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Radio example\",\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"first_radio_answer\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"second_radio_answer\")\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.RADIO,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Rank #1\", # More ranks can be created like this\u0026nbsp; with N number of options\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;required = True,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 1\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 2\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 3\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;,)\n\u0026nbsp;\u0026nbsp;]\n)\n\n\nontology = client.create_ontology(\"RLHF classification example\", ontology_builder.asdict(), media_type=lb.MediaType.Conversational)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eFor more information about ontology creation, please refer to the \u003ca href=\"https://docs.labelbox.com/reference/ontology-examples?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edocumentation\u003c/u\u003e\u003c/a\u003e for more examples.\u003c/p\u003e\u003ch2 id=\"best-practices-for-ontology-design\"\u003eBest practices for ontology design\u003c/h2\u003e\u003ch3 id=\"leverage-existing-ontologies-wisely\"\u003eLeverage existing ontologies wisely\u003c/h3\u003e\u003cp\u003eLabelbox allows users to reuse ontologies from previous projects, saving time and ensuring consistency across related tasks. However, be cautious when modifying shared ontologies:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCopy existing ontologies: To prevent unintended changes to previous projects, create a copy of an existing ontology. This creates a new schema node while retaining all your classes.\u003c/li\u003e\u003cli\u003eUsers can customize the ontology for their current project. After copying, they can freely modify the ontology to suit the new project's needs without affecting earlier work.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"optimize-object-ordering-for-logical-workflows\"\u003eOptimize object ordering for logical workflows\u003c/h3\u003e\u003cp\u003eThe order of objects in the ontology can significantly impact the labeling process:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePrioritize common objects: Create the most frequently used objects first. They'll appear at the top of the list, making them easily accessible to labelers.\u003c/li\u003e\u003cli\u003eDesign a logical flow: For complex tasks like model response comparisons, structure the ontology to guide labelers through a step-by-step analysis:\u003c/li\u003e\u003c/ul\u003e\u003col\u003e\u003cli\u003eStart with individual model evaluation criteria.\u003c/li\u003e\u003cli\u003ePlace comparative questions (e.g., \"Which model response is best?\") at the end.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis approach ensures labelers have thoroughly analyzed each option before making final comparisons.\u003c/p\u003e\u003ch3 id=\"enhance-visual-clarity-with-color-coding\"\u003eEnhance visual clarity with color coding\u003c/h3\u003e\u003cp\u003eImprove the visual experience for labelers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConsistent color schemes: Assign and edit colors for each object in the ontology.\u003c/li\u003e\u003cli\u003eMaintain color consistency: Use the same colors throughout the project to reduce cognitive load and improve labeling speed and accuracy.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"provide-easy-access-to-labeling-instructions\"\u003eProvide easy access to labeling instructions\u003c/h3\u003e\u003cp\u003eMake sure labelers have all the information they need at their fingertips:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAttach PDF instructions: Upload labeling guidelines as a PDF document.\u003c/li\u003e\u003cli\u003eSide-by-side viewing: Labelers can reference the instructions within Quantumworks Lab, displayed alongside the project for convenient access.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"use-advanced-classification-features\"\u003eUse advanced classification features\u003c/h3\u003e\u003cp\u003eTake advantage of Quantumworks Lab's classification capabilities to create more nuanced and accurate labels:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImplement nested classifications: This allows for more detailed object identification. For example, after drawing a segmentation mask over a tree, labelers can further classify it as healthy or unhealthy.\u003c/li\u003e\u003cli\u003eSet required questions: Ensure critical information is always captured by making certain questions mandatory for each asset.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy following these best practices, users will create more efficient labeling jobs, leading to higher quality data and improved model performance.\u003c/p\u003e\u003ch2 id=\"labelbox-labeling-services\"\u003eLabelbox labeling services\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFor Enterprise plan users, Quantumworks Lab offers data labeling services, connecting them with professional labelers to process large amounts of data quickly and efficiently. Key features include:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Rapid Data Processing\u003c/strong\u003e: Quickly handle large volumes of data without the overhead of hiring additional staff.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Specialized Expertise\u003c/strong\u003e: Access labelers with specialized knowledge, including:\u003c/p\u003e\u003col\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eMedical experts\u003c/li\u003e\u003cli\u003eVarious language specialists\u003c/li\u003e\u003cli\u003eOther certified specialties\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003c/ol\u003e\u003cp\u003e\u003cstrong\u003e3) Flexibility\u003c/strong\u003e: Scale your labeling service up or down based on project needs without long-term commitments.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4) Quality Assurance\u003c/strong\u003e: Professional labelers are trained to maintain high standards of accuracy and consistency.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5) Time and Resource Savings\u003c/strong\u003e: Eliminate the need for recruitment, training, and management of an in-house labeling team.\u003c/p\u003e\u003cp\u003eBy leveraging labeling services, enterprise users can significantly accelerate their data labeling projects, especially when dealing with complex datasets or when requiring domain-specific expertise. This service complements Quantumworks Lab's robust data import and management capabilities, providing a comprehensive solution for large-scale AI and machine learning projects.\u003c/p\u003e\u003cp\u003eTo leverage labeling services, Quantumworks Lab provides programmatic methods to request labeling services as, shown here:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Getting labeling service information:\u003c/strong\u003e Users can retrieve information about the labeling service for a specific project:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003elabeling_service = project.get_labeling_service()\nprint(labeling_service)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e This will return details such as the service ID, project ID, creation date, status, and more.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Requesting labeling services for faster results:\u003c/strong\u003e Once data and an ontology with instructions has been added, users can initiate a boost request:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003elabeling_service.request()\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e This call initiates the labeling services service for your project.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Monitoring your labeling service’s status\u003c/strong\u003e:The Quantumworks Lab labeling service requested can be easily monitored via the UI as shown below:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXekVuCZ2W9OORuo0lpeibkcuoC79eXyUJ1KIU4s1OYsnE_VgNaLcMBLaJ5OHsc57ae_3BfSIM3hv4GacVmkhbOvbPNb-PetLKLK4vUN7fDitccn5y-PIhV_nZj_OU7TeOln9Y-tRbyClHvSN3vXPHVczmk?key=Eb16GHK2ItC9fBn058nPbA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"799\"\u003e\u003c/figure\u003e\u003ch2 id=\"simple-export-via-the-labelbox-sdk\"\u003eSimple export via the Quantumworks Lab SDK\u003c/h2\u003e\u003cp\u003eOnce the labeling project is complete, users can easily export the labels using the SDK, as shown below.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eexport_task = project.export(params=export_params, filters=filters)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis simple command allows users to retrieve labeled data that is ready for use in machine learning pipelines. Please refer to \u003ca href=\"https://docs.labelbox.com/reference/export-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edocumentation\u003c/u\u003e\u003c/a\u003e for flexible ways of exporting a project with filters.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab Python SDK offers teams with a convenient and powerful way to programmatically manage human data labeling projects. By providing control over every aspect of the labeling process - from data import and ontology design to project monitoring and data export - the SDK enables AI teams with the ability to incorporate high-quality labeled data into their workflows seamlessly.\u003c/p\u003e\u003cp\u003eWe hope you found this guide helpful for gaining a deeper understanding of how to capitalize on an SDK-driven approach to simplify complex tasks and enhance productivity. Whether you’re working on small-scale projects or large, distributed labeling efforts, the Quantumworks Lab SDK offers the full-suite of tooling needed to efficiently manage your\u0026nbsp; data labeling needs and accelerate their AI development process.\u003c/p\u003e\u003cp\u003eIf you're interested in implementing an SDK approach to jumpstart your human data jobs for RLHF and model evaluation,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out, or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more.\u003c/p\u003e","comment_id":"66cceba90693fd000117e1ca","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-26-at-2.03.08-PM.png","featured":false,"visibility":"public","created_at":"2024-08-26T20:55:05.000+00:00","updated_at":"2024-08-28T15:34:08.000+00:00","published_at":"2024-08-26T21:03:56.000+00:00","custom_excerpt":"Learn how to harness the SDK to manage human data labeling jobs for RLHF and model evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/programmatically-launch-human-data-jobs-for-rlhf-and-evaluation/","excerpt":"Learn how to harness the SDK to manage human data labeling jobs for RLHF and model evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bfded301c06500016e8b1c","uuid":"a2c40e88-6376-4824-b57f-e0e3673d8b1b","title":"Evaluating leading text-to-speech models","slug":"evaluating-leading-text-to-speech-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in speech generation evaluation, visit \u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe evolution of Text-to-Speech (TTS) models has been groundbreaking, with various companies offering high-quality solutions for a range of applications. Whether it's enhancing accessibility, automating customer service, or generating voiceovers, TTS models play a vital role in today's digital landscape. \u003c/p\u003e\u003cp\u003eWe recently evaluated the performance of six leading TTS models—Google TTS, Cartesia, AWS Polly, OpenAI TTS, Deepgram, and Eleven Labs—using key evaluation metrics such as Word Error Rate (WER) and human preference ratings.\u003c/p\u003e\u003cp\u003eTo provide an in-depth analysis, we evaluated each model on 500 prompts. Each prompt was reviewed by three expert labelers to ensure accuracy and provide diverse perspectives.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"evaluation-process\"\u003e\u003cstrong\u003eEvaluation Process\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate speech outputs across several key criteria:\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer\"\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWER measures the accuracy of the generated speech by calculating the number of insertions, deletions, and substitutions compared to a reference transcription. The lower the WER, the better the model performed in terms of generating accurate and coherent speech.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1120\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"word-errors\"\u003eWord errors\u003c/h3\u003e\u003cp\u003eRaters evaluated the presence of word errors in the generated TTS audio by counting the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Insertion Errors\u003c/strong\u003e: Additional words not present in the original text.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Deletion Errors\u003c/strong\u003e: Words from the original text that were omitted.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Substitution Errors\u003c/strong\u003e: Incorrect replacements for original words.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: No word errors or only minor errors (0-1 error).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some errors (2-3 errors).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Significant errors (4 or more errors).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"speech-naturalness\"\u003eSpeech naturalness\u003c/h3\u003e\u003cp\u003eRaters assessed how human-like the generated speech sounded.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Very human-like, natural flow with appropriate pauses and inflections.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some human qualities but with occasional robotic or awkward elements.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Clearly robotic or artificial, with choppy or monotone speech.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"pronunciation-accuracy\"\u003ePronunciation accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated how clearly and correctly words were pronounced in the TTS output.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: All words are pronounced clearly and correctly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: 1-2 words are mispronounced or unclear.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: 3 or more words are mispronounced or difficult to understand.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"noise\"\u003eNoise\u003c/h3\u003e\u003cp\u003eRaters evaluated the level of background noise or artifacts in the audio.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNone\u003c/strong\u003e: No detectable noise or artifacts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Minor noise, such as a faint hiss, but not distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Noticeable noise or artifacts that may be somewhat distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Significant noise or artifacts that interfere with comprehension.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"context-awareness\"\u003eContext awareness\u003c/h3\u003e\u003cp\u003eRaters assessed the TTS system's ability to adjust to context, including tone, emphasis, and punctuation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Excellent adaptation to context, with clear tonal shifts and pauses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Adequate adaptation but misses some nuances.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Little to no adaptation; reads text monotonously without context cues.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"prosody-accuracy\"\u003eProsody accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated the rhythm, stress, and intonation (prosody) of the generated speech.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Natural-sounding rhythm and intonation that closely mimics human speech.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Mostly natural but with occasional awkwardness in stress or rhythm.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Monotonous or robotic intonation with unnatural pauses and emphasis\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"prompts-curation-process\"\u003ePrompts curation process\u003c/h2\u003e\u003cp\u003eThe diverse set of prompts used in our TTS evaluation provides a comprehensive test of model capabilities across multiple dimensions. \u003c/p\u003e\u003cp\u003eConsider these examples:\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Wow! Did you see the fireworks last night? They were spectacular!\"\u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003e This short, exclamatory sentence tests the model's ability to convey excitement and emphasis. It challenges the TTS system to appropriately modulate tone and pitch to express enthusiasm.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"I wish you were on the right path...I-I-I truly do, but you're not,' he said sadly.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis prompt evaluates the model's handling of emotional nuance, hesitation (indicated by the stuttering \"I-I-I\"), and the use of ellipsis for pauses. It also tests the system's ability to convey sadness as specified in the dialogue tag.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Alright, listen up, team! The mission briefing is as follows: Operation Nighthawk commences at 0300 hours. Rendezvous at coordinates 51°30'26.5\"N 0°07'39.0\"W. Your login credentials for the secure server are: Username: NightOwl_Alpha, Password: 3ch3l0n_X9#. Remember, this is a stealth operation. Radio silence must be maintained except for emergency protocol Sierra-Tango-Seven. If compromised, use the failsafe code: Zulu-Yankee-4-7-2-Bravo. Any questions? No? Then synchronize your watches... now. Good luck, and godspeed.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis complex prompt tests multiple aspects:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical pronunciation of alphanumeric strings and military jargon\u003c/li\u003e\u003cli\u003eConveying urgency and authority\u003c/li\u003e\u003cli\u003eHandling of coordinates and precise numerical information\u003c/li\u003e\u003cli\u003eProper pacing for a lengthy, detailed instruction set\u003c/li\u003e\u003cli\u003eAppropriate pausing, indicated by punctuation and paragraph breaks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese prompts demonstrate how we cover a range of linguistic features, emotional tones, and formatting challenges. They span from simple exclamations to complex, technical instructions, ensuring a thorough assessment of prosody, emphasis, and versatility.\u003c/p\u003e\u003cp\u003eThis diversity in prompts matters because it enables a comprehensive evaluation that reflects real-world applicability. By covering such a broad spectrum of scenarios, we can identify specific strengths and weaknesses in each TTS model, allowing for fair comparisons and pinpointing areas for improvement. Whether it's expressing joy over fireworks, conveying emotional conflict, or delivering a detailed mission briefing, these prompts help us ensure that TTS models can handle the variety and complexity of human communication.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overall-ranking-best-to-worst-model-samples\"\u003eOverall Ranking best to worst model samples\u003c/h2\u003e\u003cp\u003eRaters ranked each TTS sample from best to worst based on performance across all categories above.\u003c/p\u003e\u003ch3 id=\"model-configurations-used-at-test-time\"\u003e\u003cstrong\u003eModel configurations used at test time\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: tts-1-hd\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: alloy\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle Text-to-Speech (TTS)\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel:\u003c/strong\u003e Default\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice: \u003c/strong\u003eDefault with neutral gender voice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eElevenLabs\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: eleven_monolingual_v1 (As of 30th September, eleven_monolingual_v1 is an outdated version and Quantumworks Lab will run a new experiment with\u0026nbsp;eleven_multilingual_v2 in the future)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: pMsXgVXv3BLzUgSXRplE\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: aura-asteria-en\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Default\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAmazon Polly\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Generative\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Ruth\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: sonic-english\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Teacher Lady\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe selected these models for our evaluation as they represent the cutting-edge offerings from leading companies in the TTS space, showcasing the latest advancements in voice synthesis technology.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"results\"\u003eResults\u003c/h1\u003e\u003cp\u003eOur evaluation of 500 diverse and complex prompts provided valuable insights into the capabilities of Google, Cartesia, AWS Polly, OpenAI, Deepgram, and Eleven Labs TTS models. Each prompt was meticulously assessed by three different raters to ensure accurate and diverse perspectives. This rigorous testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of these leading TTS models:\u003c/p\u003e\u003ch3 id=\"rank-distribution-percentages\"\u003eRank distribution percentages\u003cstrong\u003e\u0026nbsp; \u0026nbsp; \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"overall-ranking\"\u003eOverall ranking\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1:\u003c/strong\u003e\u0026nbsp; (Open AI TTS) - occurred 607 times, 42.93%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2:\u003c/strong\u003e\u0026nbsp; (Cartesia) - occurred 460 times, 32.53%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 410 times, 29.00%\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 4:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 338 times, 23.90%. Followed by Eleven labs as a close second with occurrence 335, 23.69%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 5:\u003c/strong\u003e\u0026nbsp; (Deepgram) - occurred 438 times, 30.98%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 6:\u003c/strong\u003e\u0026nbsp; (Google TTS) - occurred 868 times, 61.39%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote: Rank is determined by the most frequently selected model for each position, which is why some AWS Polly appears twice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1122\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003eFig: % of times each model was ranked first.\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer-results\"\u003eWord Error Rate (WER) Results\u003c/h3\u003e\u003cp\u003eThe WER scores offer an objective look at how accurately each model converts text to speech.\u003c/p\u003e\u003cul\u003e\u003cli\u003eDeepgram: 5.67%\u003c/li\u003e\u003cli\u003eOpenAI TTS: 4.19%\u003c/li\u003e\u003cli\u003eGoogle TTS: 3.36%\u003c/li\u003e\u003cli\u003eAWS Polly: 3.18%\u003c/li\u003e\u003cli\u003eCartesia: 3.87%\u003c/li\u003e\u003cli\u003eEleven Labs: 2.83%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs achieved the lowest WER at 2.83%, making it the most accurate model. Deepgram, however, had the highest WER at 5.67%, indicating room for improvement in transcription accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" title=\"Chart\" width=\"2000\" height=\"1126\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1123\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-specific-performance-results\"\u003eModel-Specific Performance Results\u003c/h2\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each TTS model based on key evaluation criteria.\u003c/p\u003e\u003ch3 id=\"openai-tts\"\u003eOpenAI TTS\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 4.1898%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 89.60% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 87.13% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 92.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 63.37% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 64.57% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOpenAI TTS excels at producing natural-sounding speech, making it a top choice for applications requiring lifelike vocal output. It demonstrates exceptional accuracy in pronunciation and performs excellently in producing clean audio with minimal background noise. The model shows strong capabilities in understanding and conveying contextual nuances in speech and is proficient in delivering appropriate intonation and rhythm. Despite ranking high in human preference, it shows moderate accuracy in word reproduction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cm24ur6wwf\" title=\"OpenAI: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gicqvkqc5e\" title=\"OpenAI: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kg9wqqftu4\" title=\"OpenAI: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"cartesia\"\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 58.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 53.11% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 93.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 80.98% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 77.02% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.8707%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCartesia demonstrates good accuracy in word reproduction. While not as consistent as OpenAI TTS in naturalness, it still performs very well in producing natural-sounding speech. It shows excellent accuracy in pronunciation and excels at producing clean audio with minimal background noise. The model displays good capabilities in understanding contextual nuances and performs well in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/3gukhi7z2j\" title=\"Cartesia: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y3lbes0sin\" title=\"Cartesia: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f0lmkto1xk\" title=\"Cartesia: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"aws-polly\"\u003e\u003cstrong\u003eAWS Polly\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 56.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 55.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.39% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 84.72% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Medium in 58.20% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.1770%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAWS Polly shows very good accuracy in word reproduction, ranking second among the models. It struggles more with producing natural-sounding speech compared to top performers but maintains excellent pronunciation accuracy. The model performs well in producing clean audio and shows good capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qawx1dwc9g\" title=\"Polly: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/klmnfcsno5\" title=\"AWS Polly: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ubzrqtjnc9\" title=\"AWS Polly: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"eleven-labs\"\u003e\u003cstrong\u003eEleven Labs\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 2.8302%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: Medium in 44.98% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 81.97% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 80.27% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 44.70% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 46.25% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs demonstrates the best accuracy in word reproduction among all models. It shows mixed results in producing natural-sounding speech but maintains excellent pronunciation accuracy. The model performs reasonably well in producing clean audio but has more noise issues than some competitors. It shows moderate capabilities in understanding contextual nuances and has room for improvement in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/x3qzhkt0ab\" title=\"Eleven Labs: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kl0lc7oqmz\" title=\"Eleven Labs: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bc19r3iqip\" title=\"Eleven Labs: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"deepgram\"\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 5.6743%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: High in 57.78% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 64.43% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 88.83% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 53.18% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 55.52% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDeepgram shows the highest error rate in word reproduction among all models. It performs moderately well in producing natural-sounding speech and demonstrates good accuracy in pronunciation, although lagging behind some competitors. The model performs well in producing clean audio with minimal background noise but shows moderate capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mngnvgtzx3\" title=\"Deepgram: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6h3bgmcg6d\" title=\"Deepgram: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uaj3x9bln8\" title=\"Deepgram: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"google-tts\"\u003e\u003cstrong\u003eGoogle TTS\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.3574%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Low in 78.01% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 77.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.46% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: Medium in 39.25% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: Low in 45.83% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z27ett9q8m\" title=\"Google TTS: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/06pqi5qc13\" title=\"Google TTS: Wow Did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kwotkto0u6\" title=\"Google TTS: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation reveals that while Eleven Labs leads in transcription accuracy with the lowest WER, OpenAI TTS emerges as the top choice when it comes to human preference, particularly for speech naturalness, pronunciation accuracy, and prosody.\u003c/p\u003e\u003cp\u003eConversely, Google TTS, despite a relatively low WER, ranks last in human preference due to poor performance in categories like speech naturalness and context awareness. This demonstrates the importance of a balanced evaluation that considers both quantitative metrics like WER and qualitative factors like user satisfaction.\u003c/p\u003e\u003cp\u003eAs the TTS field continues to evolve, the key to further advancements will lie in combining high transcription accuracy with natural, expressive, and context-aware speech. Models like OpenAI TTS and Eleven Labs are setting the standard, but there remains room for improvement across all models to fully meet the growing demands of diverse TTS applications.\u003c/p\u003e\u003cp\u003eIn summary, while WER is crucial, the broader context of user experience—focusing on factors like speech naturalness, context awareness, and prosody accuracy—is essential in evaluating the overall effectiveness of TTS models.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-speech models presented here represents a significant advancement in assessing AI-generated speech. By leveraging this methodology alongside Quantumworks Lab's cutting-edge platform, AI teams can dramatically accelerate the development and refinement of sophisticated, domain-specific speech generation models.\u003c/p\u003e\u003cp\u003eOur solution offers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eEfficient dataset curation\u003c/li\u003e\u003cli\u003eAutomated evaluation techniques\u003c/li\u003e\u003cli\u003eHuman-in-the-loop quality assurance\u003c/li\u003e\u003cli\u003eCustomizable workflows for TTS model assessment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-speech model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdohXa2KeUawTMTWS7MfKSUYC6fVPCHjtmcFCRt5ycmB7S4k4-JElTwnmRb6ufW2kR7F3Bqu1PwzbvL83LKcqgz8jbkoY02tefklFLp98wvqm2liOKBglEGaRITIk6Wx1tY3fi-43TaCJnpwEwRVCz7q101?key=Qy_gZn4bA_q73M6n7nwEIA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"1024\"\u003e\u003c/figure\u003e","comment_id":"66bfded301c06500016e8b1c","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-12.12.46-PM.png","featured":false,"visibility":"public","created_at":"2024-08-16T23:20:51.000+00:00","updated_at":"2024-11-26T00:11:38.000+00:00","published_at":"2024-08-17T19:13:25.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/evaluating-leading-text-to-speech-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}]},"__N_SSG":true},"page":"/guides/[id]","query":{"id":"how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 12:51:15 GMT -->
</html>