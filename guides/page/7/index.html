<!DOCTYPE html><html>
<!-- Mirrored from labelbox.com/guides/page/7/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 13:30:53 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Guides | Quantumworks Lab</title><meta name="description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><link rel="preconnect" href="../../../../fonts.googleapis.com/index.html" data-next-head=""/><link rel="preconnect" href="../../../../fonts.gstatic.com/index.html" crossorigin="" data-next-head=""/><link href="../../../../fonts.googleapis.com/css2f4aa.css?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,600;0,700;1,100;1,200;1,300;1,600;1,700&amp;family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" data-next-head=""/><meta property="og:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta property="og:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta property="og:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="og:image" content="/static/images/guides-social.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Guides | Quantumworks Lab" data-next-head=""/><meta name="twitter:description" content="Covering everything you need to know in order to build AI products faster." data-next-head=""/><meta name="twitter:site" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:creator" content="@Quantumworks Lab" data-next-head=""/><meta name="twitter:url" content="https://labelbox.com/guides/" data-next-head=""/><meta property="twitter:image" content="/static/images/guides-social.png" data-next-head=""/><meta name="robots" content="noimageindex"/><meta name="google-site-verification" content="SRhmoKSVCTohF2mf6v399S1hBWnpttMgky5tXdr-3yg"/><meta name="google-site-verification" content="lI3zXS3UkxbozsCAWHpRCzkujEMbo92e1smM4A7_6lA"/><meta name="theme-color" content="#2876D4"/><meta name="referrer" content="origin-when-crossorigin"/><link rel="shortcut icon" href="../../../static/images/favicon-v4-black.png"/><meta http-equiv="Content-Security-Policy" content=" default-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27;;  script-src * &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; https://cdn.logrocket.io https://cdn.lr-ingest.io https://cdn.lr-in.com https://cdn.lr-in-prod.com;  connect-src * data: &#x27;unsafe-inline&#x27;;  img-src * data: blob: &#x27;unsafe-inline&#x27; https://*.logrocket.io https://*.lr-ingest.io https://*.logrocket.com https://*.lr-in.com https://*.lr-in-prod.com;  frame-src *;  style-src * &#x27;unsafe-inline&#x27;;  worker-src * &#x27;self&#x27; blob:;  child-src * &#x27;self&#x27; blob:; "/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-112801961-1"></script><script type="text/javascript" src="../../../../cdn.bizible.com/scripts/bizible.js" async=""></script><script id="GTM">
                            window.dataLayer = window.dataLayer || [];
                            function gtag(){dataLayer.push(arguments);}
                            gtag('js', new Date());

                            gtag('config', 'UA-112801961-1');

                            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                            '../../../../www.googletagmanager.com/gtm5445.html?id='+i+dl;f.parentNode.insertBefore(j,f);
                            })(window,document,'script','dataLayer','GTM-NXF2T87');
                        </script><link rel="stylesheet" href="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script src="../../../../code.jquery.com/jquery-3.4.1.min.js"></script><script async="" defer="" id="Intercom" src="../../../static/scripts/intercom.js"></script><script id="Munchkin1" src="../../../../munchkin.marketo.net/munchkin.js" type="text/javascript"></script><script id="Munchkin2" src="../../../static/scripts/munchkin.js"></script><script src="../../../../discover.labelbox.com/rs/622-PVG-762/images/dpi-ppc-tracking-script.js"></script><style data-styled="" data-styled-version="5.3.11">.jsdymq{height:22px;width:30px;position:relative;}/*!sc*/
.jsdymq.dark-mode > div{background-color:white;}/*!sc*/
.jsdymq > div{height:4px;width:100%;background-color:black;position:absolute;left:0;margin:auto;-webkit-transition:.3s;transition:.3s;}/*!sc*/
.jsdymq > div#one{top:0;}/*!sc*/
.jsdymq > div#two{top:0;bottom:0;}/*!sc*/
.jsdymq > div#three{bottom:0;}/*!sc*/
.jsdymq.-open > div#one{top:40%;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
.jsdymq.-open > div#two{opacity:0;}/*!sc*/
.jsdymq.-open > div#three{bottom:40%;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}/*!sc*/
data-styled.g23[id="BurgerIcon__Wrapper-sc-1rg1iu4-0"]{content:"jsdymq,"}/*!sc*/
.hgDgWF{position:-webkit-sticky;position:sticky;width:100vw;top:0;left:0;background-color:rgba(255,255,255);-webkit-backdrop-filter:blur(150px);backdrop-filter:blur(150px);-webkit-backdrop-filter:blur(150px);z-index:100 !important;}/*!sc*/
.hgDgWF.dark-mode{background:#121619;}/*!sc*/
data-styled.g27[id="HeaderMobile__Outer-sc-1yu1mfn-0"]{content:"hgDgWF,"}/*!sc*/
.iPVOAg{height:72px !important;padding:0 24px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.iPVOAg > a > img.labelbox-logo{width:120px;}/*!sc*/
.iPVOAg > i.material-icons{font-size:36px;color:#2876D4;}/*!sc*/
data-styled.g28[id="HeaderMobile__Wrapper-sc-1yu1mfn-1"]{content:"iPVOAg,"}/*!sc*/
.eJChXt{padding-bottom:100px;}/*!sc*/
.eJChXt .sub-item{border-left:2px solid rgba(177,194,216,.21);}/*!sc*/
.eJChXt h6{font-size:20px;font-weight:500;margin-bottom:10px;}/*!sc*/
.eJChXt a{color:#49535F;}/*!sc*/
.eJChXt .IN-widget,.eJChXt #twitter-widget-0{margin:10px 0;}/*!sc*/
.eJChXt .footer-grid{grid-template-columns:repeat(5,1fr);margin:40px 0 100px;text-align:left;}/*!sc*/
.eJChXt .MuiTypography-body2{font-size:18px;}/*!sc*/
.eJChXt .MuiTypography-caption{display:inline-block;width:100%;line-height:26px;text-align:center;}/*!sc*/
@media (max-width:960px){.eJChXt{text-align:center;}.eJChXt .MuiTypography-caption{margin-top:10px;}}/*!sc*/
data-styled.g31[id="Footer__StyledFooter-sc-u68pnv-0"]{content:"eJChXt,"}/*!sc*/
.kDcKSx.no-overflow{overflow-x:hidden;}/*!sc*/
.kDcKSx.no-header{padding-top:0;}/*!sc*/
data-styled.g32[id="Layout__Wrapper-sc-jbj1sg-0"]{content:"kDcKSx,"}/*!sc*/
.cKNvnl a{color:#2563eb;}/*!sc*/
data-styled.g34[id="Footer__FooterSection-sc-172m51x-0"]{content:"cKNvnl,"}/*!sc*/
</style><script defer="" async="" id="Cookiebot" type="text/javascript" src="../../../../consent.cookiebot.com/uc.js" data-cbid="f530ad95-5299-43fb-9606-954f44911c4c"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><script src="../../../../platform.linkedin.com/in.js" type="text/javascript"> <!-- -->lang: en_US</script><link rel="preload" href="../../../_next/static/css/77a4236f8e9a0455.css" as="style"/><link rel="stylesheet" href="../../../_next/static/css/77a4236f8e9a0455.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../../../_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script data-partytown-config="">
            partytown = {
              lib: "/_next/static/~partytown/"
            };
          </script><script data-partytown="">!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=w[p]||{};c[f]=(c[f]||[])})(window,'partytown','forward');/* Partytown 0.9.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(t){p=r.createElement(t?"script":"iframe"),t||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(t?"atomics.js?v=0.9.2":"sandbox-sw.html?"+Date.now()),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);</script><script src="../../../_next/static/chunks/webpack-66e4f841c974e844.js" defer=""></script><script src="../../../_next/static/chunks/framework-f0f34dd321686665.js" defer=""></script><script src="../../../_next/static/chunks/main-e0f96f365dc7fe29.js" defer=""></script><script src="../../../_next/static/chunks/pages/_app-75d6dce28a8def7d.js" defer=""></script><script src="../../../_next/static/chunks/8220-8c3b7d4a24781c26.js" defer=""></script><script src="../../../_next/static/chunks/430-c66e465cc32ba99e.js" defer=""></script><script src="../../../_next/static/chunks/3817-e3d316d0f77ffd1b.js" defer=""></script><script src="../../../_next/static/chunks/385-070eb47cdc107155.js" defer=""></script><script src="../../../_next/static/chunks/8853-67d854f782ed49e3.js" defer=""></script><script src="../../../_next/static/chunks/4587-67f46a96540c8153.js" defer=""></script><script src="../../../_next/static/chunks/3341-ef29c07033cbc96b.js" defer=""></script><script src="../../../_next/static/chunks/5008-6b2f21a0ee7e9705.js" defer=""></script><script src="../../../_next/static/chunks/pages/guides/page/%5bid%5d-19f708b94cd03eda.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_buildManifest.js" defer=""></script><script src="../../../_next/static/Tltx2tBe97VOu7U0vVrr8/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NXF2T87" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><main><div class="Layout__Wrapper-sc-jbj1sg-0 kDcKSx transition-all duration-300 ease-in undefined  overflow"><div class=""></div><div class="HeaderMobile__Outer-sc-1yu1mfn-0 hgDgWF"><div class="HeaderMobile__Wrapper-sc-1yu1mfn-1 iPVOAg lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><a href="../../../index.html"><img width="106" height="24" alt="logo" src="../../../static/images/logo-v4.svg"/></a><div class="BurgerIcon__Wrapper-sc-1rg1iu4-0 jsdymq"><div id="one"></div><div id="two"></div><div id="three"></div></div></div></div><div class="py-24 lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="grid grid-cols-12 "><div class="col-span-12 md:col-span-3"><div class="sticky top-24"><img src="../../../static/images/guide.svg" class="h-10"/><h1 class="font-future text-2xl md:text-4xl font-bold my-5">Guides</h1><p class="text-base max-w-xs text-neutral-500  pr-6">Covering everything you need to know in order to build AI products faster.</p><div class="pb-4 md:pb-0"><div class="flex relative  md:max-w-xs my-10  md:pr-6"><input type="text" class="bg-transparent border-[1px] border-solid border-black w-full rounded-md pl-10 p-2 focus-visible:outline-none" placeholder="Search..."/><img class="absolute top-3 left-0 ml-2 w-6" src="../../../static/images/library/large_search_icon.svg"/></div></div><div class="hidden md:flex md:flex-col"><a href="../../index.html" class="text-base text-neutral-900 font-medium hover:text-neutral-800 mb-4">Latest</a><a href="../../tag/build-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Build AI</a><a href="../../tag/use-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Use AI</a><a href="../../tag/explore-manage-data/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Explore &amp; manage data</a><a href="../../tag/label-data-for-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Label data for AI</a><a href="../../tag/train-fine-tune-ai/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">Train &amp; fine-tune AI</a><a href="../../tag/mlops/index.html" class="text-base text-neutral-500 hover:text-neutral-800 mb-4">MLOps</a></div></div></div><div class="col-span-12 md:col-span-9"><div class="grid grid-cols-12 gap-6"><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index4b8a.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-3012--4-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data</p><p class="text-base max-w-2xl undefined line-clamp-3">In this guide, we&#x27;ll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../intro-to-model-metrics/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexebef.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FGroup-2721--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../intro-to-model-metrics/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">An introduction to model metrics</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index3e9d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2FScreen-Shot-2023-03-01-at-8.57.36-AM.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">Using Quantumworks Lab and Weights &amp; Biases to fine tune your computer vision projects</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how you can use Quantumworks Lab and Weights &amp; Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-train-evaluate-and-improve-your-ml-models/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexcf8d.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2966--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-train-evaluate-and-improve-your-ml-models/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to get started in Quantumworks Lab Model: Train, evaluate, and improve your ML models</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we&#x27;ll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-kickstart-and-scale-your-data-labeling-efforts/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexc47c.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2973--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-kickstart-and-scale-your-data-labeling-efforts/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to kickstart and scale your data labeling efforts</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-evaluate-and-optimize-your-data-labeling-projects-results/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2977--3-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2977--3-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2977--3-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2977--3-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2977--3-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2977--3-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2977--3-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2977--3-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/index2f15.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2977--3-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-evaluate-and-optimize-your-data-labeling-projects-results/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to evaluate and optimize your data labeling project&#x27;s results</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-create-a-quality-strategy-for-your-data-labeling-project/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2976--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2976--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2976--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2976--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2976--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2976--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2976--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2976--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexf335.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2976--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-create-a-quality-strategy-for-your-data-labeling-project/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to create a quality strategy for your data labeling project</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-define-your-data-labeling-projects-success-criteria/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2975--3-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2975--3-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2975--3-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2975--3-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2975--3-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2975--3-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2975--3-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2975--3-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexc1f9.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2975--3-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-define-your-data-labeling-projects-success-criteria/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to define your data labeling project&#x27;s success criteria</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to effectively define your labeling project&#x27;s success criteria so all tasks lead to consistent output with high quality. </p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-define-a-task-for-your-data-labeling-project/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2974--1-.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2974--1-.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2974--1-.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2974--1-.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2974--1-.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2974--1-.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2974--1-.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2974--1-.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexf18b.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2023%2F01%2FGroup-2974--1-.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-define-a-task-for-your-data-labeling-project/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to define a task for your data labeling project</p><p class="text-base max-w-2xl undefined line-clamp-3">Learn how to align on key components of your project: define a task, create an ontology, and determine timelines for your labeling project.</p></a></div></div></div></div></div><div class="col-span-12 md:col-span-6 h-100"><div class="h-100"><div class="bg-transparent rounded-lg h-100 flex flex-col"><a href="../../how-to-find-similar-data-in-one-click/index.html" target="_self" class="relative aspect-video  false border border-neutral-200 rounded-lg"><img loading="lazy" decoding="async" data-nimg="fill" class="rounded-lg " style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2965.png&amp;w=640&amp;q=70 640w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2965.png&amp;w=750&amp;q=70 750w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2965.png&amp;w=828&amp;q=70 828w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2965.png&amp;w=1080&amp;q=70 1080w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2965.png&amp;w=1200&amp;q=70 1200w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2965.png&amp;w=1920&amp;q=70 1920w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2965.png&amp;w=2048&amp;q=70 2048w, /_next/image/?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2965.png&amp;w=3840&amp;q=70 3840w" src="../../../_next/image/indexe3f3.html?url=https%3A%2F%2Flabelbox-guides.ghost.io%2Fcontent%2Fimages%2F2022%2F12%2FGroup-2965.png&amp;w=3840&amp;q=70"/></a><div class="py-6 px-1 flex flex-col flex-grow justify-content-between"><div><a href="../../how-to-find-similar-data-in-one-click/index.html"><p class="text-xl lg:text-2xl leading-6 lg:leading-9 mb-2 max-w-3xl font-bold ">How to find similar data in one click</p><p class="text-base max-w-2xl undefined line-clamp-3">Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Learn more about how to find similar data in one click with Labelbox. </p></a></div></div></div></div></div><div class="col-span-12"><div class="flex align-items-center justify-content-center mx-auto mt-8"><a class="mr-9 text-neutral-700 mb-1" href="../6/index.html">&lt;</a>Page 7 of 10<a class="ml-9 text-neutral-700 mb-1" href="../8/index.html">&gt;</a></div></div></div></div></div></div><div class=""><div class="my-24 w-full h-[1px] bg-neutral-200"></div><section id="start-for-free-footer" class="
      max-w-xl
      m-auto flex flex-col gap-4 items-center justify-items-center text-center"><div class="Footer__FooterSection-sc-172m51x-0 cKNvnl flex flex-col gap-y-6 justify-center"><div class="w-160 m-auto pb-10"></div><h2 class="font-medium text-4xl sm:text-5xl lg:text-6xl  text-neutral-900 font-future">Try Quantumworks Lab today</h2><p class="text-neutral-500 font-medium  text-lg md:text-xl max-w-3xl m-auto">Get started for free or see how Quantumworks Lab can fit your specific needs by <a href="../../../sales/index.html">requesting a demo</a></p></div><a href="https://app.labelbox.com/signup" class="group transition-all inline-flex justify-center items-center gap-[.5em] lb-button !outline-none focus:!outline-none focus-visible:!outline-none focus:!shadow-none focus:!drop-shadow-none active:shadow-none w-fit text-xl py-4 px-6 rounded-xl font-medium leading-[32px] bg-neutral-800 mix-blend-multiply hover:bg-black dark:bg-neutral-50 text-neutral-50 dark:text-neutral-900 mt-6" id="" target="_self" style="outline:0 !important">Start for free</a></section></div><footer class="Footer__StyledFooter-sc-u68pnv-0 eJChXt"><div class="undefined lg:w-[88vw] max-w-6xl lg:max-w-8xl m-auto px-6"><div class="py-24"><div class=" w-full h-[1px] bg-neutral-200"></div></div><div class="hidden md:block"><img src="../../../static/images/favicon-v4-black.png" alt="Quantumworks Lab logo" loading="lazy" height="36" width="36"/></div><section class="hidden md:grid footer-grid"></section><section class="social-media"></section><div class="text-center "><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">© Quantumworks Lab, Inc <br/>We enable breakthroughs</p><div class="flex flex-row flex-wrap justify-content-center gap-4 mt-4"><a href="https://docs.labelbox.com/page/terms-of-service" class=" " target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Terms of Service</p></a><div class="mx-1 border"></div><a href="https://docs.labelbox.com/page/privacy-notice" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Privacy Notice</p></a><div class="mx-1 border hidden sm:block"></div><a href="https://docs.labelbox.com/page/copyright-dispute-policy" target="_blank"><p class="font-normal text-base text-neutral-900 dark:text-neutral-50" style="font-family:&quot;IBM Plex Mono&quot;, sans-serif;font-size:14px;font-feature-settings:unset">Copyright Dispute Policy</p></a></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"6406883577b361003d3619d5","uuid":"4ab62fd8-b9da-474e-a833-f24758c230db","title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","slug":"using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data","html":"\u003cp\u003eOne of the biggest challenges that ML teams face is how difficult it is to select the right data to improve their ML models. From working with hundreds of teams, we’ve seen that ML teams possess a vast amount of unlabeled data, but lack a structured process for effectively finding and prioritizing \u003cem\u003especific data\u003c/em\u003e that can dramatically improve model performance. \u003c/p\u003e\u003cp\u003eThis manifests itself in the form of trying to find specific examples of an edge case where your model is struggling, or in the case of wanting to surface all occurrences of a rare data point that needs to be labeled in priority. In these cases, what is the best way for your team to efficiently surface this high-impact data?\u003c/p\u003e\u003ch2 id=\"what-will-you-learn-in-this-guide\"\u003eWhat will you learn in this guide? \u003c/h2\u003e\u003cp\u003eIn this guide, we'll show you how you can use a foundation model, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data. This technique will help your team quickly enrich your data with the latest advances in off-the-shelf models and embeddings.\u003c/p\u003e\u003cp\u003eBy the end of this guide, you’ll know how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGenerate custom embeddings with Hugging Face using a single line of code and upload your data to Quantumworks Lab in order to better explore and visualize your data.\u003c/li\u003e\u003cli\u003eBetter understand the distribution of your data and quickly find similar high-impact data.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab as a native similarity search engine, where you can leverage both off-the-shelf embeddings computed by Quantumworks Lab (for image, text, and documents) and upload your own custom embeddings to quickly find all instances of similar data.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"what-are-embeddings\"\u003eWhat are embeddings? \u003c/h2\u003e\u003cp\u003eIn machine learning, an embedding, or feature vector, is an array of numbers assigned to an asset by a neural net. Assets that have similar content will also have similar embeddings. \u003c/p\u003e\u003cp\u003eFor example, in a dataset comprising images of apples and oranges, an appropriate embedding used for image similarity will show that all the vectors corresponding to apples have similar values. The vectors for all images of oranges will also be grouped together. \u003c/p\u003e\u003cp\u003eIn other words, the neural network acts as a feature extractor: it extracts an embedding vector that contains rich information about the data.\u003c/p\u003e\u003ch3 id=\"off-the-shelf-embeddings-vs-custom-embeddings\"\u003eOff-the-shelf embeddings vs custom embeddings\u003c/h3\u003e\u003cp\u003eWhen you connect your data to Quantumworks Lab, we automatically compute \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io#supported-embeddings\"\u003eoff-the-shelf\u003c/a\u003e\u003cstrong\u003e \u003c/strong\u003eembeddings on your data – this includes CLIP embeddings for images and PDFs and All-mpnet-base-v2 embeddings for text. These off-the-shelf embeddings are a useful starting point for you to explore your data and conduct similarity searches. \u003c/p\u003e\u003cp\u003eHowever, in some cases where your data has unique attributes, you may want to use your own \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io#how-to-upload-custom-embeddings\"\u003ecustom embeddings\u003c/a\u003e to power your data selection. Quantumworks Lab allows you to upload up to 100 custom embeddings in addition to the off-the-shelf embeddings that are automatically computed. \u003c/p\u003e\u003cp\u003eYou can easily compare the results of these custom and provided off-the-shelf embeddings in Quantumworks Lab to discover the best embeddings to use for data selection.\u003c/p\u003e\u003ch2 id=\"how-to-upload-custom-embeddings\"\u003eHow to upload custom embeddings\u003c/h2\u003e\u003cp\u003eFirst, connect your data with Labelbox. You can integrate your cloud storage bucket with Quantumworks Lab via IAM delegated access:\u003c/p\u003e\u003cp\u003e\u003cem\u003eHow to set up a delegated access integration with Quantumworks Lab\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-amazons3-and-labelbox/?ref=labelbox-guides.ghost.io\"\u003eAmazon S3\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-gcp-storage-labelbox/?ref=labelbox-guides.ghost.io\"\u003eGCP Storage\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-azure-and-labelbox/?ref=labelbox-guides.ghost.io\"\u003eMicrosoft Azure Blob Storage\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve successfully uploaded your data, Quantumworks Lab will automatically compute off-the-shelf embeddings on your data.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1013\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can then compute and upload custom embeddings from Hugging Face on your data:\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/huggingface/huggingface.ipynb?ref=labelbox-guides.ghost.io\" class=\"kg-btn kg-btn-accent\"\u003eGoogle Colab notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003e\u003cem\u003eFollow along in this \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/huggingface/huggingface.ipynb?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eColab notebook \u003c/em\u003e\u003c/a\u003e\u003cem\u003ewith examples shown using ResNet-50 embeddings from Hugging Face.\u003c/em\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003eImport Quantumworks Lab into your notebook\u003c/li\u003e\u003c/ol\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# for Quantumworks Lab\n!pip3 install -q Quantumworks Lab[data]\nimport Quantumworks Lab as lb\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e2. Import the \u003ca href=\"https://github.com/Quantumworks Lab/advlib/tree/main/pylib/advlib?ref=labelbox-guides.ghost.io\"\u003eADVLib\u003c/a\u003e. This is a library built by Quantumworks Lab for you to upload custom embeddings.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# for custom embeddings in Quantumworks Lab\n!pip3 install -q 'git+https://github.com/Quantumworks Lab/advlib.git'\n#ndjson\n!pip3 install -q ndjson\nimport ndjson\nimport time\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e3. Select the data rows (images or text) in Quantumworks Lab on which you want to add custom embeddings.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# get images from a Quantumworks Lab dataset\ndataset = client.get_dataset(\"clemr01l42uil07y36qkq7ygn\")\ndrs = list(dataset.export_data_rows(timeout_seconds=9999))\ndata_row_ids = [dr.uid for dr in drs]\ndata_row_urls = [dr.row_data for dr in drs]\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e4. Use Hugging Face to generate your custom embeddings by loading a specific neural network (e.g. Resnet50).\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# import HuggingFace\n!pip3 install -q transformers\n!pip3 install -q timm\n\n# load a neural network from HuggingFace \nimport transformers\ntransformers.logging.set_verbosity(50)\nimport torch\nimport torch.nn.functional as F\nimport PIL, requests\nfrom tqdm import tqdm\n\n# get ResNet-50\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = transformers.ResNetModel.from_pretrained(\"microsoft/resnet-50\")\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e5. Generate custom embeddings by iterating over your image or text data. \u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: \u003c/em\u003eThis should take approximately ~2 minutes for 512 images. For the similarity search function to work in Quantumworks Lab, you must upload at least 1,000 embeddings. \u003c/p\u003e\u003cul\u003e\u003cli\u003eRetrieve your images/text and run model inference \u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# process images\nimg_hf = image_processor(imgs, return_tensors=\"pt\")\n\n# generate resnet embeddings, thanks to inference\nwith torch.no_grad():\n\tlast_layer = model(**img_hf, output_hidden_states=True).last_hidden_state\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\u003cli\u003eRemember to do global pooling on the last layer of your embedding to reduce dimensionality\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eresnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\nresnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e6. Create the payload to upload custom embeddings to Quantumworks Lab in the form of an NDJSON file.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# create the payload\npayload = []\nfor (dr_id,resnet_embedding) in zip(dr_ids, resnet_embeddings):\n\tpayload.append({\"id\": dr_id, \"vector\": resnet_embedding})\n\n# write to NDJson file\nwith open('payload.ndjson', 'w') as f:\n\tndjson.dump(payload, f)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e7. Pick an existing custom embedding or create a custom embedding.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# max pool to reduce dimensionality\nresnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\nresnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e8. Upload your payload of custom embeddings into Labelbox.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e!advtool embeddings import \u0026lt;EMB ID\u0026gt; ./payload.ndjson\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e9. Use Quantumworks Lab Catalog UI to start conducting similarity searches.\u003c/p\u003e\u003ch2 id=\"how-to-quickly-find-instances-of-similar-data\"\u003eHow to quickly find instances of similar data\u003c/h2\u003e\u003cp\u003eOnce you have uploaded your custom embeddings to Quantumworks Lab, you can focus on curating data in Catalog that will dramatically improve your model’s performance.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eIdentify an edge case or rare example image/text you want to use to find similar data.\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis can include examples of data on which your model might be struggling. For example, let’s say the model is incorrectly classifying images with sparse patches of grass as having been affected by a wildfire. \u003c/p\u003e\u003cp\u003eIn the example below, the model appears to struggle on recognizing images with ‘no wildfire'\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1530\" height=\"736\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png 1530w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e2. Surface all instances of similar data.\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-16-47--1--1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1775\" height=\"959\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/2023-03-06_17-16-47--1--1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/2023-03-06_17-16-47--1--1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/2023-03-06_17-16-47--1--1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-16-47--1--1.gif 1775w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can run \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003esimilarity searches\u003c/a\u003e to find all instances of similar data. A similarity search will automatically surface all similar data rows – you can select multiple data rows as anchors to continue to refine your similarity search. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Combine a similarity search with other search filters.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo filter the dataset even further, you can combine a similarity search with other \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003esearch filters\u003c/a\u003e. This includes filtering on metadata, media attribute, annotation, and more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1516\" height=\"824\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png 1516w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e4. Compare similarity search results.\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-23-53--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1772\" height=\"964\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/2023-03-06_17-23-53--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/2023-03-06_17-23-53--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/2023-03-06_17-23-53--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-23-53--1-.gif 1772w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can compare the results of the similarity search on different embeddings (across off-the-shelf and custom embeddings). This gives you an understanding of which embeddings are most effective towards providing your desired results. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5. Add all instances of similar data to a labeling project or save it as a slice.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve found additional examples of similar data rows on which your model is struggling, you can queue them to your labeling project in priority or save the filters as a slice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-29-38--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"954\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/2023-03-06_17-29-38--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/2023-03-06_17-29-38--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/2023-03-06_17-29-38--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-29-38--1-.gif 1774w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003eBy saving your similarity search as a slice, any new incoming data that matches the search criteria will automatically show up in the slice. This enables automatic data curation.\u003c/p\u003e\u003cp\u003eLearn more about other key ML workflows that you can perform using similarity search \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003ein this guide\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eLeveraging embeddings as a powerful similarity search technique can help you find specific data points within an ocean of data. With a similarity search, you can easily query and curate specific data that will dramatically improve your model performance. If you’re interested in learning more, please check out the additional resources below.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAdditional resources:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-kickstart-and-scale-your-data-labeling-efforts/?ref=labelbox-guides.ghost.io\"\u003eHow to kickstart and scale your data labeling efforts\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003eHow to filter and sort your data\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003eHow to find similar data in one click\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"6406883577b361003d3619d5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-3012--4-.png","featured":false,"visibility":"public","created_at":"2023-03-07T00:41:25.000+00:00","updated_at":"2024-02-02T18:28:44.000+00:00","published_at":"2023-03-08T19:31:53.000+00:00","custom_excerpt":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-imapctful-data","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/","excerpt":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","og_description":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","twitter_image":null,"twitter_title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","twitter_description":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","meta_title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","meta_description":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6375c52617f6c9003d7b0fd9","uuid":"5f8b37d4-a683-4515-a584-0116c30aa131","title":"An introduction to model metrics","slug":"intro-to-model-metrics","html":"\u003cp\u003eModel metrics help you evaluate the performance of a model and allows you to qualitatively compare different models. You can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. \u003c/p\u003e\u003ch3 id=\"why-does-model-accuracy-not-give-a-complete-picture-of-the-models-performance\"\u003eWhy does model accuracy not give a complete picture of the model's performance?\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"392\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAccuracy tells us the model's overall performance, but this metric doesn't provide all the information needed to accurately assess a model's performance. For a more holistic picture, we'll need to consider other metrics, based on the specific context that the model is used in.\u003c/p\u003e\u003cp\u003eGenerally, accuracy tends to be high in situations where a class has a very low probability of occurring, so a model can achieve high accuracy by simply predicting the most common class. For instance, the probability of finding cancer in computed topography scans or of finding swimming pools from satellite images of homes is low, so the model's accuracy can be high even if the model's ability to detect true positives is very poor.\u003c/p\u003e\u003ch3 id=\"precision\"\u003ePrecision\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1522\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png 1522w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003ePrecision metrics focus on consistency and agreement among labelers, which are primarily derived from Quantumworks Lab's built-in consensus capability. Quantumworks Lab uses and tests the effectiveness of over 15 similar metrics for a wide range of supported annotations.\u003c/p\u003e\u003cp\u003ePrecision is a valuable metric when the negative cost of a false positive is high. For example, in spam detection models, a false positive would cause a vital email to be hidden and marked as spam when in fact, it is non-spam. A false positive in this case would negatively impact the user experience for seeing essential and urgent emails on time.\u003c/p\u003e\u003cp\u003ePopular precision metrics include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eKrippendorff's Alpha is a popular metric used to assess the agreement among raters because it works well for two or more raters, can handle missing data, and supports nominal, ordinal, and ranking data types.\u003c/li\u003e\u003cli\u003eStandard deviation measures the dispersion of a set of ratings from their mean (average) value. In the context of AI data quality, it quantifies how much variation or spread exists in the ratings given by different AI trainers for the same item or task.\u003c/li\u003e\u003cli\u003ePercent agreement is a straightforward measure of inter-rater reliability that calculates the proportion of times different raters agree in their judgments. This is particularly useful in classification tasks (enums).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"\"\u003e\u003c/h3\u003e\u003ch3 id=\"recall\"\u003eRecall\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1616\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 1616w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eRecall is a helpful metric to use when the cost of false negative is high, and you want to minimize it. For example, in fraud detection models, a false negative would cause a fraudulent transaction to be successfully processed when it should have been flagged as fraudulent. This would obviously have a negative impact on the finances of the user. Recall is also helpful for most medical condition predictions, where you would minimize false negatives to increase recall.\u003c/p\u003e\u003ch3 id=\"f-1\"\u003eF-1\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-03-at-12.47.58-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"938\" height=\"316\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-03-at-12.47.58-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-03-at-12.47.58-PM.png 938w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn some cases with imbalanced data problems, both precision and recall are important – we can consider the F1 score as an evaluation metric. An F1 score helps in the detection of skewed datasets and rare classes. Generally, it is best to have high precision and recall so that your F1 score is high. \u003c/p\u003e\u003cp\u003eTo demonstrate how accuracy only provides a partial assessment of a model's performance, we can compare the model metrics of two models below: \u003c/p\u003e\u003cfigure class=\"kg-card kg-gallery-card kg-width-wide\"\u003e\u003cdiv class=\"kg-gallery-container\"\u003e\u003cdiv class=\"kg-gallery-row\"\u003e\u003cdiv class=\"kg-gallery-image\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png\" width=\"1142\" height=\"808\" loading=\"lazy\" alt=\"\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/div\u003e\u003cdiv class=\"kg-gallery-image\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png\" width=\"1142\" height=\"792\" loading=\"lazy\" alt=\"\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/figure\u003e\u003cp\u003eThe accuracy in model A is 73.65%, and model B is 83.69%. Based on accuracy alone, model B seems to perform better. However, if you compare their recall scores, then model A has a better recall of 87.38% vs model B's 82.97% recall. Taking this into account, model A performs better since the cost of a false negative is high. \u003c/p\u003e\u003ch3 id=\"what-do-model-metrics-look-like-in-labelbox\"\u003eWhat do model metrics look like in Quantumworks Lab? \u003c/h3\u003e\u003cp\u003eRather than having you manually compute and upload metrics, \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e auto-computes metrics such as precision, recall, F-1, confusion matrix, etc. on individual predictions for you. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/2022-12-19_22-08-24--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1773\" height=\"884\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/2022-12-19_22-08-24--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/2022-12-19_22-08-24--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/2022-12-19_22-08-24--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/2022-12-19_22-08-24--1-.gif 1773w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLabelbox Model will auto-generate metrics on individual predictions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eYou can simply upload your model predictions and ground truths to receive auto-generated metrics on model precision, recall, F1-score, TP/TN/FP/FN, and confusion matrix.\u003c/li\u003e\u003cli\u003eIf the auto-generated metrics aren’t sufficient for your use case, you can upload your own custom metrics as well.\u003c/li\u003e\u003cli\u003eVisualize, filter, sort, and drill into your metrics, confidence scores, predictions, and annotations. This allows you to easily surface mispredictions, mislabeled data, and allows you to quickly identify improvements to your training data.\u003c/li\u003e\u003cli\u003eYou can interact and click into the NxN confusion matrix or click into the IOU / Precision / Recall histograms to surface and view specific data rows in “gallery view.” For instance, you can understand where your model is not performing well, where your labels are off, or where your model is the least confident.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1031\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAuto-computed confusion matrix\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eUpload confidence scores alongside every prediction and tune the confidence and IOU thresholds in the Quantumworks Lab Model UI to see how model metrics change as the thresholds change.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eFind the distribution of annotations and predictions in every model run via histograms\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/histogram-1.jpeg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1980\" height=\"1337\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/histogram-1.jpeg 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/histogram-1.jpeg 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/histogram-1.jpeg 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/histogram-1.jpeg 1980w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse the prediction and annotation distribution histogram to surface important information about your model runs\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn addition, you can easily understand the distribution of your annotations and predictions via histograms. This makes curating datasets for labeling and analyzing model performance easier than ever. You can now use distributions to find the most predicted or least-predicted class and surface classes represented in training data, but rarely predicted by the model. \u003c/p\u003e\u003ch3 id=\"labelbox-leaderboards-a-new-era-of-evaluation-for-generative-ai\"\u003e\u003cstrong\u003eLabelbox leaderboards: A new era of evaluation for generative AI\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn the rapidly evolving landscape of artificial intelligence, traditional benchmarks are no longer sufficient to capture the full capabilities of AI models. As AI grows increasingly complex, challenges like data contamination, overfitting to public benchmarks, scalability issues, and the absence of standardized evaluation criteria necessitate a more advanced approach to model metrics. \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://labelbox.com/leaderboards/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e are the first to tackle these challenges by conducting structured evaluations on subjective AI model outputs using human experts and a scientific process that provides detailed feature-level metrics and multiple ratings. Leaderboards are available for \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eImage Generation\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/speech-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSpeech Generation\u003c/u\u003e\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/leaderboards/video-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eVideo Generation\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eBy combining expert human evaluations with our \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAlignerr\u003c/u\u003e\u003c/a\u003e workforce, reliable methodology, and continuous updates, Quantumworks Lab is redefining AI evaluation. Our approach complements traditional leaderboards by offering a  comprehensive and human-based assessment of AI models. \u003c/p\u003e\u003cp\u003eRead more about Quantumworks Lab leaderboards on our blog \u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cstrong\u003e \u003c/strong\u003eLabelbox offers a robust platform coupled with expert human evaluation services to efficiently generate and visualize these metrics, empowering you to make informed decisions and improve your models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can learn more about Quantumworks Lab auto-metrics in our \u003ca href=\"https://docs.labelbox.com/docs/evaluate-model-performance?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e or by reviewing \u003ca href=\"https://docs.labelbox.com/reference/upload-image-predictions?ref=labelbox-guides.ghost.io\"\u003ehow to upload image predictions in Quantumworks Lab\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your model evaluation, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e","comment_id":"6375c52617f6c9003d7b0fd9","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-2721--1-.png","featured":false,"visibility":"public","created_at":"2022-11-17T05:22:46.000+00:00","updated_at":"2024-11-27T03:03:43.000+00:00","published_at":"2023-03-03T17:53:39.000+00:00","custom_excerpt":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/intro-to-model-metrics","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/intro-to-model-metrics/","excerpt":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":"An introduction to model metrics","og_description":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-2721--1--2.png","twitter_title":"An introduction to model metrics","twitter_description":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","meta_title":"An introduction to model metrics","meta_description":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63ff856122e0a7003dacb795","uuid":"6f3a4aaf-3582-413a-857c-b7686a5bdb48","title":"Using Quantumworks Lab and Weights \u0026 Biases to fine tune your computer vision projects","slug":"using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects","html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\u003cp\u003eRepeatable, scalable, and diagnosable production artificial intelligence (AI), requires a sophisticated machine learning operations (MLOps) ecosystem. MLOps is the backbone of machine learning (ML) engineering, focused on streamlining development of AI/ML models and deploying, and monitoring those models in production. \u003c/p\u003e\u003cp\u003eIn this post, we’ll explore an MLOps architecture that uses both Quantumworks Lab and \u003ca href=\"https://wandb.ai/site?ref=labelbox-guides.ghost.io\"\u003eWeights \u0026amp; Biases\u003c/a\u003e to develop a computer vision model for a manufacturing focused use case. The goal of the model is to reduce defects on a production manufacturing line using automated visual inspection and the model requires human judgment to curate (supervise) the training data for model development.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/rx0-34Eba9ac5-xI9vIRIt8eZW7uCrVspPzHvmlLuXsF18qJCo-lWBZinRHHwm48UXGvnS3pdYgXOkEBqaeIkH5hk6epd9RfIHPtOEeVgZ1S2p8q_yMVOkOhXIypGKnl8zrwT4JpkzKXRW0IT70pBb6sDnYnl3pG1jDBCMPfbGFPbcnC9M6ai5gWBcwDhw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"465\" height=\"346\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample image of reducing defects on a production manufacturing line via the Quantumworks Lab interface.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eKey Components of AI Development\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eDeveloping a production caliber model is an extremely iterative process. Rarely, if ever, is a model trained once and ready for production. Typically, there are a series of experiments that need to be run across several combinations of datasets and models, followed by expert analysis to determine which one yields the best results. Each experiment must be meticulously tracked and analyzed as ML teams cycle through the development process. \u003c/p\u003e\u003cp\u003eIt’s not uncommon for data science teams to jump into model training without a clear understanding of their data. This can greatly increase the number of model training iterations needed to achieve the desired result, drive up development cost and increase the risks of achieving shipping ML-powered products on time.  \u003c/p\u003e\u003cp\u003eAn efficient, data centric model development approach is valuable, and consists of two key ML Ops components, a data-centric AI platform (Quantumworks Lab) and a model diagnostics platform (Weights \u0026amp; Biases).\u003c/p\u003e\u003ch3 id=\"improving-your-data-and-models\"\u003eImproving your data and models\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eWhen your model is being developed/trained or when it is in production, there will be indicators that your model is either drifting or performing poorly. This will be noticed in the different statistics your model outputs - confidence, accuracy, loss, a fluctuation in the number of detections or the classes that are being detected, all of which will be tracked and triggered in a model diagnostics platform such as Weights \u0026amp; Biases. \u003c/p\u003e\u003cp\u003eBut what about improving the actual dataset? Or understanding which specific data my model is performing poorly on and determining which new data will most improve my model? How do I quickly get that data labeled and back into model training? When transitioning from model diagnostics to data diagnostics, we recommend leveraging tools to quickly understand the data that your model is performing poorly on, find more data similar to that, and curate subsequent datasets in order to iterate through the model development process faster. \u003c/p\u003e\u003cp\u003eThe bottom line is that model diagnostic tools and AI platforms supplement each other and should be used together. Let's dive deeper into what this looks like in practice.\u003c/p\u003e\u003ch2 id=\"how-it-looks-in-practice\"\u003eHow it looks in practice\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eOnce Quantumworks Lab and Weights \u0026amp; Biases have been installed and are set up in your pipeline, what does it look like to embark on the model development journey for the first time (or subsequent times)? Let’s look at an example workflow, most of which can be performed in a seamless process by leveraging both the \u003ca href=\"https://labelbox-python.readthedocs.io/en/latest/index.html?ref=labelbox-guides.ghost.io\"\u003eLabelbox\u003c/a\u003e and \u003ca href=\"https://docs.wandb.ai/quickstart?ref=labelbox-guides.ghost.io\"\u003eW\u0026amp;B\u003c/a\u003e SDKs:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1300\" height=\"706\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 1300w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003col\u003e\u003cli\u003eBegin by first \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003econnecting your data source(s)\u003c/a\u003e in Quantumworks Lab Catalog for easy data visualization and curation.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/View-your-dataset-in-Catalog--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"995\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/View-your-dataset-in-Catalog--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/View-your-dataset-in-Catalog--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/View-your-dataset-in-Catalog--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/03/View-your-dataset-in-Catalog--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eQuickly search, explore and manage your data in one place. View raw data, metadata, and ground truth labels as shown in the mdetal cast defect example above.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e2. Label and review your data in Quantumworks Lab Annotate\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/MTDzpAqLb7Fnqe5_3GbURP2RlB56Dp4lGfprQf_difu_gRWtUxUWw3Ftkn-nhEjC25ZC66rxrZuoIc0WN7U66lBBVXL4z-p1iU1WjlFvAdiSHjOHygivHBtyVI5z2LN1IUpwBVF-FONNhYSfTUlPEvoY8RVKUp91u31dNmeDqANDdvIQShZCOBJpKgEcNQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"339\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEasily set up a consistent ontology for your defect detection use case.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/ezgif.com-optimize--6-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1491\" height=\"976\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/ezgif.com-optimize--6-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/ezgif.com-optimize--6-.gif 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/ezgif.com-optimize--6-.gif 1491w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLeverage auto-annotation to speed up your manual labeling tasks.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e3. Easily split data into \u003ca href=\"https://docs.labelbox.com/docs/curate-data-splits?ref=labelbox-guides.ghost.io\"\u003etrain, test, and validate\u003c/a\u003e sets in Quantumworks Lab Model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Model_Run_Comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"880\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Model_Run_Comparison--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Model_Run_Comparison--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/Model_Run_Comparison--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/03/Model_Run_Comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eYou can use SDK to customize how you want to split your data rows for a given model run.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003e4. Send your data to Weights \u0026amp; Biases from Quantumworks Lab for handling model training and hyperparameter search and tuning.\u003c/p\u003e\u003cp\u003e5. Weights \u0026amp; Biases will handle model training and do a hyperparameters search to run a series of model experiments to be compared with each other. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/StJ0uM0O4-VRXym-dfdNG4JF2ArLtF3qHyoed2yp-F6fYE7dB7ztXzVn5u6AtaPlwp-eSu9fO6VugHVYneYoyOmfZSxk8cCjoEq9YdIkjJ43g7vbZg5B7U49VpraoHWt-uY2WHbDyb4EgTxtxTylEnw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"327\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eTo run a Hyperparameter search using Weights \u0026amp; Biases Sweeps you can \u003c/span\u003e\u003ca href=\"https://colab.research.google.com/drive/13eKhoSbn13kHRRQexbXEfQPz8OwjjcEh?ref=labelbox-guides.ghost.io#scrollTo=EhOKTaHcg4Xl\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003erun this colab notebook here\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e6. Visualize your results in W\u0026amp;B dashboards to quickly diagnose your model performance and create reports in order to share with colleagues and streamline communication. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/ugAyjHH-6i8xJCCdujBBbmC8L7NKrivhJ2j7_UW6Fd5rMf0S7EFPRT8r_v9x9yVrqxRxjqFqYiaikYIrzhdOY_IOjpLt_Qf9hYzBCsMVVxS-SUbKbsSQSSJYKEvuGf1qI9trJwCy_Y4HLDuh6Hgc7JE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"313\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHere we can see the results of a search over 40 experiments to find the best training settings. \u003c/span\u003e\u003ca href=\"https://wandb.ai/morgan/industrial-images/sweeps/t88dzqwe?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eClick here to see this live Weights \u0026amp; Biases dashboard\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e7. Quickly visualize your results in Quantumworks Lab Model to quickly diagnose the data that is related to your model, rapidly query all of your data sources to find data similar to where your model performs poorly (edge cases), and seamlessly queue that data for labeling.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/01/Model-metrics-view--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1753\" height=\"959\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003e8. Rapidly repeat this iterative model development process until you have a production-worthy model, deploy that model, and continuously monitor\u003c/p\u003e\u003ch2 id=\"the-labelbox-ai-platform\"\u003eThe Quantumworks Lab AI platform\u003c/h2\u003e\u003cp\u003eOne of the biggest mistakes made when creating an AI ecosystem is not integrating a data-centric approach into your MLOps pipeline and serves as a foundational tool for data curation and fast iterative improvement through the model development lifecycle. \u003c/p\u003e\u003cp\u003eUsing Quantumworks Lab, ML Teams can easily connect to their sources (i.e. file systems, data lake platforms etc.) of unstructured data and quickly begin exploring and prioritizing their data curation efforts using Quantumworks Lab \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003eOnce data has been assessed and prioritized for curation \u0026amp; labeling, a combination of weak supervision and human supervised labeling campaigns are supported with the use of Quantumworks Lab \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003eFinally, the training data is curated into data splits for test, train and validation and can be easily integrated with a model diagnostics solution (Weights and Biases) to efficiently manage the first of many model training experiments and iterations. This integration is accomplished with the Quantumworks Lab \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eMode\u003c/a\u003el product.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/03/Screen-Shot-2023-03-01-at-8.26.03-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1568\" height=\"898\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe Quantumworks Lab platform allows teams to more quickly go from data curation, annotation and fast iterative improvement through the model development lifecycle.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"weights-biases%E2%80%99-model-diagnostics-platform\"\u003eWeights \u0026amp; Biases’ Model Diagnostics Platform\u003c/h2\u003e\u003cp\u003eA model diagnostics platform should offer quick iteration, easy collaboration, and a centralized system of record for ML teams. Because ML development is often closer to a science than traditional software engineering, experimentation is at its core, and tracking the progress of these experiments is critical.\u003c/p\u003e\u003cp\u003eUsing Weights \u0026amp; Biases’ experiment tracking, ML teams can easily log their work in a standardized way, knowing that they can return to the results of their experiments days, months, or years later. \u003c/p\u003e\u003cp\u003eEase of collaboration is critical for ML teams so that they can move quickly, and Weights \u0026amp; Biases’ Reports enables colleagues to share quick notes, training journals, and polished analysis to teammates and managers to unlock decision making and keep the team moving forward.\u003c/p\u003e\u003cp\u003eFinally, knowing that your multiple model checkpoints are securely stored gives you the full picture of which model is best to select for deployment and to send to the W\u0026amp;B Model Registry, where your MLOps or DevOps team can pull it down for deployment.\u003cbr\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/4PUY0WUteKeig2EtGDEgFP_VVBSQSWkEV3ILX0b_rPUIFMDC2LsEdajU6cykdxVjjinMfRd9dEzEijMLgj0MkaS78BN4g_-yiPwbh3deCfVAWW7IEML2h_dI-YUUwCfUYG4LwErUmaIouAyL-ZqLR0TDH_l_cjnu4_ZhaXNiEzQ8Ds5We-caYw7dQdwc8A\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"144\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003eAI model development is an iterative process, which can be tedious and time consuming without a reliable MLOps pipeline. An efficient model development environment can be broken into two parts: a data-centric workflow for data ingestion, exploration, understanding, and preparation, and a model diagnostics platform for model training, tracking, evaluation, and versioning. Quantumworks Lab and Weights \u0026amp; Biases are leading platforms that were designed to complement this iterative process in mind, and when combined, can scale and help AI development teams build better models, faster than ever before.\u003c/p\u003e\u003cp\u003eWe'll be releasing more technical guides in the coming weeks on how to best utilize Quantumworks Lab and Weights \u0026amp; Biases so stay tuned! \u003cbr\u003e\u003c/p\u003e","comment_id":"63ff856122e0a7003dacb795","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-8.57.36-AM.png","featured":false,"visibility":"public","created_at":"2023-03-01T17:03:29.000+00:00","updated_at":"2023-12-18T22:47:42.000+00:00","published_at":"2023-03-01T18:56:05.000+00:00","custom_excerpt":"Learn how you can use Quantumworks Lab and Weights \u0026 Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},"url":"https://labelbox-guides.ghost.io/using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/","excerpt":"Learn how you can use Quantumworks Lab and Weights \u0026 Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Using Quantumworks Lab and Weights \u0026 Biases | Quantumworks Lab","meta_description":"Use Quantumworks Lab and Weights \u0026 Biases to build better computer vision models with step-by-step workflows of data curation, annotation, \u0026 diagnostics","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6348256660b562003d250b50","uuid":"f78ce8ff-29aa-4034-9953-1b6b50823eea","title":"How to get started in Quantumworks Lab Model: Train, evaluate, and improve your ML models","slug":"how-to-train-evaluate-and-improve-your-ml-models","html":"\u003cp\u003eThe quality of your data will dictate your model’s performance. ML teams have historically had to rely on manual methods of curating data and debugging model errors. For teams who are looking to go through fast, data-centric iterations, this is not an ideal way to quickly scale and reach production AI. \u003c/p\u003e\u003cp\u003eIn order to ship performant models, you need to be able to quickly train models with collaborative data-centric tools. Quantumworks Lab Model can help you ship better models faster by leveraging collaborative tools to curate, debug, diagnose, and optimize your machine learning data and models.\u003c/p\u003e\u003cp\u003eThis guide will walk you through how to get started with \u003ca href=\"https://app.labelbox.com/models?ref=labelbox-guides.ghost.io\"\u003eModel in the Quantumworks Lab platform\u003c/a\u003e. We’ll walk through a COCO object detection example and show you how to get onboarded in Model with your first project, model, and model run. \u003c/p\u003e\u003cp\u003eBy the end of the tutorial, you will have learned how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUpload and version a dataset in a Quantumworks Lab Model run\u003c/li\u003e\u003cli\u003eExport and train an object detection model from a pre-trained model\u003c/li\u003e\u003cli\u003eVisualize model predictions against ground truth annotations\u003c/li\u003e\u003cli\u003eView auto-generated metrics (F1, precision, recall, IOU, confusion matrix, etc.) and the distribution of annotations and predictions\u003c/li\u003e\u003cli\u003eEvaluate model performance and improve your model and data with error analysis and active learning\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eKey definitions in Quantumworks Lab Model:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eA \u003ca href=\"https://docs.labelbox.com/docs/create-a-model?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003emodel\u003c/em\u003e\u003c/a\u003e\u0026nbsp;is a large language model (LLM) integrated into Quantumworks Lab Model or your custom configuration specified by an ontology of data.\u003c/li\u003e\u003cli\u003eAn \u003ca href=\"https://docs.labelbox.com/docs/experiments?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cem\u003eexperiment\u003c/em\u003e \u003c/u\u003e\u003c/a\u003eis a directory where you can create, manage, and compare a set of \u003cem\u003emodel runs\u003c/em\u003e related to the same machine learning task (e.g object detection on COCO). \u003c/li\u003e\u003cli\u003eA \u003ca href=\"https://docs.labelbox.com/docs/create-a-model-run?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003emodel run\u003c/em\u003e\u003c/a\u003e is a model training experiment within a model directory. Each model run provides a versioned data snapshot of the data rows, annotations, and training/validation/test splits for that model run\u003c/li\u003e\u003cli\u003eYou can specify \u003ca href=\"https://docs.labelbox.com/docs/add-model-run-config-to-track-hyperparameters?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003emodel run configuratio\u003c/em\u003ens\u003c/a\u003e to create, version, and track your hyperparameters and any training-related configurations for a model run\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"part-1-seed-a-coco-dataset-and-create-a-project-and-model-run\"\u003ePart 1: Seed a COCO dataset and create a project and model run\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/174ebhOCA8XeQ-WavIDZw3U1EeUHh8MaV?ref=labelbox-guides.ghost.io#scrollTo=UxrfeTs6fint\" class=\"kg-btn kg-btn-accent\"\u003ePart 1: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/74ypf3u3ba\" title=\"How to get started in Quantumworks Lab Model (Part 1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"640\" height=\"360\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eFor this onboarding tutorial, we’ll be working with a COCO dataset example. You can follow along by using the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/174ebhOCA8XeQ-WavIDZw3U1EeUHh8MaV?ref=labelbox-guides.ghost.io#scrollTo=UxrfeTs6fint\"\u003e\u003cem\u003eGoogle Colab Notebook\u003c/em\u003e\u003c/a\u003e\u003cem\u003e and the accompanying video tutorials, but feel free to also bring your own dataset into Quantumworks Lab Model to create a Quantumworks Lab project, model and model run for your use case.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBefore you begin this tutorial, you’ll need to sign into your Quantumworks Lab account or \u003c/strong\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003ecreate a free account\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"step-1-upload-the-coco-dataset-to-labelbox\"\u003eStep 1: Upload the COCO dataset to Quantumworks Lab\u003c/h3\u003e\u003cul\u003e\u003cli\u003eUse the provided helper functions in the Colab Notebook to upload the provided COCO dataset to Quantumworks Lab\u003c/li\u003e\u003cli\u003eOnce you’ve successfully run the helper function, you should be able to see the COCO dataset appear in \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1068\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-2-upload-the-coco-dataset%E2%80%99s-bounding-box-labels-to-labelbox\"\u003eStep 2: Upload the COCO dataset’s bounding box labels to Quantumworks Lab\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWhile labels are typically created by an internal or external team of labelers, we are going to use an already labeled public dataset for this tutorial\u003c/li\u003e\u003cli\u003eUse the provided helper function to directly upload the dataset’s labels to Quantumworks Lab\u003c/li\u003e\u003cli\u003eSimilarly to the step above, you should be able to see your COCO data rows and the appropriate labels in \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-populate-a-model-run-version-model-run-data-and-model-hyperparameters\"\u003eStep 3: Populate a model run: Version model run data and model hyperparameters\u003c/h3\u003e\u003cul\u003e\u003cli\u003eNow, we’re ready to populate the above data (dataset + labels) in a model run\u003c/li\u003e\u003cli\u003eFollow the steps in the Colab Notebook to configure the ontology (containing classes of objects that we want to detect in the image dataset)\u003c/li\u003e\u003cli\u003eYou can also specify your model run’s hyperparameters\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-view-your-populated-model-run-in-the-model-tab\"\u003eStep 4: View your populated model run in the Model tab\u003c/h3\u003e\u003cul\u003e\u003cli\u003eOnce you’ve completed these steps, you should be able to see your versioned data in the Model tab\u003c/li\u003e\u003cli\u003eYou can view your data rows, labels, and inspect your model run configuration (hyperparameters) in the Quantumworks Lab app \u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1052\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eNext, we’re going to use the model run data that we’ve uploaded in Quantumworks Lab to train a model.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"part-2-fine-tune-a-faster-r-cnn-model-upload-predictions-to-model-and-run-error-analysis\"\u003ePart 2: Fine-tune a Faster R-CNN model, upload predictions to Model, and run error analysis\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1mSl3RU8tz2NMQUbE5RZC9rjsmXCTjgjT?ref=labelbox-guides.ghost.io#scrollTo=kvrRY4pJk1ul\" class=\"kg-btn kg-btn-accent\"\u003ePart 2: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/yxk8wmgpmg\" title=\"How to get started in Quantumworks Lab Model (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-1-train-an-object-detection-model-from-a-pre-trained-faster-r-cnn-model\"\u003eStep 1: Train an object detection model from a pre-trained Faster R-CNN model\u003c/h3\u003e\u003cul\u003e\u003cli\u003eExport the labels from the model run you created – the labels will be versioned by the model run\u003c/li\u003e\u003cli\u003eUse the provided helper function to transform the labels into a format that the Pytorch model can accept\u003c/li\u003e\u003cli\u003eWe’ve provided a Faster R-CNN model for fine-tuning – this replaces the last box prediction head with a new layer so that it can predict the model ontology and classes that we are concerned with\u003c/li\u003e\u003cli\u003eTrain the Faster R-CNN model for 1 epoch (4 minutes) – after this, you’ll have a model that performs decently well for this use case\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-generate-predictions-with-the-trained-model\"\u003eStep 2: Generate predictions with the trained model \u003c/h3\u003e\u003cul\u003e\u003cli\u003eGenerating predictions will help us visualize model performance in Quantumworks Lab Model and can help identify model errors\u003c/li\u003e\u003cli\u003eAssemble predictions and results of the model into a format that can be ingested back into Quantumworks Lab\u003c/li\u003e\u003cli\u003eTurn predictions into NumPy arrays and create annotation payloads for each object \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWant to upload a \u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/bring-your-own-models-to-labelbox-with-new-custom-model-integration/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003ecustom model\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e instead?\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAt Quantumworks Lab, we want to ensure you are able to upload your own models quickly and easily, with no manual onboarding. With just a few clicks, you can seamlessly integrate your own custom models into our platform to enhance prediction, accelerate model evaluation, and improve data enrichment.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdlv2idAhfmVrY-LfZhsvYcU2L7e9a5gJ-9XLJW_f98QwSSm6SlOvKk36PC-8parhUQmEc02JMpOhffaz9IjfDYGNOzfzGZNsHb9qCRDqi3r4_KUoVr2KvLV6Tgo8PHbHL1DtUM?key=d5f31BHEZOcEnpQg1zrr_5zJ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"339\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\u0026nbsp;Import a Custom Model from the Model homepage\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"step-3-upload-predictions-back-into-your-labelbox-model-run\"\u003eStep 3: Upload predictions back into your Quantumworks Lab model run \u003c/h3\u003e\u003cul\u003e\u003cli\u003eUpload the model predictions to the Quantumworks Lab app in order to visualize how the model is performing\u003c/li\u003e\u003cli\u003eOther than confidence scores, you don’t have to worry about computing metrics. Quantumworks Lab Model will auto-compute model metrics such as F1 scores, precision, IOU,  confusion matrix, false positives/false negatives/true positives/true negatives, etc\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-visualize-model-predictions-and-auto-generated-metrics-in-labelbox-model\"\u003eStep 4: Visualize model predictions and auto-generated metrics in Quantumworks Lab Model \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/GIF--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/GIF--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/GIF--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/GIF--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/GIF--1-.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAfter you’ve completed the steps above, you should now be able to go to the Models tab to visualize model predictions and model metrics\u003c/li\u003e\u003cli\u003eClick on “Metrics View” to inspect model metrics and the model’s performance on each class\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can run \u003ca href=\"https://docs.labelbox.com/docs/find-model-errors?ref=labelbox-guides.ghost.io\"\u003eerror analysis\u003c/a\u003e on specific classes or data rows of interest:\u003c/p\u003e\u003cul\u003e\u003cli\u003eStart by inspecting how the model is doing on each class\u003c/li\u003e\u003cli\u003eIf you notice the model is struggling on a specific class, you can click into the histogram to view data rows on which the model is struggling. You can refine the search query to further drill into and inspect model performance\u003c/li\u003e\u003cli\u003eLeverage “detailed view” to better inspect disagreements and find patterns of model failures on images\u003c/li\u003e\u003cli\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"learn-more\"\u003eLearn More \u003c/h2\u003e\u003cp\u003eBy following this step-by-step tutorial, you’ve now successfully created a model, a model run, and have uploaded model predictions into Quantumworks Lab for further analysis. \u003c/p\u003e\u003cp\u003eYou can also refer to the below guides for a more in-depth walkthrough on how to improve data selection and model performance:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eHow to search, surface, and prioritize data within a project\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/active-learning?ref=labelbox-guides.ghost.io\"\u003eHow to prioritize high-value data\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-label-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix label errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-model-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix model errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003eHow to find similar data in one click\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’re happy to help answer any questions. Reach out to us anytime on our \u003ca href=\"https://labelbox.com/sales?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e page. \u003c/p\u003e","comment_id":"6348256660b562003d250b50","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2966--1-.png","featured":false,"visibility":"public","created_at":"2022-10-13T14:49:10.000+00:00","updated_at":"2024-11-27T02:19:06.000+00:00","published_at":"2023-01-25T23:49:15.000+00:00","custom_excerpt":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-train-evaluate-and-improve-your-ML-models","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-train-evaluate-and-improve-your-ml-models/","excerpt":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":"How to get started in Quantumworks Lab Model: train, evaluate, and improve your ML models","og_description":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","twitter_image":null,"twitter_title":"How to get started in Quantumworks Lab Model: train, evaluate, and improve your ML models","twitter_description":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","meta_title":"How to get started in Quantumworks Lab Mode: Train evaluate, and improve your ML models","meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63bdef75be228d003d1e9ce3","uuid":"abe288c8-c70e-407b-b727-da6a90605cb3","title":"How to kickstart and scale your data labeling efforts","slug":"how-to-kickstart-and-scale-your-data-labeling-efforts","html":"\u003cp\u003eYour model performance will only ever be as strong as the quality of your training data. A common bottleneck for many AI teams is how to obtain vast amounts of high-quality training data for their use case at scale in the most time efficient and cost-effective way possible. \u003c/p\u003e\u003cp\u003eWhen it comes to deciding how to label your data, you might consider one of the following options:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompletely outsource this task to a labeling service — these external teams often receive training on the specific labeling tasks required and quickly proceed to label large datasets\u003c/li\u003e\u003cli\u003eLeverage AI-powered solutions from a labeling platform to speed up the labeling process\u003c/li\u003e\u003cli\u003eManage homegrown or open source tools and rely on your own internal team of labelers to label your dataset\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOn the surface, the above options may seem sufficient, but they have their disadvantages. The labeling process itself is opaque, so by relying on completely outsourcing your task, you risk having little insight into metrics such as labeling quality, throughput, and efficiency. If you’re working with sensitive data, outsourcing labeling can be a greater challenge regarding security concerns. Many service providers also don’t provide access to a labeling platform, hindering AI teams from experimenting within the labeling process and taking advantage of techniques like automation and active learning. In addition, utilizing in-house or open source tools can quickly become hard to manage, resulting in an exorbitant amount of time and resources in maintenance and scale. This can lead to delays from quality management and labeling iteration, poor ontology creation and management, miscommunication between stakeholders, SMEs, labelers, and more.\u003c/p\u003e\u003cp\u003eTo appropriately scale and maintain the quality required for your production use case, you’ll need to leverage a \u003ca href=\"https://labelbox.com/learn/library/complete-guide-data-engines-for-ai/?ref=labelbox-guides.ghost.io\"\u003edata engine\u003c/a\u003e. An effective data engine combines data management, quality and performance monitoring, and advanced techniques and labeling services to help improve the speed and efficiency of your labeling operations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Frame_2963.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1368\" height=\"1010\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Frame_2963.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Frame_2963.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/Frame_2963.png 1368w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eLabelbox provides \u003c/em\u003e\u003c/i\u003e\u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003elabeling services\u003c/em\u003e\u003c/i\u003e\u003c/a\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e and AI expertise, on-demand. You can outsource labeling work and partner with ML experts to fine-tune the above workflows to ensure clarity on tasks and achieve your quality targets.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRegardless of your use case, if you’re working with an external labeling team or partnering with a service provider, you’ll want to make sure that you’re set up for success. Carefully outlining your labeling project and task, defining your project’s success criteria, measuring and maintaining quality, scaling your labeling operations, and evaluating your project’s results are all key steps to ensuring that you are producing high-quality training data.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-define-a-task-for-your-labeling-project\"\u003eStep 1: Define a task for your labeling project\u003c/h2\u003e\u003col\u003e\u003cli\u003eAlign on the key components of your labeling task so that it can be effectively communicated to your labeling team\u003c/li\u003e\u003cli\u003eDetermine the \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#supported-data-types\"\u003edata type\u003c/a\u003e or industry vertical — this allows your Labeling Team Manager to appropriately match you with a team of labelers well-suited for your task\u003c/li\u003e\u003cli\u003eOutline any specific labeling or compliance requirements for this task — this will often require a specialized workforce that is trained in your specific industry or task\u003c/li\u003e\u003cli\u003eDefine your data volume and agree on a project timeline — this will help allocate resources for your project and set expectations upon project start\u003c/li\u003e\u003cli\u003eCreate \u003ca href=\"https://labelbox.com/guides/how-to-create-and-manage-ontologies/?ref=labelbox-guides.ghost.io\"\u003ean ontology\u003c/a\u003e with the goals of proper labeling, efficiency, and reusability in mind\u003c/li\u003e\u003cli\u003eProvide labeling instructions for the labeling team to use — instructions should provide context to the task, explain what the task entails, describe the labeling steps, and be treated as a “living document”\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003eHow to define a task for your data labeling project\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"step-2-define-your-labeling-project%E2%80%99s-success-criteria\"\u003e\u003cbr\u003eStep 2: Define your labeling project’s success criteria \u003c/h2\u003e\u003col\u003e\u003cli\u003eUnderstand your project’s timeline and scope — this includes any deadlines, projected data volume, and the  average time per label\u003c/li\u003e\u003cli\u003eSelect the grading requirements for your project — this will help determine what is a “good” or “bad” quality label.\u003c/li\u003e\u003cli\u003eDecide if you want to implement a \u003ca href=\"https://labelbox.com/guides/how-to-define-your-data-labeling-projects-success-criteria/?ref=labelbox-guides.ghost.io#slas-quality-speed-throughput\"\u003equality SLA\u003c/a\u003e with your labeling team — this is a bidirectional commitment with your labeling team that is built on throughput and quality calculations \u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide,\u003cstrong\u003e \u003c/strong\u003e\u003ca href=\"https://labelbox.com/guides/how-to-define-your-data-labeling-projects-success-criteria/?ref=labelbox-guides.ghost.io\"\u003eHow to define your data labeling project’s success criteria.\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"step-3-create-a-quality-strategy-for-your-labeling-project\"\u003eStep 3: Create a quality strategy for your labeling project\u003c/h2\u003e\u003cp\u003eAfter defining and setting expectations on how quality is defined, you'll want to spend some time developing a quality strategy.\u003c/p\u003e\u003col\u003e\u003cli\u003eMake sure you have quality monitoring tools in place — Quantumworks Lab’s \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003ebenchmark\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003econsensus\u003c/a\u003e tools help measure labeling accuracy and labeling consistency so you can gauge your project’s labeling efficiency\u003c/li\u003e\u003cli\u003eIncorporate manual review and feedback throughout your projects’ duration, as labeling data is a collaborative process. Quantumworks Lab’s \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003eworkflow feature\u003c/a\u003e allows you to set up customized review steps based on your quality strategy\u003c/li\u003e\u003cli\u003eIf you have a quality SLA, regularly monitor and review your data to determine whether the SLA has been met\u003c/li\u003e\u003cli\u003eEnsure that there is an open two-way communication channel between your labeling team and key stakeholders — this can resemble Quantumworks Lab platform features such as \u003ca href=\"https://docs.labelbox.com/docs/issues-comments?ref=labelbox-guides.ghost.io\"\u003eissues \u0026amp; comments\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/boost-workforce-notifications?ref=labelbox-guides.ghost.io\"\u003eupdates\u003c/a\u003e, Slack, Google Docs, etc.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-create-a-quality-strategy-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003eHow to create a quality strategy for your data labeling project\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"step-4-scale-your-labeling-operations-while-maintaining-quality\"\u003e\u003cbr\u003e\u003cstrong\u003eStep 4:\u003c/strong\u003e \u003cstrong\u003eScale your labeling operations while maintaining quality\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eOnce a desired quality strategy has been implemented, a key question becomes how to maintain consistency and quality as team size or data volume grows.\u003c/p\u003e\u003col\u003e\u003cli\u003eManage your labeling workflow by making use of iteration with small batches and an initial calibration phase\u003c/li\u003e\u003cli\u003eThe \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io#calibration-phase\"\u003ecalibration phase\u003c/a\u003e is often a smaller subset of your task — it is used to train the labeling team on labeling instructions, the ontology, and to help them become familiar with the data in the project\u003c/li\u003e\u003cli\u003eProvide feedback and work with your labeling team to iterate on the data until the desired quality threshold is reached\u003c/li\u003e\u003cli\u003eMonitor overall quality and speed of your labeling operations as you enter the \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io#production-phase\"\u003eproduction phase\u003c/a\u003e of your project\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003eHow to scale up your labeling operations while maintaining quality\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"step-5-evaluate-and-optimize-your-labeling-project%E2%80%99s-results\"\u003eStep 5: Evaluate and optimize your labeling project’s results\u003c/h2\u003e\u003cp\u003eAfter a task is completed and you have entered the production phase, it’s important to evaluate and consider factors that can guide you toward greater optimization of future batches.\u003c/p\u003e\u003cp\u003eAfter a task is completed and you have entered the production phase, it’s important to evaluate and consider factors that can guide you toward greater optimization of future batches. \u003c/p\u003e\u003col\u003e\u003cli\u003eCrowdsource feedback from your labelers — understanding their challenges with the given task can help clarify future labeling instructions, discover edge cases, and suggest ways to improve efficiency\u003c/li\u003e\u003cli\u003eReview project results and labeler performance against your existing ontology and labeling instructions — see if project results reveal an opportunity to improve ontology structure or guidance\u003c/li\u003e\u003cli\u003eSave labeling time and cost by leveraging active learning techniques to prioritize high-impact data — Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e can help you quickly identify label and model errors, find all instances of similar data to edge cases or mislabeled data rows, and more\u003c/li\u003e\u003cli\u003eDetermine how well your project’s results aligned with your quality strategy outlined in step 3 — see if you notice areas for improvement or if further customization to improve review efficiency is needed with \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eworkflows\u003c/a\u003e\u003c/li\u003e\u003cli\u003eEvaluate whether the labeling team size and skillset was appropriate for your use case and the desired production capability \u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-and-optimize-your-data-labeling-projects-results/?ref=labelbox-guides.ghost.io\"\u003eHow to evaluate and optimize your data labeling project’s results\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003ePowered by Quantumworks Lab’s data engine, experience the next level of data labeling service with direct access to curated data labeling teams for your projects in any expert domain or popular languages. Set new standards in quality and throughput at half the cost.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003eContact us\u003c/a\u003e today to access the best data labeling services with specialized labeling teams that match your use case. You can also sign up and \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started with Quantumworks Lab for free\u003c/a\u003e. \u003c/p\u003e","comment_id":"63bdef75be228d003d1e9ce3","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2973--1-.png","featured":false,"visibility":"public","created_at":"2023-01-10T23:06:29.000+00:00","updated_at":"2024-09-12T23:47:29.000+00:00","published_at":"2023-01-12T21:31:12.000+00:00","custom_excerpt":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-kickstart-and-scale-your-data-labeling-efforts","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-kickstart-and-scale-your-data-labeling-efforts/","excerpt":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2973--1--3.png","og_title":"How to kickstart and scale your data labeling efforts","og_description":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2973--1--1.png","twitter_title":"How to kickstart and scale your data labeling efforts","twitter_description":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","meta_title":"How to kickstart and scale your data labeling efforts","meta_description":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b5df77f94604003d092822","uuid":"2b424b77-c04e-4b37-8b3e-e9343aa9c81e","title":"How to evaluate and optimize your data labeling project's results","slug":"how-to-evaluate-and-optimize-your-data-labeling-projects-results","html":"\u003cp\u003eLeading machine learning teams establish high quality labeling workflows by starting with \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003esmall iterative batches\u003c/a\u003e to develop an effective feedback loop with labeling teams early on in the process. This approach allows ML teams to quickly identify issues and make necessary changes to their labeling instructions and ontologies, enabling labeling teams to become more proficient with the task before large volumes of data are labeled. Following this waterfall approach, teams are able to generate higher quality labeled data quickly and at scale. \u003c/p\u003e\u003cp\u003eAfter a task has been completed and an ML team has entered into the production phase, it's important to evaluate and consider several factors that can guide you toward greater optimization of future batches. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"what-should-ml-teams-consider-once-a-labeling-project-is-complete\"\u003eWhat should ML teams consider once a labeling project is complete? \u003c/h2\u003e\u003ch3 id=\"feedback-from-labelers\"\u003eFeedback from labelers\u003c/h3\u003e\u003cp\u003eAfter project completion, you should consider sourcing feedback from your labelers. Machine learning teams often benefit from asking labelers to share their thoughts on the given labeling task to better understand what was challenging, to discover edge cases, and receive suggestions on how to improve efficiency. \u003c/p\u003e\u003cp\u003eA great way to crowdsource this feedback is through Quantumworks Lab's \u003ca href=\"https://docs.labelbox.com/docs/issues-comments?ref=labelbox-guides.ghost.io\"\u003eissues \u0026amp; comments\u003c/a\u003e feature that can be used on individual data rows. This allows the labeling team and your ML team to collaborate and provide feedback on specific examples or assets. \u003ca href=\"https://docs.labelbox.com/docs/boost-workforce-notifications?ref=labelbox-guides.ghost.io\"\u003eUpdates\u003c/a\u003e within a project can also be used to provide more holistic feedback on the project – enabling three-way communication between you, the Quantumworks Lab Boost admin, and the labeling workforce. Lastly, you can also rely on shared Google Docs and team syncs to collect feedback from the labeling team. \u003c/p\u003e\u003ch3 id=\"labeling-instructions-and-ontology-changes\"\u003eLabeling instructions and ontology changes\u003c/h3\u003e\u003cp\u003eAfter a project has been completed, you can review project results and labeler performance to pinpoint areas for improving your labeling instructions or ontology. \u003c/p\u003e\u003cp\u003eQuestions that you can consider during review include: \u003c/p\u003e\u003cul\u003e\u003cli\u003eDo the results of the project show any areas of labeling uncertainty that can be improved by editing the task's instructions?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003cem\u003e \u003c/em\u003e\u003c/strong\u003eCreating category definitions that are mutually exclusive can improve labeling team alignment and reduce the chance of confusion between object types. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003cem\u003e \u003c/em\u003e\u003c/strong\u003eAdding numbered steps can be a great way to organize complex labeling tasks. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003cem\u003e \u003c/em\u003e\u003c/strong\u003eIncluding decision trees and rule of thumb guidance can be helpful for subjective tasks that require critical thinking. \u003c/p\u003e\u003cul\u003e\u003cli\u003eDo project results reveal opportunities for improving ontology structure? \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003c/strong\u003e Think about if the ontology is organized in the most efficient manner. For instance, can the labeling time can be improved using nested classifications vs having a list of single objects?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip: \u003c/em\u003e\u003c/strong\u003eReview whether the ontology includes too many objects or too few objects to achieve project goals.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003c/strong\u003e If warranted, share the \"why\" behind your ontology requirements or provide additional project context to help labelers better contextualize tasks. \u003c/p\u003e\u003ch3 id=\"improve-your-data-selection\"\u003eImprove your data selection \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/catalog-06--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1344\" height=\"1024\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/catalog-06--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/catalog-06--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/catalog-06--1-.png 1344w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAfter the successful completion of a labeling project, it can be tempting to proceed by creating projects or batches of even larger datasets to label. However, to make sure that you're training your model on the most impactful examples, you can leverage Quantumworks Lab's \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e to curate and prioritize future batches. \u003c/p\u003e\u003cp\u003eWith Catalog and Model, you can quickly identify label and model errors, find all instances of similar data to edge cases or mislabeled data rows and send them to Annotate in order to retrain your model on specific slices of data. Smartly selecting high value data to label in priority is key to maximizing labeling efficiency and reduce project costs. \u003c/p\u003e\u003cp\u003eRefer to the below guides and resources to learn more about how to improve data selection with Quantumworks Lab: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eHow to search, surface, and prioritize data within a project\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/active-learning?ref=labelbox-guides.ghost.io\"\u003eHow to prioritize high-value data\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-label-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix label errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-model-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix model errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003eHow to find similar data in one click\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"review-your-quality-strategy\"\u003eReview your quality strategy\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/workflow--3-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1360\" height=\"1024\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/workflow--3-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/workflow--3-.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/workflow--3-.png 1360w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOne key area to review after the completion of a project is how well your \u003ca href=\"http://labelbox.com/guides/how-to-create-a-quality-strategy-for-your-data-labeling-project?ref=labelbox-guides.ghost.io\"\u003equality assurance strategy\u003c/a\u003e performed. ML teams will benefit from asking the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003econsensus\u003c/a\u003e was used on the project, were there enough votes to provide sufficient insights?\u003c/li\u003e\u003cli\u003eWas the number of \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003ebenchmarks\u003c/a\u003e used or was the percentage of consensus coverage applied sufficient to assess performance across the dataset?\u003c/li\u003e\u003cli\u003eWere the benchmark or consensus agreement results as expected, or lower than anticipated?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003c/strong\u003e If consensus scores are consistently high or do not provide additional valuable insights, consider lowering the number of votes, coverage percentage, or removing altogether to reduce labeling time.\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf using SLAs, how well were expectations and requirements met?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYour team should also assess how well the labeling review process was. If you notice areas for improvement or the need for customization to improve review efficiency, you can customize your review step with \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eworkflows\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eTo learn more about workflows, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process?ref=labelbox-guides.ghost.io\"\u003eHow to customize your annotation review process\u003c/a\u003e\u003c/p\u003e\u003ch3 id=\"labeling-team-size-and-skillset\"\u003eLabeling team size and skillset\u003c/h3\u003e\u003cp\u003eEvaluating the size and makeup of the labeling team is important at every phase of your labeling operations. At the end of a project, it's recommended that you review whether the labeling team size was too small to meet the desired production capability or too large for the required volume. Additionally, it is key for teams to consider whether future batches of data will require labelers with specialized training (such as having industry-specific or language experience). \u003c/p\u003e\u003cp\u003eKeeping communication open with the labeling team is also crucial during these considerations. This is especially important when working with external labeling teams, who often benefit from having advance notice of any new batches or changing requirements so that they can effectively allocate or maintain their resources. \u003c/p\u003e\u003cp\u003eClear and timely communication of future project needs, such as the anticipated readiness of the next project or batch, the expected size of the dataset, changes to the instructions or labeling requirements, and ideal completion dates, should be communicated to help ensure a smooth ramp up on the next project. \u003c/p\u003e\u003cp\u003eYou can easily access data labeling services with specialized expertise that are fit for your specific use case through \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e. Collaborate with the workforce in real-time to monitor high-quality data,, all while managing and keeping human labeling costs to a minimum using AI-assisted tools and automation techniques. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003eContact our Boost team\u003c/a\u003e to get paired with a specialized workforce team or learn more about how one of our customers, NASA's Jet Propulsion Laboratory, used \u003ca href=\"https://labelbox.com/customers/nasa-jpl/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost to deliver quality labels at 5x their previous speed\u003c/a\u003e. \u003c/p\u003e","comment_id":"63b5df77f94604003d092822","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2977--3-.png","featured":false,"visibility":"public","created_at":"2023-01-04T20:20:07.000+00:00","updated_at":"2023-10-26T18:15:02.000+00:00","published_at":"2023-01-12T21:31:07.000+00:00","custom_excerpt":"Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-evaluate-and-optimize-your-data-labeling-project's-results","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-evaluate-and-optimize-your-data-labeling-projects-results/","excerpt":"Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. ","reading_time":4,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2977--3--2.png","og_title":"How to evaluate and optimize your data labeling project's results","og_description":"Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2977--3--1.png","twitter_title":"How to evaluate and optimize your data labeling project's results","twitter_description":"Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. ","meta_title":"How to evaluate and optimize your data labeling project's results","meta_description":"Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b594a0f94604003d092665","uuid":"ec556dd1-ec16-401f-ba8d-685831eb0220","title":"How to create a quality strategy for your data labeling project","slug":"how-to-create-a-quality-strategy-for-your-data-labeling-project","html":"\u003cp\u003eThe outcome of a labeling project is often a reflection of how it began. A project that was put together in a rushed or careless manner often leads to non-optimal or even unusable training data. \u003ca href=\"https://labelbox.com/guides/data-labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eHigh-quality training data is the ultimate goal of all labeling projects\u003c/a\u003e, so spending time to develop a quality strategy at the beginning of your labeling project is crucial. \u003c/p\u003e\u003cp\u003eWhile carving out time at the beginning of your labeling project might take additional time, it will ultimately save you time and effort on reviews and corrections downstream. \u003c/p\u003e\u003cp\u003eQuality strategy refers to how you ensure and maintain that your labeling project is producing high quality training data. Your quality strategy might include the use of automated quality monitoring features, manual review and feedback, insight gleaned from the monitoring of quality SLA, and above all regular two-way communication with your labeling. \u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"labelboxs-quality-features\"\u003eLabelbox's quality features \u003c/h1\u003e\u003cp\u003eInsight into labeling quality is crucial in not only understanding labeling progress, but also in maximizing labeling efficiency to better manage labeling time and cost. The Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eAnnotation platform\u003c/a\u003e has several features that can assist with quality monitoring. \u003c/p\u003e\u003ch3 id=\"benchmark\"\u003eBenchmark\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003eBenchmark\u003c/a\u003e is a Quantumworks Lab QA tool that enables you to designate a labeled asset as a \"gold standard\" and automatically compares all labels on that asset to the benchmark label.\u003c/p\u003e\u003cp\u003eYou can create this label yourself or choose a \"perfect\" label done by your labeling team and mark it as a benchmark. The asset is then distributed to all labelers on the project and labelers will receive a benchmark score depending on how close their label is to the benchmark label. \u003c/p\u003e\u003cp\u003eBenchmark scores range from 0% (no match between the benchmark label and the labeler's annotations) to 100% (complete match between the benchmark and the labeler's annotations). \u003c/p\u003e\u003cp\u003eYou can learn more about how to set up benchmark labels and how benchmark scores are calculated in our \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-04-at-10.42.13-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1138\" height=\"422\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-04-at-10.42.13-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-04-at-10.42.13-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-04-at-10.42.13-AM.png 1138w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow does benchmark support quality? \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eBenchmark measures labeling \u003cem\u003eaccuracy\u003c/em\u003e. You can use benchmark on tasks that have one objectively correct answer and measure labeler performance against a \"gold standard\". \u003c/p\u003e\u003cp\u003eIt is helpful to determine a target benchmark score (per labeler or per label). The target percentage may depend on task complexity and quality expectations. A 90% benchmark score might be difficult to achieve on one task, but fall below quality expectations on another task. Therefore, you should think about and communicate your target benchmark score expectations to your labelers and reviewers.  \u003c/p\u003e\u003cp\u003eBenchmark scores should be monitored throughout the labeling project. Monitoring at the labeler level can help identify labelers who may struggle more with the task than others, resulting in lower benchmark scores, and you can retrain them or remove them from the project accordingly. Tracking the average benchmark scores across the project can help your team identify general challenges with the task. For example, a drop in scores once a new batch of data has been added may point to a new pattern in the data that needs to be addressed with updated labeling instructions. \u003c/p\u003e\u003ch3 id=\"consensus\"\u003eConsensus\u003c/h3\u003e\u003cp\u003eThe \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003eConsensus\u003c/a\u003e feature generates an agreement rate by comparing annotations provided by all labelers working on the exact same image.\u003c/p\u003e\u003cp\u003eWhen adding a new batch of data to your labeling project, you have the ability to enable or disable consensus for that batch, configure and set batch-specific priority, coverage, and the number of labels. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.19.12-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"660\" height=\"322\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-12-at-4.19.12-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.19.12-PM.png 660w\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow does consensus support quality? \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eConsensus measures labeling \u003cem\u003econsistency\u003c/em\u003e. Use consensus on tasks that are subjective where you want to go with a majority vote. You can also use it to identify data labeled with low consistency that needs to be checked by an expert.\u003c/p\u003e\u003cp\u003eAs with benchmarks, there is not a universal \"good\" or \"bad\" consensus score percentage. A “good” consensus score percentage will depend on the complexity of the task along with your quality expectations. When used for truly subjective tasks, consensus scores will not indicate the quality of a label, but rather the level of consistency between your labelers' subjective opinions. \u003c/p\u003e\u003cp\u003eConsensus scores should be monitored throughout the life of a labeling project. Monitoring consensus scores at the labeler-level can help identify a labeler that might consistently disagree with other labelers, signaling that the labeler might not have a strong understanding of the task. Average consensus scores across the project can help measure overall consistency. Low scores on an objective task may point towards gaps in labeler training or can allude to ambiguous wording in labeling instructions, leaving room for different interpretations by your labelers. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"manually-review-data-with-custom-workflows\"\u003eManually review data with custom workflows \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/workflow-diagram--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"672\" height=\"512\"\u003e\u003c/figure\u003e\u003cp\u003eWhile manual review and providing feedback can be somewhat time intensive, it is an important component in establishing quality in the early stages of your labeling project and should be a part of your quality strategy. \u003c/p\u003e\u003cp\u003eTo ensure quality, labeling should begin on small batches with manual reviews. Once quality is established, manual reviews can taper off to only apply to a percentage of the data and you can start to rely more on manual reviews on the labeling team's side and review tools like benchmark or consensus. \u003c/p\u003e\u003cp\u003eIn Quantumworks Lab Annotate, you can set up customized review steps based on your decided quality strategy in your project's \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflow tab\u003c/a\u003e. As you work with large, complex projects, having to review all labeled data rows becomes increasingly time-consuming and expensive. You can leverage workflows to create a highly-customizable, step-by-step review pipeline to drive efficiency and automation into your review process. \u003c/p\u003e\u003cp\u003eOn the Quantumworks Lab Annotation platform, you can set up customized review steps as decided in your quality strategy via the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e feature. Upon creating a new project, the default setting includes one review step (\"Initial review task\") on 100% of labels.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Workflows---review-tasks---reject--2-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1771\" height=\"912\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Workflows---review-tasks---reject--2-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Workflows---review-tasks---reject--2-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Workflows---review-tasks---reject--2-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/Workflows---review-tasks---reject--2-.gif 1771w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can create a \"new review task\" and can customize your review process with up to 10 review tasks. For example, you can add a second review step on a sample of data performed by your internal review team after the labeling team has already performed their review. This allows for greater QA before data rows are transitioned to \"Done\" and are ready to be used for production. You further customize your review process by routing labels that include a specific annotation type to a separate review step (e.g for all data rows with this annotation type to be performed by an expert reviewer). \u003c/p\u003e\u003cp\u003eFor a more in-depth walkthrough and demonstration of workflows, please refer to our guide on \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003ehow to customize your annotation review process\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow does manual review support quality? \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eManual review can go hand-in-hand with sharing constructive feedback with individual labelers or the entire labeling team. \u003c/p\u003e\u003cp\u003eRather than simply approving or rejecting a label, you can take the time to share written feedback on why a label was rejected. This can be extremely helpful at the beginning of a new task as your labeling team gets started on your project. The labeling team can then use this feedback to better understand your expectations, fix mistakes, and avoid similar mistakes moving forward. \u003c/p\u003e\u003cp\u003eLabeling data is an inherently collaborative process that requires continuous feedback between labelers and reviewers to ensure high-quality outcomes. You can make use of \u003ca href=\"https://docs.labelbox.com/docs/issues-comments?ref=labelbox-guides.ghost.io\"\u003eissues \u0026amp; comments\u003c/a\u003e on the Quantumworks Lab platform to quickly pinpoint and share written feedback directly on the label. This feature also supports two-way communication, allowing labelers to respond or ask clarifying questions. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/comment-gif--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/comment-gif--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/comment-gif--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/comment-gif--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/comment-gif--1-.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"sla-monitoring\"\u003eSLA monitoring\u003c/h2\u003e\u003cp\u003eImplementing a \u003ca href=\"https://labelbox.com/guides/how-to-define-your-data-labeling-projects-success-criteria/?ref=labelbox-guides.ghost.io#slas-quality-speed-throughput\"\u003equality SLA\u003c/a\u003e for your project is optional. However, once the decision for an SLA is made, it requires consistent monitoring to be effective. \u003c/p\u003e\u003cp\u003eA labeling project with an SLA in place, such as a minimum labeling quality threshold to be reached, requires regular monitoring and review to determine whether the SLA has been met on a batch of data. \u003c/p\u003e\u003cp\u003eA labeling project with an SLA in place, i.e. a minimum labeling quality threshold to be reached, requires regular reviews from your side to determine whether the SLA has been met on a given batch of data. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow does SLA monitoring support quality?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhile you could rely solely on the SLA agreement with your labeling partner to meet the required quality threshold, a rigorous review process provides an opportunity to further improve quality. During your review of each batch of data, you should consider the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIs one labeler making significantly more mistakes that others? If so, consider retraining or replacing that labeler.\u003c/li\u003e\u003cli\u003eIs there one error category that shows up more frequently than other errors? If so, consider fleshing out the labeling instructions for this error or put together a small training project and retrain labelers on this error.\u003c/li\u003e\u003cli\u003eAre there changes in the data from batch to batch that lead to poor quality due to lack of experience with this new/different data? If so, consider updating your instructions with new examples found in this new batch of data. Walk the labeling team through these new changes and let them know to ask questions if they are unsure of how to label this new data. \u003c/li\u003e\u003cli\u003eAre you finding errors that do not fit into one of the error categories established as part of the quality SLA? If so, consider adding a new error category for the next batch and highlight this change to your labeling team so that labelers and reviewers are aware of what to look out for. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"create-communication-channels\"\u003eCreate communication channels \u003c/h2\u003e\u003cp\u003eDirect communication is of utmost importance before and during the labeling and feedback process. Communication channels should be carefully chosen, set up, and communicated clearly to all involved parties ahead of starting a project. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/annotate-collaberation--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1344\" height=\"1024\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/annotate-collaberation--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/annotate-collaberation--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/annotate-collaberation--1-.png 1344w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eSome common communication channels are listed below:\u003c/p\u003e\u003cul\u003e\u003cli\u003eChat programs (e.g. Slack)\u003c/li\u003e\u003cli\u003eEmail\u003c/li\u003e\u003cli\u003eShared documents (e.g. Google docs or slides) \u003c/li\u003e\u003cli\u003eRegular scheduled meetings (in person or remotely, ideally with screen sharing capabilities)\u003c/li\u003e\u003cli\u003eCommunication features on the labeling platform (e.g. \u003ca href=\"https://docs.labelbox.com/docs/boost-workforce-notifications?ref=labelbox-guides.ghost.io\"\u003eUpdates\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/issues-comments?ref=labelbox-guides.ghost.io\"\u003eIssues\u003c/a\u003e on the Quantumworks Lab platform)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce communication methods have been chosen, it is worthwhile to establish further guidelines around communication such as: availability, response times, definition of recipients/stakeholders for different topics, structure of messaging (e.g different chat groups or channels, multiple email threads versus one big thread), accessibility of different file formats shared (e.g Google docs versus MS doc). Initially, these details might not seem super important, however clear definitions of communication can aid with communication efficiency and affect the final outcome and quality of your project. \u003c/p\u003e\u003cp\u003eYou can rely on a combination of communication channels, such as weekly Q\u0026amp;A or feedback meetings coupled with a group chat to quickly unblock progress between weekly meetings. Communication channels are useful in providing day-to-day status updates from the labeling team or aid in the announcement of new projects or data volumes by the project owner. \u003c/p\u003e\u003cp\u003eYour chosen method of communication should foster direct communication between the labeling team and a subject-matter expert on the project owner's side. Two-way communication is encouraged at all times during the labeling process as it will help streamline and accelerate the review/rework and feedback process. \u003c/p\u003e\u003cp\u003eTo make communication efficient and direct, consider applying the following principles:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAs they aren't as familiar with the data themselves, meet all questions by the labeling team with patience and understanding. \u003c/li\u003e\u003cli\u003eMonitor and respond to your chosen communication channels regularly, this will help unblock the labeling team in a timely manner. \u003c/li\u003e\u003cli\u003eDefine how questions and feedback should be shared. For example, are screenshots or direct links to labels on the platform the most helpful?\u003c/li\u003e\u003cli\u003eBe sure to communicate the availability of all parties involved (time zone work hours, holidays, etc.).\u003c/li\u003e\u003cli\u003eBe mindful of any cultural differences. Labelers may be initially hesitant to ask you questions.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCommunication is always a two-way street – encourage labelers to reach out to subject-matter experts with any questions or concerns and be sure to respond to questions and share feedback throughout the duration of the labeling task. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow does effective communication support quality?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eImagine receiving a set of tasks, with ambiguous instructions, and having no way of following up to ask clarifying questions. It wouldn't be a huge surprise if you misinterpreted the instructions based on your assumptions and ended up getting every task wrong. In this case, it would be incredibly useful to receive specific feedback that could be used to correct mistakes and understand how to correctly carry out the task. \u003c/p\u003e\u003cp\u003eThis is where direct two-way communication can greatly help the labeling team gain assurance and learn more about your expectations. Understanding task instructions and correcting any labeling errors early on in the project's lifecycle can greatly help with labeling quality. Identifying any errors and misconceptions at the project's onset allows labelers to avoid making those same mistakes on future data rows. \u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eWhile outlining your quality strategy is essential to ensuring that your labeling task is producing high quality training data, you'll also want to make sure that you're able to scale up your labeling operations without sacrificing quality. \u003c/p\u003e\u003cp\u003eA key question for many ML teams is how to maintain consistency and quality as team size or data volume grows. To learn more about how to scale your labeling operations while maintaining quality, please refer to the next guide in this series: \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003ehow to scale up your labeling operations while maintaining quality\u003c/a\u003e. \u003c/p\u003e","comment_id":"63b594a0f94604003d092665","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2976--1-.png","featured":false,"visibility":"public","created_at":"2023-01-04T15:00:48.000+00:00","updated_at":"2024-05-06T21:11:57.000+00:00","published_at":"2023-01-12T21:31:04.000+00:00","custom_excerpt":"Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-create-a-quality-strategy-for-your-data-labeling-project","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-create-a-quality-strategy-for-your-data-labeling-project/","excerpt":"Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. ","reading_time":9,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2976--1--1.png","og_title":"How to create a quality strategy for your data labeling project","og_description":"Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2976--1--2.png","twitter_title":"How to create a quality strategy for your data labeling project","twitter_description":"Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. ","meta_title":"How to create a quality strategy for your data labeling project","meta_description":"Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b48d65f94604003d0925bf","uuid":"5391783f-4c20-4647-9702-f1b9713933d6","title":"How to define your data labeling project's success criteria","slug":"how-to-define-your-data-labeling-projects-success-criteria","html":"\u003cp\u003eEvery ML project begins with the desired outcome of creating \"high-quality\" training data. But what does that really mean in practice and how exactly do you know when that has been achieved?\u003c/p\u003e\u003cp\u003eAnnotation requirements often evolve throughout the lifetime of an ML project, so it's necessary to define your success criteria at the outset of a project and be nimble enough to recognize when requirements may need to be adjusted as things evolve.\u003c/p\u003e\u003cp\u003eLeveraging a workflow that allows for \u003ca href=\"https://labelbox.com/guides/data-labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003elabeling\u003c/a\u003e to be done in \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003esmall batches and that begins with a calibration phase\u003c/a\u003e can help clearly define \"high quality\" and ensure that quality is maintained.\u003c/p\u003e\u003cp\u003eIt's crucial that you take time at the beginning of a project to set expectations on labeling speed, acceptable errors, and determine how quality is defined in order to set you up for an easy to follow quality SLA. This will ensure that your labeling team fully understands the task at hand and the level of performance expected from them. While it may seem like a lot to think through at the onset of a labeling project, it will create mutual understanding that leads to a consistent output with high quality.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"timeline-and-scope-your-deadline-data-volume-and-the-average-time-per-label\"\u003eTimeline and scope: Your deadline, data volume, and the average time per label\u003c/h2\u003e\u003cp\u003eOne of the first steps in helping you get a handle on how you define success is understanding the scope and timeline of your project. Some questions to consider as you plan for high quality labels are:\u003c/p\u003e\u003cul\u003e\u003cli\u003eDoes this project have a definite volume with a deadline?\u003c/li\u003e\u003cli\u003eIs there no specific deadline, but a fixed volume?\u003c/li\u003e\u003cli\u003eDo you have a notion of how long you expect it to take to label each asset?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eRegardless of if there is a predetermined volume with a deadline or not, throughput should be part of your success criteria. Knowing your deadline and volume upfront can set expectations for your labeling team that they will need to complete 'x' amount of assets per day or week. If you have a fixed volume, but no deadline, this is when a \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003ecalibration phase\u003c/a\u003e comes in handy as an initial batch can be used to determine expected throughput. If you have a realistic expected time per asset, that can be used as well.\u003c/p\u003e\u003cp\u003eA project is considered successfully completed when it is finished in a timely manner or by a given deadline. Understanding labeling throughput and setting expectations allows you and your labeling team to have a mutual understanding of success.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"what-is-considered-good-quality-and-how-is-it-measured\"\u003eWhat is considered \"good quality\" and how is it measured? \u003c/h2\u003e\u003cp\u003eIt may be enticing to say that your project has been successful when labels have simply been deemed \"good\" and of \"high quality\". However, this needs to be defined further so that your labeling team knows what \"good\" looks like and how to achieve it.\u003c/p\u003e\u003cp\u003eIf your labeling project requires a simple binary label, it is easy to understand what is good (correct) and what is bad (incorrect) labeling. As the complexity of your labeling task grows, so does the need for assigning value to errors and stating specific requirements for what is \"good\".\u003c/p\u003e\u003cp\u003eAs the owner of your project and labels, you'll need to select the grading requirements for your project. In essence, you have to decide how each asset will be evaluated to have passed or failed in its initial labeling. For this, we suggest creating a definition of what causes an asset to be fully rejected.\u003c/p\u003e\u003cp\u003eFor example, let's say you have a computer vision project that requires multiple classes of bounding boxes per image. If 1 of 7 bounding boxes is mislabeled on an image, is this considered a grave enough error that results in the whole image being rejected? Or is it a lesser error where you would simply create an issue for correction, but not necessarily reject the entire image? Perhaps missing a bounding box entirely is a more serious error than a misclassification and that's a criteria for what leads to rejection.\u003c/p\u003e\u003cp\u003eIt is helpful to outline grave or fatal errors, as well as the lesser errors at the outset of a project or following your review during calibration. Defining these nuances will help your labeling team better understand your feedback and know where to focus their internal QA. We recommend making a list of these errors and keeping it handy while you review. Not only will this allow you to focus your review as well, but you can copy/paste these named errors in whatever feedback format you use to easily be able to track trends.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"slas-quality-speed-throughput\"\u003eSLAs (quality, speed, throughput)\u003c/h2\u003e\u003cp\u003eQuality SLAs (service-level agreements) with your labeling team are a bidirectional commitment that is mutually agreed upon. Quality SLAs are built on throughput and quality calculations.\u003c/p\u003e\u003cp\u003eShould you want to implement a quality SLA on a project, we recommend starting with a calibration phase to allow you to get a real world grasp on throughput (time per individual asset or amount of assets per week) and the types of errors that are surfaced when labeling is done by a team who is not intimately familiar with your project.\u003c/p\u003e\u003cp\u003eDetermining and setting a percentage for quality is at your discretion based on the type and amount of errors that you find during the first calibration phase. We suggest discussing this with the labeling team to ensure you're setting a threshold that is achievable given the complexity of your task.\u003c/p\u003e\u003cp\u003eWhen it comes to tracking whether or not a SLA is being met, throughput and quality are the two components you’ll be looking at:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eThroughput\u003c/strong\u003e can easily be gauged by ensuring the stated number of assets are in fact being completed on the cadence (daily, weekly, etc) agreed to.\u003c/li\u003e\u003cli\u003eAs for ensuring that the \u003cstrong\u003equality\u003c/strong\u003e necessary is being achieved, this will require your continued review of a sample throughout the lifespan of your project. This is when having a reusable list of error types comes in handy, while you review the assets that received serious errors will be considered rejected. Calculating quality can be as simple as dividing the number of assets rejected by the total number you reviewed. Having a clear way of understanding how many assets of a reviewed sample were rejected and why will help your labeling team understand if “good quality” is being achieved.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBelow you will find a table that outlines the milestones of implementing a quality SLA and assigns responsibility for each to a particular party. As stated, a well executed quality SLA is bidirectional and involves the continued involvement of the project owner, ensuring good quality labels is a shared responsibility.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.41.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1110\" height=\"1190\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-12-at-4.41.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-12-at-4.41.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.41.35-PM.png 1110w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAfter defining and setting expectations on how quality is defined, you'll want to spend some time to develop a quality strategy. Your quality strategy will resemble how you will monitor and ensure that your project ends up with high quality data. This can include the use of automated quality monitoring features, manual review and feedback, regular two-way communication with your labeling team, and more. \u003c/p\u003e\u003cp\u003eLearn more about \u003ca href=\"https://labelbox.com/guides/how-to-create-a-quality-strategy-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003ehow to create a quality strategy for your data labeling project\u003c/a\u003e in our next guide.\u003c/p\u003e","comment_id":"63b48d65f94604003d0925bf","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2975--3-.png","featured":false,"visibility":"public","created_at":"2023-01-03T20:17:41.000+00:00","updated_at":"2024-05-06T21:10:56.000+00:00","published_at":"2023-01-12T21:30:59.000+00:00","custom_excerpt":"Learn how to effectively define your labeling project's success criteria so all tasks lead to consistent output with high quality. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-define-your-data-labeling-projects-success-criteria","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-define-your-data-labeling-projects-success-criteria/","excerpt":"Learn how to effectively define your labeling project's success criteria so all tasks lead to consistent output with high quality. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2975--3--2.png","og_title":"How to define your data labeling project's success criteria","og_description":"Learn how to effectively define your labeling project's success criteria so all tasks lead to consistent output with high quality. ","twitter_image":null,"twitter_title":"How to define your data labeling project's success criteria","twitter_description":"Learn how to effectively define your labeling project's success criteria so all tasks lead to consistent output with high quality. ","meta_title":"How to define your data labeling project's success criteria","meta_description":"Learn how to effectively define your labeling project's success criteria so all tasks lead to consistent output with high quality. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b34450f94604003d092366","uuid":"32b97a0e-237d-4690-8906-19aa50b4a3fd","title":"How to define a task for your data labeling project","slug":"how-to-define-a-task-for-your-data-labeling-project","html":"\u003cp\u003eLarge volumes of high-quality training data are crucial to the success of any machine learning model. A labeling project is where you orchestrate and manage all of your labeling operations within Labelbox.\u003c/p\u003e\u003cp\u003eThe first step in the labeling process is to align on the key components of the labeling task within a project. This sets the tone of the project and allows Quantumworks Lab to make labeling more efficient down the line.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"defining-the-data\"\u003eDefining the data\u003c/h2\u003e\u003ch3 id=\"data-type\"\u003eData type \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/annotate-one-platform--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"672\" height=\"512\"\u003e\u003c/figure\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003eCreating a project\u003c/a\u003e and configuring your labeling task on the Quantumworks Lab platform begins alignment on which type of data needs to be labeled.\u003c/p\u003e\u003cp\u003eLabelbox supports the following \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#supported-data-types\"\u003edata types\u003c/a\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImages\u003c/li\u003e\u003cli\u003eVideos\u003c/li\u003e\u003cli\u003eText\u003c/li\u003e\u003cli\u003eConversational text\u003c/li\u003e\u003cli\u003ePDF documents\u003c/li\u003e\u003cli\u003eGeospatial / Tiled imagery\u003c/li\u003e\u003cli\u003eAudio\u003c/li\u003e\u003cli\u003eHTML\u003c/li\u003e\u003cli\u003eDICOM (medical imagery)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBased on your chosen data modality, the Labeling Team Manager can leverage prior knowledge and the existing experience of different teams of labelers to work on your projects.\u003c/p\u003e\u003cp\u003eFor instance, some workforce teams have an excellent track record of successfully labeling projects with a specific type of imagery, while others have extended experience in working with the video editor or in specialized text projects. By curating labeling teams who are already well-versed in your use case and match your project needs, they can get started quicker with little friction.\u003c/p\u003e\u003ch3 id=\"industry-verticals\"\u003e\u003cstrong\u003eIndustry verticals\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/catalog-hero--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1011\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/catalog-hero--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/catalog-hero--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/catalog-hero--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/catalog-hero--1-.png 2089w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eSimilarly, our labeling partners are experienced in various industries such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eManufacturing\u003c/li\u003e\u003cli\u003eReal estate\u003c/li\u003e\u003cli\u003eFood service\u003c/li\u003e\u003cli\u003eAgrotech\u003c/li\u003e\u003cli\u003eHealthcare\u003c/li\u003e\u003cli\u003eInsurance\u003c/li\u003e\u003cli\u003eRetail\u003c/li\u003e\u003cli\u003eand many more\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLeveraging labelers who have experience in the industry of interest for your task should be discussed, as it allows the Labeling Team Manager to allocate the best workforce who meets your needs.\u003c/p\u003e\u003cp\u003eSome teams have large experience in aerial roof tagging for insurance companies, while others have been working on long-term microscopy pictures in the medical field, and many more variations of all kinds of tasks. Understanding the scope and frame of the task allows Quantumworks Lab to set your team up for success with the right workforce.\u003c/p\u003e\u003cp\u003eA workforce team who is well-versed in your industry or use case will need less time to get calibrated on your task. This means they'll be able to label more data in less time, which results in cheaper labeling costs as you only pay for the time when labeling screen-time occurs.\u003c/p\u003e\u003cp\u003eThere might be some use cases where general experience in a specific industry or data type might not be sufficient enough to meet your requirements. Quantumworks Lab also offers the option for you to onboard expert labelers for your project needs. You can learn more below under the \"Specific labeling requirements\" section.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"leveraging-labelboxs-suite-of-tools\"\u003eLeveraging Quantumworks Lab's suite of tools \u003c/h2\u003e\u003ch3 id=\"annotate\"\u003eAnnotate\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/annotate-header-image--3-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1170\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/annotate-header-image--3-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/annotate-header-image--3-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/annotate-header-image--3-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/annotate-header-image--3-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox's \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e is designed to give you complete visibility and control over every aspect of your labeling operations across data modalities.\u003c/p\u003e\u003cp\u003eWhile setting up your labeling project, you'll need to acknowledge the supported file formats and annotation types in order to prevent issues down the line.\u003c/p\u003e\u003cp\u003eSimilarly, it is important to understand how to use Annotate to set up your project and labeling task, collaborate with your internal or external teams, and how to ensure that you're minimizing labeling time and spend. \u003c/p\u003e\u003cp\u003eFor instance, only one labeler can work per data row. If you have long videos to annotate, we might recommend splitting them into multiple files so more annotators can work on the data. Ultimately this will depend on your own speed and time requirements, however the Labeling Team Manager is available to work with you to determine what will work best for your team's use case.\u003c/p\u003e\u003cp\u003eYou can learn more about Annotate in our \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"catalog\"\u003eCatalog\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/catalog-hero--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1250\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/catalog-hero--2-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/catalog-hero--2-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/catalog-hero--2-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/catalog-hero--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox's \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e is a data curation tool for you to organize, search, visualize, and explore your unstructured data. \u003c/p\u003e\u003cp\u003eUtilizing Catalog for data selection is a huge advantage in having a quality batch of data to label, according to specific parameters required for your task. You can leverage Catalog's features, such as \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003efilters\u003c/a\u003e, a one-click \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003esimilarity search\u003c/a\u003e, \u003ca href=\"https://docs.labelbox.com/docs/adding-metadata?ref=labelbox-guides.ghost.io\"\u003emetadata\u003c/a\u003e, and more, to ensure that the data you're queueing to your project is well-structured for your business requirements. \u003c/p\u003e\u003cp\u003eYou can learn more about Catalog in our \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"specific-labeling-requirements\"\u003eSpecific labeling requirements \u003c/h2\u003e\u003ch3 id=\"expert-workforce\"\u003eExpert workforce\u003c/h3\u003e\u003cp\u003eFor more specific and specialized tasks, Quantumworks Lab has the ability to onboard labelers who are qualified in particular domains:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical labelers with determined skills, such as people with software programming certifications\u003c/li\u003e\u003cli\u003eMedical labelers like nurses, clinicians, dermatologists, neurologists, and surgeons\u003c/li\u003e\u003cli\u003eLabelers fluent in one of 20+ languages covered by our partners\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYour Labeling Team Manager can source a specialist, based on the expertise needed, who can get started on your task. Since experts can take longer to source, it is essential to determine and request this requirement along with additional information needed prior to the start of labeling. \u003c/p\u003e\u003ch3 id=\"compliance\"\u003eCompliance\u003c/h3\u003e\u003cp\u003eAs Quantumworks Lab partners are spread out across different countries, it's important that geographical location is acknowledged and discussed so your business requirements are met. Depending on your discussed compliance and project needs, the Labeling Team Manager will ensure that the right workforce is onboarded accordingly. \u003c/p\u003e\u003cp\u003eLabelbox partners are compliant with the following certifications:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSOC2 Type I\u003c/li\u003e\u003cli\u003eSOC2 Type II\u003c/li\u003e\u003cli\u003eGDPR\u003c/li\u003e\u003cli\u003eHIPAA\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"labeling-forecast\"\u003eLabeling forecast  \u003c/h2\u003e\u003ch3 id=\"volume\"\u003eVolume\u003c/h3\u003e\u003cp\u003eDefining your data volume is a key element to consider when outlining your labeling task. This allows early expectations to be set in context of throughput and subsequently helps the Labeling Team Manager and the workforce to organize the labeling in the most efficient manner.\u003c/p\u003e\u003cp\u003eFor instance, the following aspects should be considered and discussed when defining your task:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLarge volumes of data\u003c/li\u003e\u003cli\u003eLong-term projects\u003c/li\u003e\u003cli\u003eShort projects\u003c/li\u003e\u003cli\u003eTurnarounds\u003c/li\u003e\u003cli\u003eUploads frequency\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBased on the above, the Labeling Team Manager can allocate the task to the team most appropriate to meet the volume demands in terms of resources and availability.\u003c/p\u003e\u003ch3 id=\"timeline\"\u003eTimeline\u003c/h3\u003e\u003cp\u003eUnderstanding the timeline of your task is also crucial in effectively ramping up and scaling labeling activity. A rough outline of when you want the project to start and the target completion data helps define a structured labeling process and makes resource management easier. \u003c/p\u003e\u003cp\u003eA task that is set up for success will consider the following: \u003c/p\u003e\u003cul\u003e\u003cli\u003eHow many data rows do you plan to upload to your project? At which frequency?\u003c/li\u003e\u003cli\u003eWhat are the expectations in terms of speed?\u003c/li\u003e\u003cli\u003eDo you have a deadline?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eQuick turnarounds on high volumes of data and tight deadlines can be delicate to navigate, so planning ahead and understanding timelines in advance can help maximize the resources available and the workforce's time can be used efficiently. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"creating-an-ontology\"\u003eCreating an ontology\u003c/h2\u003e\u003cp\u003eAnother important aspect of a well-defined labeling task is the \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eontology\u003c/a\u003e. It needs to be built in a way that will work for the task at hand, and that follows the most logical workflow for a labeler. Ontologies and features should be created and managed with the goals of proper labeling, efficiency, and reusability in mind.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/2acf976-Screen_Shot_2021-12-09_at_12.28.01_PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1840\" height=\"1404\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/2acf976-Screen_Shot_2021-12-09_at_12.28.01_PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/2acf976-Screen_Shot_2021-12-09_at_12.28.01_PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/2acf976-Screen_Shot_2021-12-09_at_12.28.01_PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/2acf976-Screen_Shot_2021-12-09_at_12.28.01_PM.png 1840w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"components\"\u003eComponents\u003c/h3\u003e\u003cp\u003eWithin an ontology, the three kinds of features are: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eObjects\u003c/strong\u003e (bounding boxes, polygons, segmentation masks, points, polylines, etc)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eClassifications\u003c/strong\u003e (radio, checklist, etc), that can be global or nested\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRelationships \u003c/strong\u003e(these are approached differently in Quantumworks Lab depending on the data type): With text data, you can define relationships between entity annotations as part of the objects. With image data, you can set relationship items in the ontology \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eA good ontology should define and answer the following: \u003c/p\u003e\u003cul\u003e\u003cli\u003eWhat should the labeling team be labeling? \u003c/li\u003e\u003cli\u003eHow should objects and/or classifications be labeled? \u003c/li\u003e\u003cli\u003eWhat additional information is helpful for your model? \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eReusing ontologies can be useful if you're planning on having multiple projects for the same or a very similar use case. Elements can be added to an existing ontology without affecting labels, so an ontology is not set in stone and you're encouraged to test and fine-tune your ontology. \u003c/p\u003e\u003cp\u003eYou can learn more about how to create and manage your ontologies in \u003ca href=\"https://labelbox.com/guides/how-to-create-and-manage-ontologies/?ref=labelbox-guides.ghost.io\"\u003ethis guide\u003c/a\u003e. \u003c/p\u003e\u003ch3 id=\"speed\"\u003eSpeed\u003c/h3\u003e\u003cp\u003eYou should choose tools that will allow labelers to label as fast as possible while maintaining the output needed for your model.\u003c/p\u003e\u003cp\u003eSample questions to consider when selecting tools would be: \u003c/p\u003e\u003cul\u003e\u003cli\u003eWould separate bounding boxes per class be better than one bounding box with a nested classification? \u003c/li\u003e\u003cli\u003eIs a segmentation mask necessary or would a polygon do? \u003c/li\u003e\u003cli\u003eDo you need every frame to be annotated? \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"quality\"\u003eQuality\u003c/h3\u003e\u003cp\u003eHigh-quality training data is critical to the success of your model. Ensure quality by designing a clear and efficient ontology that makes it easier for you to organize the output.\u003c/p\u003e\u003cp\u003eSample questions to consider include: \u003c/p\u003e\u003cul\u003e\u003cli\u003eDoes the ontology need to be extremely complex, or can you consider separate projects to label different things on the same data?\u003c/li\u003e\u003cli\u003eIs the free text field necessary? Avoid options that can lead to inconsistencies, typos and misspellings.\u003c/li\u003e\u003cli\u003eIs it best to skip or should there be an annotation denoting that the answer is unknown, that there was nothing to label? Do you want to understand why some assets do not meet the criteria, or is it fine to have a bucket of unlabeled assets?\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"creating-labeling-instructions\"\u003eCreating labeling instructions \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.10.49-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1450\" height=\"1208\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-12-at-4.10.49-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-12-at-4.10.49-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.10.49-PM.png 1450w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce all the modalities of the task have been properly defined, you have to provide \u003ca href=\"https://docs.labelbox.com/docs/labeling-instructions?ref=labelbox-guides.ghost.io\"\u003elabeling instructions\u003c/a\u003e to the workforce. Even with an extremely simple ontology, it is necessary to offer additional information related to the labeling task. \u003c/p\u003e\u003cp\u003eLabeling instructions compliment the ontology and can be in the form of a document or a video demo. You can include anything that you deem useful and relevant to explaining the rules of your labeling task in a way that is easy for labelers to follow. \u003c/p\u003e\u003cp\u003eGood instructions will go into detail with specific examples and clearly lay out major labeling rules. Labeling instructions should provide context to the task, explain what the task entails, describe the labeling steps, and serve as a \"living document\".\u003c/p\u003e\u003cp\u003eInstructions can be altered depending on task progress and any changes in your requirements. Since changes can be made, it is advised for you to keep track of what made a label meet your success criteria so you can tailor the instructions to help the team understand important criteria of what makes a \"good label\". \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"defining-labeling-rules\"\u003eDefining labeling rules\u003c/h2\u003e\u003ch3 id=\"definition-of-all-ontology-items\"\u003eDefinition of all ontology items\u003c/h3\u003e\u003cp\u003eIt is important that you make sure to list and define the items that you want labeled. As mentioned in the \"Creating an ontology\" section above, you should be clear on the features and rules behind each expected annotation: \u003c/p\u003e\u003cul\u003e\u003cli\u003eObjects\u003c/li\u003e\u003cli\u003eClassifications\u003c/li\u003e\u003cli\u003eRelationships\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor example, if the project contains several entities/objects to be labeled with multiple classifications to choose from, explain each entity/object and each classification in sufficient detail: \u003c/p\u003e\u003cul\u003e\u003cli\u003eHow tight around the object does the bounding box need to be? \u003c/li\u003e\u003cli\u003eHow do labelers pinpoint a precise point on a blurry image with shapes that are not sharp? \u003c/li\u003e\u003cli\u003eIs there a maximum number of points a polygon can have before it becomes excessive for your model? \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhen defining your labeling rules, you should aim to: \u003c/p\u003e\u003cul\u003e\u003cli\u003eProvide clear definitions of all concepts for text projects to prevent ambiguities in the labeling\u003c/li\u003e\u003cli\u003eTry to format your instructions in a way that is easy to read, and make sure the golden rules pop out in a clear and distinguishable manner to minimize any chance that these could be missed\u003c/li\u003e\u003cli\u003eDefine your approach on a frame basis for video projects\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-by-step-labeling-workflow\"\u003eStep-by-step labeling workflow\u003c/h3\u003e\u003cp\u003eMake sure to describe each step in the labeling workflow so that your labelers are not lost in the ontology:\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor projects with multiple objects per asset, is there a specific order in which you need the annotations to be added?\u003c/li\u003e\u003cli\u003eDescribe your expectations for the review process\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"examples\"\u003eExamples\u003c/h3\u003e\u003cp\u003eThe best way to convey the results you want is by providing clear examples of the data to the labelers in the form of screenshots in your instructions. There are several approaches to this:\u003c/p\u003e\u003cul\u003e\u003cli\u003eProvide screenshots of unlabeled and labeled data for the labelers to have an overview of the assets they will be working on and what the outcome should be. Try to include several images that represent the variations of the full set.\u003c/li\u003e\u003cli\u003eClarify the variability of the data by sharing “edge cases” in your guidelines. An asset that would stand out from the rest of the set should be explained in detail, so the labelers know how to approach these types of cases.\u003c/li\u003e\u003cli\u003eInclude incorrectly labeled (negative) examples as well. This helps the team to identify mistakes to avoid. Common mistakes can be mentioned to prevent making them in this task.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003eAll of these key components contribute to defining a task that is set for success on the Quantumworks Lab platform. These aspects also help the Labeling Team Manager ensure that your project outcomes are successful. \u003c/p\u003e\u003cp\u003eAfter the initial task setup, the next step in your labeling journey is to define your success criteria. Along with your volumes and deadlines, learn more about how to get a notion of the average time per label, describe how a \"good label\" is measured, and learn about SLAs in our next guide: How to define your data labeling project's success criteria. \u003c/p\u003e","comment_id":"63b34450f94604003d092366","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2974--1-.png","featured":false,"visibility":"public","created_at":"2023-01-02T20:53:36.000+00:00","updated_at":"2024-10-02T22:27:59.000+00:00","published_at":"2023-01-12T21:30:52.000+00:00","custom_excerpt":"Learn how to align on key components of your project: define a task, create an ontology, and determine timelines for your labeling project.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-define-a-task-for-your-data-labeling-project/","excerpt":"Learn how to align on key components of your project: define a task, create an ontology, and determine timelines for your labeling project.","reading_time":9,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2974--1--2.png","og_title":"How to define a task for your labeling project","og_description":"A crucial first step in the labeling process is to align on key components of the labeling task. Learn how to define a task, create an ontology, and determine timelines for your labeling project.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2974--1--1.png","twitter_title":"How to define a task for your labeling project","twitter_description":"A crucial first step in the labeling process is to align on key components of the labeling task. Learn how to define a task, create an ontology, and determine timelines for your labeling project.","meta_title":"How to define a task for your labeling project","meta_description":"A key first step in the labeling process is to align on components of the labeling task. Learn how to define a task, create an ontology, and determine timelines for your labeling project. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"638e795d26221b003dd3cb95","uuid":"2d10380a-0764-4d34-b1b5-4b2e576b7be5","title":"How to find similar data in one click","slug":"how-to-find-similar-data-in-one-click","html":"\u003cp\u003eThe most successful training datasets are carefully visualized, curated, and debugged to increase model performance at each iteration.\u003c/p\u003e\u003cp\u003eML teams mine data by looking for all examples of rare assets or edge cases that will dramatically improve model performance. Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Building a similarity search engine that scales to hundreds of millions of data points and generates instant results is difficult for even the most advanced ML teams. \u003c/p\u003e\u003cp\u003eWith similarity search, you can easily query and explore your unstructured data and develop a holistic understanding of your training data. Plus, it helps break down silos across datasets, so teams can focus on curating and labeling the data that will dramatically improve model performance.\u003c/p\u003e\u003cp\u003eLabelbox provides a native similarity search engine, where you can leverage both off-the-shelf embeddings (for image, text, and documents)\u003cstrong\u003e \u003c/strong\u003eor upload your own custom embeddings to quickly find all instances of similar data.\u003c/p\u003e\u003ch2 id=\"how-to-conduct-a-similarity-search-query\"\u003eHow to conduct a similarity search query\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mz9zvhw4mo\" title=\"Instantly find ​similar data in one click [Catalog] Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1) Hover and click on the bottom right icon of any data row OR select all data rows of interest and click \"Similar to selection\" \u003c/p\u003e\u003cp\u003e2) This will automatically surface similar data rows – you can select multiple data rows as anchors to refine your similarity search \u003c/p\u003e\u003cp\u003e3) Combine similarity search with other filters and save these searches as \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e. This will allow you to revisit all current and incoming data rows that match the specific search criteria.\u003c/p\u003e\u003chr\u003e\u003cp\u003eWith Quantumworks Lab' similarity search, you can unlock the following workflows: \u003c/p\u003e\u003ch2 id=\"explore-visualize-and-understand-your-data-in-one-click\"\u003eExplore, visualize, and understand your data in one click\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/2022-11-28_08-44-21--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1756\" height=\"967\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eBefore you train your ML model, you can explore all of your data in Catalog\u003c/li\u003e\u003cli\u003eIn just a few clicks, you can surface all examples of data rows of interest, and either save them as a slice of data of interest, or send them to a labeling project as a batch\u003c/li\u003e\u003cli\u003eIn the example above, we can filter all images of a single flower from almost five million data rows in Catalog with just one click\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"quickly-mine-edge-cases-or-rare-examples\"\u003eQuickly mine edge cases or rare examples \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/2022-11-28_09-03-44--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1764\" height=\"975\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAfter training your model, you might find an edge case where your model is struggling\u003c/li\u003e\u003cli\u003eYou can use similarity search in Catalog to easily confirm whether this is a pattern of model failures, or simply a one-off mistake\u003c/li\u003e\u003cli\u003eIn the above example, the model appears to struggle with images with many flowers, so we can quickly mine edge cases to find all images containing many flowers\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"find-all-labeling-mistakes-in-your-project-and-send-them-to-re-labeling\"\u003eFind all labeling mistakes in your project and send them to re-labeling\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/bleh--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWithin a labeling project, you might identify data rows with problems – such as labeling quality issues or mislabeled data\u003c/li\u003e\u003cli\u003eYou can leverage similarity search to find all similar labeled data (which might contain labeling errors and need additional review) and submit them to a specific review step \u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"select-even-more-high-impact-data-to-label\"\u003e\u003cbr\u003eSelect even more high-impact data to label \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/2022-11-28_09-18-49--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1764\" height=\"972\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce you’ve identified data rows on which your model is struggling, you can find all similar unlabeled data in your datasets, label that data, and retrain the model to improve performance\u003c/li\u003e\u003cli\u003eIn this example, the model has low confidence with green bananas, so we used a similarity search and filter to show only unlabeled images of green bananas —\u0026nbsp;which can then be labeled and used to train the model\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"automatically-curate-data\"\u003e\u003cbr\u003eAutomatically curate data \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/My-Movie-47--4-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eBy saving your similarity search as a slice, any new incoming data point uploaded to Quantumworks Lab — and that matches the similarity search — will show up in the slice\u003c/li\u003e\u003cli\u003eWith data curation pipelines that update even when you're offline, you can continuously upload data from production, and data points that look similar to data of interest will show up in the corresponding slice\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"find-duplicate-data\"\u003eFind duplicate data \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/2022-11-28_09-24-41--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1765\" height=\"964\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eEasily find all instances of duplicate data that you don't want to appear in your labeling project by leveraging similarity search\u003c/li\u003e\u003cli\u003eOnce you've found all similar duplicate data, you can save this search as a slice and this will automatically filter and all similar images, including past and incoming data that gets added to Catalog\u003c/li\u003e\u003cli\u003eYou can then take action on duplicate data, such as deleting them from your Quantumworks Lab instance\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"automatically-label-data-from-catalog\"\u003eAutomatically label data from Catalog\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/My-Movie-47--2-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eLeverage similarity search and metadata to automatically identify and label data in bulk without sending data to a labeling project\u003c/li\u003e\u003cli\u003eSave search criteria for a cluster of similar data as a data slice, so that all new and old data that matches that criteria will automatically get added to that slice\u003c/li\u003e\u003cli\u003eYou can then select data rows of interest within the slice, or select the entire slice and tag these data rows with metadata\u003c/li\u003e\u003cli\u003eIn the above example, we surfaced a cluster of data rows containing green stamps using similarity search, selected the data rows of interest, and added a metadata tag called 'green stamps'\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003eYou can learn more about \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003esimilarity search\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e in our documentation. \u003c/p\u003e","comment_id":"638e795d26221b003dd3cb95","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2965.png","featured":false,"visibility":"public","created_at":"2022-12-05T23:06:05.000+00:00","updated_at":"2023-10-27T17:06:03.000+00:00","published_at":"2022-12-19T14:46:22.000+00:00","custom_excerpt":"Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Learn more about how to find similar data in one click with Labelbox. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-find-similar-data-in-one-click/","excerpt":"Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Learn more about how to find similar data in one click with Labelbox. ","reading_time":4,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2965-2.png","og_title":"How to find similar data in one click","og_description":"Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Learn more about how to find similar data in one click with Labelbox. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2965-1.png","twitter_title":"How to find similar data in one click","twitter_description":"Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Learn more about how to find similar data in one click with Labelbox. ","meta_title":"How to find similar data in one click | Quantumworks Lab","meta_description":"Find examples of edge cases that will improve model performance. Learn more about how to find similar data in one click with Labelbox. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"total":95,"allPosts":[{"id":"684755a1e82e4e00013fe307","uuid":"fab31394-c337-43cc-bc44-725a7b69cc61","title":"Quantumworks Lab Leaderboards: Redefining AI evaluations with human-centric assessments","slug":"labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments","html":"\u003cp\u003eIn the rapidly evolving landscape of artificial intelligence, traditional benchmarks are no longer sufficient to capture the true capabilities of AI models. At Quantumworks Lab, we're excited to introduce our groundbreaking \u003ca href=\"https://labelbox.com/leaderboards/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e—an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks. \u003c/p\u003e\u003ch2 id=\"the-limitations-of-current-benchmarks-and-leaderboards\"\u003e\u003cstrong\u003eThe limitations of current benchmarks and leaderboards\u003c/strong\u003e\u003c/h2\u003e\u003ch3 id=\"benchmark-contamination\"\u003e\u003cstrong\u003eBenchmark contamination\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOne of the most pressing issues in AI evaluation today is benchmark contamination. As large language models are trained on vast amounts of internet data, they often inadvertently include the very datasets used to evaluate them. This leads to inflated performance metrics that don't accurately reflect real-world capabilities. For example:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe LAMBADA dataset, designed to test language understanding, has been found in the training data of several popular language models, with an \u003ca href=\"https://hitz-zentroa.github.io/lm-contamination/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLM Contamination Index\u003c/u\u003e\u003c/a\u003e of 29.3%.\u003c/li\u003e\u003cli\u003ePortions of the SQuAD question-answering dataset have been discovered in the pretraining corpora of multiple large language models.\u003c/li\u003e\u003cli\u003eEven coding benchmarks like HumanEval have seen their \u003ca href=\"https://arxiv.org/pdf/2407.07565?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esolutions leaked online\u003c/u\u003e\u003c/a\u003e, potentially contaminating future model training.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis contamination makes it increasingly difficult to trust traditional benchmark results, as models may be “cheating” by memorizing test data rather than demonstrating true understanding or capability.\u003c/p\u003e\u003ch3 id=\"existing-leaderboards-a-step-forward-but-not-enough\"\u003e\u003cstrong\u003eExisting leaderboards: A step forward, but not enough\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWhile several leaderboards have emerged to address the limitations of traditional benchmarks, they each come with their own set of challenges.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eLMSYS chatbot arena\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLMSYS Chatbot Arena, despite its broad accessibility, faces notable challenges in providing objective AI evaluations. Its reliance on non-expert assessments and emphasis on chat-based evaluations may introduce personal biases, potentially favoring engaging responses over true intelligence. Researchers worry that this approach could lead companies to prioritize optimizing for superficial metrics rather than genuine real-world performance. Furthermore, LMSYS's commercial ties raise concerns about impartiality and the potential for an uneven evaluation playing field, as usage data may be selectively shared with certain partners.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScale AI's SEAL\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eScale’s Safety, Evaluations, and Alignment Lab (SEAL), released few months ago, offers detailed insights/evaluations for topics such as reasoning, coding, and agentic tool use. However, the infrequent updates and primary focus on language models, while useful, may not capture the full spectrum of rapidly advancing multimodal AI capabilities.\u003c/p\u003e\u003ch2 id=\"challenges-in-ai-evaluation\"\u003e\u003cstrong\u003eChallenges in AI evaluation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThese and other existing leaderboards all run into core challenges with AI evaluations:\u003c/p\u003e\u003cp\u003e1) Data contamination and overfitting to public benchmarks\u003c/p\u003e\u003cp\u003e2) Scalability issues as models improve and more are added\u003c/p\u003e\u003cp\u003e3) Lack of standards for evaluation instructions and criteria\u003c/p\u003e\u003cp\u003e4) Difficulty in linking evaluation results to real-world outcomes\u003c/p\u003e\u003cp\u003e5) Potential bias in human evaluations\u003c/p\u003e\u003ch2 id=\"introducing-the-labelbox-leaderboards-a-comprehensive-approach-to-ai-evaluation\"\u003e\u003cstrong\u003eIntroducing the Quantumworks Lab Leaderboards: A comprehensive approach to AI evaluation\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Quantumworks Lab Leaderboards are the first to tackle these challenges by conducting structured evaluations on subjective AI model outputs using human experts and a scientific process that provides detailed feature-level metrics and multiple ratings. Leaderboards are available for \u003ca href=\"https://labelbox.com/leaderboards/complex-reasoning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eComplex Reasoning\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/multimodal-reasoning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eMultimodal Reasoning\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eImage Generation\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/speech-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSpeech Generation\u003c/u\u003e\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/leaderboards/video-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eVideo Generation\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eOur goal is to go beyond traditional leaderboards and benchmarks by incorporating the following elements:\u003c/p\u003e\u003ch3 id=\"1-multimodal-and-niche-focus\"\u003e\u003cstrong\u003e1. Multimodal and niche focus\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eUnlike leaderboards that primarily focus on text-based large language models, we evaluate a diverse range of AI modalities and specialized applications, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImage generation and analysis\u003c/li\u003e\u003cli\u003eAudio processing and synthesis\u003c/li\u003e\u003cli\u003eVideo creation and manipulation\u003c/li\u003e\u003cli\u003eComplex and multimodal reasoning\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"2-expert-human-evaluation\"\u003e\u003cstrong\u003e2. Expert human evaluation\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFor every evaluation, public or private, it’s critical for the raters to reflect your target audience. We place expert human judgment, using our \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAlignerr\u003c/u\u003e\u003c/a\u003e workforce, at the core of the evaluation process to ensure:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSubjective quality assessment:\u003c/strong\u003e Humans assess aspects like aesthetic appeal, realism, and expressiveness.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContextual understanding:\u003c/strong\u003e Evaluators consider the broader context and intended use.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAlignment with human preferences:\u003c/strong\u003e Raters ensure evaluations reflect criteria that matter to end-users.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResistance to contamination:\u003c/strong\u003e Human evaluations on novel tasks are less prone to data contamination.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"3-reliable-and-transparent-methodology\"\u003e\u003cstrong\u003e3. Reliable and transparent methodology\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWe are committed to performing trustworthy evaluations using a variety of sophisticated metrics. Quantumworks Lab balances privacy with openness by providing detailed feature-level metrics (e.g. prompt alignment, visual appeal, and numerical count for text-image models) and multiple ratings.\u003c/p\u003e\u003cp\u003eIn addition to critical human experts performing the evaluations, our methodology utilizes the \u003ca href=\"https://labelbox.com/product/platform/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLablebox Platform\u003c/a\u003e to generate advanced metrics on both the rater and model performance. We provide the following metrics across our three leaderboards:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eElo rating system:\u003c/strong\u003e Adapted from competitive chess, our Elo system provides a dynamic rating that adjusts based on head-to-head comparisons between models. This allows us to capture relative performance in a way that's responsive to improvements over time.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTrueSkill rating:\u003c/strong\u003e Originally developed for Xbox Live, TrueSkill offers a more nuanced rating that accounts for both a model's performance and the uncertainty in that performance. This is particularly useful for newer models or those with fewer evaluations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank percentages:\u003c/strong\u003e We track how often each model achieves each rank (1st through 5th) in direct comparisons. This provides insight into not just average performance, but consistency of top-tier results.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAverage rating:\u003c/strong\u003e A straightforward metric that gives an overall sense of model performance across all evaluations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to these key metrics, our methodology incorporates the following characteristics to ensure a balanced and fair evaluation:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eExpert evaluators:\u003c/strong\u003e Utilizing skilled professionals from our Alignerr platform to provide nuanced, context-aware assessments.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComprehensive and novel datasets:\u003c/strong\u003e Curated to reflect real-world scenarios while minimizing contamination.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTransparent reporting:\u003c/strong\u003e Detailed insights into our methodologies and results without compromising proprietary information.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"4-continuously-updated-evaluations\"\u003e\u003cstrong\u003e4. Continuously updated evaluations\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOur leaderboard isn't static; we plan to regularly update our evaluations to include the latest models and evaluation metrics, ensuring stakeholders have access to current and relevant information.\u003c/p\u003e\u003ch2 id=\"leaderboard-insights-a-glimpse-into-the-image-generation-leaderboard\"\u003e\u003cstrong\u003eLeaderboard insights: A glimpse into the image generation leaderboard\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo illustrate the power of our comprehensive evaluation approach, let's explore the \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eimage generation leaderboard\u003c/a\u003e. For each evaluation of the latest image-generating models, we capture and publish four key pieces of data to help understand capabilities and areas of opportunity for each model.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Elo ratings:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXf_TDIoHT0uGUl1DnLLMI3qkaV4g9M9yrK4GVWRc1Ze495hZYjwXE5Yq0wZefgLQecqsXA8cS7bSmTNz923B8CYgza7d2PkPn25crjiCrd0I3W2MG53hWiTgo-N8BTL8y11b3vuog?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT Image 1 leads with 1069.17, followed by GPT 4.1 at 1039.62 and Recraft v3 at 1039.37\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2) TrueSkill ratings:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdIblItim2rjJ5kSmyMtNqbhzIQZM24C6TwogiUeuNFCwBNyUPydDpZ3vOlYPSimzksITCDgQ_ej4Czavte2eI8aT0OEjCB0FwDDagBBP-yQLgt2T_FV8dTy0DdFqWUfk4cbMSYcg?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT Image 1 again leads with 982.86, with GPT 4.1 following at 979.89\u003c/li\u003e\u003cli\u003eThis indicates high expected performance for GPT Image 1with relatively low uncertainty\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Rank percentages:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe8jjQCJegixK47fNhDT67NDHe8fhN6U0Kwm-mbkrIa9HWWFcwKbIubLDO7uy0_KtVVu1gHOcIk0Zh9VOHGgV6zrBBkGxbaCHW9a5Uu48lSo8L-J_48nAq6Q-bFqD9k-aYwPvY3?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT 4.1 achieves the top rank 60.14% of the time, followed by GPT Image 1 at 59.31%\u003c/li\u003e\u003cli\u003eThis shows GPT 4.1 consistency in achieving top results, but also highlights GPT Image 1 and DALL-E 3’s strong performance\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4) Average rank (lower better):\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemzdbe3m9BuWRyjcZTnpDJMSAn8a4UuMJTaepEJHsQwJ_JocoixeC4UQcftoenMWuZ3y9tToZU7UtSwxRMNCVdLwytjw72Tq_8bMC5MAJOxB0VYMpE4P4-EcVxs-uqZsEPlM6K?key=Ac1zIy2DfMoJh5TpVF8KNw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"297\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eNote: Lower score is better here so GPT 4.1 leads in average rank.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eGPT 4.1 slightly edges out GPT Image 1 with an average rating of 1.4 vs 1.41 (lower is better)\u003c/li\u003e\u003cli\u003eThis suggests GPT 4.1 performs well in direct comparisons despite lower Elo and TrueSkill ratings\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese metrics provide a multi-faceted view of model performance, allowing users to understand not just which model is \"best\" overall, but which might be most suitable for their specific use case. For instance, while GPT Image 1 and GPT 4.1 leads in most metrics, DALL-E 3’s and Imagen 3’s strong average rating suggests it is a reliable choice for consistent performance across a range of tasks.\u003c/p\u003e\u003ch2 id=\"join-the-revolution-beyond-the-benchmark\"\u003e\u003cstrong\u003eJoin the revolution: Beyond the benchmark\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Quantumworks Lab Leaderboards represent a significant advance in AI evaluation, pushing past traditional leaderboards by incorporating expert human evaluations for subjective generative AI models using comprehensive metrics. We are uniquely able to achieve this thanks to our modern AI data factory that combines human experts and our scalable platform with years of operational excellence evaluating AI models.\u003c/p\u003e\u003cp\u003eWe invite you to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCheck out the \u003ca href=\"http://labelbox.com/leaderboards?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e to explore our latest evaluations across various AI modalities and niche applications.\u003c/li\u003e\u003cli\u003e\u003cu\u003eLet us know\u003c/u\u003e if you have suggestions or want a specific model included in future assessments.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eContact us\u003c/u\u003e\u003c/a\u003e to learn more about how we can help you evaluate and improve your AI models across all modalities.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eReady to go beyond the benchmark? Let's redefine AI evaluation — together—and drive the field toward more meaningful, human-aligned progress that truly captures the capabilities of next-generation AI models.\u003c/p\u003e","comment_id":"684755a1e82e4e00013fe307","feature_image":"https://labelbox-guides.ghost.io/content/images/2025/06/guide_LeaderboxHero2.png","featured":false,"visibility":"public","created_at":"2025-06-09T21:44:01.000+00:00","updated_at":"2025-06-11T17:30:22.000+00:00","published_at":"2025-06-10T23:15:53.000+00:00","custom_excerpt":"Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"664e43b39132db0001f20548","name":"Michael Haag","slug":"michael","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/michael/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"664e43b39132db0001f20548","name":"Michael Haag","slug":"michael","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/michael/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/labelbox-leaderboards-redefining-ai-evaluation-with-human-assessments/","excerpt":"Introducing our groundbreaking Quantumworks Lab Leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Quantumworks Lab Leaderboards: Redefining AI Evaluation with Human Experts","meta_description":"Introducing our groundbreaking Quantumworks Lab leaderboards: an innovative, scientific process to rank multimodal AI models that goes beyond conventional benchmarks.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66cceba90693fd000117e1ca","uuid":"baef90d7-868a-4181-b171-0c1bfec81d5c","title":"Programmatically launch human data jobs for RLHF and evaluation","slug":"programmatically-launch-human-data-jobs-for-rlhf-and-evaluation","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eLabelbox’s Python SDK provides AI teams with a powerful approach to orchestrate human data labeling projects. In this guide, we’ll walk through how to harness the Python SDK to manage human data labeling jobs for RLHF and evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.\u003c/p\u003e\u003ch2 id=\"getting-started-set-up-the-labelbox-python-sdk\"\u003eGetting started: Set up the Quantumworks Lab Python SDK\u003c/h2\u003e\u003cp\u003eLet's begin by first setting up the Quantumworks Lab Python SDK in four simple steps:\u003c/p\u003e\u003cp\u003e1) \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCreate an API key\u003c/u\u003e\u003c/a\u003e to start using Quantumworks Lab Python SDK\u003c/p\u003e\u003cp\u003e2) pip install \"Quantumworks Lab[data]\" in terminal or !pip install \"Quantumworks Lab[data]\" in your notebook\u003c/p\u003e\u003cp\u003e3) Authentication can be done by saving your key to an environment variable:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003euser@machine:~$ export LABELBOX_API_KEY=\"\u0026lt;your_api_key\u0026gt;\"\nuser@machine:~$ python3\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e4) Then, import and initialize the API Client. \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab as lb \nclient = lb.Client()\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"importing-your-data-into-labelbox-methods-and-supported-formats\"\u003eImporting your data into Quantumworks Lab: Methods and supported formats\u003c/h2\u003e\u003cp\u003eNow that the SDK has been set up,\u0026nbsp; let's look at an example of uploading LLM response evaluation data for RLHF:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003e# Create a dataset\ndataset = client.create_dataset(\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"RLHF asset upload example\"+str(uuid.uuid4()),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;iam_integration=None\n)\n# Upload assets\ntask = dataset.create_data_rows([\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_1.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;},\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_2.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;},\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;{\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"row_data\": \"https://storage.googleapis.com/labelbox-datasets/conversational-sample-data/pairwise_shopping_3.json\",\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\"global_key\": str(uuid.uuid4())\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;}\n\u0026nbsp;\u0026nbsp;])\ntask.wait_till_done()\nprint(\"Errors:\",task.errors)\nprint(\"Failed data rows:\", task.failed_data_rows)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLearn more about all supported data types and editors \u003ca href=\"https://docs.labelbox.com/docs/label-data?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003ch2 id=\"creating-an-ontology-using-the-sdk\"\u003eCreating an ontology using the SDK\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWith the data imported, the next step is to create your ontology for the project. The ontology defines the structure and relationships within the data for your labeling process. Below is an example of how to create an ontology using the Quantumworks Lab Python SDK:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003eimport Quantumworks Lab as lb\nontology_builder = lb.OntologyBuilder(\n\u0026nbsp;\u0026nbsp;classifications=[\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.TEXT,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Free form text example\"),\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.CHECKLIST,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Checklist example\",\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"first_checklist_answer\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"second_checklist_answer\")\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;),\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.RADIO,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Radio example\",\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;scope=lb.Classification.Scope.INDEX,\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"first_radio_answer\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"second_radio_answer\")\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Classification(\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;class_type=lb.Classification.Type.RADIO,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;name=\"Rank #1\", # More ranks can be created like this\u0026nbsp; with N number of options\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;required = True,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;options=[\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 1\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 2\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;lb.Option(value=\"Option 3\"),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;,)\n\u0026nbsp;\u0026nbsp;]\n)\n\n\nontology = client.create_ontology(\"RLHF classification example\", ontology_builder.asdict(), media_type=lb.MediaType.Conversational)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eFor more information about ontology creation, please refer to the \u003ca href=\"https://docs.labelbox.com/reference/ontology-examples?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edocumentation\u003c/u\u003e\u003c/a\u003e for more examples.\u003c/p\u003e\u003ch2 id=\"best-practices-for-ontology-design\"\u003eBest practices for ontology design\u003c/h2\u003e\u003ch3 id=\"leverage-existing-ontologies-wisely\"\u003eLeverage existing ontologies wisely\u003c/h3\u003e\u003cp\u003eLabelbox allows users to reuse ontologies from previous projects, saving time and ensuring consistency across related tasks. However, be cautious when modifying shared ontologies:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCopy existing ontologies: To prevent unintended changes to previous projects, create a copy of an existing ontology. This creates a new schema node while retaining all your classes.\u003c/li\u003e\u003cli\u003eUsers can customize the ontology for their current project. After copying, they can freely modify the ontology to suit the new project's needs without affecting earlier work.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"optimize-object-ordering-for-logical-workflows\"\u003eOptimize object ordering for logical workflows\u003c/h3\u003e\u003cp\u003eThe order of objects in the ontology can significantly impact the labeling process:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePrioritize common objects: Create the most frequently used objects first. They'll appear at the top of the list, making them easily accessible to labelers.\u003c/li\u003e\u003cli\u003eDesign a logical flow: For complex tasks like model response comparisons, structure the ontology to guide labelers through a step-by-step analysis:\u003c/li\u003e\u003c/ul\u003e\u003col\u003e\u003cli\u003eStart with individual model evaluation criteria.\u003c/li\u003e\u003cli\u003ePlace comparative questions (e.g., \"Which model response is best?\") at the end.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis approach ensures labelers have thoroughly analyzed each option before making final comparisons.\u003c/p\u003e\u003ch3 id=\"enhance-visual-clarity-with-color-coding\"\u003eEnhance visual clarity with color coding\u003c/h3\u003e\u003cp\u003eImprove the visual experience for labelers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConsistent color schemes: Assign and edit colors for each object in the ontology.\u003c/li\u003e\u003cli\u003eMaintain color consistency: Use the same colors throughout the project to reduce cognitive load and improve labeling speed and accuracy.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"provide-easy-access-to-labeling-instructions\"\u003eProvide easy access to labeling instructions\u003c/h3\u003e\u003cp\u003eMake sure labelers have all the information they need at their fingertips:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAttach PDF instructions: Upload labeling guidelines as a PDF document.\u003c/li\u003e\u003cli\u003eSide-by-side viewing: Labelers can reference the instructions within Quantumworks Lab, displayed alongside the project for convenient access.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"use-advanced-classification-features\"\u003eUse advanced classification features\u003c/h3\u003e\u003cp\u003eTake advantage of Quantumworks Lab's classification capabilities to create more nuanced and accurate labels:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImplement nested classifications: This allows for more detailed object identification. For example, after drawing a segmentation mask over a tree, labelers can further classify it as healthy or unhealthy.\u003c/li\u003e\u003cli\u003eSet required questions: Ensure critical information is always captured by making certain questions mandatory for each asset.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy following these best practices, users will create more efficient labeling jobs, leading to higher quality data and improved model performance.\u003c/p\u003e\u003ch2 id=\"labelbox-labeling-services\"\u003eLabelbox labeling services\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFor Enterprise plan users, Quantumworks Lab offers data labeling services, connecting them with professional labelers to process large amounts of data quickly and efficiently. Key features include:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Rapid Data Processing\u003c/strong\u003e: Quickly handle large volumes of data without the overhead of hiring additional staff.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Specialized Expertise\u003c/strong\u003e: Access labelers with specialized knowledge, including:\u003c/p\u003e\u003col\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eMedical experts\u003c/li\u003e\u003cli\u003eVarious language specialists\u003c/li\u003e\u003cli\u003eOther certified specialties\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003c/ol\u003e\u003cp\u003e\u003cstrong\u003e3) Flexibility\u003c/strong\u003e: Scale your labeling service up or down based on project needs without long-term commitments.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4) Quality Assurance\u003c/strong\u003e: Professional labelers are trained to maintain high standards of accuracy and consistency.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5) Time and Resource Savings\u003c/strong\u003e: Eliminate the need for recruitment, training, and management of an in-house labeling team.\u003c/p\u003e\u003cp\u003eBy leveraging labeling services, enterprise users can significantly accelerate their data labeling projects, especially when dealing with complex datasets or when requiring domain-specific expertise. This service complements Quantumworks Lab's robust data import and management capabilities, providing a comprehensive solution for large-scale AI and machine learning projects.\u003c/p\u003e\u003cp\u003eTo leverage labeling services, Quantumworks Lab provides programmatic methods to request labeling services as, shown here:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Getting labeling service information:\u003c/strong\u003e Users can retrieve information about the labeling service for a specific project:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003elabeling_service = project.get_labeling_service()\nprint(labeling_service)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e This will return details such as the service ID, project ID, creation date, status, and more.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Requesting labeling services for faster results:\u003c/strong\u003e Once data and an ontology with instructions has been added, users can initiate a boost request:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-Python\"\u003elabeling_service.request()\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e This call initiates the labeling services service for your project.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Monitoring your labeling service’s status\u003c/strong\u003e:The Quantumworks Lab labeling service requested can be easily monitored via the UI as shown below:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXekVuCZ2W9OORuo0lpeibkcuoC79eXyUJ1KIU4s1OYsnE_VgNaLcMBLaJ5OHsc57ae_3BfSIM3hv4GacVmkhbOvbPNb-PetLKLK4vUN7fDitccn5y-PIhV_nZj_OU7TeOln9Y-tRbyClHvSN3vXPHVczmk?key=Eb16GHK2ItC9fBn058nPbA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"799\"\u003e\u003c/figure\u003e\u003ch2 id=\"simple-export-via-the-labelbox-sdk\"\u003eSimple export via the Quantumworks Lab SDK\u003c/h2\u003e\u003cp\u003eOnce the labeling project is complete, users can easily export the labels using the SDK, as shown below.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eexport_task = project.export(params=export_params, filters=filters)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis simple command allows users to retrieve labeled data that is ready for use in machine learning pipelines. Please refer to \u003ca href=\"https://docs.labelbox.com/reference/export-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edocumentation\u003c/u\u003e\u003c/a\u003e for flexible ways of exporting a project with filters.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab Python SDK offers teams with a convenient and powerful way to programmatically manage human data labeling projects. By providing control over every aspect of the labeling process - from data import and ontology design to project monitoring and data export - the SDK enables AI teams with the ability to incorporate high-quality labeled data into their workflows seamlessly.\u003c/p\u003e\u003cp\u003eWe hope you found this guide helpful for gaining a deeper understanding of how to capitalize on an SDK-driven approach to simplify complex tasks and enhance productivity. Whether you’re working on small-scale projects or large, distributed labeling efforts, the Quantumworks Lab SDK offers the full-suite of tooling needed to efficiently manage your\u0026nbsp; data labeling needs and accelerate their AI development process.\u003c/p\u003e\u003cp\u003eIf you're interested in implementing an SDK approach to jumpstart your human data jobs for RLHF and model evaluation,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out, or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more.\u003c/p\u003e","comment_id":"66cceba90693fd000117e1ca","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-26-at-2.03.08-PM.png","featured":false,"visibility":"public","created_at":"2024-08-26T20:55:05.000+00:00","updated_at":"2024-08-28T15:34:08.000+00:00","published_at":"2024-08-26T21:03:56.000+00:00","custom_excerpt":"Learn how to harness the SDK to manage human data labeling jobs for RLHF and model evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/programmatically-launch-human-data-jobs-for-rlhf-and-evaluation/","excerpt":"Learn how to harness the SDK to manage human data labeling jobs for RLHF and model evaluation. With just a few steps, you can set up the SDK, import various types of data, and launch, monitor, and export labeling projects programmatically, all while ensuring data quality and scalability.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bfded301c06500016e8b1c","uuid":"a2c40e88-6376-4824-b57f-e0e3673d8b1b","title":"Evaluating leading text-to-speech models","slug":"evaluating-leading-text-to-speech-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in speech generation evaluation, visit \u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe evolution of Text-to-Speech (TTS) models has been groundbreaking, with various companies offering high-quality solutions for a range of applications. Whether it's enhancing accessibility, automating customer service, or generating voiceovers, TTS models play a vital role in today's digital landscape. \u003c/p\u003e\u003cp\u003eWe recently evaluated the performance of six leading TTS models—Google TTS, Cartesia, AWS Polly, OpenAI TTS, Deepgram, and Eleven Labs—using key evaluation metrics such as Word Error Rate (WER) and human preference ratings.\u003c/p\u003e\u003cp\u003eTo provide an in-depth analysis, we evaluated each model on 500 prompts. Each prompt was reviewed by three expert labelers to ensure accuracy and provide diverse perspectives.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"evaluation-process\"\u003e\u003cstrong\u003eEvaluation Process\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate speech outputs across several key criteria:\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer\"\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWER measures the accuracy of the generated speech by calculating the number of insertions, deletions, and substitutions compared to a reference transcription. The lower the WER, the better the model performed in terms of generating accurate and coherent speech.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1120\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.16-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"word-errors\"\u003eWord errors\u003c/h3\u003e\u003cp\u003eRaters evaluated the presence of word errors in the generated TTS audio by counting the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Insertion Errors\u003c/strong\u003e: Additional words not present in the original text.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Deletion Errors\u003c/strong\u003e: Words from the original text that were omitted.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Substitution Errors\u003c/strong\u003e: Incorrect replacements for original words.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: No word errors or only minor errors (0-1 error).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some errors (2-3 errors).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Significant errors (4 or more errors).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"speech-naturalness\"\u003eSpeech naturalness\u003c/h3\u003e\u003cp\u003eRaters assessed how human-like the generated speech sounded.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Very human-like, natural flow with appropriate pauses and inflections.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Some human qualities but with occasional robotic or awkward elements.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Clearly robotic or artificial, with choppy or monotone speech.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"pronunciation-accuracy\"\u003ePronunciation accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated how clearly and correctly words were pronounced in the TTS output.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: All words are pronounced clearly and correctly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: 1-2 words are mispronounced or unclear.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: 3 or more words are mispronounced or difficult to understand.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.28.35-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch3 id=\"noise\"\u003eNoise\u003c/h3\u003e\u003cp\u003eRaters evaluated the level of background noise or artifacts in the audio.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNone\u003c/strong\u003e: No detectable noise or artifacts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Minor noise, such as a faint hiss, but not distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Noticeable noise or artifacts that may be somewhat distracting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Significant noise or artifacts that interfere with comprehension.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"context-awareness\"\u003eContext awareness\u003c/h3\u003e\u003cp\u003eRaters assessed the TTS system's ability to adjust to context, including tone, emphasis, and punctuation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Excellent adaptation to context, with clear tonal shifts and pauses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Adequate adaptation but misses some nuances.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Little to no adaptation; reads text monotonously without context cues.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"prosody-accuracy\"\u003eProsody accuracy\u003c/h3\u003e\u003cp\u003eRaters evaluated the rhythm, stress, and intonation (prosody) of the generated speech.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Natural-sounding rhythm and intonation that closely mimics human speech.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Mostly natural but with occasional awkwardness in stress or rhythm.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Monotonous or robotic intonation with unnatural pauses and emphasis\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"prompts-curation-process\"\u003ePrompts curation process\u003c/h2\u003e\u003cp\u003eThe diverse set of prompts used in our TTS evaluation provides a comprehensive test of model capabilities across multiple dimensions. \u003c/p\u003e\u003cp\u003eConsider these examples:\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Wow! Did you see the fireworks last night? They were spectacular!\"\u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003e This short, exclamatory sentence tests the model's ability to convey excitement and emphasis. It challenges the TTS system to appropriately modulate tone and pitch to express enthusiasm.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"I wish you were on the right path...I-I-I truly do, but you're not,' he said sadly.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis prompt evaluates the model's handling of emotional nuance, hesitation (indicated by the stuttering \"I-I-I\"), and the use of ellipsis for pauses. It also tests the system's ability to convey sadness as specified in the dialogue tag.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cblockquote class=\"kg-blockquote-alt\"\u003e\u003cstrong\u003e\u003cem\u003e\"Alright, listen up, team! The mission briefing is as follows: Operation Nighthawk commences at 0300 hours. Rendezvous at coordinates 51°30'26.5\"N 0°07'39.0\"W. Your login credentials for the secure server are: Username: NightOwl_Alpha, Password: 3ch3l0n_X9#. Remember, this is a stealth operation. Radio silence must be maintained except for emergency protocol Sierra-Tango-Seven. If compromised, use the failsafe code: Zulu-Yankee-4-7-2-Bravo. Any questions? No? Then synchronize your watches... now. Good luck, and godspeed.\" \u003c/em\u003e\u003c/strong\u003e\u003c/blockquote\u003e\u003cp\u003eThis complex prompt tests multiple aspects:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical pronunciation of alphanumeric strings and military jargon\u003c/li\u003e\u003cli\u003eConveying urgency and authority\u003c/li\u003e\u003cli\u003eHandling of coordinates and precise numerical information\u003c/li\u003e\u003cli\u003eProper pacing for a lengthy, detailed instruction set\u003c/li\u003e\u003cli\u003eAppropriate pausing, indicated by punctuation and paragraph breaks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese prompts demonstrate how we cover a range of linguistic features, emotional tones, and formatting challenges. They span from simple exclamations to complex, technical instructions, ensuring a thorough assessment of prosody, emphasis, and versatility.\u003c/p\u003e\u003cp\u003eThis diversity in prompts matters because it enables a comprehensive evaluation that reflects real-world applicability. By covering such a broad spectrum of scenarios, we can identify specific strengths and weaknesses in each TTS model, allowing for fair comparisons and pinpointing areas for improvement. Whether it's expressing joy over fireworks, conveying emotional conflict, or delivering a detailed mission briefing, these prompts help us ensure that TTS models can handle the variety and complexity of human communication.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overall-ranking-best-to-worst-model-samples\"\u003eOverall Ranking best to worst model samples\u003c/h2\u003e\u003cp\u003eRaters ranked each TTS sample from best to worst based on performance across all categories above.\u003c/p\u003e\u003ch3 id=\"model-configurations-used-at-test-time\"\u003e\u003cstrong\u003eModel configurations used at test time\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: tts-1-hd\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: alloy\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle Text-to-Speech (TTS)\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel:\u003c/strong\u003e Default\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice: \u003c/strong\u003eDefault with neutral gender voice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eElevenLabs\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: eleven_monolingual_v1 (As of 30th September, eleven_monolingual_v1 is an outdated version and Quantumworks Lab will run a new experiment with\u0026nbsp;eleven_multilingual_v2 in the future)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: pMsXgVXv3BLzUgSXRplE\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: aura-asteria-en\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Default\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAmazon Polly\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Generative\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Ruth\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: sonic-english\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice\u003c/strong\u003e: Teacher Lady\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe selected these models for our evaluation as they represent the cutting-edge offerings from leading companies in the TTS space, showcasing the latest advancements in voice synthesis technology.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"results\"\u003eResults\u003c/h1\u003e\u003cp\u003eOur evaluation of 500 diverse and complex prompts provided valuable insights into the capabilities of Google, Cartesia, AWS Polly, OpenAI, Deepgram, and Eleven Labs TTS models. Each prompt was meticulously assessed by three different raters to ensure accurate and diverse perspectives. This rigorous testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of these leading TTS models:\u003c/p\u003e\u003ch3 id=\"rank-distribution-percentages\"\u003eRank distribution percentages\u003cstrong\u003e\u0026nbsp; \u0026nbsp; \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.34.33-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch2 id=\"overall-ranking\"\u003eOverall ranking\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1:\u003c/strong\u003e\u0026nbsp; (Open AI TTS) - occurred 607 times, 42.93%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2:\u003c/strong\u003e\u0026nbsp; (Cartesia) - occurred 460 times, 32.53%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 410 times, 29.00%\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 4:\u003c/strong\u003e\u0026nbsp; (AWS Polly) - occurred 338 times, 23.90%. Followed by Eleven labs as a close second with occurrence 335, 23.69%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 5:\u003c/strong\u003e\u0026nbsp; (Deepgram) - occurred 438 times, 30.98%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 6:\u003c/strong\u003e\u0026nbsp; (Google TTS) - occurred 868 times, 61.39%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote: Rank is determined by the most frequently selected model for each position, which is why some AWS Polly appears twice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1122\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-1.19.31-PM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003eFig: % of times each model was ranked first.\u003c/p\u003e\u003ch3 id=\"word-error-rate-wer-results\"\u003eWord Error Rate (WER) Results\u003c/h3\u003e\u003cp\u003eThe WER scores offer an objective look at how accurately each model converts text to speech.\u003c/p\u003e\u003cul\u003e\u003cli\u003eDeepgram: 5.67%\u003c/li\u003e\u003cli\u003eOpenAI TTS: 4.19%\u003c/li\u003e\u003cli\u003eGoogle TTS: 3.36%\u003c/li\u003e\u003cli\u003eAWS Polly: 3.18%\u003c/li\u003e\u003cli\u003eCartesia: 3.87%\u003c/li\u003e\u003cli\u003eEleven Labs: 2.83%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs achieved the lowest WER at 2.83%, making it the most accurate model. Deepgram, however, had the highest WER at 5.67%, indicating room for improvement in transcription accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1121\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.17-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" title=\"Chart\" width=\"2000\" height=\"1126\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.28-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1123\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-10.11.39-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-specific-performance-results\"\u003eModel-Specific Performance Results\u003c/h2\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each TTS model based on key evaluation criteria.\u003c/p\u003e\u003ch3 id=\"openai-tts\"\u003eOpenAI TTS\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 4.1898%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 89.60% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 87.13% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 92.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 63.37% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 64.57% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOpenAI TTS excels at producing natural-sounding speech, making it a top choice for applications requiring lifelike vocal output. It demonstrates exceptional accuracy in pronunciation and performs excellently in producing clean audio with minimal background noise. The model shows strong capabilities in understanding and conveying contextual nuances in speech and is proficient in delivering appropriate intonation and rhythm. Despite ranking high in human preference, it shows moderate accuracy in word reproduction.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cm24ur6wwf\" title=\"OpenAI: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gicqvkqc5e\" title=\"OpenAI: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kg9wqqftu4\" title=\"OpenAI: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"cartesia\"\u003e\u003cstrong\u003eCartesia\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 58.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 53.11% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 93.49% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 80.98% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: High in 77.02% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.8707%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCartesia demonstrates good accuracy in word reproduction. While not as consistent as OpenAI TTS in naturalness, it still performs very well in producing natural-sounding speech. It shows excellent accuracy in pronunciation and excels at producing clean audio with minimal background noise. The model displays good capabilities in understanding contextual nuances and performs well in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/3gukhi7z2j\" title=\"Cartesia: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y3lbes0sin\" title=\"Cartesia: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f0lmkto1xk\" title=\"Cartesia: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"aws-polly\"\u003e\u003cstrong\u003eAWS Polly\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: High in 56.29% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: High in 55.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.39% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 84.72% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Medium in 58.20% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.1770%\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAWS Polly shows very good accuracy in word reproduction, ranking second among the models. It struggles more with producing natural-sounding speech compared to top performers but maintains excellent pronunciation accuracy. The model performs well in producing clean audio and shows good capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qawx1dwc9g\" title=\"Polly: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/klmnfcsno5\" title=\"AWS Polly: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ubzrqtjnc9\" title=\"AWS Polly: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"eleven-labs\"\u003e\u003cstrong\u003eEleven Labs\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 2.8302%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: Medium in 44.98% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 81.97% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 80.27% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 44.70% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 46.25% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEleven Labs demonstrates the best accuracy in word reproduction among all models. It shows mixed results in producing natural-sounding speech but maintains excellent pronunciation accuracy. The model performs reasonably well in producing clean audio but has more noise issues than some competitors. It shows moderate capabilities in understanding contextual nuances and has room for improvement in delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/x3qzhkt0ab\" title=\"Eleven Labs: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kl0lc7oqmz\" title=\"Eleven Labs: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bc19r3iqip\" title=\"Eleven Labs: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"deepgram\"\u003e\u003cstrong\u003eDeepgram\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWord Error Rate (WER): 5.6743%\u003c/li\u003e\u003cli\u003eSpeech Naturalness: High in 57.78% of cases\u003c/li\u003e\u003cli\u003ePronunciation Accuracy: High in 64.43% of cases\u003c/li\u003e\u003cli\u003eNoise: None in 88.83% of cases\u003c/li\u003e\u003cli\u003eContext Awareness: Medium in 53.18% of cases\u003c/li\u003e\u003cli\u003eProsody Accuracy: Medium in 55.52% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDeepgram shows the highest error rate in word reproduction among all models. It performs moderately well in producing natural-sounding speech and demonstrates good accuracy in pronunciation, although lagging behind some competitors. The model performs well in producing clean audio with minimal background noise but shows moderate capabilities in understanding contextual nuances and delivering appropriate intonation and rhythm.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mngnvgtzx3\" title=\"Deepgram: Wow did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6h3bgmcg6d\" title=\"Deepgram: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uaj3x9bln8\" title=\"Deepgram: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"google-tts\"\u003e\u003cstrong\u003eGoogle TTS\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWord Error Rate (WER)\u003c/strong\u003e: 3.3574%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeech naturalness\u003c/strong\u003e: Low in 78.01% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePronunciation accuracy\u003c/strong\u003e: High in 77.30% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNoise\u003c/strong\u003e: None in 89.46% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext awareness\u003c/strong\u003e: Medium in 39.25% of cases\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProsody accuracy\u003c/strong\u003e: Low in 45.83% of cases\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z27ett9q8m\" title=\"Google TTS: I wish you were on the right path Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/06pqi5qc13\" title=\"Google TTS: Wow Did you see the fireworks last night Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kwotkto0u6\" title=\"Google TTS: The mission Audio\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"100%\" height=\"218px\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation reveals that while Eleven Labs leads in transcription accuracy with the lowest WER, OpenAI TTS emerges as the top choice when it comes to human preference, particularly for speech naturalness, pronunciation accuracy, and prosody.\u003c/p\u003e\u003cp\u003eConversely, Google TTS, despite a relatively low WER, ranks last in human preference due to poor performance in categories like speech naturalness and context awareness. This demonstrates the importance of a balanced evaluation that considers both quantitative metrics like WER and qualitative factors like user satisfaction.\u003c/p\u003e\u003cp\u003eAs the TTS field continues to evolve, the key to further advancements will lie in combining high transcription accuracy with natural, expressive, and context-aware speech. Models like OpenAI TTS and Eleven Labs are setting the standard, but there remains room for improvement across all models to fully meet the growing demands of diverse TTS applications.\u003c/p\u003e\u003cp\u003eIn summary, while WER is crucial, the broader context of user experience—focusing on factors like speech naturalness, context awareness, and prosody accuracy—is essential in evaluating the overall effectiveness of TTS models.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-speech models presented here represents a significant advancement in assessing AI-generated speech. By leveraging this methodology alongside Quantumworks Lab's cutting-edge platform, AI teams can dramatically accelerate the development and refinement of sophisticated, domain-specific speech generation models.\u003c/p\u003e\u003cp\u003eOur solution offers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eEfficient dataset curation\u003c/li\u003e\u003cli\u003eAutomated evaluation techniques\u003c/li\u003e\u003cli\u003eHuman-in-the-loop quality assurance\u003c/li\u003e\u003cli\u003eCustomizable workflows for TTS model assessment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-speech model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdohXa2KeUawTMTWS7MfKSUYC6fVPCHjtmcFCRt5ycmB7S4k4-JElTwnmRb6ufW2kR7F3Bqu1PwzbvL83LKcqgz8jbkoY02tefklFLp98wvqm2liOKBglEGaRITIk6Wx1tY3fi-43TaCJnpwEwRVCz7q101?key=Qy_gZn4bA_q73M6n7nwEIA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"1024\"\u003e\u003c/figure\u003e","comment_id":"66bfded301c06500016e8b1c","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-17-at-12.12.46-PM.png","featured":false,"visibility":"public","created_at":"2024-08-16T23:20:51.000+00:00","updated_at":"2024-11-26T00:11:38.000+00:00","published_at":"2024-08-17T19:13:25.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/evaluating-leading-text-to-speech-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-speech models using both human preference ratings and automated evaluation techniques.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66bff4f801c06500016e8bba","uuid":"e5bf7da8-d960-4a06-9050-b06f63a8a4c0","title":"Metrics-based RAG Development with Quantumworks Lab","slug":"metrics-based-rag-development-with-labelbox","html":"\u003ch1 id=\"overview\"\u003e\u003cstrong\u003eOverview\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOne of the biggest challenges with RAG Application development is evaluating the quality of responses. As it currently stands, there are a series of different benchmark metrics, tools, and frameworks to help with RAG application evaluation.\u003c/p\u003e\u003cp\u003eLLM evaluation tools (such as RAGAS and DeepEval) provide a plethora of quality scores designed to evaluate the efficiency of the RAG model, ranging from retrieval to generation. In this guide, we’ll take a metrics based approach to enhance RAG development by focusing on 2 target metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext recall\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eThis measures (on a scale from 0-1) how well the retrieved content aligns with the Ground Truth Answer\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eContext precision\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003eIdeally, the most relevant context chunks should be retrieved first. This metric measures whether the RAG application can return the most relevant chunks (with respect to the Ground Truth) at the top.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.56.04-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"generating-ground-truth-data-and-%E2%80%98synthetic%E2%80%99-data\"\u003e\u003cstrong\u003eGenerating ground truth data and ‘\u003cem\u003esynthetic’\u003c/em\u003e data\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo calculate performance metrics, we have to compare the RAG response with ground truth. Ground truths are factually verified responses to a given user query and must be linked back to a source document.\u003c/p\u003e\u003cul\u003e\u003cli\u003eOn the other hand, ground truth data can be automatically generated by pre-trained models. LLM Frameworks, such as LangChain, have modules that generate Question / Answer pairs based on a source document. While creating Synthetic ground truth is less labor intensive, drawbacks include quality issues, lacking real-world nuances, and inheriting model bias.\u003c/li\u003e\u003cli\u003eGround truth data is typically manually created by experts through the collection and verification of internal source documents. While labor intensive and slower to collect, manual data labeling optimizes for accuracy and quality.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.57.12-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo start off, we’ll use the Quantumworks Lab Platform to orchestrate a ground truth creation workflow with 2 approaches.\u003c/p\u003e\u003col\u003e\u003cli\u003eManual ground truth generation\u003c/li\u003e\u003cli\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"manual-ground-truth-generation\"\u003e\u003cstrong\u003eManual ground truth generation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eBy uploading common user questions (prompts) to the Quantumworks Lab Platform, we can set up an annotation project. Now subject matter experts can craft a thoughtful answer and provide the appropriate source.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"938\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.35-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"synthetic-ground-truth-generation-with-human-in-the-loop\"\u003e\u003cstrong\u003eSynthetic ground truth generation with Human-in-the-Loop\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1154\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.58.46-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeveraging the latest Foundation Models on Quantumworks Lab Foundry, we can combine an automated ground truth generation process with a Human in the Loop component for quality assurance.\u003c/p\u003e\u003cp\u003eTo start off, we will include the source document for each user query (PDF in this case). We can then prompt a foundation model (Google Gemini 1.5 Pro in this case) for each user question and attach the PDF context needed to answer the question.\u003c/p\u003e\u003cp\u003eThe provided prompt in this case is: *For the given text and PDF attachment, answer the following. Given the attached PDF, generate an answer using the information from the attachment. Respond with I don't know if the PDF attachment doesn't contain the answer to the question.*\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"724\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.21-PM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eNext, we can include the response generated from Gemini \u003cstrong\u003eas pre-labels\u003c/strong\u003e to an annotation project.\u0026nbsp;\u003c/p\u003e\u003cp\u003eInstead of starting from scratch while answering the question, human experts can modify and verify the Gemini response along with the corresponding document and user query, optimizing for quality and efficiency.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1480\" height=\"570\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.00.14-PM.png 1480w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"improving-rag\"\u003eImproving RAG\u003c/h1\u003e\u003cp\u003eA RAG based application consists of many components. Summarized in a recent research survey by Gao et al. (2023), a few factors that impact RAG system performance include (but not limited to):\u003c/p\u003e\u003cul\u003e\u003cli\u003eImproving the retriever (e.g. Reranking)\u003c/li\u003e\u003cli\u003eFine-tuning the embedding model for source documents\u003c/li\u003e\u003cli\u003eAdding document metadata tagging (for rerouting or semantic search)\u003c/li\u003e\u003cli\u003eQuery rewriting\u003c/li\u003e\u003cli\u003eOptimizing the chunking strategy\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’ll focus on the following strategies for improving metrics based RAG evaluation:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eOptimizing the chunking strategy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-tuning the embedding model\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1246\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-6.00.09-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"chunking-strategy\"\u003e\u003cstrong\u003eChunking strategy\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhile keeping LLM generation model (GPT Turbo 3.5) and Embeddings Model (sentence-transformers/distiluse-base-multilingual-cased-v2) consistent, we will compare the retrieval metrics derived from RAGAs.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will export the question and response pairs generated from Quantumworks Lab to a Python Environment.\u003c/li\u003e\u003cli\u003eNext we will generate the RAG response based on the following chunking strategies:\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRecursive character splitter\u003c/strong\u003e (500 token size with 150 chunk overlap)\u003cul\u003e\u003cli\u003eThis is a heuristic approach and involves dynamically splitting text based on a set of rules (e.g. new lines / tables / page breaks) while maintaining coherence.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpacy text splitter\u003c/strong\u003e\u003cul\u003e\u003cli\u003espaCy uses a rule-based approach combined with machine learning models to perform tokenization.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnce the RAG response and context have been generated, we create a table with the following fields required for RAGAs metrics evaluation: (Question, Ground Truth, RAG Answer, Context)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"432\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-5.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-5.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-5.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eBy calculating RAGAs metrics evaluation using the necessary Query, Response, Ground Truth, and Source information, we can derive performance results for the entire dataset and aggregate the findings. The Context Precision and Context Recall metrics for Recursive and Spacy are shown below:\u003c/p\u003e\u003ch3 id=\"recursive-text-chunking\"\u003e\u003cstrong\u003eRecursive text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1008\" height=\"624\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-4.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-4.png 1008w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"spacy-text-chunking\"\u003e\u003cstrong\u003eSpacy text chunking\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1054\" height=\"652\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-3.png 1054w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eFrom the results, we observe that Spacy yielded slightly better context precision. Given the volume of our dataset (~100 QA pairs), the difference is likely statistically insignificant. However, we can conclude that choosing the Chunking strategy and parameters will certainly affect the output of the chatbot.\u003c/p\u003e\u003cp\u003eGiven the distribution of our Recall and Precision metrics, both techniques seem to struggle with the same data rows. This indicates that the key semantics are not captured by our default embeddings (\u003cstrong\u003esentence-transformers/distiluse-base-multilingual-cased-v2\u003c/strong\u003e). We’ll test out another embedding model to see if we can improve RAG performance metrics.\u003c/p\u003e\u003ch2 id=\"embeddings-model\"\u003e\u003cstrong\u003eEmbeddings model\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAn Embedding Model is responsible for generating embeddings, which is a vector representation to any piece of information, such as a chunk of text in a RAG application. A solid embeddings model is able to better capture semantic meaning of a user question and the corpus of source documents, therefore returning the most relevant chunks as the context for answer generation.\u003c/p\u003e\u003cp\u003eWhile holding the chunking strategy constant (Recursive), we will change the underlying embeddings model in the RAG workflow before evaluating with RAGAs. The Huggingface\u003ca href=\"https://huggingface.co/spaces/mteb/leaderboard?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003eLeaderboard\u003c/u\u003e\u003c/a\u003e includes performance benchmarks for various embeddings models. Beyond retrieval performance, model size, latency, and context length are also important when choosing an embeddings model.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo start off, we will generate responses using the \u003cstrong\u003ebge-base-en-v1.5\u003c/strong\u003e model and evaluate with RAGAs\u003cul\u003e\u003cli\u003eWhile ranking 35 on the leaderboards, this model is extremely lightweight (109 million parameters)\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1096\" height=\"632\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-2.png 1096w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eFrom the results, changing the embeddings model (from distiluse-base-multilingual-cased-v2 to bge-base-en-v1.5) significantly improved the retrieval metrics, around ~10 points for both context recall and precision.\u003cul\u003e\u003cli\u003eThis indicates that the retrieved context chunks are more relevant to the ground truth and ranked higher on average as well.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"fine-tuning-an-embeddings-model\"\u003e\u003cstrong\u003eFine-tuning an embeddings model\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eMost embedding models are trained on general knowledge. Specific domains, such as internal organization information, may limit the effectiveness of these models. Therefore, we can choose to fine-tune an embeddings model with the goal of better understanding and generating domain specific information.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo fine tune embeddings, we need labeled examples of positive and negative context chunks.\u003cul\u003e\u003cli\u003eUsing a training dataset, we’ll upload the user query and the top X chunks to Quantumworks Lab\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing Quantumworks Lab Annotate, subject matter experts can determine if each chunk is relevant in answering the given question.\u003c/li\u003e\u003cli\u003e\u003c/li\u003e\u003cli\u003eUsing the Quantumworks Lab SDK we can now export our labels and convert to the following format for fine tuning: {\"query\": str, \"pos\": List[str], \"neg\":List[str]}\u003cul\u003e\u003cli\u003eThe fine tuning process for the BGE family of models is also described\u003ca href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/examples/finetune/README.md?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eUsing the fine tuned embeddings model, we can generate updated responses and evaluate.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom the example in the screenshot, bge tends to incorrectly extract the citations section, which the expert labelers can correct and mark as irrelevant.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1005\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the resulting metrics, we can see that by fine tuning on 50 data rows is able to improve context recall and context precision by 2-3 points each. We expect this to improve as more labels are provided to continuously fine tune the embeddings model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1098\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/image.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/image.png 1098w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eEvaluating the quality of responses in developing RAG Applications is essential for creating efficient and reliable systems. By incorporating the Quantumworks Lab platform in metrics based RAG development, from ground truth generation to feedback for embedding models, developers can enhance the overall performance and reliability of RAG applications. \u003c/p\u003e","comment_id":"66bff4f801c06500016e8bba","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-16-at-5.55.12-PM.png","featured":false,"visibility":"public","created_at":"2024-08-17T00:55:20.000+00:00","updated_at":"2024-09-03T20:00:44.000+00:00","published_at":"2024-08-17T01:07:55.000+00:00","custom_excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/metrics-based-rag-development-with-labelbox/","excerpt":"Learn how to optimize your Retrieval-Augmented Generation (RAG) applications by focusing on key metrics like context recall and precision. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66b28fe6c2887d000107cdde","uuid":"40792083-45ad-4606-8583-7781bc74c305","title":"Unlocking precision: The \"Needle-in-a-Haystack\" test for LLM evaluation","slug":"unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation","html":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\u003cp\u003eSelecting the optimal large language model (LLM) for specific tasks is crucial for maximizing efficiency and accuracy. One of the key challenges faced by teams is selecting the best models for pre-labeling tasks, especially when dealing with large datasets and complex annotations.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox’s Model Foundry provides a robust platform for evaluation and determining the most suitable model for various applications. To illustrate this, the Quantumworks Lab Labs team conducted an experiment simulating the \"Needle-in-a-Haystack\" test. This test involves identifying specific elements within vast amounts of data, ensuring the model’s precision and reliability.\u003c/p\u003e\u003cp\u003eBy utilizing Quantumworks Lab Model Foundry’s advanced experiments and evaluation tools, teams can compare multiple LLMs to identify the one that delivers the highest accuracy and efficiency for pre-labeling on complex tasks, thus saving time and enhancing the quality of predictions.\u003c/p\u003e\u003cp\u003eIn this blog post, we’ll dive into the intricacies of the \"Needle-in-a-Haystack\", exploring how to leverage Foundry to find the best model for your pre-labeling or data enrichment needs.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"the-needle-in-a-haystack-test\"\u003eThe \"Needle-in-a-Haystack\" test\u003c/h1\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test is a specialized evaluation method designed to gauge the performance of large language models (LLMs) in identifying specific, often infrequent, elements in large datasets.\u0026nbsp;\u003c/p\u003e\u003cp\u003eImagine you have a massive dataset filled with a mix of common and rare pieces of information, similar to a haystack with a few needles hidden inside. The challenge is to determine how effectively a model can find those needles (rare information) without getting distracted by the surrounding hay (common information ).\u0026nbsp; This rare information could be anything from specific keywords in a text document to unique objects in a video.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"669\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/08/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePrevious analysis of Claude-2.1 \u003c/span\u003e\u003ca href=\"https://twitter.com/GregKamradt/status/1722386725635580292?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eperformed by Greg Kamradt\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e, member of the ARC Prize team.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"628\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/08/image-2.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePrevious analysis of GPT-4 \u003c/span\u003e\u003ca href=\"https://twitter.com/GregKamradt/status/1722386725635580292?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eperformed by Greg Kamradt\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e, member of the ARC Prize team.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"why-is-this-a-good-test-for-model-foundry\"\u003e\u003cstrong\u003eWhy is this a good test for Model Foundry?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test is a great fit for Model Foundry for several reasons:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReal-world relevance:\u003c/strong\u003e This test simulates real-world conditions, where critical information is buried in a large dataset. This ensures that models are being simulated in environments that match actual applications.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eComprehensive evaluation:\u003c/strong\u003e Quantumworks Lab Model Foundry offers advanced tools that make setting up experiments, running evaluations, and comparing results efficient and easy.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eBetter decision making:\u003c/strong\u003e The insights gained from the Needle in a Haystack test can facilitate stronger decision-making when we are choosing the most suitable LLM for a task. This ensures investment in models that offer the best performance for application.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch1 id=\"creating-the-needle-in-a-haystack-internally\"\u003eCreating the \"Needle-in-a-Haystack\" internally\u003c/h1\u003e\u003cp\u003eThe first step in our experiment was to create a detailed labeling instructions set that we could eventually send to LLMs for pre-labeling. It is important to note that we decided to use Text data for our study. Various other asset types such as Video and Image can also emulate a similar test.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1254\" height=\"698\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.34-PM.png 1254w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"instructions-overview\"\u003e\u003cstrong\u003eInstructions overview\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe wanted to build a dataset that consisted of conversations between users and a customer support chatbot, focusing on banking and financial transactions. Each conversation would be categorized into specific issues related to accounts, banking services, and transactions.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs a result, our instruction set would include detailed descriptions of each category, example conversations to guide the labeling process, and clear decision-making guidelines to help annotators distinguish between closely-related issues.\u003c/p\u003e\u003ch2 id=\"ontology\"\u003e\u003cstrong\u003eOntology\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe ontology included categories such as:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eACCOUNT_TERMINATION\u003c/li\u003e\u003cli\u003eACCOUNT_RECOVERY\u003c/li\u003e\u003cli\u003eACCOUNT_SECURITY_BREACH\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_UPDATE_DETAILS\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eBANKING_LOAN_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_FEE_DISPUTE\u003c/li\u003e\u003cli\u003eBANKING_OVERDRAFT_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_WIRE_TRANSFER_HELP\u003c/li\u003e\u003cli\u003eBANKING_SAVINGS_PLANS\u003c/li\u003e\u003cli\u003eBANKING_INVESTMENT_SERVICES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eBANKING_INSURANCE_PRODUCTS\u003c/li\u003e\u003cli\u003eBANKING_MOBILE_APP_SUPPORT\u003c/li\u003e\u003cli\u003eBANKING_DEBIT_CARD_ACTIVATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eTRANSACTION_DISPUTE\u003c/li\u003e\u003cli\u003eTRANSCATION_REFUND\u003c/li\u003e\u003cli\u003eTRANSACTION_VERIFICATION\u003c/li\u003e\u003cli\u003eTRANSACTION_FRAUD_REPORT\u003c/li\u003e\u003cli\u003eTRANSACTION_LIMIT_INCREASE\u003c/li\u003e\u003cli\u003eTRANSACTION_HISTORY_QUERY\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAs seen, we chose closely-correlated categories and provided precise instructions so that while there were many similarities between subcategories, there were slight differences and nuances that our chosen LLM would have to notice and use to drive the decision-making process.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"building-the-dataset\"\u003e\u003cstrong\u003eBuilding the dataset\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCreating the dataset involved curating and structuring the data row to reflect real-world scenarios that modeled the above ontology. This ensured the dataset was comprehensive and challenging for the models.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"dataset-composition\"\u003e\u003cstrong\u003eDataset composition\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData Row Content: \u003c/strong\u003eEach data row represented a conversation between a user and customer support chatbot.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"sample-data-rows\"\u003e\u003cstrong\u003eSample data rows\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eACCOUNT_TERMIATION: User conversations requesting closure of their account\u003c/li\u003e\u003cli\u003eBANKING_LOAN_SERVICES: Inquiries about applying for or managing loans\u003c/li\u003e\u003cli\u003eTRANSACTION_FRAUD_REPORT: Reports of suspected fraudulent activities\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo view our labeling instructions, click the link \u003ca href=\"https://storage.googleapis.com/labelbox-datasets/lb_rahul/pdfs/Customer%20Support%20Ticket%20LLM%20Instructions.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"model-evaluation-using-foundry-llms\"\u003eModel evaluation using Foundry LLMs\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1246\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.02.49-PM.png 1246w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"model-selection\"\u003e\u003cstrong\u003eModel selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe decided to evaluate our dataset on four leading LLMs currently on the market:\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"gemini-15-pro\"\u003e\u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eReleased by Google as part of the Gemini series;\u003c/li\u003e\u003cli\u003eKnown for its strong multimodal capabilities;\u003c/li\u003e\u003cli\u003eDesigned for complex reasoning and task completion.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gpt-4o\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eDeveloped by OpenAI;\u003c/li\u003e\u003cli\u003eAn advanced iteration of the GPT (Generative Pre-trained Transformer) series;\u003c/li\u003e\u003cli\u003eKnown for its strong natural language understanding and generation;\u003c/li\u003e\u003cli\u003eOptimized for faster response times and efficient computational resource usage.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"claude-35-sonnet\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eCreated by Anthropic;\u003c/li\u003e\u003cli\u003ePart of the Claude 3 model family;\u003c/li\u003e\u003cli\u003eKnown for its strong performance in writing and complex tasks;\u003c/li\u003e\u003cli\u003eCapable of engaging in nuanced conversations and providing detailed explanations.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gemini-15-flash\"\u003e\u003cstrong\u003eGemini 1.5 Flash\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eAnother model in the Google Gemini series;\u003c/li\u003e\u003cli\u003eOptimized for speed and efficiency;\u003c/li\u003e\u003cli\u003eDesigned for tasks requiring quick responses;\u003c/li\u003e\u003cli\u003eSuitable for applications where real-time responses are crucial.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"analysis-and-insights\"\u003e\u003cstrong\u003eAnalysis and insights\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1314\" height=\"712\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.08-PM.png 1314w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce we had created Model Foundry Predictions on our dataset for all four LLMs, we placed them into a Model Experiment for model evaluation. Creating an experiment allowed us to dive deeply into the intricacies of each model to determine their overall performance on a needle in a haystack application.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFrom the exhibits above, we can see which models performed best from an precision perspective:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eGemini 1.5 Pro (81.55%)\u003c/li\u003e\u003cli\u003eClaude 3.5 Sonnet (80.98%)\u003c/li\u003e\u003cli\u003eGPT-4o (79.02%)\u003c/li\u003e\u003cli\u003eGemini 1.5 Flash (76.96%)\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1220\" height=\"772\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.40-PM.png 1220w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eConfusion matrices and Precision graphs are also available in Model Experiment, giving us a better understanding of the above precision scores.\u003c/p\u003e\u003cp\u003eFrom the graphs and further analysis, we can see the categories in the ontology that each model struggled with. Note that a struggle indicates a precision score of less than 0.75.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1306\" height=\"734\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-03-at-1.03.56-PM.png 1306w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"gemini-15-pro-1\"\u003e\u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"claude-35-sonnet-1\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_MISC_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gpt-4o-1\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_CREDIT_CARD_ISSUES\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eBANKING_MISC_QUERY\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"gemini-15-flash-1\"\u003e\u003cstrong\u003eGemini 1.5 Flash\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBANKING_FEE_DISPUTE\u003c/li\u003e\u003cli\u003eBANKING_INSURANCE_PRODUCTS\u003c/li\u003e\u003cli\u003eACCOUNT_ID_CONFIRMATION\u003c/li\u003e\u003cli\u003eBANKING_SECURITY_FEATURES\u003c/li\u003e\u003cli\u003eBANKING_POLICY_INFO\u003c/li\u003e\u003cli\u003eBANKING_EXTERNAL_QUERY\u003c/li\u003e\u003cli\u003eBANKING_MISC_QUERY\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBased on the performance breakdown, we can draw several insights:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTop performers\u003c/strong\u003e: Gemini 1.5 Pro and Claude 3.5 Sonnet emerge as the leading models for this particular needle in a haystack task, with very similar performance profiles.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCommon challenges\u003c/strong\u003e: All models struggled with certain categories, particularly ACCOUNT_ID_CONFIRMATION and BANKING_CREDIT_CARD_ISSUES. This suggests these categories may be inherently more difficult to classify or may require more specific training data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrecision vs. Speed\u003c/strong\u003e: While Gemini 1.5 Pro achieved the highest accuracy, teams should consider their specific needs. If real-time responses are crucial, Gemini 1.5 Flash might be a better choice despite its lower accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRoom for improvement\u003c/strong\u003e: Even the top-performing models have areas where they struggle. This information can be valuable for fine-tuning models or adjusting the labeling instructions for future iterations.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch1 id=\"leveraging-model-foundry-for-decision-making-and-pre-labeling\"\u003eLeveraging Model Foundry for decision making and pre-labeling\u003c/h1\u003e\u003cp\u003eThe experiment demonstrates the power of Quantumworks Lab Model Foundry in facilitating data-driven decision-making for model selection and optimizing the pre-labeling process. By providing comprehensive evaluation tools and visualizations, Model Foundry enables teams to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompare \u003cstrong\u003emultiple\u003c/strong\u003e models \u003cstrong\u003esimultaneously\u003c/strong\u003e\u003c/li\u003e\u003cli\u003eIdentify \u003cstrong\u003especific\u003c/strong\u003e strengths and weaknesses of \u003cstrong\u003eeach\u003c/strong\u003e \u003cstrong\u003emodel\u003c/strong\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eMake informed decisions based on \u003cstrong\u003eprecision\u003c/strong\u003e, \u003cstrong\u003erecall\u003c/strong\u003e, and \u003cstrong\u003eoverall accuracy\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePinpoint\u003c/strong\u003e areas for potential model \u003cstrong\u003eimprovement\u003c/strong\u003e or \u003cstrong\u003efine-tuning\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to model evaluation, Model Foundry significantly enhances the pre-labeling workflow:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEfficient pre-labeling\u003c/strong\u003e: Once the best-performing model is identified, it can be seamlessly integrated into the pre-labeling pipeline, significantly reducing manual labeling efforts\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eQuality assurance\u003c/strong\u003e: By understanding model strengths and weaknesses, teams can strategically allocate human resources to review and correct pre-labels in categories where models struggle\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIterative improvement\u003c/strong\u003e: As more data is labeled and models are retained, teams can continuously evaluate and update their pre-labeling model, ensuring ongoing optimization of the labeling process.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost reduction\u003c/strong\u003e: By selecting the most accurate model for pre-labeling, teams can minimize the need for manual corrections, leading to substantial time and cost savings in large-scale labeling projects.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy leveraging Model Foundry for both decision-making and pre-labeling processes, teams can significantly enhance the efficiency and accuracy of their entire data labeling pipeline.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"next-steps\"\u003eNext Steps\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"634\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/2024/08/image-1.png 1000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo further improve model performance and decision-making, consider the following steps:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eFine-tune models on challenging categories.\u003c/li\u003e\u003cli\u003eConduct additional experiments with different data types or industry-specific datasets.\u003c/li\u003e\u003cli\u003eImplement regular evaluations and feedback loops to identify areas for improvement and adapt to changing requirements.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy continually refining your approach and leveraging the insights gained from Model Foundry, you can ensure that your team is always using the most effective LLM for your specific needs, driving efficiency and accuracy in your AI-powered workflows.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eThe \"Needle-in-a-Haystack\" test, as implemented through Quantumworks Lab Model Foundry, proves to be an effective method for evaluating LLM performance on complex, nuanced tasks. By simulating real-world scenarios and leveraging Model’s advanced evaluation tools, teams can select the most suitable model for their specific pre-labeling needs.\u003c/p\u003e\u003cp\u003eIn our experiment, Gemini 1.5 Pro and Claude 3.5 Sonnet demonstrated superior performance, but the choice between them (or other models) would depend on the specific requirements of the project, including factors like speed, resource efficiency, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs the field of AI continues to evolve rapidly, tools like Quantumworks Lab Model Foundry become increasingly valuable, enabling teams to stay at the forefront of the space by consistently evaluating and selecting the best models for their unique challengers.\u003c/p\u003e","comment_id":"66b28fe6c2887d000107cdde","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-2.55.05-PM.png","featured":false,"visibility":"public","created_at":"2024-08-06T21:04:38.000+00:00","updated_at":"2024-09-03T20:04:14.000+00:00","published_at":"2024-08-06T22:08:47.000+00:00","custom_excerpt":"Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our \"Needle-in-a-Haystack\" experiment. Learn how to enhance accuracy and efficiency in complex data annotations.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"66b29da3c2887d000107ce2e","name":"Raj Jain","slug":"raj","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/raj/"},{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"66b29da3c2887d000107ce2e","name":"Raj Jain","slug":"raj","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/raj/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/unlocking-precision-the-needle-in-a-haystack-test-for-llm-evaluation/","excerpt":"Discover how to choose the perfect large language model for your pre-labeling tasks by diving into our \"Needle-in-a-Haystack\" experiment. Learn how to enhance accuracy and efficiency in complex data annotations.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66b1410331089400019364ee","uuid":"2207c60e-9f86-4ef2-9613-168f8707aee5","title":"A comprehensive approach to evaluating text-to-video models","slug":"a-comprehensive-approach-to-evaluating-text-to-video-models","html":"\u003cp\u003eThe emergence of text-to-video AI models has marked a significant milestone in artificial intelligence, with models from Runway ML (Gen-3), Luma Labs, and Pika transforming written descriptions into dynamic and lifelike videos. This technology is reshaping industries from video production to digital marketing, democratizing visual storytelling.\u003c/p\u003e\u003cp\u003eHowever, despite their impressive capabilities, these models often fall short of human expectations, producing results that lack prompt adherence, realism, or fidelity to the input text. To accelerate the development of text-to-video models, it is crucial to establish comprehensive evaluation methodologies to pinpoint areas for improvement.\u003c/p\u003e\u003cp\u003eThis article presents a rigorous approach to assessing the strengths and limitations of Runway ML (Gen-3), Luma Labs, and Pika using human preference ratings. Let’s dive into how we systematically analyzed these leading models.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"human-preference-evaluation\"\u003e\u003cstrong\u003eHuman preference evaluation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human raters per prompt, allowing us to tap into a network of expert raters to evaluate video outputs across several key criteria:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1245\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.08.17-AM.png 2246w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eChecklist for evaluating text-to-video models using human preferences.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"prompt-adherence\"\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRaters assessed how well each generated video matched the given text prompt on a scale of high, medium, or low. For example, in the given prompt: “A peaceful Zen garden with carefully raked sand, bonsai trees, and a small koi pond.” Raters looked to see if there was prompt adherence by looking at the presence of key concepts for the prompt.\u003c/p\u003e\u003cul\u003e\u003cli\u003eIs there a garden?\u003c/li\u003e\u003cli\u003eDoes it look peaceful?\u003c/li\u003e\u003cli\u003eIs the sand present, and is it raked?\u003c/li\u003e\u003cli\u003eAre there bonsai trees?\u003c/li\u003e\u003cli\u003eIs the a small koi pond?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: If all or most of the key concepts are present.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e:\u0026nbsp; If half the key concepts are present.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: If less than half of key concepts are present .\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdENKR1QEaokNDikPODa-kTi209qknyV3PBGvEm5xa4QWqugwI5sx0zmlAZOIH2XrpQRjB9xuAAb4gshoWZfSl7mjgNmNDIWcL_bXxK4pqk0TnzP0-Xn7EG0LTznK4sBhXk1ZxuiG4p_WwoCXqx2d3dj5V_?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"509\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 1: \"A romantic Parisian street scene with couples walking and street musicians playing\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"video-realism\"\u003e\u003cstrong\u003eVideo realism\u003c/strong\u003e \u003c/h4\u003e\u003cp\u003eRaters assessed how closely the video resembled reality.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Realistic lighting, textures, and proportions.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Somewhat realistic but with slight issues in shadows or textures.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Animated or artificial appearance.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd4QhDzWX5l35W_ecjUSerUNd0sTSA_DtOLU2DjwPLenslTtGrRyf4tPjhEcwQdIxFw50JqfnPwk86brTWRtowMUFRHbMdG1hr-dqGZ69-nYFebJ9KVMslxPNvHQdjrBWPA6soRH5LZ3uUDZBggfO_tJSFl?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"504\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 2: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"video-resolution\"\u003e\u003cstrong\u003eVideo resolution\u003c/strong\u003e \u003c/h4\u003e\u003cp\u003eThis criterion evaluates the level of detail and overall clarity in the video.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: Fine details visible (e.g., individual leaves, fabric textures).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: Major elements are clear, but finer details are somewhat lacking.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: Overall blurry or lacking significant detail.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdbrYYgTHolz9LOsLQzJHJVpzFx7TWbyIXrutMv52jbQl6nu_qHjrEV5kFXbCH01iYVLRTgoJV3BDjOIdZKU8TOod8uA_Ev-5QemuPmYTYiymsr2XN7nMs6F2AbWoVph_NnkZ54BRHe2Ldq8-IJK2J1ggNj?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"502\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 3: \"An ancient temple in the jungle with hidden traps and treasures waiting to be discovered\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"artifacts\"\u003e\u003cstrong\u003eArtifacts \u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRaters identified any visible artifacts, distortions, or errors, such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnnatural distortions in objects or backgrounds\u003c/li\u003e\u003cli\u003eMisplaced or floating elements\u003c/li\u003e\u003cli\u003eInconsistent lighting or shadows\u003c/li\u003e\u003cli\u003eUnnatural repeating patterns\u003c/li\u003e\u003cli\u003eUnnatural movements\u003c/li\u003e\u003cli\u003eBlurred or pixelated areas\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eScoring\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh\u003c/strong\u003e: If all or 5 of the errors are present.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e: If 2 or 3 errors are present.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLow\u003c/strong\u003e: \u0026nbsp;If 1 or 0 errors are present.\u0026nbsp;\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcOz4iP5QSqnQJpBB5F4neKuaS-jElkj40E21sIahJZmgm2qEE2oBxWOhDBTriFa26xOPwHbpQUGPjzUMFIfIjzWxl8I8vI8Z904UjiO-Jut14igYMNzk55VBU0U82is1tavV3FJA7uSU1kXtjCEuAmhB6I?key=xH3vH8otpzvwcUCC5VHzFQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"502\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample prompt 4: \"Cat following a mouse\".\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"results\"\u003e\u003cstrong\u003eResults\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOur evaluation of 25 diverse set of complex prompts, generated by GPT-4, was stress-tested and provided valuable insights into the capabilities of Runway ML, Luma Labs and Pika. Each prompt was assessed by three different raters to ensure more accurate and diverse perspectives. This rigorous stress testing highlighted the strengths and areas for improvement for each model.\u003c/p\u003e\u003cp\u003eLet's delve into the insights and performance metrics of the top models: Runway ML (Gen-3), Luma Labs, and Pika.\u003c/p\u003e\u003ch3 id=\"overall-ranking-for-human-preference-evaluations\"\u003e\u003cstrong\u003eOverall ranking for human-preference evaluations\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRank 1\u003c/strong\u003e: Runway ML (Gen-3) ranked 1st in 65.22% of cases, Luma Labs in 18.84% and Pika in 15.94%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 2\u003c/strong\u003e: Luma Labs ranked 2nd in 59.42% of cases, Runway ML (Gen-3) in 21.74% and Pika in 18.84%\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRank 3\u003c/strong\u003e: Pika ranked 3rd in 65.22% of cases, Luma Labs in 21.74% and\u0026nbsp; Runway ML in 13.04%\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1248\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.21-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRank 1 standings: Runway ML (Gen-3), Luma Labs, Pika\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"model-specific-performance-results\"\u003e\u003cstrong\u003eModel-Specific Performance results\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eLet's understand the relative strengths and weaknesses of each model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1248\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.09.50-AM.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"\u003e\u003c/figure\u003e\u003ch4 id=\"runway-ml-gen-3\"\u003e\u003cstrong\u003eRunway ML (Gen-3)\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 59.42% of cases. This model excels at accurately reflecting the input prompts, making it a reliable choice for generating intended content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 17.39% of cases. Although it performs well relative to other models, it still has room for improvement in minimizing artifacts and errors.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 46.38% of cases. Runway ML generates realistic videos nearly half the time, indicating strong capabilities in producing lifelike content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 56.52% of cases. This model is proficient in delivering high-resolution videos, enhancing the viewing experience.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uyngtw0382\" title=\"RUNWAYML_____Gen-3 Alpha 2891026367, A grand fantasy cast Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"576\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRunway ML (Gen-3): \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"luma-labs\"\u003e\u003cstrong\u003eLuma Labs\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 37.68% of cases. While not as consistent as Runway ML, it still performs reasonably well in adhering to prompts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 17.39% of cases. Similar to Runway ML, it needs improvements to reduce visual defects.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 20.29% of cases. This model struggles more with realism, making it less suitable for applications requiring lifelike video content.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 30.43% of cases. Luma Labs offers moderate video resolution quality but lags behind Runway ML.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/pyz5tqupa7\" title=\"LUMA_______A_grand_fantasy_castle_surrounded_by_lush_landscapes_and_mythical_creatures__85043b Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"530\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLuma Labs: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"pika\"\u003e\u003cstrong\u003ePika\u003c/strong\u003e\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrompt adherence\u003c/strong\u003e: High in 36.23% of cases. Comparable to Luma Labs, Pika maintains a fair level of consistency with input prompts.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArtifacts and Errors\u003c/strong\u003e: High in 43.48% of cases. Pika has the highest occurrence of artifacts and errors, indicating significant areas for enhancement.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Realism\u003c/strong\u003e: High in 20.29% of cases. Like Luma Labs, Pika also faces challenges in producing realistic videos.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo Resolution\u003c/strong\u003e: High in 23.19% of cases. Pika offers the least in terms of video resolution among the three models, suggesting a need for improvement in this aspect.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/k1kvephgc9\" title=\"PIKA____A_grand_fantasy_castle_surrounded_by_lush_landscapes_and_mythical_creatures._seed3125151661828847 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePika: \"A grand fantasy castle surrounded by lush landscapes and mythical creatures\".\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIt's worth noting that the scope of this study was constrained by two key factors: \u003c/p\u003e\u003cul\u003e\u003cli\u003eThe absence of a public API for large-scale video generation from prompts; \u003c/li\u003e\u003cli\u003eOur deliberate use of a diverse prompt dataset. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis dataset encompassed a wide range of complexity, from simple to intricate descriptions. Additionally, we attempted to use automatic evaluations, such as assessing video quality based on all video frames and evaluations by large language models (LLMs) that support video. However, due to conflicting results, these methods were omitted from the blog post.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOur comprehensive evaluation of state-of-the-art text-to-video models reveals a clear preference hierarchy among Runway ML (Gen-3), Luma Labs, and Pika. Runway ML (Gen-3) emerges as the top performer, securing the first rank in 65.22% of cases, thanks to its high prompt adherence and superior video resolution. However, it still exhibits a notable occurrence of artifacts and errors, suggesting room for enhancement.\u003c/p\u003e\u003cp\u003eLuma Labs, while trailing behind Runway ML, demonstrates moderate performance, particularly in maintaining prompt consistency and video resolution. Its primary weakness lies in generating realistic videos, which is crucial for lifelike content applications. On the other hand, Pika, ranking third, shows the highest need for improvement, especially in minimizing artifacts and enhancing video resolution.\u003c/p\u003e\u003cp\u003eWhile each model has its strengths and weaknesses, Runway ML (Gen-3) stands out for its robust performance across most evaluation criteria, making it the preferred choice for generating high-quality, realistic videos. As the field of text-to-video generation continues to evolve, addressing the identified shortcomings will be key to advancing the capabilities of these models.\u003c/p\u003e\u003cp\u003eBy targeting these key areas, we can drive the next wave of innovations in text-to-video technology, creating more sophisticated and versatile text-to-video systems that cater to a broader range of applications and user needs.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-video models presented here represents a significant advance in assessing AI-generated videos. When combined with Quantumworks Lab's platform, AI teams can accelerate the development and refinement of sophisticated, domain-specific video generation models with greater efficiency and quality through our dataset curation, automated evaluation techniques, and human-in-the-loop QA.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-video model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more. \u003c/p\u003e\u003cp\u003eWe'd love to hear from you and discuss how we can support your AI evaluation needs.\u003c/p\u003e","comment_id":"66b1410331089400019364ee","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-10.13.57-AM.png","featured":false,"visibility":"public","created_at":"2024-08-05T21:15:47.000+00:00","updated_at":"2024-09-03T20:08:05.000+00:00","published_at":"2024-08-05T22:14:00.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/a-comprehensive-approach-to-evaluating-text-to-video-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-video models using human preference ratings, as well as challenges with automated evaluation techniques.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66a978d931089400019364a5","uuid":"bbe7d436-1e06-4439-bce3-780f72151bdf","title":"A comprehensive approach to evaluating text-to-image models","slug":"a-comprehensive-approach-to-evaluating-text-to-image-models","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo explore our latest in image generation evaluation, visit\u0026nbsp;\u003c/em\u003e\u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cstrong\u003e\u003cem\u003eLabelbox leaderboards\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u003cem\u003e. By leveraging our modern AI data factory, we're redefining AI evaluation. Our new, innovative approach combines human expertise with advanced metrics to surpass traditional benchmarks.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAs text-to-image AI models continue to evolve, it's become increasingly important to develop robust evaluation methods that can assess their performance across multiple dimensions. In this post, we'll explore a comprehensive approach to evaluating 3 leading text-to-image models - \u003cem\u003eDALL-E 3, Stable Diffusion, and Imagen 2 \u003c/em\u003e- using both human preference ratings and automated evaluation techniques.\u003c/p\u003e\u003ch2 id=\"the-rise-of-text-to-image-models\"\u003eThe rise of text-to-image models\u003c/h2\u003e\u003cp\u003eText-to-image generation has seen remarkable progress in recent years. Models like \u003cem\u003eDALL-E 3, Stable Diffusion, and Imagen 2\u003c/em\u003e can now produce strikingly realistic and creative images from natural language descriptions. This technology has many useful applications, from graphic design and content creation to scientific visualization and beyond.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe5C5TuOA7TCKRT_7O_PQ5hnqXPPAlM4jOO4KvRk2DR0y0k9OWNYJzNhGIL5rqanTMlYK9reVzCb_pwx__rvW6rTmRkRljBX6aIjnA3DuEH1L_-ahgR0MnJU9JD-vWdQnDi8pZeoRIvpXD0kskzRf3WdSxH?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"723\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemOVe_M8fg0ddRqLYwN1MaKDcuJtPGMXgkP1hlaXtbjU7ZKsbunWQZMVM34ttGlsa8ulDjWoCxW-KVagWUiNG4xdUc3yCYdWMcEKsC1Pl8da6kyk0UgjjL66-qxaua9sYL4IUNTOSNsyggkztxBhXbIur6?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"733\"\u003e\u003c/figure\u003e\u003cp\u003eAs these models become more advanced, having reliable ways to compare their performance and identify areas for improvement are paramount. Let’s next dive into how we developed a two-fold evaluation approach for getting more granularity into their performance.\u003c/p\u003e\u003ch2 id=\"1-human-preference-evaluation\"\u003e[1] Human preference evaluation\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfTKedTbSWhlq5swtSUlaHwVE--84lNrAPDAGpko3p_AnEz0jooz4-lin5Mpg-SwHG3CJktHiRqSHTCJcuavjLCJOau4-GxczB2Odq4mQOXSaH7ijz1wCWZic_o0B_npX0x5qgxAqiHULmNouzCOv2l8nMy?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"902\"\u003e\u003c/figure\u003e\u003cp\u003eTo capture subjective human preference, we first set up the evaluation project using our specialized and highly-vetted Alignerr workforce, with consensus set to three human labelers per data row, allowing us to tap into a network of expert raters to evaluate image outputs across several key criteria:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAlignment with prompt: \u003c/strong\u003eRaters assessed how well each generated image matched the given text prompt on a scale of high, medium, or low. For example, for the prompt \"A red apple on a wooden table\":\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Image shows a clear, realistic red apple on a wooden table\u003c/li\u003e\u003cli\u003eMedium: Image shows an apple, but it's green or the table isn't clearly wooden\u003c/li\u003e\u003cli\u003eLow: Image shows an unrelated scene\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePhotorealism: \u003c/strong\u003eThis criterion evaluates how closely the image resembled a real photograph:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Realistic lighting, textures, and proportions\u003c/li\u003e\u003cli\u003eMedium: Somewhat realistic but with slight issues in shadows or textures\u003c/li\u003e\u003cli\u003eLow: Cartoonish or artificial appearance\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eDetail: \u003c/strong\u003eRaters then determined the level of detail and overall clarity:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHigh: Fine details visible (e.g., individual leaves, fabric textures)\u003c/li\u003e\u003cli\u003eMedium: Major elements clear, but finer details somewhat lacking\u003c/li\u003e\u003cli\u003eLow: Overall blurry or lacking significant detail\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eArtifacts: \u003c/strong\u003eFinally, raters identified any visible artifacts, distortions, or errors, such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnnatural distortions in objects or backgrounds\u003c/li\u003e\u003cli\u003eMisplaced or floating elements\u003c/li\u003e\u003cli\u003eInconsistent lighting or shadows \u003c/li\u003e\u003cli\u003eUnnatural repeating patterns\u003c/li\u003e\u003cli\u003eBlurred or pixelated areas\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"2-automated-evaluations\"\u003e[2] Automated evaluations\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXduxNvNrJswCQvT7DGZfSyEoIGUOLdlCVN1c3UTP_Bbz-s-sL-P246muageVr1aPVyH7DlaXtqkmGS6Lf9fm_MuztOdJAddLr2muQLG7MzHCC4BqHfBLlN9YAO7teoaN1Qma8wTq8kRv53xfh-V1iGRVMqO?key=lqEno5Mm9WfqtiqLpZE_8Q\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"907\"\u003e\u003c/figure\u003e\u003cp\u003eTo complement human ratings, we implemented several automated evaluation techniques. Here’s an example for one \u003ca href=\"https://storage.googleapis.com/text_image_eval/gen_images/014ac7aa527c953fd0a7aeb08e238dea/results.json?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eImage Quality Score: \u003c/strong\u003eWe calculated an objective image quality score based on several key metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSharpness: Using Laplacian variance to assess image clarity.\u003c/li\u003e\u003cli\u003eContrast: Evaluating the range between minimum and maximum pixel values.\u003c/li\u003e\u003cli\u003eNoise Estimation: Employing a filter-based approach to quantify image noise.\u003c/li\u003e\u003cli\u003eStructural Similarity Index (SSIM): Comparing the image to a slightly blurred version to assess structural integrity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese metrics were combined into a comprehensive quality score in order to provide an objective measure of the image's technical attributes.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePrompt adherence: \u003c/strong\u003eWe utilized the CLIP (Contrastive Language-Image Pre-training) model to measure similarity between the text prompt and the generated image in a shared embedding space. This approach provides an automated assessment of how well the image aligns with the given prompt, offering insights into the model's ability to accurately interpret and visualize textual descriptions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDetailed scoring: \u003c/strong\u003eWe employed Claude, an advanced AI model, to provide detailed scoring and analysis of the generated images. This multifaceted evaluation includes several key components.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cu\u003eElement accuracy scores\u003c/u\u003e: For each key element in the prompt, Claude assesses its presence, provides a description, and assigns an accuracy score out of 10 reflecting how well these elements matched the prompt.\u003c/li\u003e\u003cli\u003e\u003cu\u003eCategory scores\u003c/u\u003e: Claude evaluates images across various categories such as objects, colors, spatial relations, activities, and materials. Each category receives a score out of 10, providing a comprehensive view of the image's content accuracy.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eClaude prompt adherence\u003c/u\u003e: Claude assigns an overall similarity score, expressed as a percentage, indicating how closely the entire image matches the given prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eUnexpected elements and inconsistencies\u003c/u\u003e: Claude identifies any unexpected elements or inconsistencies in the image that do not align with the intended prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eOverall impression:\u003c/u\u003e Claude provides an overall impression of the image, summarizing how well it captures the essence of the prompt. This includes a qualitative assessment of the image's strengths and areas for improvement.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"results\"\u003eResults\u0026nbsp;\u003c/h2\u003e\u003cp\u003eOur evaluation of 100 images across a diverse set of complex prompts, generated by GPT-4, stress-tested and provided valuable insights into the capabilities of Stable Diffusion, DALL-E, and Imagen 2.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHere’s a summary of the human-preference evaluations:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eModel rankings and initial findings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eStable Diffusion ranked first in 50.7% of cases, DALL-E in 38%, and Imagen 2 in 11.3%.\u003c/li\u003e\u003cli\u003eStable Diffusion ranked second (37%), followed by DALL-E (33%) and Imagen 2 (30%)\u003c/li\u003e\u003cli\u003eImagen 2 was ranked third (59%), while DALL-E and Stable Diffusion were ranked last less often (29% and 12% respectively\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ePerformance metrics (as percentages of maximum possible scores):\u003c/p\u003e\u003cul\u003e\u003cli\u003eStable Diffusion: 84.3% prompt alignment, 85.3% photorealism, 91.7% detail/clarity.\u003c/li\u003e\u003cli\u003eDALL-E: 84.3% prompt alignment, 58.3% photorealism, 83.7% detail/clarity.\u003c/li\u003e\u003cli\u003eImagen 2: 61.3% prompt alignment, 74.7% photorealism, 71.3% detail/clarity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eHere’s a summary of the detailed auto evaluation metrics:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eStable Diffusion emerged as a consistent performer across various metrics:\u003c/p\u003e\u003cul\u003e\u003cli\u003eClaude’s prompt adherence score: 82.27%. More specifically, attributed to accurate/realistic colors (89.40%) and depicting objects (89.30%)\u003c/li\u003e\u003cli\u003eHowever, image quality (34.98%) was relatively low\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDALL-E excelled in prompt interpretation and visualization:\u003c/p\u003e\u003cul\u003e\u003cli\u003eClaude’s prompt adherence score: 87.04%. More specifically, attributed to depicting objects well (91.60%) and displaying moving activities accurately (81.10%)\u003c/li\u003e\u003cli\u003eStrongest in translating textual descriptions into visual elements\u003c/li\u003e\u003cli\u003eHowever, image quality (30.11%) was lowest\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eImagen 2 performed the worst, but had a higher technical quality for images:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLowest Claude prompt adherence score (76.86%) and aspect accuracy (70.92%)\u003c/li\u003e\u003cli\u003eMuch weaker in moving activities (72.20%) and detailed attributes (77.70%)\u003c/li\u003e\u003cli\u003eHigher image quality (55.55%) than the other two models\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eAnalysis and insights:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"831\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-5.01.40-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eComparing the auto evaluation metrics to the human preference evaluations reveals some additional interesting findings:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStable Diffusion's balanced performance:\u003c/strong\u003e Stable Diffusion emerged as the top performer overall in human evaluations, ranking first in 50.7% of cases. It showed consistent high scores across human-evaluated metrics, particularly excelling in detail/clarity (91.7%) and photorealism (85.3%). However, the auto evaluation revealed a relatively low image quality score (34.98%), suggesting that technical image quality doesn't always correlate with human perception of quality.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDALL-E's strengths and weaknesses:\u003c/strong\u003e While DALL-E ranked first in 38% of human evaluations, it showed a significant weakness in human-perceived photorealism (58.3%). Interestingly, it had the highest Claude prompt adherence score (87.04%) in the auto evaluation, which aligns with its strong performance in human-evaluated prompt alignment (84.3%). This suggests DALL-E excels at interpreting and executing prompts, but may struggle with realistic rendering.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImagen 2's technical quality vs. human preference:\u003c/strong\u003e Imagen 2 consistently ranked lower in human preferences, struggling particularly with prompt alignment (61.3%). However, it had the highest technical image quality score (55.55%) in the auto evaluation. This discrepancy highlights that technical image quality doesn't necessarily translate to human preference or perceived prompt adherence.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrompt alignment discrepancies\u003c/strong\u003e: While human evaluations showed Stable Diffusion and DALL-E tied in prompt alignment (84.3% each), the auto evaluation gave DALL-E a higher score (87.04%) compared to Stable Diffusion (82.27%). This suggests that human and AI perceptions of prompt adherence may differ slightly.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePhotorealism and image quality\u003c/strong\u003e: The human-evaluated photorealism scores don't align with the auto-evaluated image quality scores. Stable Diffusion led in human-perceived photorealism (85.3%) but had low technical image quality (34.98%). Conversely, Imagen 2 had the highest technical image quality (55.55%) but ranked second in human-perceived photorealism (74.7%).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDetail and clarity vs. technical metrics\u003c/strong\u003e: Stable Diffusion stood out in human-evaluated detail and clarity (91.7%), which aligns with its high auto-evaluated scores in depicting objects (89.30%) and accurate colors (89.40%). This suggests a correlation between these technical aspects and human perception of detail and clarity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIt's also important to note that this study didn't include Midjourney due to its Discord-only integration, which made it challenging to implement into our evaluation study. While Midjourney is recognized for its high-quality output, its unconventional access method can be a barrier for users seeking traditional API or web-based interactions. Additionally, Google’s Imagen 2 implements strict safety and content filters across a wide range of topics, which did limit versatility and required additional pre-processing. Such factors, alongside the technical and human-based perceptual metrics evaluated in our study, also influence the overall usability and adoption of AI image generation models in real-world scenarios.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eOur comprehensive evaluation of leading text-to-image models, combining human preference ratings with automated metrics, reveals intriguing contrasts between quantitative performance and human perception. Stable Diffusion emerged as the overall top performer in human evaluations, excelling in detail/clarity and photorealism despite a lower technical image quality score. This underscores the complex relationship between technical metrics and human perception of quality. DALL-E demonstrated strength in prompt interpretation and adherence across both human and automated evaluations, although it showed weakness in human-perceived photorealism. Imagen 2, while scoring highest in technical image quality, consistently ranked lower in human preferences, particularly struggling with prompt alignment.\u003c/p\u003e\u003cp\u003eAs these technologies continue to evolve, our results indicate that each model has distinct strengths and areas for improvement. Stable Diffusion offers balanced performance across various criteria, making it suitable for a wide range of applications. DALL-E excels in prompt interpretation and execution, making it ideal for tasks requiring precise visualization of detailed descriptions. Imagen 2's high technical quality suggests it could be particularly useful in applications where image fidelity is prioritized, although improvements in prompt adherence would enhance its overall performance. Future research should focus on bridging the gap between technical metrics and human perception, as well as addressing specific weaknesses identified in each model, such as DALL-E’s photorealism or Imagen 2’s prompt alignment. By refining these aspects, we can push the boundaries of AI-generated imagery and develop more versatile and capable text-to-image systems that better meet the needs of various applications and user preferences.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003eGet started today\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe comprehensive approach to evaluating text-to-image models presented here represents a significant advance in assessing AI-generated images. When combined with Quantumworks Lab's platform, AI teams can accelerate the development and refinement of sophisticated, domain-specific image generation models with greater efficiency and quality through our dataset curation, automated evaluation techniques, and human-in-the-loop QA.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your own text-to-image model assessment, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more. We'd love to hear from you and discuss how we can support your AI evaluation needs.\u003c/p\u003e","comment_id":"66a978d931089400019364a5","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-30-at-4.35.13-PM.png","featured":false,"visibility":"public","created_at":"2024-07-30T23:35:53.000+00:00","updated_at":"2024-11-26T00:11:57.000+00:00","published_at":"2024-07-31T00:02:09.000+00:00","custom_excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"66a9822f31089400019364c9","name":"Krish Maniar","slug":"krish","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/krish/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"66a9822f31089400019364c9","name":"Krish Maniar","slug":"krish","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/krish/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/a-comprehensive-approach-to-evaluating-text-to-image-models/","excerpt":"Discover how to employ a more comprehensive approach to evaluating leading text-to-image models using both human preference ratings and automated evaluation techniques.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"669a9314e017de000190bda5","uuid":"88ed362b-5dd4-48eb-950c-0e2798e34a3f","title":"Using Quantumworks Lab to improve data quality via AutoQA \u0026 advanced labeler review","slug":"using-labelbox-to-improve-data-quality-via-autoqa-advanced-labeler-review","html":"\u003cp\u003eWorking with leading generative AI teams who are building frontier models and developing task-specific generative AI products, we've seen first hand the importance of data quality and robust QA processes in order to deliver performant models. The availability of human-evaluated data sets the companies apart in their AI offerings. In this solution accelerator, we will guide you through the different workflows and demonstrate how Quantumworks Lab can expedite the quality review process for creating better data for generative AI use cases.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dkbjlz0nnn\" title=\"Kushal AudoQA - 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe generative AI use cases we’ll focus on for this guide is multi-turn conversations, similar to what you’d find interacting with an LLM. As an introduction, Quantumworks Lab offers multimodal chat for two main options:\u003c/p\u003e\u003ch3 id=\"1-live-online-based-evaluation\"\u003e\u003cstrong\u003e1. Live online-based evaluation\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn this scenario, Quantumworks Lab provides an experience similar to a \u003ca href=\"https://labelbox.com/blog/announcing-multimodal-chat-for-genai-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003echatbot arena\u003c/a\u003e where multiple models or multiple versions of the same model are compared against each other. A labeler then ranks the responses to determine which one is better. For example, you can input a prompt and receive three different responses from different versions of a model, such as Gemini, GPT, or Claude. It's important to note that the labelers don’t know which response corresponds to which model to prevent bias.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExample:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eInput a prompt.\u003c/li\u003e\u003cli\u003eReceive three responses from different model versions.\u003c/li\u003e\u003cli\u003eThe labeler then ranks the responses based on quality (defined as per your business-specific needs).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"2-offline-multimodal-chat\"\u003e\u003cstrong\u003e2. Offline multimodal chat\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThis option allows you to upload existing multi-turn conversations to Quantumworks Lab for evaluation (e.g. accuracy, relevance, tone, etc.).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExample:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eUpload a multi-turn conversation between a human and a chatbot.\u003c/li\u003e\u003cli\u003eEvaluate and label the conversation across different axes, such as relevance, factuality, and fluency.\u003c/li\u003e\u003cli\u003eThis can be done on a per-message level or for the entire conversation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy leveraging these workflows, teams can effectively utilize Quantumworks Lab for various chatbot-based use cases.\u003c/p\u003e\u003ch2 id=\"prompt-and-response-generation-walkthrough\"\u003e\u003cstrong\u003ePrompt and response generation walkthrough\u003c/strong\u003e\u003cbr\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mfgloie4dw\" title=\"Kushal AutoQA - 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a first step, let’s walk through a common workflow in Quantumworks Lab involving prompt and response generation, which is crucial for training a model. This process involves creating question and answer pairs and offers three main options: workforce-generated prompts and responses, guided prompt and response creation, and responding to uploaded prompts.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor workforce-generated prompts, labelers input questions and corresponding responses, selecting the appropriate category for each pair.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXemODUkHK4G4d1-GDQpXuY-GpXvdZKokbuOZ7pqhwH4YtEqYP-c2UNwakRhkJKp_jja7mEgD-pvdSUAe0F4QAVu5DX6cAB3pqCww2cWOGqc3fG9nMwl99D5VqUJnPyoh2nzblS1VQo3EKAvIJ0t4690fUET?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"823\"\u003e\u003c/figure\u003e\u003cp\u003eAs shown above, the guided prompt and response creation option uses an image, code snippet, video, or text as a basis for labelers to generate relevant question and answer pairs.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rk8jhk0ro6\" title=\"Kushal AutoQA - 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLastly, if you have existing prompts, labelers can evaluate these prompts on multi-turn conversations and provide suitable responses. This flexible workflow supports various input types, enabling efficient model training through comprehensive prompt and response generation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXfp0NYehNcvRTlOFRnRnvl_c88o9_XZO4MQcTGAhlxO0zFJiKJhOxoSC-SnfAb2rPOQAovFmjRKWEW422hl6gqrONAYrDTo_dasu3GxgQ71K3_bVCYrJay_cdRd3fEyQz-89-puoPPKlghrbm1l49dhsSxV?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"819\"\u003e\u003c/figure\u003e\u003cp\u003eAfter identifying the data to be labeled and setting up the labeling schema, which includes fields for the prompt, response, and category, you can attach detailed instructions via PDF. These instructions are accessible to labelers during the labeling process and labelers can start labeling from scratch by generating prompt-response combinations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXf-pM7Cev56U0tGeAKUQ2qsmDjGPHfwGn_sLk8yb_7sKCuzuWhystmTA2--SOTMp5C9uPmIvxzr1KnkMmbAfuD6YG6GzDmfU_tVf6l7cJ3TU1BoJfLNqyAILHuD7q-QFx4CMYBInrek29GFll6Q3lupNyYW?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"885\"\u003e\u003c/figure\u003e\u003ch2 id=\"ai-assisted-alignment-ai-assisted-labeling\"\u003e\u003cstrong\u003eAI-assisted alignment (AI-assisted labeling)\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dxv4cpcs3z\" title=\"Kushal AutoQA - 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLabelbox allows you to use a large language model or a custom model for pre-labeling by using our model-assisted labeling option.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can select any available model such as \u003cem\u003eGPT, Gemini, Claude\u003c/em\u003e, etc or bring in your own custom one. By generating a preview, you receive pre-labels that labelers can then modify and correct without having to create labels from scratch. While this speeds up the labeling process, it can introduce bias.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf your task requires unbiased labeling, you can enforce labelers to create labels from scratch without any model assistance. The platform offers flexibility, enabling you to choose between speed and accuracy based on your task requirements.\u003c/p\u003e\u003ch2 id=\"labelbox-ai-assisted-alignment-autoqa-aka-ai-critic\"\u003e\u003cstrong\u003eLabelbox AI-assisted alignment (AutoQA aka AI critic)\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ugdbth629w\" title=\"Kushal AutoQA - 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a next step, let’s walk through how you can leverage Quantumworks Lab’s platform to utilize an LLM to act as an AI critic or judge to help auto QA your data during the review process. Quantumworks Lab employs an LLM to review prompt and response pairs, providing scores and feedback and critique on the quality of labels and why things were good or bad. This feedback helps identify insights and scores to figure out which labels require further review.\u003c/p\u003e\u003ch3 id=\"scoring-and-feedback\"\u003e\u003cstrong\u003eScoring and feedback\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXd49bah2g-Y-dJi7rv07qxkQxH3QVspBNwHJMYT06TKhZPS61KEjDIKdfGsMAWaMLXwVFHHXSAgnfePp1TjzyBtG_15Xmz91syjfZN2XDU34jovG1Akyao2_7o_h1LvX-CBG6oJnJWuYDOJ05rvpGL4-sK-?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"829\"\u003e\u003c/figure\u003e\u003cp\u003eAs an example shown above, you can see that for this specific data row, the labeling score is 4.75,\u0026nbsp; and includes ideas for improving the reasoning. With this view, you can go through all of your different data rows and filter the data based on scores to identify labels that need additional scrutiny. For example, you might filter for scores below 4 and set a range, and move these labels to a custom queue for further review. Creating custom queues for specific score ranges, like between 3 and 4, allows for a more organized review and QA process.\u003c/p\u003e\u003cp\u003eReviewers in the loop can view label instructions, scores, and improvement ideas for each label. This information is crucial for understanding the quality of your labels and making necessary adjustments. You can filter and select labels based on their scores and move them to appropriate queues for further action. As a best practice, the Quantumworks Lab team works closely with our customers to come up with the best scoring for your review and evaluation needs.\u003c/p\u003e\u003cp\u003eFurthermore, Quantumworks Lab allows you to create custom review workflows tailored to your specific needs. As shown below, you can define different metrics and criteria for scoring, such as \u003cem\u003eBLEU\u003c/em\u003e scores, which is common specifically for Gen AI use cases around free-form text.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXdDrCCs3pFr5vZc1XiH4NKSRy10Z52AwFMxspY8rlrM8VwWSD97jtCucEWQl9y6siSKXRVDwnrZBP78CPRobkNjdp2MhwpGUt9PrZGbmPdFlTo0vkvDOD5XnRx1DL9vKuYXJ5FKFvv4gx3AIdgU9eNOpiKs?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1096\"\u003e\u003c/figure\u003e\u003cp\u003eThis flexibility ensures that the review process aligns with all of your project requirements and quality standards.\u003c/p\u003e\u003ch3 id=\"exporting-your-data\"\u003e\u003cstrong\u003eExporting your data\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXe3mrwO8jvCOqnuLQNzGBZcY_h4iVwJygVYyRXE0JUDEk4yijWUglS6tjJI29fGsZESN1kSyiIqbFoBcH4LqQdUHaf72pb5Az2PMV0SAYzNQE4JaXaJ6PncP_wGBU17MHmu7pHCpuvioc-OXqB9VvN2yj38?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"786\"\u003e\u003c/figure\u003e\u003cp\u003eOnce labels are finished with the review process, labels are moved to the done stage and are ready for export out of the Quantumworks Lab platform. You can export the data by selecting it and triggering the export JSON. This process ensures that you have high-quality data ready.\u003c/p\u003e\u003cp\u003eBecause Quantumworks Lab tracks in-depth metrics around labeling operations (average label time, average review time, etc.) disaggregated by labeling member, you can very easily calculate additional inter-annotator agreement metrics, such as Krippendorff’s Alpha Score and Cohen’s Cappa for qualitative metrics (e.g. likert scales), beyond the off-the-shelf benchmark and consensus metrics that Quantumworks Lab already calculates.\u003c/p\u003e\u003ch3 id=\"providing-feedback-to-labelers-and-ensuring-quality\"\u003e\u003cstrong\u003eProviding feedback to labelers and ensuring quality\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXdNzGDU92vOA2K8yP34ISKLkN1FD-qRMenZxYSsJZFRfI3mdgVMAWymkkZV4eukzSUuxyt1nEobCbJJoVPlXIrACbfz6c6-HCDCjf9KqIlAkIVw2D_ivX6SVpfzJMeUc2MwOfW4YYhrpBwuriI-2aSEkJkc?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"817\"\u003e\u003c/figure\u003e\u003cp\u003eCustomers and reviewers can also easily collaborate and provide real-time feedback to labelers by raising any issues or instructions, and labelers can be notified to make necessary changes. As an example, you can instruct your labelers to “incorporate the feedback from the LLM to make this response better.”\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcD95jtRW_w_sxoXYDqzq5_U2KUBcaMwhKlfyFedseFMCRNbUjTEm0RNeb4TqIghead-cgTt1AosJ-yELJpndZfwL25GZOjIFJPPX_lDNF0gMhRMnmeiTg2jVCmjMBUiJl0lILmbeytnwpi3Y3bcZa5HaRI?key=_aussd-24X8AKL3WyEbNng\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"862\"\u003e\u003c/figure\u003e\u003cp\u003eThe review and feedback loop helps maintain high-quality data and can be tailored to include as many review steps as needed to meet your quality control standards, allowing you to select how much data you want auto-QA’d, how much data you want Quantumworks Lab to internally review amongst themselves or whether you want to review it on your own. Quantumworks Lab provides full control and typically works with customers to determine how many review steps you want and what are the different scores you want to evaluate against.\u003c/p\u003e\u003cp\u003eThis ongoing process ensures a consistent and efficient labeling engine; as new data comes in, it gets quickly labeled and if it meets the criteria, it moves to an appropriate queue which then gets reviewed by subject matter experts, and finally gets you the batch of high-quality labels delivered.\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003e\u003c/h2\u003e\u003cp\u003eWe hope that this walkthrough gives you a better understanding of how Quantumworks Lab makes it easier than ever before to iterate quickly with real-time, granular visibility into labels and implement autoQA workflows for data quality. In addition, AI labs and generative AI companies can benefit from tapping into \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ediverse pools\u003c/a\u003e of expertise in order to improve the underlying data and model performance with Labelbox.\u0026nbsp;\u003c/p\u003e\u003cp\u003eReady to improve the data quality for your generative AI initiatives? \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eContact us\u003c/u\u003e\u003c/a\u003e to learn more and we’d love to hear from you.\u0026nbsp;\u003c/p\u003e","comment_id":"669a9314e017de000190bda5","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/Screenshot-2024-07-19-at-9.36.05-AM.png","featured":false,"visibility":"public","created_at":"2024-07-19T16:23:48.000+00:00","updated_at":"2024-07-19T18:03:53.000+00:00","published_at":"2024-07-19T16:32:08.000+00:00","custom_excerpt":"Learn how you can use Quantumworks Lab to accelerate the quality review process for creating better data for generative AI use cases using autoQA and advanced labeler reviews.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/using-labelbox-to-improve-data-quality-via-autoqa-advanced-labeler-review/","excerpt":"Learn how you can use Quantumworks Lab to accelerate the quality review process for creating better data for generative AI use cases using autoQA and advanced labeler reviews.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"668c2f59878a56000112a4e0","uuid":"93014cdf-3555-4d2b-b480-2b9db686e2db","title":"Using multimodal chat to enhance a customer’s online support experience","slug":"using-multimodal-chat-to-enhance-a-customers-online-support-experience","html":"\u003cp\u003eCustomer service systems are prime targets for multimodal chat as this new technology leverages an intuitive interface for answering user questions\u0026nbsp; while enabling companies to unlock previously hidden insights about customer journeys and sentiment.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can streamline the process of labeling and managing data for various use cases, including intent classification, dialogue retrieval, state tracking, and model evaluation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. We’ll illustrate the various labeling tasks that Quantumworks Lab can accomplish to ensure the chatbot is intelligent and effective, providing customers with relevant and timely answers.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://files.readme.io/ce6e056-Labelbox_2024-06-04_10-37-061.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"678\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eLabelbox offers a \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/docs/live-multimodal-chat-evaluation?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003enumber of editor types\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, including the live multimodal chat editor.\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"labeling-tasks\"\u003e\u003cstrong\u003eLabeling Tasks\u003c/strong\u003e\u003c/h1\u003e\u003ch2 id=\"multimodal-intent-classification-dialogue-retrieval-and-state-tracking\"\u003e\u003cstrong\u003eMultimodal Intent Classification, Dialogue Retrieval and State Tracking\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xs53lmro56\" title=\"Customer Query Resolution Training - Part 1 - Training Data Collection + Using Classifications Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 1: Generate prompt-response pairs for supervised fine-tuning of agentic workflows.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eLabel the customer’s intent and sentiments in a given turn of the chat, so you can identify the appropriate next steps accordingly.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1427\" height=\"1333\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/first-one.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/07/first-one.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/07/first-one.png 1427w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe customer appears frustrated, so the sentiment is labeled as “frustrated.”\u003c/p\u003e\u003cp\u003eFurthermore, the customer appears to be reporting their order was damaged and is requesting a new one, so both of those requests are labeled using checklist classifications.\u003c/p\u003e\u003cp\u003eWith these labels, you know the customer’s intent (get a new piece) and can also record the current state of the conversation (damage reported but no action taken yet). As the conversation proceeds, you can update the state based on what actions are taken and keep checking the intent and tone of the customer to ensure they are satisfied.\u003c/p\u003e\u003cp\u003eLabelbox allows you to label every aspect of every turn-by-turn conversation, enabling you to develop fine-grained data sets of annotated real-world conversations.\u003c/p\u003e\u003cp\u003eYou can use these labels to train your agent to retrieve similar past interactions and follow similar response patterns or simply be inspired by them.\u003c/p\u003e\u003ch2 id=\"model-evaluation-ranking-and-selection\"\u003e\u003cstrong\u003eModel evaluation, ranking and selection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eYou trained a few models offline with the previously generated data using supervised fine-tuning or RLHF. How do you know which of the models is the best performing, and how often do they make mistakes?\u003c/p\u003e\u003cp\u003eLabelbox allows you to connect these models hosted behind your custom endpoints. Then, with the Live Multimodal Chat editor, labelers can interact with all of them simultaneously by feeding prompts, comparing and contrasting responses, and even rewriting the best ones to be even better.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1vpskixih4\" title=\"Customer Query Resolution Training - Part 2 - LLM Development \u0026amp; Evaluation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 2: Evaluate a single model, generate multiple responses and write your own gold standard response.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eWhen evaluating a single model, Quantumworks Lab allows you to generate multiple responses and also add your own response. This is very useful for use cases where all of the model’s responses are lacking in quality and a human-written response provides the gold standard for further downstream training. Furthermore, in the course of the labeling conversation, you can choose one of these responses to be passed in as prior context into later turns, allowing you to choose the path of the conversation in real-time for better training data generation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uzesocscxk\" title=\"Customer Query Resolution Training - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 3: Evaluate multiple models at once in “chat arena” style.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/secone-one-1-1-1-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"450\" height=\"640\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox also supports evaluating multiple models in a “chat arena” style setup.\u003c/p\u003e\u003cp\u003eSay you have trained candidate models and now want to compare them against each other and also foundation models like GPT 4o. Connect your own models with Quantumworks Lab as \u003cem\u003ecustom\u003c/em\u003e models, and easily choose any latest and greatest foundation model from Foundry within Labelbox. The chat arena excludes information about which response was generated from which model so as not to bias the labeling process.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWith Quantumworks Lab's multimodal chat functionalities, you can generate high-quality datasets to train, evaluate, and test your custom LLMs or pit off-the-shelf LLMs like GPT4o and Gemini 1.5 against each other in a Chatbot Arena-style competition to see which one suits your needs best.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/b3f423vxob\" title=\"Customer Query Resolution Training - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePart 4: Survey of all LLM capabilities in Quantumworks Lab\u003c/em\u003e\u003c/p\u003e\u003cp\u003eAs the landscape of LLMs evolves rapidly, Quantumworks Lab will continue to build functionality to ensure our customers can align models to their use cases quickly and reliably. With differentiated data, streamlined operations, and transparent orchestration, you can build and refine foundation models that meet your specific needs.\u003c/p\u003e\u003ch2 id=\"evaluate-multimodal-chat-data-today-for-free\"\u003eEvaluate multimodal chat data today for free\u003c/h2\u003e\u003cp\u003eReady to deliver on the next-generation of generative AI?\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eSign up for a free\u003c/u\u003e\u003c/a\u003e\u0026nbsp;Quantumworks Lab account to try it out or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to learn more and we’d love to hear from you.\u003c/p\u003e","comment_id":"668c2f59878a56000112a4e0","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/mmbazel_an_ecommerce_chatbot_responding_to_a_support_problem_in_fd177f4f-0440-4c4a-8fa3-07fe82488f05-1.webp","featured":false,"visibility":"public","created_at":"2024-07-08T18:26:33.000+00:00","updated_at":"2024-07-10T16:09:14.000+00:00","published_at":"2024-07-09T16:06:15.000+00:00","custom_excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-multimodal-chat-to-enhance-a-customers-online-support-experience/","excerpt":"In this guide we’ll show how Quantumworks Lab can be used to collect training data for an ecommerce chatbot that responds to customer queries about online shopping. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6657a8439c01af00013c4eb1","uuid":"31996de5-5a27-4754-878d-e496d9de1c00","title":"Working with videos using Gemini 1.5 and multimodal models","slug":"working-with-videos-using-gemini-1-5-and-multimodal-models","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u0026nbsp;\u003c/h2\u003e\u003cp\u003eGiven the pace of innovation in AI, teams are continually looking to integrate various data types like text, images, and video as a way to unlock new functionality for delivering next-gen applications and experiences. The development of multimodal models, which can process and understand diverse data inputs, is one of the most promising advancements.\u0026nbsp;\u003c/p\u003e\u003cp\u003eNotably, combining video processing with the capabilities of large language models (LLMs) is a breakthrough feature for teams who want to highlight specific objects, scenes, and actions from high-volumes of video content.\u003c/p\u003e\u003cp\u003eHowever, many multimodal models, such as Gemini 1.5, require teams to convert videos to 1 frame per second (FPY) for analysis. Converting FPS, while tedious, aligns the video data with the model’s optimal processing capabilities, ensuring that no critical information is lost while maintaining compatibility with the model’s precision.\u003c/p\u003e\u003cp\u003eIn this blog post, we’ll show how easy it is to convert videos to 1FPS and upload them to Quantumworks Lab Catalog for generating predictions in Model Foundry.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"preparing-videos-for-inference-with-multimodal-models\"\u003ePreparing videos for inference with multimodal models\u003c/h1\u003e\u003cp\u003eThe two main approaches for ensuring videos meet the exact 1FPS requirements of multimodal models like Gemini 1.5 include:\u003c/p\u003e\u003col\u003e\u003cli\u003eVideo Upload: Converting a video to 1 FPS and uploading this converted video into Quantumworks Lab Catalog.\u003c/li\u003e\u003cli\u003eFrame Extraction Upload: Converting a video to 1 FPS, extracting each of the video frames, and uploading the extracted video frame images to Catalog.\u0026nbsp;\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eBy doing so, users will be able to use various multimodal models like GPT- 4v, Claude 3 Opus and Amazon Rekognition (as well as additional models natively supported by Quantumworks Lab). It is important to note that 1FPS is not necessary for Model Foundry’s use on video datarows, but this approach may be helpful when using certain multimodal models.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"approach-1upload-a-1-fps-video-to-catalog\"\u003eApproach #1 - Upload a 1 FPS Video to Catalog\u003c/h2\u003e\u003cp\u003eThe first approach is converting a video to 1 frame per second (FPS) and uploading the converted video to Catalog.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can follow along in this \u003ca href=\"https://colab.research.google.com/drive/1mJv6L5PbSZf4TmskWG1XMAqj4A4QS1Ye?usp=sharing\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"steps\"\u003e\u003cstrong\u003eSteps\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eStep 1: Download Video From Google Cloud Storage (GCS)\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1114\" height=\"202\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.09.34-AM.png 1114w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe function ‘convert_video_to_1fps_and_download’ is defined to handle the video conversion and upload process.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIt starts by downloading the specified video file from a Google Cloud Storage bucket to the local Colab environment.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 2: Convert the Video to 1 FPS\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1044\" height=\"82\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.10.00-AM.png 1044w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eUsing the ‘ffmpeg’ tool, the video is converted to 1 frame per second (FPS). This step is crucial and can be easily modified to change the FPS by altering the ‘-vf fps=1’ parameter in the ‘subprocess.run’ command. For example, to convert the video to 2 FPS, you would change ‘fps=1’ to ‘fps =2.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 3: Upload the Converted Video Back to GCS\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.14.20-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"843\" height=\"74\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.14.20-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.14.20-AM.png 843w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAfter conversion, the video is saved locally and then uploaded back to the GCS bucket.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 4:\u0026nbsp; Upload to Catalog\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1055\" height=\"238\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.15.06-AM.png 1055w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe converted video is then integrated with Quantumworks Lab by creating a dataset and adding the video as a data row.\u003c/li\u003e\u003cli\u003eThe code uses the Quantumworks Lab SDK to create a dataset and upload the converted video, making it ready for further processing and labeling in Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"approach-2upload-extracted-video-frames-to-catalog\"\u003eApproach #2 - Upload Extracted Video Frames to Catalog\u003c/h2\u003e\u003cp\u003eThe second approach is converting a video to 1 frame per second (FPS), extracting video frames, and uploading the extracted video frame images to Catalog.\u003c/p\u003e\u003cp\u003eYou can follow along in this \u003ca href=\"https://colab.research.google.com/drive/1ZYye--AOyHUzXaF24mU1nHuGS0myViJl?usp=sharing\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e\u003cu\u003e.\u003c/u\u003e\u003c/p\u003e\u003ch3 id=\"steps-1\"\u003e\u003cstrong\u003eSteps\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eStep 1: Download Video From Google Cloud Storage (GCS)\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1175\" height=\"232\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.05-AM.png 1175w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe function ‘process_video_from_gcs’ is designed to handle the entire workflow of downloading the video, extracting frames, and uploading them back to GCS.\u003c/li\u003e\u003cli\u003eIt downloads the specified video file from a GCS bucket to the local Colab environment.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 2: Extract Video Frames at 1 FPS\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1148\" height=\"39\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.16.44-AM.png 1148w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe frames are saved locally in a specified directory.\u003c/li\u003e\u003cli\u003eUsing the ‘ffmpeg’ tool, the video is processed to extract frames at 1 frame per second. Again, this can be easily adjusted by changing the ‘-vf fps=1’ parameter in the ‘subprocess.run’ command to any desired frame rate.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 3: Rename and Organize Frames\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.17.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"896\" height=\"447\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.17.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.17.28-AM.png 896w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eEach extracted frame is renamed sequentially for better organization, using a consistent naming pattern (e.g. ‘image_frame_0001.jpg’).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eStep 4: Upload Frames to GCS\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe renamed frames are uploaded back to the GCS bucket into a directory specific to the frames of the video.\u0026nbsp;\u003c/li\u003e\u003cli\u003ePublic URLs and global keys for each uploaded frame are generated and stored for later use.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.18.07-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"672\" height=\"235\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-30-at-10.18.07-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/05/Screenshot-2024-05-30-at-10.18.07-AM.png 672w\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eStep 5: Upload to Catalog\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe extracted frames are uploaded to Catalog by creating a dataset and adding each frame as an individual data row, using the Quantumworks Lab SDK.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"additional-considerations\"\u003eAdditional Considerations\u0026nbsp;\u003c/h1\u003e\u003cp\u003eWhen deciding between uploading 1FPS videos or extracted video frames to Catalog, there are some important features to consider:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFor 1FPS Videos\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcGfqLUwq4Vt_KSDpedsvAvF3Z9rP3bdesbBACTSMAf6NDeDlRxKSCsTw_QSV_GulMPPGda9XMK-kKw1a6osW8UFL1jjEe_8QW0JxMvyHqRI-jkbBN4OUdiCYk22o5kb4AbVR_ZoYaoac-YASMQGwcROso?key=kFXSU0zZGAC6bV-HbWQZiQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"437\" height=\"175\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTemporal Context Preservation\u003c/strong\u003e:\u003cul\u003e\u003cli\u003eProvides a continuous video stream and maintains the temporal relationships and sequences between frames\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSimplified Workflow\u003c/strong\u003e:\u0026nbsp;\u003cul\u003e\u003cli\u003eManaging a single video file is typically simpler than handling multiple image files, reducing the number of files to manage\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLimited Flexibility in Frame Manipulation\u003c/strong\u003e:\u003cul\u003e\u003cli\u003eVideos offer less flexibility for individual frame manipulation and augmentation compared to separate image files\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eFor Extracted Video Frames\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXeSVX1V08D-2UrE4e5c5VflZgVPJU7EguUFQR1Faa7-JI4iGLa9ZwwzpywKgKQtgz7-jSZ--82W9x2JrfWPgKYl93of_NbdlWphywYp6Sz77LsQKr9kG34CBEcMqht_fOoHL0ZrmVBOjZ47Zhwpg5A-MFs?key=kFXSU0zZGAC6bV-HbWQZiQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1081\" height=\"418\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIncreased Optionality\u003c/strong\u003e:\u0026nbsp;\u003cul\u003e\u003cli\u003eProcess only frames that are most relevant, allowing for greater control over the dataset\u003c/li\u003e\u003cli\u003eExtracted frames can be pre-processed or filtered according to specific criteria, potentially enhancing the quality of input data\u003c/li\u003e\u003cli\u003eFocus the analysis on the most important moments in the video\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLoss of Temporal Context\u003c/strong\u003e:\u003cul\u003e\u003cli\u003eIndividual frames lack the temporal continuity present in videos, which might be crucial based on the specific use case.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIncreased File Management\u003c/strong\u003e:\u0026nbsp;\u003cul\u003e\u003cli\u003eHandling a large number of individual image files within Quantumworks Lab Catalog\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"next-steps\"\u003eNext steps\u0026nbsp;\u003c/h2\u003e\u003cp\u003eOnce a video dataset has been converted to 1FPS via one of the two approaches highlighted above, Gemini 1.5 and other multimodal models can be used to \u003ca href=\"https://labelbox.com/guides/harnessing-ai-for-efficient-video-labeling/?ref=labelbox-guides.ghost.io#introduction\"\u003e\u003cu\u003eharness AI for efficient video labeling\u003c/u\u003e\u003c/a\u003e, enabling precise and accurate frame classification to enhance data insights and model training.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u0026nbsp;\u003c/h1\u003e\u003cp\u003eIn this blog post, we explored the importance of preparing video data for multimodal models like Gemini 1.5, which analyze video data at 1 frame per second (FPS). This ensures maximum compatibility with the model's processing capabilities for accurate and efficient analysis.\u003c/p\u003e\u003cp\u003eChoosing between uploading 1 FPS videos and extracted video frames depends on your project's specific needs. As a rule of thumb, uploading videos preserves temporal context and simplifies file management, while extracting frames allows for detailed analysis and greater control, but with more file handling.\u003c/p\u003e\u003cp\u003eBy understanding these considerations, you can effectively leverage multimodal models like Gemini 1.5, optimizing your workflow for enhanced performance and accuracy in video classification tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you are not already using Quantumworks Lab, you can \u003ca href=\"https://app.labelbox.com/signup?utm_keyword=Quantumworks Lab%2520annotation\u0026utm_source=house\u0026utm_medium=email\u0026utm_campaign=524\u0026gclid=Cj0KCQjw2PSvBhDjARIsAKc2cgM16FFzTqltH9d4iQrrMQBwayH1ftA6F6A8dBgcdgz7MIR6dpv773oaAmgzEALw_wcB\u0026landingPageAnonymousId=%22b04806c5-ff2b-4a09-9379-a9837b1801a9%22\u0026referrer_url=https://www.google.com/\"\u003eget started for free\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003econtact us\u003c/a\u003e to learn more about using multimodal models for better video classification.\u0026nbsp;\u003c/p\u003e","comment_id":"6657a8439c01af00013c4eb1","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/05/mmbazel_a_photorealistic_depiction_of_analyzing_video_frames_of_26a0629d-30a4-4c38-8bd2-30877fefb937-1.png","featured":false,"visibility":"public","created_at":"2024-05-29T22:12:19.000+00:00","updated_at":"2024-05-30T17:22:24.000+00:00","published_at":"2024-05-30T17:03:49.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"}],"tags":[{"id":"681516a56e02120001d5ab84","name":"#mikiko","slug":"hash-mikiko","description":null,"feature_image":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/404/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/working-with-videos-using-gemini-1-5-and-multimodal-models/","excerpt":"Introduction \n\nGiven the pace of innovation in AI, teams are continually looking to integrate various data types like text, images, and video as a way to unlock new functionality for delivering next-gen applications and experiences. The development of multimodal models, which can process and understand diverse data inputs, is one of the most promising advancements. \n\nNotably, combining video processing with the capabilities of large language models (LLMs) is a breakthrough feature for teams who ","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6656135f9132db0001f20551","uuid":"f173905b-1c7a-4f87-8064-ac8af0ccdaa7","title":"AI foundations: Understanding embeddings","slug":"ai-foundations-understanding-embeddings","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eEmbeddings transform complex data into meaningful vector representations, enabling powerful applications across various domains.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis post covers the theoretical foundations, practical examples, and demonstrates how embeddings enhance key use cases at Labelbox.\u003c/p\u003e\u003cp\u003eWhether you’re new to the concept or looking to deepen your understanding, this guide will provide valuable insights into the world of embeddings and their practical uses.\u003c/p\u003e\u003chr\u003e\u003ch1 id=\"what-is-an-embedding\"\u003eWhat is an embedding?\u003c/h1\u003e\u003cp\u003eAt its core, an embedding is a vector representation of information. This information can be of any modality—video, audio, text, and more.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe process of generating embeddings involves a deep learning model trained on specific data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach index in a vector represents a numerical value, often a floating-point value between 0 and 1.\u003c/p\u003e\u003ch2 id=\"the-relationship-between-dimensions-and-detail\"\u003eThe relationship between dimensions and detail\u003c/h2\u003e\u003cp\u003eThe dimensions of a vector describe the level of detail it can capture. Higher dimensionality means higher detail and accuracy in the similarity score.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, a vector with only two dimensions is unlikely to store all relevant information, leading to simpler but less accurate representations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1362\" height=\"626\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.30-AM.png 1362w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"vector-representations-of-images\"\u003eVector representations of images\u003c/h2\u003e\u003cp\u003eFor instance, consider an image. The underlying compression algorithm converts this image into a vector representation, preserving all relevant information. These embeddings can then be used to determine the similarity between different pieces of data. For example, images of a cat and a dog would have different embeddings, resulting in a low similarity score.\u003c/p\u003e\u003cp\u003eAnother example: Let's consider a model trained on images of helicopters, planes, and humans. If we input an image of a helicopter, the model generates a vector representation reflecting this. If the image contains both a helicopter and humans, the representation changes accordingly. This process helps in understanding how embeddings are generated and used.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1240\" height=\"618\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.57.55-AM.png 1240w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Helicopter or not helicopter?\" captured via embeddings.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"importance-and-applications-of-embeddings\"\u003eImportance and applications of embeddings\u003c/h1\u003e\u003ch2 id=\"why-embeddings-matter\"\u003eWhy embeddings matter\u0026nbsp;\u003c/h2\u003e\u003cp\u003eAs we’ve described in the prior sections, understanding what embeddings are and how they work is crucial for several reasons:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCapturing Semantic Relationships\u003c/strong\u003e: Embeddings help capture complex relationships within data, enabling more accurate predictions and recommendations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Search Capabilities\u003c/strong\u003e: Embeddings facilitate efficient and accurate similarity searches, essential in applications like search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"applications-of-embeddings\"\u003eApplications of embeddings\u003c/h2\u003e\u003cp\u003eEmbeddings are extremely useful in search problems, especially within recommender systems.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHere are a few examples:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImage Similarity\u003c/strong\u003e: Finding images similar to an input image, like searching for all images of dogs based on a sample image.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eText-to-Image Search\u003c/strong\u003e: Using a textual query, such as \"image of a dog,\" to find relevant images.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContent Recommendations\u003c/strong\u003e: Identifying articles or movies similar to ones you've enjoyed, enhancing search engines and recommendation systems.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1296\" height=\"874\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-10.58.24-AM.png 1296w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEmbeddings are the essential ingredient to a smooth and enjoyable family movie night via recommendations systems, which can suggest movies similar to ones you've enjoyed in the past.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"how-embeddings-are-generated\"\u003eHow embeddings are generated\u003c/h1\u003e\u003cp\u003eDeep learning models, trained on either generalized or specific datasets, learn patterns from data to generate embeddings. Models trained on specific datasets, like cat images, are good at identifying different breeds of cats but not dogs. Generalized models, trained on diverse data, can identify various entities.\u003c/p\u003e\u003cp\u003eModern models produce embeddings of over 1024 dimensions, increasing accuracy and detail. However, this also raises challenges related to the cost of embedding generation, storage, and computational resources.\u003c/p\u003e\u003cp\u003eSome of the earliest work with creating embedding models included word2vec for NLP and convolutional neural networks for computer vision.\u003c/p\u003e\u003ch2 id=\"the-significance-of-word2vec\"\u003eThe significance of word2vec\u003c/h2\u003e\u003cp\u003eThe introduction of word2vec by Mikolov et al. in 2013 marked a significant milestone in creating word embeddings. Word2vec enabled the capture of semantic relationships between words, such as the famous king-queen and man-woman analogy, demonstrating how vectors can represent complex relationships.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis breakthrough revolutionized natural language processing (NLP), enhancing tasks like machine translation, sentiment analysis, and information retrieval.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWord2vec laid the foundation for subsequent embedding techniques and deep learning models in NLP.\u003c/p\u003e\u003ch2 id=\"equivalent-of-word2vec-for-images\"\u003eEquivalent of word2vec for Images\u003c/h2\u003e\u003cp\u003eFor images, convolutional neural networks (CNNs) serve as the equivalent of word2vec. Models like AlexNet, VGG, and ResNet revolutionized image processing by creating effective image embeddings.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese models convert images into high-dimensional vectors, preserving spatial hierarchies and semantic information.\u0026nbsp;\u003c/p\u003e\u003cp\u003eJust as word2vec transformed text data into meaningful vectors, CNNs transform images into embeddings that capture essential features, enabling tasks like object detection, image classification, and visual similarity search.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k_icon.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"250\" height=\"250\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eImageNet is a large-scale visual database designed for use in visual object recognition research, containing millions of labeled images across thousands of categories. Models like AlexNet, VGG, and ResNet demonstrated the power of CNNs using datasets like ImageNet.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"uploading-custom-embeddings-to-labelbox\"\u003eUploading custom embeddings to Quantumworks Lab\u0026nbsp;\u0026nbsp;\u003c/h2\u003e\u003cp\u003eLabelbox now supports importing and exporting custom embeddings seamlessly, allowing for better integration into workflows. This new feature provides flexibility in how embeddings are utilized, making it easier to leverage embeddings for various applications.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eimport Quantumworks Lab as lb\nimport transformers\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nimport requests\nimport numpy as np\n\n# Add your API key\nAPI_KEY = \"your_api_key_here\"\nclient = lb.Client(API_KEY)\n\n# Get images from a Quantumworks Lab dataset\nDATASET_ID = \"your_dataset_id_here\"\ndataset = client.get_dataset(DATASET_ID)\nexport_task = dataset.export_v2()\nexport_task.wait_till_done()\n\ndata_row_urls = [dr_url['data_row']['row_data'] for dr_url in export_task.result]\n\n# Get ResNet-50 from HuggingFace\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = transformers.ResNetModel.from_pretrained(\"microsoft/resnet-50\")\n\nimg_emb = []\n\nfor url in data_row_urls:\n    response = requests.get(url, stream=True)\n    image = Image.open(response.raw).convert('RGB').resize((224, 224))\n    img_hf = image_processor(image, return_tensors=\"pt\")\n    with torch.no_grad():\n        last_layer = model(**img_hf, output_hidden_states=True).last_hidden_state\n        resnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\n        resnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\n        img_emb.append(resnet_embeddings.cpu().numpy())\n\ndata_rows = []\n\nfor url, embedding in zip(data_row_urls, img_emb):\n    data_rows.append({\n        \"row_data\": url,\n        \"embeddings\": [{\"embedding_id\": new_custom_embedding_id, \"vector\": embedding[0].tolist()}]\n    })\n\ndataset = client.create_dataset(name='image_custom_embedding_resnet', iam_integration=None)\ntask = dataset.create_data_rows(data_rows)\nprint(task.errors)\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eTo compute your ow embeddings and send them to Quantumworks Lab, you can use models from Hugging Face. Here’s a brief example using ResNet-50:\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003cscript src=\"https://gist.github.com/mmbazel-labelbox/edb8efbab1904106009e1ed630199e29.js\"\u003e\u003c/script\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eThis code snippet demonstrates how to create and upload custom embeddings, which can then be used for similarity searches within Labelbox.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1694\" height=\"1176\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.00.41-AM.png 1694w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"why-you-should-care-about-custom-embeddings\"\u003eWhy you should care about custom embeddings\u003c/h3\u003e\u003cp\u003eCustom embeddings provide several advantages:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTailored Representations\u003c/strong\u003e: Custom embeddings can be tailored to your specific dataset, improving accuracy in domain-specific tasks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Performance\u003c/strong\u003e: They allow for more precise similarity searches and recommendations by capturing nuances specific to your data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFlexibility\u003c/strong\u003e: Custom embeddings offer flexibility in handling various types of data and use cases, making them a versatile tool in machine learning workflows.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"using-embeddings-for-better-search\"\u003eUsing embeddings for better search\u003c/h1\u003e\u003cp\u003eEarlier we mentioned that search (a core activity of search engines, recommendation systems, data curation, etc) is a common and important application of embeddings. How? By comparing how similar two (or many more) items are to each other.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"measuring-similarity\"\u003eMeasuring similarity\u003c/h2\u003e\u003cp\u003eThe most popular algorithm for measuring similarity between two vectors is cosine similarity. It calculates the angle between two vectors: the smaller the angle, the more similar they are. Cosine similarity scores range from 0 to 1, or -1 to 1 for dissimilarity.\u003c/p\u003e\u003cp\u003eFor example, if vectors W and V are close, their cosine similarity might be 0.79. If they are opposite, the similarity is lower, indicating dissimilarity. This method helps in visually and computationally understanding the similarity between different assets.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J0ha8suIAyD2qrFVEWI9TqDRLb2c8ForgqTmyvKQ8ZKsXLeHf5H96V0tQSMV_bCbqBsLWcBb3ljxSP8vKYDKElnlTv1zHq36uQG7mwzyP9HELFlHgGf7FTZO8AWH_ndg5OAXm3yLmbbEpVPg1Cvp108\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"718\" height=\"521\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOriginal source: \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Similarity-between-vectors-can-be-estimated-by-calculating-the-cosine-of-the-angle-th_fig1_333734049?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\"Similarity between vectors can be estimated by calculating the cosine of the angle θ between them.\"\u003c/span\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBecause embeddings are essentially vectors, we can apply many of the same operations used to analyze vectors to embeddings.\u003c/p\u003e\u003ch3 id=\"strategies-for-similarity-computation\"\u003eStrategies for similarity computation\u003c/h3\u003e\u003cp\u003eAdditional strategies for performing similarity computation include:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eBrute Force Search\u003c/strong\u003e: Comparing one asset against every other asset. This provides the highest accuracy but requires significant computational resources and time. For instance, running brute force searches on a dataset with millions of entries can lead to significant delays and high computational costs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eApproximate Nearest Neighbors (ANN)\u003c/strong\u003e: Comparing one asset against a subset of assets. This method reduces computational resources and latency while maintaining high accuracy. ANN algorithms like locality-sensitive hashing (LSH) and KD-trees provide faster search times, making them suitable for real-time applications, although they may sacrifice some accuracy.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHashing Methods\u003c/strong\u003e: Techniques like MinHash and SimHash provide efficient similarity searches by hashing data into compact representations, allowing quick comparison of hash values to estimate similarity. These methods are particularly useful for text and document similarity.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTree-based methods (KD-trees, R-trees, and VP-trees) and graph-based methods (like Hierarchical Navigable Small World, also known as HNSW) are also options.\u003c/p\u003e\u003ch2 id=\"leveraging-embeddings-for-data-curation-management-and-analysis-in-labelbox\"\u003eLeveraging embeddings for data curation, management, and analysis in Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThrough search, embeddings improve the efficiency and accuracy of data curation, management, and analysis in several ways:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eAutomated Data Organization\u003c/strong\u003e: Embeddings help group similar data points together, making it easier to organize and manage large datasets.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnhanced Search and Retrieval\u003c/strong\u003e: By transforming data into embeddings, search and retrieval operations become faster and more accurate, enabling quick access to relevant information.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImproved Data Analysis\u003c/strong\u003e: Embeddings capture underlying patterns and relationships within data, facilitating more effective analysis and insights extraction.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAnd these are exactly the ways we use embeddings in our products at Quantumworks Lab to \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimprove search for data\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, in \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e, users can find images similar to an input image, such as searching for basketball players on a court. Our natural language search allows users to input a textual query, like \"dog,\" and find relevant images.\u003c/p\u003e\u003cp\u003eLabelbox offers several features to streamline embedding workflows, including:\u003c/p\u003e\u003ch3 id=\"data-curation-and-management\"\u003eData curation and management\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/custom-embeddings?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCustom Embeddings\u003c/u\u003e\u003c/a\u003e in Catalog: Surfaces high-impact data, with automatic computation for various data types.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/manage-selections?ref=labelbox-guides.ghost.io#smart-select\"\u003e\u003cu\u003eSmart Select\u003c/u\u003e\u003c/a\u003e in Catalog: Curates data using random, ordered, and cluster-based methods.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/cluster-view?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCluster View\u003c/u\u003e\u003c/a\u003e: Discovers similar data rows and identifies labeling or model mistakes.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1620\" height=\"838\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.31-AM.png 1620w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Cluster view to explore relationships between data rows, identify edge cases, and select rows for pre-labeling or human review.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eNatural Language\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSimilarity Search\u003c/u\u003e\u003c/a\u003e: Supports video and audio assets.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-metrics?ref=labelbox-guides.ghost.io#automatic-metrics\"\u003e\u003cu\u003ePrediction-level Custom Metrics\u003c/u\u003e\u003c/a\u003e: Filters and sorts by custom metrics, enhancing model error analysis.\u003c/li\u003e\u003cli\u003ePre-label Generation from \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFoundry\u003c/u\u003e\u003c/a\u003e Models: Streamlines project workflows.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/video-annotations?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnhanced Video Player\u003c/u\u003e\u003c/a\u003e: Aids in curating, evaluating, and visualizing video data.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1630\" height=\"904\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-04-at-11.01.56-AM.png 1630w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWould a rose smell as sweet if it was merely one of a thousand easily discovered in a flower dataset using a natural language search? Trick question: you can't smell photos. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"embeddings-schema\"\u003eEmbeddings schema\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/working-with-features?ref=labelbox-guides.ghost.io#create-features-from-the-schema-tab\"\u003e\u003cu\u003eSchema Tab\u003c/u\u003e\u003c/a\u003e: Includes an Embeddings subtab for viewing and creating custom embeddings.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"api-and-inferencing\"\u003eAPI and inferencing\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/run-foundry-apps?ref=labelbox-guides.ghost.io#run-app-using-rest-api\"\u003e\u003cu\u003eInferencing Endpoints\u003c/u\u003e\u003c/a\u003e: Generate predictions via REST API without creating data rows or datasets.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"additional-workflow-enhancements\"\u003eAdditional workflow enhancements\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBoost Express: Request additional labelers for data projects.\u003c/li\u003e\u003cli\u003eExport v2 Workflows and Improved SDK: Streamlines data management.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThese features ensure that embeddings are utilized effectively, making your workflows more efficient and your models more accurate.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eEmbeddings are a powerful tool in machine learning, enabling efficient and accurate similarity searches and recommendations. We hope this post has clarified what embeddings are and how they can be utilized.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor more on uploading custom embeddings, check out our \u003ca href=\"https://docs.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/page/tutorials?ref=labelbox-guides.ghost.io\"\u003edeveloper guides\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAdditional resources on embeddings can be found here:\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jbwzrxh814\" title=\"Embeddings Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWatch the video: \u003ca href=\"https://labelbox.wistia.com/medias/jbwzrxh814?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eUnderstanding embeddings in machine learning\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eLearn more in our community about:\u003cu\u003e \u003c/u\u003e\u003ca href=\"https://community.labelbox.com/t/how-to-upload-custom-embedding-s-and-why-you-should-care/1169?ref=labelbox-guides.ghost.io#why-would-i-need-embedding-2\"\u003e\u003cu\u003eHow to upload custom embeddings and why you should care\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThanks to our contributors Tomislav Peharda, Paul Tancre, and Mikiko Bazeley! \u003c/p\u003e","comment_id":"6656135f9132db0001f20551","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/05/YT-Thumbnails--23-.jpg","featured":false,"visibility":"public","created_at":"2024-05-28T17:24:47.000+00:00","updated_at":"2024-09-04T18:02:27.000+00:00","published_at":"2024-05-28T20:32:29.000+00:00","custom_excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},"url":"https://labelbox-guides.ghost.io/ai-foundations-understanding-embeddings/","excerpt":"Learn how to utilize embeddings for data vector representations and discover key use cases at Quantumworks Lab, including uploading custom embeddings for optimized performance.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"662804d1d733f0000145b1c4","uuid":"3db0aafe-d6b7-470a-8ff0-37835d066fe4","title":"How to harness AI for efficient video labeling","slug":"harnessing-ai-for-efficient-video-labeling","html":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eWorking in collaboration with numerous leading companies in artificial intelligence, we're observing a surge in enthusiasm for using advanced models to initially label data, integrating human expertise later to refine and tailor previously labor-intensive and time-consuming tasks.\u003c/p\u003e\u003cp\u003eThese AI models are transforming one of the most daunting tasks in machine learning—the creation of high-quality video datasets. Utilizing such models allows machine learning teams to leverage automated tools to pre-label or enrich data, facilitating a range of applications from monitoring driver behavior to detecting objects in manufacturing environments.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis blog post will explore how models like Gemini 1.5 Pro, Grounding DINO, and SAM are redefining the video labeling landscape, while boosting efficiency and speed. \u003c/p\u003e\u003cp\u003eBy automating the labor-intensive labeling tasks, these models not only accelerate the workflow, but also liberate time for users and decrease labeling costs.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"steps\"\u003eSteps\u003c/h2\u003e\u003ch3 id=\"step-1-select-video\"\u003eStep 1: Select video\u0026nbsp;\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eTo select a video: \u003c/p\u003e\u003cul\u003e\u003cli\u003eNarrow in on a subset of data. Users can use Quantumworks Lab Catalog filters, including media attributes, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eUsers can click “Predict with Foundry” once the data of interest is selected.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-choose-a-model-of-interest\"\u003eStep 2: Choose a model of interest\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eTo select a model: \u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, users will be prompted to choose a model of interest for a model run.\u003c/li\u003e\u003cli\u003eThen, select a model from the ‘model gallery’ based on the type of task - such as video classification, video object detection, and video segmentation.\u003c/li\u003e\u003cli\u003eTo locate a specific model, users can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine-learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-configure-model-settings-and-submit-a-model-run\"\u003eStep 3: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eOnce the model of interest is selected, users can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of settings, which can be found in the Advanced model setting.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can generate preview predictions on up to five data rows to understand how the current model settings affect the final predictions.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows users to confirm the configuration settings confidently:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf users are unhappy with the generated preview predictions, they can edit the model settings and continue to generate preview predictions until they're satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eAfter users are satisfied with the preview predictions, a model run can be submitted.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-send-the-images-to-annotate\"\u003eStep 4: Send the images to Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eUsers can transfer the results to a labeling project using the UI via the \"Send to Annotate\" feature. Labelers can then quickly review labels for accuracy.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"pre-labeling-use-cases\"\u003ePre-labeling Use Cases\u003c/h2\u003e\u003ch3 id=\"example-1-segmentation-mask-using-grounding-dino-sam\"\u003eExample 1: Segmentation mask using Grounding DINO + SAM\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eSegmentation masks\u003c/a\u003e are used for autonomous vehicles, medical imagery, retail applications, face recognition and analysis, video surveillance, satellite image analysis, etc. Masks are some of the most time-consuming annotations to make for video. Below, we see an example of how this can be automated with Grounding DINO + SAM so the reviewers can make small edits if needed instead of starting from scratch.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7k184d0h65\" title=\"Pre-labeling Use Case: Segmentation mask using Grounding DINO + SAM Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"example-2-bounding-box-using-grounding-dino\"\u003eExample 2: Bounding box using Grounding DINO\u003c/h3\u003e\u003cp\u003eBounding boxes are utilized in similar scenarios as segmentation masks, but these scenarios demand less precision than those requiring pixel-level (masks) detail. Bounding boxes can be automated using Grounding DINO, as illustrated below with detection of a person in video.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ie9s2r3zff\" title=\"Pre-labeling Use Case: Bounding Box using Grounding DINO Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"example-3-global-classification-using-gemini-15-pro\"\u003eExample 3: Global classification using Gemini 1.5 Pro\u003c/h3\u003e\u003cp\u003eGlobal classification for video is used when the overall classification for video is required like when a driver safety system needs to detect if a driver is distracted. Gemini 1.5 Pro can analyze an hour long video and provide answers about events that took place in the video. This automation reduces the need for human intervention, allowing personnel to focus on reviewing videos only when they are flagged with specific classifications. \u003c/p\u003e\u003ch3 id=\"example-4-frame-based-classification-using-gemini-15-pro\"\u003eExample 4: Frame based classification using Gemini 1.5 Pro\u003c/h3\u003e\u003cp\u003eFrame-based classification is utilized in scenarios similar to segmentation masks. Gemini 1.5 Pro can analyze an hour-long video and identify the specific timestamps for a particular event. \u003c/p\u003e\u003ch2 id=\"additional-considerations\"\u003e\u003cstrong\u003eAdditional considerations\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAdditional considerations as users incorporate Foundry labels into their projects and workflows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo produce frame based binary classifications in Gemini 1.5 Pro, users are recommended to experiment with the prompt and provide as much context as possible to get the best results. \u003cul\u003e\u003cli\u003eFor example, the following yields better results:\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003c!--kg-card-begin: html--\u003e\n“For the given video, what timestamps have a banana and be as thorough as possible about checking each second for a banana. Make sure there is no overlap in timestamps. Return the result as a dictionary without any new lines. {“banana”: “\u003ctimestamps-with-banana\u003e“, “no_banana”:  “\u003ctimestamps-without-banana\u003e“}” than using “For the given video, what timestamps have a banana. Return the result as a dictionary without any new lines. {“banana”: “\u003ctimestamps-with-banana\u003e” }”\n\u003c!--kg-card-end: html--\u003e\n\u003cul\u003e\u003cli\u003eUsers can also \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA/B test different models \u003c/u\u003e\u003c/a\u003efrom Foundry to find the model that best fits the use case using Quantumworks Lab Model.\u003c/li\u003e\u003cli\u003eIf we do not support your use case or any questions arise, feel free to contact our \u003ca href=\"https://docs.labelbox.com/docs/contacting-customer-support?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esupport team\u003c/u\u003e\u003c/a\u003e, as we would love to hear your feedback to improve Foundry.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAnnotating video data has traditionally been a tedious and time-consuming task. The integration of advanced AI models from Quantumworks Lab Foundry into the video labeling process marks a significant transformation in how video data is annotated. By leveraging Foundry's capabilities, users can drastically speed up their video labeling projects. This acceleration not only diminishes the time required to bring products to market but also substantially reduces the costs involved in model development.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our additional resources on how to utilize state-of-the-art AI models in Foundry, including using \u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003emodel distillation\u003c/a\u003e and fine-tuning to leverage the power of foundation models:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-accelerate-labeling-projects/?ref=labelbox-guides.ghost.io#conclusion\"\u003e\u003cu\u003eHow to accelerate labeling projects using GPT–4V in Foundry\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io#what-are-the-benefits-of-using-image-segmentation-for-my-machine-learning-model\"\u003e\u003cu\u003eHow to create high-quality image segmentation masks quickly and easily\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://event.on24.com/wcc/r/4460171/5D5357846BEAB1FD1E12A5ED146DE6E1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomated Labeling With Any Foundation Model\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFine tune LLMs\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"662804d1d733f0000145b1c4","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/image.png","featured":false,"visibility":"public","created_at":"2024-04-23T18:58:25.000+00:00","updated_at":"2024-11-22T23:53:12.000+00:00","published_at":"2024-04-23T23:40:19.000+00:00","custom_excerpt":"Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/harnessing-ai-for-efficient-video-labeling/","excerpt":"Learn how models like Gemini 1.5 Pro, Grounding DINO, and SAM can be used to pre-label videos. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"661fffd84efc960001cf4cac","uuid":"30ac0bda-fee0-4ffa-93bd-357a123fa51a","title":"How to turn LangSmith logs into conversational data with Quantumworks Lab","slug":"turn-langchain-logs-into-conversational-data-with-labelbox","html":"\u003cp\u003eTo design task-specific LLMs, a step-by-step approach that will improve their day-to-day usability needs to be taken while ensuring safety and relevance and obtaining user feedback. The success of these in real-world scenarios depends on the availability of reliable, high-quality training data and with alignment from human preferences.\u003c/p\u003e\u003cp\u003eLangChain, one of the most popular frameworks for building LLM-powered applications, is complemented by LangSmith, a unified developer platform for building, testing, and monitoring LLM applications. Together, LangChain and LangSmith provide the framework and platform for you to manage the entire LLM-powered application lifecycle.\u003c/p\u003e\u003cp\u003eLabelbox offers a comprehensive data-centric AI platform that includes a native labeling editor, model-assisted labeling, human-labeling workflows, human labeling workforce, and diagnostics to align task-specific models and develop intelligent applications across various data modalities, including text, documents, audio, images, and video, while also providing features like data catalog, model automation, and workforce services to optimize labeling operations and scale human evaluation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.24.59-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1252\" height=\"686\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-22-at-8.24.59-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-22-at-8.24.59-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.24.59-PM.png 1252w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this tutorial guide, we’ll walk through combining Quantumworks Lab and LangSmith by getting conversation data created with \u003ca href=\"https://www.langchain.com/langsmith?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLangSmith\u003c/u\u003e\u003c/a\u003e to \u003ca href=\"https://labelbox.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBelow is also a video guide for more details: \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9qrdqetxgs\" title=\"Langsmith to Quantumworks Lab Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch1 id=\"steps\"\u003eSteps\u0026nbsp;\u003c/h1\u003e\u003ch2 id=\"what-you%E2%80%99ll-need\"\u003eWhat you’ll need\u003c/h2\u003e\u003cul\u003e\u003cli\u003eLabelbox API key\u003c/li\u003e\u003cli\u003eLangSmith API key\u003c/li\u003e\u003cli\u003eOpenAI API\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can also follow-along with this \u003ca href=\"https://colab.research.google.com/drive/1tjiTgL5rgcsfgMDOYfzqMJab0IANFpfO?usp=sharing\u0026ref=labelbox-guides.ghost.io#scrollTo=bJCokRW6PRe6\"\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"setup\"\u003eSetup\u003c/h2\u003e\u003cp\u003eUtilizing \u003ca href=\"https://python.langchain.com/docs/langsmith/walkthrough?ref=labelbox-guides.ghost.io#exporting-datasets-and-runs\"\u003e\u003cu\u003eLangSmith Python SDK\u003c/u\u003e\u003c/a\u003e, you can run a variety of LLMs on a test dataset, and example prompts to evaluate a model’s performance. First, you must create a dataset inside \u003ca href=\"https://smith.langchain.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLangSmith\u003c/u\u003e\u003c/a\u003e. Make sure to keep track of the name you give your dataset and set the type as Chat:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.33-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1250\" height=\"622\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-22-at-8.25.33-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-22-at-8.25.33-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.33-PM.png 1250w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAfter you create your dataset, add a few example prompts that can be used to evaluate your model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.47-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1252\" height=\"626\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-22-at-8.25.47-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-22-at-8.25.47-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.47-PM.png 1252w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.59-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1250\" height=\"594\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/09/Screenshot-2024-09-22-at-8.25.59-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/09/Screenshot-2024-09-22-at-8.25.59-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/09/Screenshot-2024-09-22-at-8.25.59-PM.png 1250w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"evaluate-model\"\u003eEvaluate Model\u003c/h2\u003e\u003cp\u003eNow that you are set up, you can evaluate your model with your example prompts. The notebook included in this tutorial goes over this process. We will be using the chain_results after running our model on our dataset. Below is an example of what that would look like:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003echain_results = run_on_dataset(\n    dataset_name=LS_DATASET_NAME,\n    llm_or_chain_factory=functools.partial(\n        create_agent, prompt=prompt, llm_with_tools=llm_with_tools\n    ),\n    evaluation=evaluation_config,\n    verbose=True,\n    client=client,\n    project_name=f\"tools-agent-test-5d466cbc-{unique_id}\",\n    # Project metadata communicates the experiment parameters,\n    # Useful for reviewing the test results\n    project_metadata={\n        \"env\": \"testing-notebook\",\n        \"model\": \"gpt-3.5-turbo\",\n        \"prompt\": \"5d466cbc\",\n    },\n)\u003c/code\u003e\u003c/pre\u003e\u003chr\u003e\u003ch2 id=\"create-labelbox-data-rows\"\u003eCreate Quantumworks Lab Data Rows\u003c/h2\u003e\u003cp\u003eWith the chain results obtained above, you can format to Quantumworks Lab conversation data using the function below. Please see the Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/reference/text-conversational?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eImport Conversation Text Data\u003c/u\u003e\u003c/a\u003e developer guides for more information.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003efrom uuid import uuid4\n\ndef import_conversational (chain_results: dict[str:str], user_id_dict: dict[str:str], output_user_name: str) -\u0026gt; dict[str:str]:\n    \"\"\"Turn chain_result dictionary object from Langchain to conversation data for Quantumworks Lab\n\n    Args:\n        chain_results (dict[str:str]): LangChain evaluation results.\n        user_id_dict (dict[str:str]): Dictionary matching chat type of LangChain(user) to Quantumworks Lab type with corresponding Quantumworks Lab UserID and Chat Alignment.\n                                      {\u0026lt;Langchain user\u0026gt;: {id: \u0026lt;Quantumworks Lab userid\u0026gt;, \"alight\", \u0026lt;Quantumworks Lab alignment (right or left)\u0026gt;}}\n        output_user_name (str): LangChain output user type.\n    Returns:\n        list[\u0026lt;Quantumworks Lab data tows\u0026gt;]\n    \"\"\"\n    lb_conversations = []\n    for conversational in chain_results[\"results\"].values():\n        lb_conversation =   {\n        \"row_data\": {\n            \"type\": \"application/vnd.labelbox.conversational\",\n            \"version\": 1,\n            \"messages\": []\n        },\n        \"global_key\": str(uuid4()),\n        \"media_type\": \"CONVERSATIONAL\",\n        }\n        if \"input\" in conversational[\"output\"]:\n            for input in conversational[\"output\"][\"input\"]:\n                lb_conversation[\"row_data\"][\"messages\"].append({\n                    \"content\": input[\"data\"][\"content\"],\n                    \"user\": {\n                        \"userId\": user_id_dict[input[\"type\"]][\"id\"],\n                        \"name\": input[\"type\"]\n                    },\n                    \"canLabel\": True,\n                    \"align\": user_id_dict[input[\"type\"]][\"align\"],\n                    \"messageId\": str(uuid4())\n                })\n        if \"output\" in conversational[\"output\"]:\n            output = conversational[\"output\"][\"output\"]\n            lb_conversation[\"row_data\"][\"messages\"].append({\n                \"content\": output,\n                \"user\": {\n                    \"userId\": user_id_dict[output_user_name][\"id\"],\n                    \"name\": output_user_name\n                },\n                \"canLabel\": True,\n                \"align\": user_id_dict[output_user_name][\"align\"],\n                \"messageId\": str(uuid4())\n            })\n        lb_conversations.append(lb_conversation)\n    return lb_conversations\u003c/code\u003e\u003c/pre\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe function uses a dictionary python object parameter (user_id_dict) to match LangSmith user types to Quantumworks Lab types along with their alignment inside the editor. The messages in this example are all marked as “canLabel: True.” This means that all messages can be annotated inside our editor. Also, the global key is set randomly through the UUID Python library, but you can have the global_key set to the ID associated with the example run to avoid importing duplicate information.\u003c/p\u003e\u003ch2 id=\"import-data-rows-into-labelbox\"\u003eImport Data Rows into Quantumworks Lab\u003c/h2\u003e\u003cp\u003eNow that you have created your data rows with your conversational data, you can import them into Labelbox. Please reference these \u003ca href=\"https://docs.labelbox.com/reference/dataset?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edocuments\u003c/u\u003e\u003c/a\u003e from Quantumworks Lab for more information. The script below demonstrates creating a Quantumworks Lab dataset and importing your data rows.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e\nimport Quantumworks Lab as lb\n\nclient = lb.Client(api_key=\"\u0026lt;YOUR_API_KEY\u0026gt;\")\n\ndataset = client.create_dataset(name='\u0026lt;dataset_name\u0026gt;')\n\ndataset.create_data_rows(\"\u0026lt;data row payload\u0026gt;\")\u003c/code\u003e\u003c/pre\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"use-cases-and-implication\"\u003eUse Cases and Implication\u003c/h1\u003e\u003cp\u003eUtilizing Quantumworks Lab with LangSmith can better develop chatbots by engaging large language models (LLMs) for conversation intent classification. This combination enables constant model tracking and evaluation because LangSmith and Quantumworks Lab ensure the chatbot performance adheres to human preferences through proper training data. Also, alongside a human-in-the-loop system, semantic search and pre-trained models for pre-labeling allow nuanced analysis.\u003c/p\u003e\u003cp\u003eCombining efforts between Quantumworks Lab and LangSmith enhances the performance of RAG-based agents by \u003ca href=\"https://labelbox.com/guides/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/?ref=labelbox-guides.ghost.io\"\u003eimproving reranking models\u003c/a\u003e and preprocessing documents to make them more relevant. Considering a broader collection of retrieved text or incorporating more human-generated labels rather than relying only on a single document can facilitate the generation of high-quality training labels, hence making the \u003ca href=\"https://labelbox.com/guides/how-to-improve-your-task-specific-chatbot-for-safety-relevancy-and-user-feedback/?ref=labelbox-guides.ghost.io\"\u003echatbot responses more relevant and accurate\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe tutorial above demonstrates the combination of LangSmith with Quantumworks Lab, enabling some of the mentioned use cases.\u003c/p\u003e\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\u003cp\u003eOnce you have your data rows inside Quantumworks Lab, you can send them to an \u003ca href=\"https://docs.labelbox.com/docs/what-is-a-project?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eannotate project \u003c/u\u003e\u003c/a\u003ewhere you can apply a variety of \u003ca href=\"https://docs.labelbox.com/docs/conversational-annotations?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eannotations\u003c/u\u003e\u003c/a\u003e or have a popular LLM model generate pre-labels that can then be evaluated with a human in the loop. After annotating your data rows, you can utilize the generated labels to train your LLM further. You can then reassess and annotate your predictions using the same workflow.\u003c/p\u003e\u003cp\u003eIf you’re interested in learning more about the capabilities offered by Quantumworks Lab and LangChain for supporting generative AI development, check out our \u003ca href=\"https://labelbox.com/blog/seamless-llm-human-evaluation-with-langsmith-and-labelbox/?ref=labelbox-guides.ghost.io\"\u003epartnership announcement\u003c/a\u003e for additional entails and use cases.\u003c/p\u003e\u003cp\u003eInterested in joining the conversation? Let us know what you think in our original post \u003ca href=\"https://community.labelbox.com/t/how-to-convert-langchain-results-to-labelbox-conversation-data/1091?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eReach out to the \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox sales team\u003c/u\u003e\u003c/a\u003e to start implementing a hybrid evaluation system for your production generative AI applications.\u003c/p\u003e","comment_id":"661fffd84efc960001cf4cac","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/Frame-3783.png","featured":false,"visibility":"public","created_at":"2024-04-17T16:59:04.000+00:00","updated_at":"2024-09-23T15:46:09.000+00:00","published_at":"2024-04-17T18:24:56.000+00:00","custom_excerpt":"Use Quantumworks Lab's human \u0026 AI evaluation capabilities to turn LangSmith chatbot and conversational agent logs into data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/turn-langchain-logs-into-conversational-data-with-labelbox/","excerpt":"Use Quantumworks Lab's human \u0026 AI evaluation capabilities to turn LangSmith chatbot and conversational agent logs into data.","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66183ccc8d5c4a00014061cc","uuid":"2afa1d4e-85b9-48d3-ae01-81c57db2916b","title":"How to use AI to improve website search relevance","slug":"how-to-improve-search-relevance","html":"\u003cp\u003eWith the latest advances in foundation models, organizations can now enhance search relevance for websites by better matching between user intent with product listings. While companies now have access to a wealth of search queries, sifting through all of these search results can be incredibly time-consuming and resource-intensive. By leveraging AI, teams can now analyze search queries and feedback at scale, to gain insights into common topics or customer sentiment. This allows businesses to identify common themes and pinpoint areas of improvement to enhance\u0026nbsp;their overall website experience to maximize for key metrics such as user retention, conversion and revenue.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for search relevance. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eImproving search relevance requires a vast amount of data in the form of search queries and accurate product descriptions. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape: \u003c/strong\u003eThe changing nature and format of customer review data from multiple sources (e.g webpages, apps, social media, etc.) poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their website search relevance for product descriptions and listings. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s assisted data enrichment and flexible training frameworks to quickly build AI systems that uncover actionable insights from customer searches. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-full\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1746\" height=\"952\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-4.22.09-PM.png 1746w\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to dramatically improve search relevance for any website or app. Specifically, this guide will walk through how you can explore and better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-use-ai-to-improve-search-relevance-for-your-website\"\u003eSee it in action: How to use AI to improve search relevance for your website\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below in either: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1-kTALPqchHXGogUdRVnya7GklKHrsWVL?ref=labelbox-guides.ghost.io#scrollTo=Vw_nNPl8baNJ\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1-kTALPqchHXGogUdRVnya7GklKHrsWVL?ref=labelbox-guides.ghost.io#scrollTo=Vw_nNPl8baNJ\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs customer queries and product descriptions across channels proliferate, brands want to learn from customer feedback to build the most user-friendly experience on their website or app. For this use case, we’ll be working with a dataset of e-commerce website queries – with the goal of analyzing the queries to demonstrate how a company could gain insight into how their customers search for products and how to optimize for relevance.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/vpl1wf0vui\" title=\"Search relevance 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003eFor the purpose of this tutorial, we’ve provided a sample open-source \u003ca href=\"https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe?ref=labelbox-guides.ghost.io\"\u003eKaggle dataset\u003c/a\u003e that can be downloaded.  \u003c/p\u003e\u003cp\u003ePlease \u003ca href=\"https://drive.google.com/file/d/1hh2MYzol6-4PSsqX7mbX2YSLLGA4jAYZ/view?usp=drivesdk\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edownload the dataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your text data as a CSV. \u003c/li\u003e\u003cli\u003eIf your text snippets sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your text data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data-by-clustering\"\u003eSearch and curate data by clustering\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1538\" height=\"708\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-29-at-9.12.17-AM.png 1538w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Smart select to cluster data and focus your model improvement on specific data rows\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExplore topics of interest\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith your data in Quantumworks Lab, you can begin to leverage Catalog to uncover interesting topics to get a sense of what customers are searching for. \u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize your data –\u0026nbsp; you can click through individual data rows to get a sense for which queries are the most popular.\u003c/li\u003e\u003cli\u003eDrill into specific topics of interest\u003cstrong\u003e \u003c/strong\u003e– leverage natural language search, for example searching “beds, mirrors, etc,” to bring up all related queries related to that topic. You can adjust the confidence threshold of your searches accordingly which can be helpful in gauging the volume of data related to the topic of interest.\u0026nbsp;\u003c/li\u003e\u003cli\u003eEasily find all instances of similar examples to data of interest.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-2-create-an-initial-model-run-of-search-relevance-assessments\"\u003ePart 2: Create an initial model run of search relevance assessments \u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/t3er59wcfk\" title=\"Search relevance 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can proceed to using Quantumworks Lab's Foundry product to model run an initial model run to accelerate search relevance assessments.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, you'll need to set up your ontology for search relevance assessment based on your project's requirements.\u003c/li\u003e\u003cli\u003eAfterwards, you can define the criteria for rating the relevance of search results to each type of query.\u003c/li\u003e\u003cli\u003eNext, you can communicate the business definition of relevance to the models directly into the prompt. You can use Foundry to add context to the prompt, allowing it to rank results as if it were part of your respective business. In this example, we'll include the prompts for what \"good relevance\", \"excellent relevance\", etc and help the model predict what would fit under this criteria. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAs an illustrative example, you can set up \"excellent relevance\" as a result that perfectly matches the search query, including all specific attributes (category, material, color, purpose, etc). This indicates that the term is exactly what the user is searching for. For the query, \"kitchen blender stainless steel\", a result for \"stainless steel countertop blender\" is highly relevant, matching the user's intended category.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"673\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.04.26-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eGenerate an initial preview to assess how well the adjusted prompt performs and you can save the adjusted prompt as an app, including data type (text), ontology, and the original prompt. This allows for easy re-use and the ability to build upon the saved app for future assessments of search relevance criteria. \u003c/p\u003e\u003cp\u003eAfter this has been set up, you can now generate the next preview to ensure quality before submitting the model run for assessments.\u003c/p\u003e\u003ch3 id=\"view-search-relevance-assessments-results\"\u003eView search relevance assessments results \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gq8nnv2ykw\" title=\"Search relevance 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce your model inferencing job has been completed, you can then navigate to the model tab and locate a variety of foundation models (e.g., Claude 3 will be used in this tutorial) to view the completed model run with your rankings.\u003c/li\u003e\u003cli\u003eOptionally, you can add an explanation classification or review the results of the ranking, which includes all 560 items/data rows.\u003c/li\u003e\u003cli\u003eBy adding the results to your project, you can next perform further analytics such as analyzing the distributions of predictions from the Metrics view.\u003c/li\u003e\u003cli\u003eNext, select all items and you can send them to Annotate, and choose \"search relevance assessment\", where you'll then be able to have humans review as an additional quality check.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"further-analyze-and-optimize-your-search-relevance-assessments-results\"\u003eFurther analyze and optimize your search relevance assessments results \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kw7in8bosa\" title=\"Search relevance 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThe last part of the walkthrough is to analyze your distribution of relevance categories within your project, noting varying levels of relevance and to review the query class for all 560 data rows to identify trends in relevance. You can do this by using automated approaches to understand query types and relevance patterns, as we show in the video above.\u003c/li\u003e\u003cli\u003eBy filtering your dataset by the search relevance assessment project, you can navigate to the Analytics view to identify trends and examples of excellent relevance and poor relevance within specific query classes (as shown below).\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"883\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.14.34-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample analytics distribution showing excellent relevance on terms like kids wall decor, sectionals and area rugs.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"838\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Screenshot-2024-04-11-at-1.15.17-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample analytics distribution showing poor relevance for beds, furniture cushions, mirrors\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAs this time, you can consider adjusting prompts to accurately reflect relevance criteria, or use metadata fields, such as query class, to further analyze relevance.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo further evaluate and enrich the data, teams can also explore incorporating human supervision in the labeling process, with a hybrid or combination approaches: fully automated, half human in the loop, half automated, or all human-in-the-loop.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can improve your data further in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eAlignerr\u003c/a\u003e: Leverage our global network of\u0026nbsp;specialized labelers for a variety of tasks.\u0026nbsp;This community of subject matter experts from several disciplines align AI models by creating high-quality data in their field of expertise. The community spans nearly every major discipline of sciences, industries and languages, worldwide.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eBy tapping into the most recent developments in foundation models, businesses can transform the effectiveness of website searches by refining the alignment between user intent and product offerings. Given the abundance of search queries that a prospective customer may use, the process of sorting through them manually is labor-intensive and time-consuming. \u003c/p\u003e\u003cp\u003eBy harnessing the power of AI, organizations can efficiently examine search queries and feedback on a large scale, uncovering recurring themes and gauging customer sentiment. \u003c/p\u003e\u003cp\u003eThis enables enterprises to detect prevalent trends and target areas for enhancement, allowing them to optimizing the overall website experience to drive key metrics like user retention, conversion rates, and revenue. Remember to optimize the \u003ca href=\"https://www.web4business.com.au/portfolio-item/the-most-important-24-pages-to-include-on-website/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ewebsite content\u003c/a\u003e as well to ensure it's meeting your end user's goals. Give the walkthrough a try and we also recommend checking out our other solution accelerators such as \u003ca href=\"https://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003epersonalized experiences\u003c/a\u003e for retail to improve customer experiences.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful search relevance websites. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"66183ccc8d5c4a00014061cc","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/thumbnail--6-.png","featured":false,"visibility":"public","created_at":"2024-04-11T19:41:00.000+00:00","updated_at":"2024-09-12T23:45:53.000+00:00","published_at":"2024-04-12T17:02:16.000+00:00","custom_excerpt":"Learn how you can dramatically improve search relevance for any website or app by using AI and foundation models to better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-improve-search-relevance/","excerpt":"Learn how you can dramatically improve search relevance for any website or app by using AI and foundation models to better understand search query topics and classify product descriptions/listings to make more data-driven business decisions around the customer experience.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"660b13c7f4cd5600015bbd7f","uuid":"01794d37-2e79-4fe4-a76f-cb512edfddbf","title":"How to pre-label images in your annotation project","slug":"how-to-pre-label-images-using-model-assisted-labeling-in-your-annotation-project","html":"\u003cp\u003eWith the recent integration of model-assisted labeling (MAL) directly into Quantumworks Lab projects, labeling teams can now quickly generate pre-labels using the most powerful foundation models available today. This is particularly valuable for computer vision projects where pre-labeling images with bounding boxes and segmentation masks can significantly reduce the time required for human labelers to annotate data.\u003c/p\u003e\u003ch2 id=\"use-model-assisted-labeling-in-your-project\"\u003eUse model-assisted labeling in your project\u003c/h2\u003e\u003cp\u003eCheck out how model-assisted labeling works within Quantumworks Lab projects:\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xulxg54oi8\" title=\"Model assisted labeling in Annotate projects Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"598\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePre-label images with model-assisted labeling in Quantumworks Lab\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWithin a Quantumworks Lab project, you now have the option to invoke powerful computer vision models to run through your project’s data and generate pre-labels in a matter of minutes. This enables your human labelers to simply correct and submit labels instead of starting from scratch.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"pre-labeling-with-bounding-boxes\"\u003ePre-labeling with bounding boxes\u003c/h2\u003e\u003cp\u003eThere are many state of the art foundation models available for adding bounding boxes to images. Each of these models come with different advantages and configuration options depending on your use case. For example, \u003ca href=\"https://labelbox.com/product/model/foundry-models/groundingdino/?ref=labelbox-guides.ghost.io\"\u003eGrounding DINO\u003c/a\u003e and \u003ca href=\"https://labelbox.com/product/model/foundry-models/owl-vit/?ref=labelbox-guides.ghost.io\"\u003eOWL-ViT\u003c/a\u003e are prompt-based models that take in general phrases, like “trees” or “black mug”, and can intelligently detect objects that fit the description. These models are great for custom use cases since they can generalize well to new objects based on the description you provide. On the other hand, \u003ca href=\"https://labelbox.com/product/model/foundry-models/yolov8-object-detection/?ref=labelbox-guides.ghost.io\"\u003eUltralytics YOLOv8\u003c/a\u003e does not require a prompt. It has been trained on an extensive list of common objects in our day to day life, so it can identify those objects in your image without any prompt. It works well for most common use cases right out of the box.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Frame-3698-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1670\" height=\"689\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Frame-3698-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Frame-3698-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Frame-3698-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Frame-3698-1.png 1670w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eObject detection models for pre-labeling bounding boxes\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"pre-labeling-with-segmentation-masks\"\u003ePre-labeling with segmentation masks\u003c/h2\u003e\u003cp\u003eFor many computer vision use cases, applying segmentation masks to images is critical especially in complex scenarios where precise delineation of objects is necessary. Drawing segmentation masks used to be a manual, tedious process of using a pen or brush tool, but using foundation models in Quantumworks Lab can save valuable human labeling time and accelerate your labeling process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe recommend considering Meta’s \u003ca href=\"https://labelbox.com/product/model/foundry-models/sam/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eSegment Anything Model\u003c/a\u003e (SAM), the best-in-class segmentation foundation model that can draw masks around any discrete object in the image. You can also run GroundingDINO or OWL-ViT to draw bounding boxes around objects of interest first, and then feed those boxes into SAM to draw masks within each box. All of this is possible with a few clicks in Quantumworks Lab projects.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/04/Cards-5--2-.png\" class=\"kg-image\" alt=\"Segment Anything Model (SAM) from Meta\" loading=\"lazy\" width=\"2000\" height=\"653\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/04/Cards-5--2-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/04/Cards-5--2-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/04/Cards-5--2-.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/04/Cards-5--2-.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSegment Anything Model (SAM) from Meta\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\u003cp\u003eLabelbox has been a pioneer of model-assisted labeling for years, starting out by using our powerful SDK to upload model-generated annotations. After working closely with our customers, we’re excited to streamline the experience and integrate MAL directly into your labeling workflow with just a few clicks and no-code required. \u003c/p\u003e\u003cp\u003eTry it when you \u003ca href=\"https://app.labelbox.com/new-project/image?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecreate your next image project\u003c/u\u003e\u003c/a\u003e in Labelbox. And don’t worry, we are continuing to invest in supercharging your labeling workflows with the power of foundation models. So stay tuned, there is more to come!\u003c/p\u003e\u003cp\u003eIf you are not already using Quantumworks Lab, you can \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started for free\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003econtact us\u003c/a\u003e to learn more about pre-labeling images using model-assisted labeling.\u003c/p\u003e","comment_id":"660b13c7f4cd5600015bbd7f","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/Labelbox-model-assisted-labeling.png","featured":false,"visibility":"public","created_at":"2024-04-01T20:06:31.000+00:00","updated_at":"2024-04-02T18:24:51.000+00:00","published_at":"2024-04-02T18:24:51.000+00:00","custom_excerpt":"With model-assisted labeling (MAL) now directly integrated into Quantumworks Lab projects, learn how you can quickly generate pre-labels using the most powerful foundation models available today.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/how-to-pre-label-images-using-model-assisted-labeling-in-your-annotation-project/","excerpt":"With model-assisted labeling (MAL) now directly integrated into Quantumworks Lab projects, learn how you can quickly generate pre-labels using the most powerful foundation models available today.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":"Pre-label images with model-assisted labeling in Quantumworks Lab","feature_image_caption":null},{"id":"65ab1fd23c5a9d00013a705f","uuid":"612cb839-1267-4900-89ed-9362aacf55a4","title":"How to accelerate labeling projects using GPT–4V in Foundry","slug":"how-to-accelerate-labeling-projects","html":"\u003cp\u003eWorking closely with hundreds of companies at the forefront of AI, we are seeing a growing interest from teams wanting to use foundation models to pre-label data before combining a human-in-the-loop workflow to inject their unique domain expertise and automate specific tasks that have been previously very time-consuming or manually intensive.\u003c/p\u003e\u003cp\u003eIn this post, we’ll walk through 3 specific examples of how teams can use Quantumworks Lab to accelerate labeling projects by using GPT-4V to create high-quality labels for various data types, including images, HTML, and text.\u0026nbsp;\u003c/p\u003e\u003cp\u003eGenerating high-quality datasets is often one of the most tedious parts of the development process for ML teams. By using Quantumworks Lab Foundry, ML teams can now quickly use LLMs to their advantage to pre-label or enrich data that span a wide range of use cases, such as identifying amenities for rental listings, classifying items in a product catalog, or categorizing support messages.\u003c/p\u003e\u003ch2 id=\"pre-labeling-use-case-1-classifying-amenities\"\u003e\u003cstrong\u003ePre-labeling Use Case #1: Classifying amenities \u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eOnline travel marketplaces such as Expedia, Booking.com, Airbnb, VRBO, etc.,  often show product listings that have a main image with additional images as supplements. These travel marketplaces can enhance their user experience and conversion rates by enriching objects and desired characteristics in product listings to give users more context about visual assets.\u003c/p\u003e\u003cp\u003eAs an example of this in action, we’ve seen ML teams upload primary and supporting images as a single entity, referred to as a data row. A data row can be considered as a task that a human or AI can do. Afterwards, the ML team will tap into\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFoundry\u003c/u\u003e\u003c/a\u003e, which helps automate labeling tasks.\u003c/p\u003e\u003cp\u003eThe example below focuses on identifying various amenities in a rental listing.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Attachment-preview-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"910\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Attachment-preview-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Attachment-preview-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Attachment-preview-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Attachment-preview-.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe main listing image is accompanied by supporting images.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003e\u003cstrong\u003eStep 1: Select images and choose a foundation model of interest\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/select-image-and-model-1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"960\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/select-image-and-model-1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/select-image-and-model-1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/select-image-and-model-1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/select-image-and-model-1.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWorkflow of selecting the images and model of choice.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data, users can use Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eCatalog\u003c/a\u003e filters, including media attributes, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eUsers can then click “Predict with Model Foundry” once the data of interest has appeared.\u003c/li\u003e\u003cli\u003eUsers will then be prompted to choose a model of interest for a model run.\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task - such as image classification, object detection, and image captioning.\u003c/li\u003e\u003cli\u003eTo locate a specific model, users can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine-learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003e\u003cstrong\u003eStep 2: Configure model settings and submit a model run\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/oX69xeF-beq-3wT3y_58xEdqAUefnmBzU55avrqbOc12PRWizA28ATCxCmmTh_KixA7CunMbpkYZCgeMizjFSkKh4IoKCR6bcEPTihnevrneo2HSZ3CW9dTZLQekxw5GjwEe3TO6lNlV-FCA3YFuoIeGzavyQNx3bCMX3vZ0H1Ys4p7yCl6Pt-SVYgqCpQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"800\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWorkflow of selecting the ontology, prompt and generating a preview of the prediction.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eOnce the model of interest is selected, users can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of settings, which can be found in the Advanced model setting. For this use case, the only Foundry model setting I changed was to select “use_image_attachments” to pass the supporting images to GPT-4V.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can then generate preview predictions on up to five data rows to understand how the current model settings affect the final predictions.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows users to confirm the configuration settings confidently:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf users are unhappy with the generated preview predictions, they can edit the model settings and continue to generate preview predictions until they're satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eAfter users are satisfied with the preview predictions, a model run can be submitted.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Image-airbnb-end-result-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"720\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Image-airbnb-end-result-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Image-airbnb-end-result-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Image-airbnb-end-result-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Image-airbnb-end-result-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eIn addition to the obvious amenities, GPT-4v identified the subtle amenities like heater, lakeview, mountain View, and stove. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003ePrompt used for Amenities classification: \u003c/p\u003e\u003cul\u003e\u003cli\u003eYou are a helpful assistant. What amenities are in the images? Respond with the following options. [Kitchen, TV, Heater, Stove, Hot tub, Skis, Lake view, Refrigerator, Microwave, Mountain view, Shower]. Return the result as a dictionary. {“Amenities” : [“\u0026lt;prediction\u0026gt;”]}\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-send-the-images-to-annotate\"\u003e\u003cstrong\u003eStep 3: Send the images to Annotate\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eUsers can transfer the results to a labeling project using the UI via the \"Send to Annotate\" feature. Labelers can then quickly review labels for accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/kZlndAKlXfBsSFIF68sr8Q34BTRIUFIuLnWQRm88a32mquEuqIwzVVFkgYPIyiyRDxdqK8dGAQAhvcjuxSuvlZSV2No5ZpL0-mNymUjeKqiEioiN2MTRlOK0q_dsFoLPb49Vh6m6GCvmaKPPtVXPg1vT4TmZcX45Wy2NQQAmFdRwOhjFPkdIsgUQez-mlg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"795\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUI for manually reviewing the labels created by LLMs. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"pre-labeling-use-case-2-recommendation-engine\"\u003e\u003cbr\u003e\u003cstrong\u003ePre-labeling Use Case #2: Recommendation engine \u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eE-commerce companies such as Amazon, Wayfair, Etsy, and eBay offer a \"similar products\" feature that improves the discoverability of products and provides an easy way to compare items, thereby increasing customer satisfaction and reducing return rates.\u003c/p\u003e\u003cp\u003eA recommendation engine often powers the product similarity feature that requires integrating text and images into a unified file. ML teams can use Quantumworks Lab to help automate this workflow as we support HTML files that state-of-the-art models can label or enrich. For the HTML product similarity task, the initial steps 1-2 remain the same, but the prompt and ontology will be adjusted to focus on classifying whether the products are identical, and GPT-4V will provide reasoning.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/side-by-side.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"725\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/side-by-side.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/side-by-side.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/side-by-side.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/side-by-side.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe see that GPT-4v predicted the products to not be the same and provided an accurate explanation for the answer. GPT-4v also classified it correctly to be a bottle type. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003ePrompt used for recommendation engine: \u003c/p\u003e\u003cul\u003e\u003cli\u003eYou are a helpful assistant. Based on the descriptions and images provided for the two products, determine whether the products being described are the same.\u003c/li\u003e\u003cli\u003eClassify Is the product the same? pick one of the options: [yes, no].\u0026nbsp;\u003c/li\u003e\u003cli\u003eClassify Item type as the following: [Food, Perishable, Liquid, Bottle, Nonperishable].\u003c/li\u003e\u003cli\u003eAnswer Explanation with why yes or no.\u0026nbsp;\u003c/li\u003e\u003cli\u003eReturn the result as a JSON. {\"Is the product the same?\" : \"\u0026lt;prediction\u0026gt;\", \"Item type\" : [\"\u0026lt;prediction\u0026gt;\"], \"Explanation\", \"\u0026lt;prediction\u0026gt;\"}\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"pre-labeling-use-case-3-support-chat-classification\"\u003e\u003cstrong\u003ePre-labeling Use Case #3: Support chat classification\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eEnterprises with a significant user base require a full-fledged customer support team to ensure smooth operations. Providing efficient customer support is typically achieved by efficiently categorizing and triaging high volumes of real-time support messages.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, support teams often have to manually classify tickets, which can be time-consuming and prone to human error. This process is necessary to sort the messages into the correct reporting categories and identify which engineering teams should handle which bugs.\u003c/p\u003e\u003cp\u003eBy using advanced large language models (LLMs) such as GPT-4V, customer chat intent classification can be automated, and then labelers can review and edit the labels if needed. \u003c/p\u003e\u003cp\u003eThe following prompt was used to classify customer messages, and default Foundry model settings were used. If GPT-4V fails to produce an expected answer, users can add an if statement to capture the edge cases, as shown below.\u003c/p\u003e\u003cul\u003e\u003cli\u003eYou are a helpful support assistant. Read the following text and classify them to the information below.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf the tier is Enterprise and urgency is Critical then Priority is Priority 0.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf the the ticket is related to feature request and Tier is Enterprise then Priority is P2. Otherwise all feature request tickets are P4.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf the tier is Free and urgency is low, then Priority is Priority 4.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf description is related to python or coding then engineering team is SDK.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIf description is related to labeling editor then engineering team is Perception.\u003c/li\u003e\u003cli\u003eIf description is related to log in issues or app crashes\u0026nbsp; then engineering team is Platform. \u0026nbsp;\u003c/li\u003e\u003cli\u003eIf urgency is critical and tier is not enterprise then priority should be Priority 2.\u003c/li\u003e\u003cli\u003eIf description is stating to support a feature then that is a feature request.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eClassify Message intent, pick one of the options: [Accidental payment, Unauthorized payment, Irrelevant, Unable to login, Reset password, Cancel subscription, App bug, Feature Request]. Classify Priority, pick one of the options: [Priority 1, Priority 2, Priority 3, Priority 4, Priority 0]. Classify Engineering team, pick one of the options: [SDK, Perception, Platform].\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1994\" height=\"676\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.25-PM-1.png 1994w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBased on the prompt, GPT4-V correctly classified this as P0, Platform team and App bug.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"678\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.34-PM-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe prompt mentioned to classify feature support requests as \"Feature Request\" and set the priority to 2 due to enterprise tier.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"676\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.55.28-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eGPT-4V initially classified this as P4. To correct it, \"If urgency is critical and tier is not enterprise, then the priority should be Priority 2\" was added to the prompt.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"678\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-21-at-9.06.46-PM-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eGPT-4V correctly used the prompt to classify the text based on the if statements. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"additional-considerations\"\u003e\u003cstrong\u003eAdditional Considerations\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAdditional considerations as users incorporate Foundry labels into their projects and workflows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIncorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman evaluators\u003c/u\u003e\u003c/a\u003e to find edge cases and updating the prompt would give the best results as users create more labels with Foundry. For example, if statements were added in the chat classification use case to account for edge cases.\u0026nbsp;\u003c/li\u003e\u003cli\u003eUsers can also \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA/B test different models \u003c/u\u003e\u003c/a\u003efrom Foundry to find the model that best fits the use case using Quantumworks Lab Model.\u003c/li\u003e\u003cli\u003eIn addition to GPT-4V, users can utilize various state-of-the-art models like Gemini 1.5 Pro, Claude 3 Opus, and more from Quantumworks Lab Foundry, as seen \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eIf we do not currently support your use case or any questions arise, feel free to contact our \u003ca href=\"https://docs.labelbox.com/docs/contacting-customer-support?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esupport team\u003c/u\u003e\u003c/a\u003e, as we would love to hear your feedback to improve Foundry.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003cbr\u003e\u003c/h2\u003e\u003cp\u003eLarge Language Models (LLMs) and generative AI are showing an enormous impact on the individual productivity of knowledge workers. As these large foundation models improve, we’ll continue to see impressive real-world use cases for automating pre-labeling more quickly and cheaply. Completing labeling projects with Quantumworks Lab Foundry combines both AI-assistance and human-in-the-loop workflows to automate one or more specific tasks. \u003c/p\u003e\u003cp\u003eCheck out our additional resources on how to utilize state-of-the-art AI models in Foundry, including using model distillation and fine-tuning to leverage the power of foundation models:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-build-generative-captioning-using-foundation-models-for-product-listings/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eHow to build generative captioning and enrich product listings faster with foundation models\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eHow to build a powerful product recommendation system for retail\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eIntroduction to model distillation\u0026nbsp;\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://event.on24.com/wcc/r/4460171/5D5357846BEAB1FD1E12A5ED146DE6E1?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAutomated Labeling With Any Foundation Model\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eFine tune LLMs\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"65ab1fd23c5a9d00013a705f","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/03/thumbnail--5-.png","featured":false,"visibility":"public","created_at":"2024-01-20T01:20:18.000+00:00","updated_at":"2024-03-27T15:19:55.000+00:00","published_at":"2024-03-26T06:03:00.000+00:00","custom_excerpt":"Learn 3 specific examples of how teams can use Quantumworks Lab to accelerate labeling projects by using multimodal models to create high-quality labels for various data types including images, HTML, and text. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"},"primary_tag":{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-accelerate-labeling-projects/","excerpt":"Learn 3 specific examples of how teams can use Quantumworks Lab to accelerate labeling projects by using multimodal models to create high-quality labels for various data types including images, HTML, and text. ","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6601bf732822410001bac2ba","uuid":"f740e28a-2932-4536-b087-575a1edd5fde","title":"How to improve your task-specific chatbot for better safety, relevancy, and user feedback","slug":"how-to-improve-your-task-specific-chatbot-for-safety-relevancy-and-user-feedback","html":"\u003cp\u003eBuilding task-specific chatbots requires a structured approach when it comes to improving their everyday usefulness, specifically for better safety, relevancy, and user feedback. Given the highly subjective tasks LLM-powered chat applications are expected to perform, a common denominator for how well they do in real-world settings depend on the availability of reliable high-quality training data and how closely aligned they are to human preferences. Working hand-in-hand with leading AI teams, we've observed a set of best practices that we wanted to share in order to help you improve the performance of your task-specific chatbots.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo ensure high levels of trust \u0026amp; safety, an LLM-powered chatbot should be able to detect intentions, entities, and topics when interacting with a user. With quality labeled examples, a task driven application can steer away from conversations that are not relevant to its intended task, ensuring a safe and smooth user experience.\u003c/li\u003e\u003cli\u003eThe large language model (LLM) at the heart of the chat application must be fine tuned with relevant responses or enhanced with human feedback from RAG techniques (e.g. \u003ca href=\"https://labelbox.com/guides/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eReranking\u003c/a\u003e) in order to ensure that the user receives the most relevant information. Examples of \u003ca href=\"https://www.ssw.com.au/rules/train-gpt/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLLMs that can be used, include GPT\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eTo prevent a chatbot from drifting or getting stale with outdated responses, ML teams must continuously monitor and evaluate model performance using ground-truth responses and / or human feedback.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this tutorial guide, we'll walk through some of these top considerations and how Quantumworks Lab can be used as a platform to help accelerate chatbot development.\u003c/p\u003e\u003ch3 id=\"part-1-trust-and-safety-%E2%80%94-understanding-intentions\"\u003e\u003cstrong\u003ePart 1: Trust and Safety — Understanding Intentions\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/u8nd7zpy0o\" title=\"Chat Pt.1 Safety Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo ensure the best user experience, an LLM-based chatbot must be properly scoped to deliver on its intended area of expertise. For example, a chatbot application for an airline company should not be responding to off topic questions, such as politics. Therefore, a chatbot that understands user intent can steer the user towards its intended areas of expertise and away from potentially harmful or unrelated conversations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/image--10-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1720\" height=\"792\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/image--10-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/image--10-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/image--10-.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/image--10-.png 1720w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAn example visual of an LLM-based chatbot which without guardrails, would go off and answer questions that are not pertinent to their intended area of expertise.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn this section, we'll leverage Quantumworks Lab to classify the intent of historical conversations as on-topic (coffee / tea) or off-topic (politics). To start off, let’s load a subset of the \u003ca href=\"https://github.com/thunlp/UltraChat?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eUltrachat\u003c/u\u003e\u003c/a\u003e dataset to Quantumworks Lab Catalog. Ultrachat is an open-source dialogue dataset powered by Turbo APIs to train powerful language models with general conversational capability.\u003c/p\u003e\u003cp\u003eTo being, let's first identify political conversations that your chatbot shouldn’t be able to answer.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing the \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eCatalog\u003c/a\u003e filters, we can cast an initial wide net of examples.\u003cul\u003e\u003cli\u003eSemantic Search — utilize the underlying vector embeddings to return relevant prompts (Politics and opinionated text)\u003c/li\u003e\u003cli\u003eOther functions (e.g. find text) — this will be used to identify targeted keywords or phrases\u003c/li\u003e\u003cli\u003eSave the filters as a \"Slice\" for further reference\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/dHdLvbrx9IA9Uxtlz5Ka2lcRVwT-U6PqOr3Dn7rJ8RtNjv5UQIys4KXULZiOl-8O80BV7jqvifcV4c7evkfG-zIi4RGiI034Ou6_iC9NixZjw90SIxjXbzFS93YzT6PJT323LhJRuORDokUxr_df2rU\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"294\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse Quantumworks Lab Catalog filters to help you identify target keywords\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can see that using semantic search and labeling functions have produced promising initial results. As a next step, let’s validate your results further with foundation models.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eWith Quantumworks Lab, you can use a pre-trained foundation model to verify the results from semantic search and other functions.\u003c/li\u003e\u003cli\u003eBy selecting the targeted Data Rows within the slice, you can apply Google’s Gemini Pro to check for political and opinionated classification.\u003c/li\u003e\u003cli\u003eThe results will be returned to you as pre-labels.\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/U-RLJxDtPSYJ99SfjhdUoAv4Nhi6B9O0eaaQMtBtrhesVDrNHvcfUYLym-q_xzn8jZPdpvunuAVrP5VY_fWOaigJH1vIMCg4vxi17LLJ_Rs82Mz759wDEGTss97OxRH_xBJh8Vkdfg4TFZpNUoQlag4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"758\"\u003e\u003c/figure\u003e\u003cp\u003eTo ensure completeness, you can next leverage a human-in-the-loop (HITL) approach\u0026nbsp;to conduct a final review for intent.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eAnnotate\u003c/a\u003e, you create an Annotation Project for Text.\u003c/li\u003e\u003cli\u003eYou can now send the Prompts and responses (as pre-labels) to your Annotation Project.\u003c/li\u003e\u003cli\u003eA human-in-the-loop (HITL) approach ensures that the prompts are checked for complex nuances by skilled professionals that the models may have missed.\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/ES6aNgATSLyk3CKSOqxwuDHPqLq0KsG7K8QeH9MGZKEFwcPqk-CmR5pNKgVfUyJ6TL0LeLSJx-YH-HMEYo-lmAILem_GiFLmUgNI36ntbiPSPdndCgq7cc2fz2aNEoS3pmRF3-h_JPCiyD7Yu36i8BM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"529\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eUtilizing the \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox SDK\u003c/a\u003e, these prompts, along with their labeled classifications, can now be seamlessly integrated with trust and safety frameworks, such as\u003ca href=\"https://github.com/NVIDIA/NeMo-Guardrails?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003eNeMo Guardrails\u003c/u\u003e\u003c/a\u003e, an open source toolkit for controlling the output of a large language model.\u003c/li\u003e\u003cli\u003eBased on the prompts and classifications labeled in the previous steps, we can feed these examples into NeMo to ensure that the application does not respond to potentially sensitive political topics.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Q00lf_YPoPlL_11VaJyfLtcYK9e7VBgb55ZvOv0SmvhWAq5McNJ4wjyului_alK0F0M4rWoUHLXpETRZAlE_zUbTg7_WrDvuptD_WQWExyiMR-EusJL58Fs8YYtoqudO0KGN6mCKcgk1M_oxDnuu-ow\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"300\"\u003e\u003c/figure\u003e\u003ch3 id=\"part-2-generating-quality-responses-for-your-llm-based-chatbot\"\u003e\u003cstrong\u003ePart 2: Generating Quality Responses for your LLM-based Chatbot \u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y8zb2m8mt1\" title=\"Chatbot Pt2. Fine Tuning Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWhen your intelligent chat application is powered by an underlying large language model, you can customize these LLMs to a defined task with 2 key approaches: Fine-tuning or Retrieval Augmented Generation (RAG).\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor this exercise, you will fine tune a task-specific LLM, and you can follow along via text below or from the video above. It’s worth noting that Quantumworks Lab can also be used for optimizing RAG based systems with techniques such as \u003ca href=\"https://labelbox.com/guides/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eReranking\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo start off, let's first generate quality responses to selected prompts. Ideally, we’ll want the chatbot to replicate the responses provided by your annotators.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab Catalog, you'll identify the relevant prompts related to coffee, and you can combine filters such as:\u003cul\u003e\u003cli\u003eSemantic search with input prompt\u003c/li\u003e\u003cli\u003eSimilarity search with ideal example data rows\u003c/li\u003e\u003cli\u003eKeywords matching\u003c/li\u003e\u003cli\u003eExclude already classified prompts that were shown in Part 1 of this tutorial\u003c/li\u003e\u003cli\u003eSave your filters as a Slice\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/4lzE9AcYtkeXWiz6ZpFvV65fK1hOJbnY4OYDoe-7a5kCzFILWHQ3yH3GXGe7rfqC4pP1QjJ4V44OwPhiSdg3YgCJXBVlwMqmvUqSgkxVYXAl2WfnWpQVsWp9yqz-02wzymgp_2hRFgn4iPuDzzrOTiU\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"703\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce you have the initial set of LLM prompts, you will next create an annotation project for \u003cstrong\u003eHumans Responses to Uploaded Prompts.\u003c/strong\u003e This allows you to quickly generate quality responses to each of the selected prompts.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/6dGzWO9-wbbS2WJnqxtXYsBuOY21s9CJAqsPAks6LH4WvUiy9Ld7aIwTcz8ZQzvGmHTWX9bIaHNq8PA6ZKGmGl2-wVtpuHIHWe3dYry75aXJeJFATMnt5uLxWJgROWD312eqX1Sh-7-FCrOS185jvvs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"776\" height=\"472\"\u003e\u003c/figure\u003e\u003cp\u003eYour team of annotators can now produce specific responses for the LLM to learn from.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eIf you need help in this domain, Quantumworks Lab provides on-demand teams of skilled professional labelers with LLM experience via Quantumworks Lab’s \u003ca href=\"https://labelbox.com/product/boost/workforce/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBoost Workforce\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/W7qsBM44eDsFDtQEHnWHzKt9pACdNp3o5MOrLpCFnVfrXmRiL-ZTk3yao-WHkSPO6SocQ02waG2WkbYVJNo-BWuPbjSzgzZNmqr0xWmSoBIXIlasNWi-pQnWRXeGWFALhB9czIz7U4OXZDhNhlbMQbQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"654\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce your dataset is generated, you can use the Python SDK to export the labels and convert it to a format for model fine-tuning (e.g. JSONL format for GCP Vertex AI)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy identifying relevant prompts and generating quality responses, you can now train a task specific LLM model and output predictions for any new requests.\u003c/p\u003e\u003ch3 id=\"part-3-model-evaluation-and-deployment\"\u003e\u003cstrong\u003ePart 3: Model Evaluation and Deployment\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9a0o6o5vne\" title=\"Chatbot pt3. Evaluation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"558\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBefore deploying your model into production, let’s evaluate the performance of the responses when compared to your ground-truth data (expected output). Using holdout prompts and responses pairs not used to train the model, you can evaluate the performance of your fine-tuned LLM.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModel\u003c/a\u003e, let’s first create a new experiment as shown below.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/dS7V4NoliqteLrHDgvgOAnvjWmyl5BvWfBFXpQpI0JfwAbnHPdtSj4riVFvYjGgSqwIJur5n8cW2POzvJWDnkNi_MBXsmsw73gRLvwFabtjJIJKT5p69clEPPP1SiPY67SqwYN1kO1AqDVZosSS7Sso\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"759\"\u003e\u003c/figure\u003e\u003cp\u003eAfterwards, you can create a model run to add the holdout dataset containing the ground-truth responses.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/PYteE32jXIijEUpH5HqvAk8I__xzRMkv9d30sqZU0ztL1lbdWBQp6GwJenkqDmcN3LkuJh-QlbQB4wV-0eisZOIUN9j7FIz4s1ODAf7zkh-vhm1h2WGjFPbOZlqt72GZEmc-4EgWo8Jo2EYU0mcW1SM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"557\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eUsing the Python SDK, you can next export the prompts to generate responses from your fine-tuned model from Google's Vertex AI.\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eUsing 3rd-party libraries, you can calculate custom metrics for evaluating NLP tasks, such as the BLEU score\u003c/li\u003e\u003cli\u003eThe predictions and custom metrics can now be uploaded back into your model run\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-width-wide\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/9DYocSmDHD410N5L4APC2cQ_LoWrzK380GO8LpvaLysfihLwLse2S7sTkFNQiui5RfRYTr7Uy5oCIL9dqAZ7PW3KTScFZ_WbLbR8rq7iu5384DxzF3932GBLTQuaiPkboL6D8hjpPHMQIP5RblomLXw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"588\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWith the metrics and predictions uploaded, you can filter by various metrics to identify the highest and lowest performing prompts, along with overall model performance.\u003c/li\u003e\u003cli\u003eYou can then select these data rows (such as this high performing prompt below) to find similar prompts within the catalog, which will be used to train the next iteration of your chat application.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/yZHhwpGt5t1TM9EoNUufy7G239bLWVWYpWMcX8m5nfhIcRTE_XZ3FRdq5tiCCpM-T9WrsDQd6Qo7XVQgduIc-r2_n1_JN0nk13UJ7XfFwpgYFzdE5_RX_tqWXsSnhBAM-nzVssr_ZQ4xoKG6H5u2qaE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"938\"\u003e\u003c/figure\u003e\u003ch3 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn this guide, we highlighted a few best practices for ensuring better safety, relevancy, and user feedback when building a task-specific chatbot. Regardless of the techniques used and the models chosen, it is crucial to generate and classify data to the highest quality for optimizing chatbot outputs. \u003c/p\u003e\u003cp\u003eBy incorporating semantic search, labeling functions, foundation models, and a human-in-the-loop approach, you will be able to generate and classify data  in higher qualities consistently. Combined with an SDK-driven approach, you can more easily train models and enhance LLM performance through faster iterations. Give the tutorial a try and we'd love to hear your feedback or ideas on how we can help you improve your LLM-based chatbot applications.\u003c/p\u003e","comment_id":"6601bf732822410001bac2ba","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/03/thumbnail--3-.png","featured":false,"visibility":"public","created_at":"2024-03-25T18:16:19.000+00:00","updated_at":"2024-05-28T17:01:29.000+00:00","published_at":"2024-03-25T19:16:18.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab for optimizing your task-specific LLM chatbot for better safety, relevancy, and user feedback.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/how-to-improve-your-task-specific-chatbot-for-safety-relevancy-and-user-feedback/","excerpt":"Learn how to leverage Quantumworks Lab for optimizing your task-specific LLM chatbot for better safety, relevancy, and user feedback.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65e8dd751ac5200001374088","uuid":"eb8d2ad8-e01c-4e9a-b3ad-0021f50d7241","title":"How to automate medical imaging with AI","slug":"how-to-automate-medical-imaging-with-ai","html":"\u003cp\u003eWith AI-powered detection, you can now easily harness the latest advances in foundation models to help accelerate your medical imaging and life sciences operations. AI is being used for a range of features and use cases - from segmentation of medical scans, detection of abnormalities in organs, or classifying cell nuclei for the purpose of predicting diseases like cancer. As the demand for more intelligent monitoring continues to rise, it's essential for teams to improve imaging efficiency, accuracy, and diagnostic capabilities. Quantumworks Lab empowers the world’s largest organizations to leverage AI solutions tailored to their unique medical detection challenges.\u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for large-scale medical imaging detection. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving medical imaging detection requires a vast amount of data in the form of images, videos, and documents. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e\u0026nbsp;The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e\u0026nbsp;Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers organizations to transform their medical imaging operations through advanced computer vision techniques. Instead of relying on time-consuming manual human review, companies can leverage Quantumworks Lab’s AI-assisted data labeling and flexible training frameworks to quickly build task-specific models that uncover actionable insights.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-9.29.44-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1704\" height=\"918\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-9.29.44-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-9.29.44-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-9.29.44-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-9.29.44-AM.png 1704w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through an end-to-end workflow on how your team can use Quantumworks Lab to build a powerful model to improve medical imaging detection. Specifically, this guide will walk through how you can explore and better understand your visual assets to make more data-driven diagnostics on histopathology images. \u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-automate-medical-imaging-with-ai\"\u003eSee it in action: How to automate medical imaging with AI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/q9qnm01eu1\" title=\"Solution Accelerator - Medical Imaging - Intro \u0026amp; Curation Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"496\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across\u0026nbsp;\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab%2520pricing\u0026utm_source=house\u0026utm_medium=email\u0026utm_campaign=mar2024\u0026\u0026landingPageAnonymousId=%22b04806c5-ff2b-4a09-9379-a9837b1801a9%22\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data with Catalog and Foundry\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below via:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://drive.google.com/file/d/1d3CZWl0YmBKxOKzYmjabip5fXtz-V8ke?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1d3CZWl0YmBKxOKzYmjabip5fXtz-V8ke?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eDatabricks Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003e\u003cstrong\u003ePart 1: Explore and prepare your data\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in the\u0026nbsp;\u003ca href=\"https://drive.google.com/file/d/1d3CZWl0YmBKxOKzYmjabip5fXtz-V8ke?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, please make a copy of the notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ll be working with a dataset of\u0026nbsp;histopathology images – with the goal of identifying cancerous regions through visual inspections. These images were taken from the open source PanNuke dataset, first published by researchers at Warwick University. The raw image dataset comes in the form of a numpy array of pixels, and first needs to be converted to jpeg stored as objects on your cloud storage. The steps taken to achieve this can be found in the Python notebooks that accompany this demo. \u003c/p\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003ePlease download the\u0026nbsp;dataset and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your data as a CSV.\u003c/li\u003e\u003cli\u003eIf your images sit as individual files in cloud storage, you can reference the URL of these files through our\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your image data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your medical imaging data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this demo, we'll be using Catalog to find relevant images of histopathology images for our dataset by taking samples of tissue cells that are stained and subsequently analyzed under a microscope - with trained pathologists identifying cancerous regions through visual inspections. AI models are increasingly being used to perform the visual inspection in order to accelerate this manual and time consuming activity, as well as reduce observer variability.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can leverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom the data shown, you can toggle over to the Analytics view within Catalog to understand the distribution of your data better and  looking for the number of images relating to each organ. In this specific data set, you can see that \"breast\" and \"colon\" datarows are the most common, making up roughly 30% and 18% respectively. For the purposes of this walkthrough, you will select the \"colon\" datarows, layer multiple filters to arrive at your relevant data, and save this as a slice for annotation, which we'll be covering next.\u003c/p\u003e\u003ch3 id=\"create-your-annotate-project\"\u003eCreate your Annotate project\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/2fncdldnr4\" title=\"Solution Accelerator - Medical Imaging - Create Annotate Project Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"496\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eNow that you have narrowed down your relevant subset of data, the next step is to and set up your Annotate project, which allows you to begin labeling the key objects or classes about an image - in this case it will be the cell nuclei of interest within each image.\u003c/p\u003e\u003cp\u003eIn order to understand the various nuclei categories that are present across the dataset, you can observe academic papers that accompany the PanNuke dataset - in the case of Colon organ images, we will look to annotate 5 main categories: \u003cem\u003eNeoplastic, Connective, Inflammatory, Epithelial, and Dead Cell \u003c/em\u003enuclei.\u003c/p\u003e\u003cp\u003eAfter identifying what we're interested in detecting, you can proceed to create an ontology. When entering your ontology, you can select the segmentation annotation type and then pass data to the project, and attach your ontology and begin tagging. There are various options for segmentation, from pen, to bounding box, to pen and brush.\u003c/p\u003e\u003ch3 id=\"annotate-your-data-rows\"\u003eAnnotate your data rows\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/j3tcc2ti9m\" title=\"Solution Accelerator - Medical Imaging - Annotating Datarows Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"498\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eNow that you have appropriately configured your annotation project with the relevant data and ontology, you can begin to annotate each of your datarows. To do this, a labeler can navigate to the top right and under “Start”, you can select the option to “Start Labeling”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eDoing so will bring up the Quantumworks Lab annotate UI, where on the right you will have the datarow of interest, in this case an image of colon tissue, and on the left you can see each of the objects (or nuclei type) included in your ontology. Your labelers can use this to begin marking up each of the objects of nuclei types present in the image.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.45.52-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"959\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-06-at-4.45.52-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-06-at-4.45.52-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-06-at-4.45.52-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.45.52-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eIn some situations especially in the medical field, you may wish to have your data rows annotated my subject matter experts such as trained pathologists or other medical professionals which may be from within your organization.\u003c/li\u003e\u003cli\u003eHowever, if you do not have an appropriate labeling workforce or technical expertise within your organization, you can utilize Quantumworks Lab’s \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eBoost Workforce\u003c/a\u003e offering. This program offers customers the ability to source labelers through Quantumworks Lab with specialized knowledge and skillset required to annotate the features of relevance across your dataset.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIt is worth exploring a few of the capabilities available within Annotate in order to perform your task. Quantumworks Lab has AI powered solutions embedded in Annotate, that make use of image embeddings to help in suggesting appropriate segment instances\u003c/p\u003e\u003cp\u003eAfter selecting the object of interest, you can see at the top the various options available:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.44.10-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"988\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-06-at-4.44.10-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-06-at-4.44.10-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-06-at-4.44.10-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.44.10-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003col\u003e\u003cli\u003eThe first is the Hover and Click option, which allows us to move our cursor over various part of the image and identify possible segment instances as you can see on the screen in front of you.\u003c/li\u003e\u003cli\u003eThe second option is the Auto Segment Box solution, where we can draw a bounding box around an object of interest, and leverage the segment anything model to automatically generate appropriate segment masks.\u003c/li\u003e\u003cli\u003eThe third option is Brush. This gives you full control control of the segment area by allow us to highlight an area using a larger brush stroke\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThe effectiveness of the Hover and Click and Segment Anything options will depend on several factors such as the quality and resolution of your images, as well as features in the image such as the definition of object boundaries and brightness of the image. However, in this case we can see that the Hover and Click options leads to good results, so you can go ahead and begin annotating accordingly.\u003c/p\u003e\u003ch3 id=\"reviewing-your-annotations\"\u003eReviewing your annotations\u003c/h3\u003e\u003cp\u003eEventually you will reach a stage where you have a sufficient number of labeled data rows to begin reviewing. In this instance, you can label roughly 1000 data rows in your batch in the first instance. Reviewing can alternatively be an ongoing activity that is run in parallel to your initial labeling, and the frequency of review is completely dependent on your labeling operations preferences or business logic. To begin reviewing the labeling workforces annotations, a reviewer can simply navigate to the top right and select start \"Review\". \u003c/p\u003e\u003cp\u003eThere are a few options available to your reviewers: \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.42.38-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"946\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-06-at-4.42.38-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-06-at-4.42.38-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-06-at-4.42.38-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-06-at-4.42.38-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003col\u003e\u003cli\u003eFirst of all, you could mark an issue or comment and then reject the data rule (as shown in the image above). This would move the data rule back into a rework bucket where a labeler can review it and see any issues attached before taking remedial action and resubmitting for a second review. \u003c/li\u003e\u003cli\u003eAlternatively, a labeler may choose to edit the image as part of the review stage themselves directly. \u003c/li\u003e\u003cli\u003eThe third and final option is simply to review the data row, acknowledge that everything seems appropriate and go ahead and approve. This will then subsequently bring up the next data row for review and the processes repeated until all data rows sit within the done bucket. Once in the done bucket, the data can be extracted and used for downstream exercises as training or fine tuning a custom model or deriving business insights from the annotations.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"part-2-using-model-foundry-to-generate-pre-labels\"\u003ePart 2: Using Model \u0026amp; Foundry to generate pre-labels\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/h7c8z0spyy\" title=\"Solution Accelerator - Medical Imaging - Model \u0026amp; Foundry Pre-labels Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"502\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn Part 1 of this walkthrough, we\u0026nbsp;covered how to curate medical images and set up your annotation project for initial labeling and review. The next part of this guide will cover how you can extract your annotations for evaluating and diagnosing your model. There are various ways that this can be done; from exporting the data directly within the Quantumworks Lab UI to using Quantumworks Lab's API to extract the data directly from your project.\u003c/p\u003e\u003cp\u003eWe'll be covering first how to set up a model experiment within the Quantumworks Lab UI. You can navigate to the model tab and see the tab for \"Experiment\" here. By creating a new experiment in the top right as shown below, you can name it something appropriate and add a description and select next. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.25.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"960\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.25.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.25.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.25.55-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.25.55-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAfterwards, select the ontology that you used in your previous annotate project and then subsequently select the project which your labeled data rows sit within. \u003c/p\u003e\u003cp\u003eYou'll see that Quantumworks Lab handles the test train and validate splits and suggests by default 80/10/10. You can tweak these by using the slide at the bottom or by editing the percentages shown in the text box. For the purpose of this demo, you'll keep this as standard and create your first model run (as shown below). \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-10.19.43-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"936\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-10.19.43-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-10.19.43-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-10.19.43-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-10.19.43-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAfter your data has been successfully passed into a model run, you can observe each of your annotations, and on the left hand side, you can see the splits across train, validate, and test. \u003c/p\u003e\u003cp\u003eYou're now in a position to copy your model run ID, and leverage the accompanying notebook in order to extract the data using Quantumworks Lab's API. The export will contain each data row along with the accompanying annotations and  the data split so you know what to train your model on versus validation and test sets. \u003c/p\u003e\u003cp\u003eYou can follow along in the \u003ca href=\"https://drive.google.com/file/d/1d3CZWl0YmBKxOKzYmjabip5fXtz-V8ke?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003enotebook\u003c/a\u003e provided to see how a YOLO model was trained for the purpose of identifying nuclei. The real advantage of model runs is that after you've trained your model, you can pass the predictions for your dataset back into Quantumworks Lab in order to evaluate your model performance. \u003c/p\u003e\u003cp\u003eAfter uploading your model predictions back to your model run, you can then dive into each data row in turn to observe your ground truth, versus your model predictions as shown below. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.28.32-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"939\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.28.32-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.28.32-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.28.32-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.28.32-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou'll see the ground truth annotations as well as the predictions from your model run, including each instance of your classes. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.29.03-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"758\" height=\"564\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.29.03-AM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.29.03-AM.png 758w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThis gives you a more granular view of how your model is performing. You can also navigate to the metrics tab in the top right. As shown below, you will see a view of various metrics from precision recall F1, and in the case of segmentation and object detection, intersection over union (IoU). \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-10.17.04-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"994\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-10.17.04-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-10.17.04-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-10.17.04-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-10.17.04-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eWe're now in a position to compare model inferences with your ground-truth annotations to see where the model may be underperforming. A disagreement between model predictions and ground truth labels can be due to a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection and additional labeling.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.30.47-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"979\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.30.47-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.30.47-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.30.47-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.30.47-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn some cases, it may be beneficial to go ahead and select a few of these edge case examples as anchors and then leverage the \"Find Similar\" feature in Quantumworks Lab Catalog. This allows you to identify where your model is underperforming on an ongoing basis, surface relevant data from across your repository that matches that type of data, and then to re-batch this data back to an annotate project for further labeling.\u003c/p\u003e\u003cp\u003eThis allows you to be targeted in the type of data that you're surfacing and to improve your overall training data iteratively. In each case, you can retrain a custom model and within the model tab and in your experiment, you can begin to compare model runs to see each time you retrain, whether you see the intended benefits in accuracy.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.32.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"994\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.32.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.32.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.32.34-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.32.34-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAs a next step, you can now begin to narrow down your data that was not already annotated in your project, and begin to layer up various filters and filter by colon tissue. You can also narrow down to a subset of our data that may be useful in order to batch label for annotation. \u003c/p\u003e\u003cp\u003eYou can also leverage the first iteration of your model in order to accelerate this process, and deploy your model within Quantumworks Lab's Foundry and use this to generate prelabels, which can be passed to Annotate for a labeler to review and to refine if necessary.\u003c/p\u003e\u003cp\u003e\u003cu\u003eNote\u003c/u\u003e: In this walkthrough, we've already deployed the custom model to Foundry. If you have any custom models within your organization that you're interested in deploying into your workspace, please reach out to your Quantumworks Lab support team.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.36.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1222\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.36.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.36.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.36.55-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.36.55-AM.png 2318w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this case, go ahead and select the sample of four hundred and select \"Predict with Foundry\" as shown above. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.37.33-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"564\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.37.33-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.37.33-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.37.33-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.37.33-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou'll see the custom YOLO-V8 model that we have trained as part of the accompanying notebook. This allows you to proceed to the model run UI where you can configure your ontology, as well as various parameters of our model run. You can also save this as an app for repeatability in the future, or you can generate previews on a sample of five or less to see how your model is performing as shown below. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.39.46-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1182\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.39.46-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.39.46-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.39.46-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.39.46-AM.png 2312w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this case, we're fairly confident that the model will perform as expected for this particular iteration, so you can go ahead and submit this as a full model run.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.41.08-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1205\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.41.08-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.41.08-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.41.08-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.41.08-AM.png 2308w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou'll need to wait a few moments before this is complete. Once your model run is complete, you can navigate back to your dataset within Catalog, and filter for your model run which narrows down to a subset of your data for which nuclei were detected (in this case roughly three hundred). Next, select all of these data rows and you can leverage the \"Send to Annotate\" feature to include these model predictions.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.42.58-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"944\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.42.58-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.42.58-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.42.58-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.42.58-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eA reminder here to do a bit of mapping to ensure that the ontology is properly mapped. To confirm, you'll get a green confirmation and see your model predictions match up with the ontology that's assigned to your project. \u003c/p\u003e\u003cp\u003eAfterwards, you can select the stage of your project that you want to send this to, whether it's straight to an initial review task or an initial labeling task. In this case, you'll want your labeling workforce to both validate and add where any nuclei that may have been missed. You can then navigate back to this particular project, and after a few moments, you can refresh and see your data rows passed into your project.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.45.52-AM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"978\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.45.52-AM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.45.52-AM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.45.52-AM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.45.52-AM-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce your data rows have been successfully passed to your project, they now sit within the \"to label\" tab. When a labeler steps into the labeling UI, they will now see the model predictions, as shown above, outlined as pre-annotations. You can add additional segmentation for any nuclei that have been missed by your initial model. \u003c/p\u003e\u003cp\u003eOnce appropriately annotated, you can go ahead and click \"submit\" and you'll now  have all your pre labels showing over several nuclei of interest which can be simply validated, as shown below. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.48.35-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"971\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-11-at-11.48.35-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-11-at-11.48.35-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-11-at-11.48.35-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-11-at-11.48.35-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThis wraps up part 2 of our guide on how you can leverage Quantumworks Lab Model to evaluate each of your model iterations, as well as speed up each iteration via Quantumworks Lab Foundry to accelerate subsequent batches of data labeling.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eBy automating the ability to analyze high-volumes of medical images with AI, Quantumworks Lab provides valuable human-in-the-loop insights for a variety of diagnostics and detection use cases. This gives leading healthcare and life sciences organizations the ability to dramatically improve medical imaging for better efficiency, accuracy, and diagnostic capabilities.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful computer vision models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab%2520pricing\u0026utm_source=house\u0026utm_medium=email\u0026utm_campaign=mar2024\u0026\u0026landingPageAnonymousId=%22b04806c5-ff2b-4a09-9379-a9837b1801a9%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"65e8dd751ac5200001374088","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-15.40.15-1.png","featured":false,"visibility":"public","created_at":"2024-03-06T21:17:41.000+00:00","updated_at":"2024-03-13T17:03:02.000+00:00","published_at":"2024-03-11T18:56:19.000+00:00","custom_excerpt":"Walk through an end-to-end tutorial on how your team can use Quantumworks Lab to build powerful models to improve medical imaging detection.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa615375d13000123d7fc","name":"Industry: Healthcare \u0026 life science","slug":"industry-healthcare-life-science","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-healthcare-life-science/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-automate-medical-imaging-with-ai/","excerpt":"Walk through an end-to-end tutorial on how your team can use Quantumworks Lab to build powerful models to improve medical imaging detection.","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66aacffc31089400019364dd","uuid":"6511d7aa-8755-4ba6-9abb-229bf527ca18","title":"How to implement generative AI","slug":"how-to-implement-generative-ai","html":"\u003cp\u003eGenerative AI became particularly popular and known to the general population with the introduction of \u003ca href=\"https://openai.com/index/chatgpt/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eChatGPT\u003c/u\u003e\u003c/a\u003e. However, this AI solution has been on the market longer than imagined.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIts power in transforming research, content creation and discovery, and customer service has sent businesses running to implement this technology. Most organizations are restructuring with the current AI wave to implement generative AI, but how is it possible? Regardless of the business domain or application use case, generative AI offers opportunities to organizations looking to tap into it.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this article, we will learn what generative AI is and how it can be implemented into various digital products relevant to businesses of different kinds.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"what-is-generative-ai\"\u003e\u003cstrong\u003eWhat is Generative AI?\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGenerative AI\u003c/u\u003e\u003c/a\u003e is a branch of artificial intelligence that has the most widespread business applications and relevance because of its versatility. It leverages machine learning algorithms to create new, original content from learned data. After learning adequately from a large and diversified dataset, the algorithm is poised to produce text, video, images, audio, and code that are original but reflective of the learning material. With these capabilities, machines can now perform tasks traditionally associated with human creativity, like writing, composing music, or generating animations.\u0026nbsp;\u003c/p\u003e\u003cp\u003eDeep learning models like neural networks that can simulate the decision-making characteristic of the human brain are used in developing generative AI. These networks can cherry-pick the most relevant components of the input data and learn from them to generate accurate content per the user's instructions.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith tools like ChatGPT and Dalle-E, generative AI technology has gained prominence over the past years. The number and capabilities of such tools are steadily growing, demonstrating the potential of generative AI technology to revolutionize content creation, automation, and business usage.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"why-generative-ai\"\u003e\u003cstrong\u003eWhy Generative AI?\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAs we have seen, generative AI is a game-changing technology, and the earlier a business implements it in its operations, the better. The significance of generative AI primarily lies in its ability to automate and enhance the creative process. Contrary to the popular opinion that this technology could potentially replace creatives, the tool is more of a helper. It serves various applications, from designing products to drafting documents, just from a prompt.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe underlying machine learning algorithm is able to achieve these functionalities by simply learning from examples and generating similar yet unique results. This capability is quite crucial for scaling content creation and driving innovation across industries.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAlso, the ability to create customized content makes generative AI ideal for personalizing user experiences. This personalization can be applied by customer service and marketing teams across various industries. Therefore, generative AI stands out for its potential to redefine how individuals and organizations approach their business problem-solving and creative processes.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"developing-generative-ai-models\"\u003e\u003cstrong\u003eDeveloping generative AI models\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eImplementing generative AI in various business processes begins with understanding the business problem and developing the relevant generative AI solution. The most technical and often challenging aspect of implementing generative AI is the development phase. It involves model design, training, evaluation, and deployment. Each of these stages requires careful planning and implementation to ensure effective alignment of the AI technology with the business needs.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"contextualizing-business-problems-with-generative-ai\"\u003e\u003cstrong\u003eContextualizing business problems with generative AI\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAs powerful as generative AI models are, they are only helpful when used appropriately to solve the right problem. Therefore, it is essential that we first understand the business problem that we are trying to solve with the generative AI model. Problem analysis will bring to light the possible application areas, alignment with the business objectives, and the expected results. Only after contextualizing the problem and understanding how the technology applies in our industry can we develop the model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eUnderstandably, each business across various sectors has different needs and are trying to use generative AI to solve different problems. For instance, a market research agency might need a generative AI system to read through large volumes of research materials, generate insights from these documents, and present the results in summarized and readable formats. Such a use case would require training a generative model on a dataset that captures this functionality and testing it to ascertain accuracy and usability.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"model-design-and-data-collection\"\u003e\u003cstrong\u003eModel design and data collection\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eModel development is critical to implementing generative AI models for whichever business application. This development starts with model design and data collection. Design simply entails defining the model architecture relevant to the application and the nature of the data it will generate. For instance, when developing text-based generative AI models, we go for large language architectures like transformers, while we choose vision model architectures if we intend to generate images or animations.\u0026nbsp;\u003c/p\u003e\u003cp\u003eDifferent architectures that match these computational models are available for use when implementing generative AI technology. Whether neural networks, diffusion models, or any other type, the choice of model architecture sets the foundation for the resultant generative AI model.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter picking the right architecture, data collection follows. Generative AI models require extensive, diverse datasets to learn from. For instance, training a Stable Diffusion model for image generation would need a minimum of a thousand to 2 billion well-labeled images to train. Besides being voluminous, this data must be representative of the task and domain of the application.\u003c/p\u003e\u003cp\u003eWhile collecting data to train generative AI models, ethical issues like privacy and security emerge. Therefore, it is important to develop frameworks for anonymizing and securing sensitive information. With the data and model architecture ready, we jump straight to model training!\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"model-training\"\u003e\u003cstrong\u003eModel training\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTraining a generative AI model involves feeding it with the collected data and iteratively adjusting its parameters to improve the output. The model is exposed to the prepared dataset to enable it to understand the interrelationship between the data points and perform the related generative tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eDepending on the approach taken, the training process can be resource-intensive. In a typical training lab where generative AI models are trained from scratch, hundreds of GPUs run non-stop to enable the model to learn adequately from the data. However, for organizations that cannot afford this costly and time-consuming training approach, there are existing foundational models like \u003ca href=\"https://labelbox.com/product/model/foundry-models/gpt4/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGPT-4\u003c/u\u003e\u003c/a\u003e that they can adapt to their various business needs using techniques like fine-tuning.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAdvanced training techniques like transfer learning, embedding, and \u003ca href=\"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ereinforcement learning from human feedback (RLHF)\u003c/u\u003e\u003c/a\u003e are also employed to refine the model’s performance further. These techniques improve the accuracy of the generative AI model and align them with human expectations and needs like helpfulness, harmlessness, and ethics.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"model-evaluation-and-deployment\"\u003e\u003cstrong\u003eModel evaluation and deployment\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAfter training, it is only fair to test the performance of the model before deploying it for use. This model testing is technically achieved through evaluating the model’s performance against predefined metrics and benchmarks. Quantitative measures like recall, precision, and accuracy, as well as comparisons of the model with human performance, are key evaluation methodologies.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf the model fails to pass the predefined evaluation benchmarks, it is retrained by augmenting the dataset and tuning the hyperparameters till the desirable result is achieved. Generative AI models that pass evaluation are deployed, making them accessible to end-users. Integrating these models into AI applications that users can interact with is part of the deployment phase.\u003c/p\u003e\u003cp\u003eHowever, the lifecycle of these models extends beyond deployment. To maintain the relevance and accuracy of these generative AI models, it is important to engage in post-deployment practices like continuous monitoring and reevaluation. Post-deployment maintenance also adapts the model to new data and user feedback, further improving it.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"using-generative-ai-models-responsibly\"\u003e\u003cstrong\u003eUsing Generative AI models responsibly\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAs much as generative AI unlocks infinite possibilities in the field of technology, their responsible use is paramount to ensure they serve society positively. This responsible use involves addressing social biases that may arise from training data or model assumptions, which may lead to systematic unfairness. Human audits, representational analysis, and fairness metrics are some of the approaches taken to mitigate such biases. The strategies taken to correct the biases can include but are not limited to, diversifying the training data, engaging in adversarial training, and adjusting model priorities.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIntellectual property (IP) and privacy issues also demand attention when implementing generative AI. To avoid legal pitfalls, the creators, trainers, and users of AI-generated content must respect copyright laws and user privacy. Therefore, it is advisable to verify the copyright status of the training data. Since these models are trained on data from different jurisdictions, understanding the evolving regulations of different regions is also considered an IP best practice.\u003c/p\u003e\u003cp\u003eAs we approach \u003ca href=\"https://labelbox.com/ai-glossary/artificial-general-intelligence-agi/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eArtificial General Intelligence (AGI)\u003c/u\u003e\u003c/a\u003e, the stakes are even higher. While AGI promises immense benefits and improvement of the generative AI, it also presents severe risks. The potential of surpassing human capabilities carries more harms, like ethical violations, that must be addressed early enough. Strategies like alignment with human values and policy development must be put in place to control AGI outcomes. AGI, like generative AI, must follow clear behavioral rules and feedback mechanisms as part of using these technologies responsibly.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"final-thoughts-on-implementing-generative-ai\"\u003e\u003cstrong\u003eFinal thoughts on implementing generative AI\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eGenerative AI has the potential to revolutionize industries and change how businesses operate by automating tasks and generating original content. The underlying principle of this technology is learning from datasets similar to the task to be accomplished and generating new and original content based on the learned patterns.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eImplementing generative AI requires a conscientious approach that balances technical prudence and ethical responsibility. Before deciding on the model architecture or training methodologies to employ, it is important to understand the business problem that the model seeks to solve. Sufficient training data should then be collected and used to train a model that satisfies the predefined business goals.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we integrate these models into various sectors, it is important to consider their ethical implications and societal impacts. As such, the implementation of Generative AI should be guided by principles that prioritize fairness and privacy while preventing misuse.\u003c/p\u003e\u003cp\u003eLabelbox is a complete solution combining the best tools and fully managed services for generative AI. Evaluate multiple models simultaneously, analyze and rewrite responses, and generate new, highly-specific datasets for RLHF and advanced training. \u003ca href=\"https://app.labelbox.com/signup?utm_keyword=Quantumworks Lab\u0026utm_content=model_product_page\u0026utm_campaign=modelfoundry\u0026utm_source=linkedin\u0026utm_medium=organic_social\u0026\u0026landingPageAnonymousId=%22583ce60e-4c83-4821-ada7-bfdc420a7a2b%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eGet started with a free trial of the platform\u003c/u\u003e\u003c/a\u003e and see how Quantumworks Lab helps you implement generative AI.\u003c/p\u003e","comment_id":"66aacffc31089400019364dd","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/08/_generative-ai.png","featured":false,"visibility":"public","created_at":"2024-07-31T23:59:56.000+00:00","updated_at":"2024-08-01T00:04:08.000+00:00","published_at":"2024-03-03T00:00:00.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-implement-generative-ai/","authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"tags":[],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/how-to-implement-generative-ai/","excerpt":"Generative AI became particularly popular and known to the general population with the introduction of ChatGPT. However, this AI solution has been on the market longer than imagined. \n\nIts power in transforming research, content creation and discovery, and customer service has sent businesses running to implement this technology. Most organizations are restructuring with the current AI wave to implement generative AI, but how is it possible? Regardless of the business domain or application use c","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to implement generative AI | Quantumworks Lab","meta_description":"Generative AI automates content creation by learning from data. Learn how to implement it and align models with business needs.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65dd3fe1255738000153641c","uuid":"17cc9bd7-9e84-499a-850d-e6ead9b48f85","title":"How to sync your cloud buckets with Quantumworks Lab","slug":"how-to-sync-your-cloud-buckets-with-labelbox","html":"\u003cp\u003eLabelbox recently introduced the ability to synchronize your cloud buckets from Amazon, Google and Microsoft Azure into Quantumworks Lab Catalog. This improvement greatly simplifies the existing integration by eliminating the need to customize JSON or configure Python scripts.\u003c/p\u003e\u003cp\u003eLabelbox supports native integrations with cloud storage from leading providers including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAmazon S3\u003c/li\u003e\u003cli\u003eGoogle Cloud Storage\u003c/li\u003e\u003cli\u003eMicrosoft Azure Blob Storage\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCloud architecture plays a critical role in ensuring scalability, flexibility, and security for your data management – and it’s always good to\u0026nbsp;\u003ca href=\"https://www.ssw.com.au/rules/cloud-architect/?ref=labelbox-guides.ghost.io\"\u003ehave a cloud expert on your team\u003c/a\u003e.\u0026nbsp;If you are familiar with how cloud storage works, you can\u0026nbsp;\u003ca href=\"https://app.labelbox.com/home?cloudBucketDrawerSelectionOpen=1\u0026cloudBucketIntegrationDrawerOpen=1%3F\u0026landingPageAnonymousId=%223bed3496-d471-4095-ba10-c34e51bd36cd%22\u0026ref=labelbox-guides.ghost.io\"\u003eintegrate your cloud bucket\u003c/a\u003e\u0026nbsp;when adding a dataset to Labelbox. This guide is intended to show you how to set up and use cloud storage integration with Quantumworks Lab Catalog.\u003c/p\u003e\u003ch3 id=\"why-use-cloud-storage-integrations\"\u003eWhy use cloud storage integrations?\u003c/h3\u003e\u003cp\u003eCloud buckets are a simple and efficient way to manage large volumes of unstructured data, especially for computer vision use cases involving working documents, images and video. The Quantumworks Lab cloud storage integration can automatically scan any set of folders in your cloud bucket and synchronize the following data types into the Quantumworks Lab Catalog:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImage\u003c/li\u003e\u003cli\u003eVideo\u003c/li\u003e\u003cli\u003eText\u003c/li\u003e\u003cli\u003eAudio\u003c/li\u003e\u003cli\u003eHTML\u003c/li\u003e\u003cli\u003eTiled imagery (COG, NITF, GeoTIFF)\u003c/li\u003e\u003cli\u003eDocuments\u003c/li\u003e\u003cli\u003eChat (Conversations)\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"how-to-sync-a-dataset-with-your-cloud-bucket\"\u003eHow to sync a dataset with your cloud bucket?\u003c/h3\u003e\u003cp\u003eAfter completing the one-time setup of delegated access to your object store (see \u003ca href=\"https://docs.google.com/document/d/17BG09nOqil7rQ6oub493o-bYrUxNSghEbMCvfxX6-MI/edit?ref=labelbox-guides.ghost.io#heading=h.8iysj0uw5p9s\"\u003e\u003cu\u003esection below\u003c/u\u003e\u003c/a\u003e), you can use the Quantumworks Lab UI to configure the synchronization of any folder to a dataset in the Quantumworks Lab Catalog with just a few clicks.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/tz0beimgxb?wtime=56s\" title=\"Quantumworks Lab Cloud Storage Integration Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce configured, you can sync any connected dataset with your cloud storage with a single click.\u003c/p\u003e\u003ch3 id=\"how-to-set-up-delegated-access-to-your-cloud-storage-provider\"\u003eHow to set up delegated access to your cloud storage provider\u003c/h3\u003e\u003cp\u003eThe one prerequisite for using the Quantumworks Lab Cloud Storage integration is \u003ca href=\"https://docs.labelbox.com/reference/cloud-storage-iam-integration?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esetting up IAM delegated access\u003c/u\u003e\u003c/a\u003e. This is a one-time setup process for each object store allowing Quantumworks Lab to access assets stored in your cloud buckets that you would like to add to Catalog or label using Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIAM delegated access works similarly whether you are using Amazon S3, Microsoft Azure or Google Cloud Storage (GCS). For example, when you use IAM delegated access to add your unlabeled data to Quantumworks Lab, you can keep your assets in Amazon S3 and grant Quantumworks Lab read-only access to your AWS cloud buckets.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/he2QAvnCZbh2UwVpU_dLEi7X8jriKJPCwsMGwiStTew6dkbQT4FasCpwngZ1Y0gjo7R1DOxn5H6Cf4GzcLmuqpiVcTEORt3rMBVbDgVMv9bh9g4RyuePgnSIBYU40VcXHlE0OZCRK6gmwmXzxUkg02w\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"645\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eDelegated Access setup in AWS (similar in GCP)\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIAM delegated access is highly flexible and allows you to control access at the granularity that you desire. \u003c/p\u003e\u003cul\u003e\u003cli\u003eYou can grant Quantumworks Lab access to all of your buckets, a single bucket, or even a particular path within a bucket. \u003c/li\u003e\u003cli\u003eYou can even set up different integrations within Quantumworks Lab for different datasets or projects. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIAM delegated access allows you to use private cloud-hosted buckets with Quantumworks Lab, which helps to ensure that your assets are kept safe.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ghtx8fcwka\" title=\"Quantumworks Lab IAM Setup for Cloud Storage integration Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eRefer to the documentation for additional information on \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esetting up IAM delegated access in Quantumworks Lab\u003c/u\u003e\u003c/a\u003e, and \u003ca href=\"https://app.labelbox.com/home?cloudBucketDrawerSelectionOpen=1\u0026cloudBucketIntegrationDrawerOpen=1\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eadd data to Quantumworks Lab\u003c/u\u003e\u003c/a\u003e from your cloud bucket with ease.\u003c/p\u003e\u003ch3 id=\"summary\"\u003eSummary\u003c/h3\u003e\u003cp\u003eStoring data in cloud buckets is a simple and effective way to manage large volumes of unstructured data – especially images, video and documents. Connecting and synchronizing your cloud storage with datasets in Quantumworks Lab has never been easier.\u003c/p\u003e\u003cp\u003eIf you’re already using cloud storage, \u003ca href=\"https://app.labelbox.com/home?cloudBucketDrawerSelectionOpen=1\u0026cloudBucketIntegrationDrawerOpen=1\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eadd data to Quantumworks Lab\u003c/u\u003e\u003c/a\u003e today. \u003c/p\u003e","comment_id":"65dd3fe1255738000153641c","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Frame-3410--2-.png","featured":false,"visibility":"public","created_at":"2024-02-27T01:50:25.000+00:00","updated_at":"2024-05-28T17:03:00.000+00:00","published_at":"2024-02-28T17:21:47.000+00:00","custom_excerpt":"Sync your unstructured data automatically and skip glue scripts with native support for S3 (AWS), GCS (GCP) and Blob Storage (Azure).  ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-sync-your-cloud-buckets-with-labelbox/","excerpt":"Sync your unstructured data automatically and skip glue scripts with native support for S3 (AWS), GCS (GCP) and Blob Storage (Azure).  ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65df9c591ac5200001373e38","uuid":"8d95e2bd-c28d-4dfa-9adf-a74623c1fb4b","title":"How to use AI to automate invoice and document processing","slug":"how-to-use-ai-to-automate-invoice-and-document-processing","html":"\u003cp\u003eWith AI-powered invoice and document processing, you can now seamlessly integrate the latest advances in foundation models into your core financial and administrative operations. As the demand for better monitoring, reporting and compliance continues to rise, it's essential for teams to ensure accurate, timely, and organized handling of financial transactions. Quantumworks Lab empowers the world’s largest financial services organizations to leverage AI solutions tailored to their unique invoice and document processing challenges.\u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for large-scale invoice \u0026amp; document processing. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving invoice and document processing via OCR analysis requires a vast amount of data in the form of document PDFs and images. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e\u0026nbsp;The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e\u0026nbsp;Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their invoice and document processing through advanced computer vision and OCR techniques. Instead of relying on time-consuming manual human review, companies can leverage Quantumworks Lab’s AI-assisted data enrichment and flexible training frameworks to quickly build task-specific models that uncover actionable insights for faster processing.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-29-at-7.53.46-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1906\" height=\"1060\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-29-at-7.53.46-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-29-at-7.53.46-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-29-at-7.53.46-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-29-at-7.53.46-AM.png 1906w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build an AI model to improve invoice and document processing from images. Specifically, this guide will walk through how you can explore and better understand unstructured data to make more data-driven business decisions.\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-use-ai-to-automate-invoice-and-document-processing\"\u003eSee it in action: How to use AI to automate invoice and document processing\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gtbfqcy3j2\" title=\"Mark demo 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across\u0026nbsp;\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and label your data with Catalog and Foundry\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003e\u003cstrong\u003ePart 1: Explore and prepare your data\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/4j3emzidh0\" title=\"Mark demo 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ll be working with a dataset of image invoices – with the goal of quickly curating data, and using OCR to understand where the text is and to identify what information these invoices contain while finding and correcting model errors. This workflow is very popular with Quantumworks Lab users because it allows teams to have a model do most of the work, while humans (aka subject matter experts) will be able to focus on the task of correcting the model, thereby reducing the amount of manual work.\u003c/p\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you will see your image data rendered in Quantumworks Lab Catalog. You can browse through the invoice dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cp\u003eYou’ll now be able to see your invoice dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore image invoices, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"using-foundry-to-pre-label-invoices\"\u003eUsing Foundry to pre-label invoices\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/i1ujgt5ofc\" title=\"Mark demo 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn this next step, we'll walk through how you can take a human-in-the-loop approach to iterate or modify pre-labels and speed up the annotation process.  \u003c/p\u003e\u003cp\u003eModel Foundry enables teams to choose from a library of models and in this case, we'll be using Amazon's Textract to generate previews and attach them as pre-labels.\u003c/p\u003e\u003cp\u003eWith\u0026nbsp;\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, you can automate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/p\u003e\u003ch3 id=\"send-to-your-annotation-project-for-human-review\"\u003eSend to your Annotation project for human review\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/e71d5d038r\" title=\"Mark demo 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe next step in order to send our annotation project for human review is to to set up your ontology. In this case, we'll call it \"test OCR\" and we'll be using bounding boxes on our images. Note that you can reuse the ontology that you've created previously or you can create a new one. \u003c/p\u003e\u003cp\u003eIn this case, we want a bounding box for text, as well as a sub-classification for the value with the goal of identifying where the text is for bounding boxes. \u003c/p\u003e\u003cp\u003eWe can now include the model predictions that we just completed, and if we're confident that the model is performing well, we can set it to an initial labeling task or as an initial review task. A labeling task means that the labeler will be able to adjust and modify before it goes to a reviewer, and your reviewer will be able to just reject or accept the labels. \u003c/p\u003e\u003cp\u003eIn this case, we have chosen to use Amazon Textract but there are a variety of OCR-specific models that are available within Foundry. Alternatively, we can choose to use your own custom model for OCR invoice detection. The benefits of this approach is that it will allow you to run predictions using your custom model as an end-to-end workflow and more quickly classify parts of interest.\u003c/p\u003e\u003cp\u003e\u003cu\u003eNote\u003c/u\u003e: If this is interesting and you're looking to adopt this method within Quantumworks Lab, please reach out to our support team as we would be happy to assist with deploying your custom model within Foundry.\u003c/p\u003e\u003ch3 id=\"human-in-the-loop-review-for-ocr-invoices\"\u003eHuman-in-the-loop review for OCR invoices\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y7czt6semk\" title=\"Mark demo 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce the initial comparison task is completed, our labelers can now start labeling and we can see how the Amazon Textract model performs on these image invoices with a human-in-the loop come workflow to correct labels for any mistakes. From this example, we can see that the model seems to be performing well so that we can submit these labels for further review and QA. \u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-invoice-processing-model-effectiveness\"\u003eEvaluate and diagnose invoice processing model effectiveness\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/eu7q715hun\" title=\"Mark demo 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe final step involves comparing or A/B testing different OCR models to see which one is the best fit for our specific use case. In this case, we'll be comparing Amazon's Textract with the Tesseract OCR model. A disagreement between model predictions and ground truth labels can be due to a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eVisually compare ground truth labels (in green) to the model predictions (in red).\u003c/li\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model for 10x faster corner-case detection – detect and visualize corner-cases where the model is underperforming. For example, you can drill into cases where ‘empty’ objects are not predicted, where the model might have difficulty identifying specific fields in the image.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\u003cp\u003eBy analyzing high-volumes of documents or images, Quantumworks Lab provides valuable human-in-the-loop insights for invoice and document processing to ensure enable financial services and insurance companies to make data-driven decisions that improve operational efficiency, compliance and revenue.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful task-specific models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"65df9c591ac5200001373e38","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-29-at-7.54.26-AM.png","featured":false,"visibility":"public","created_at":"2024-02-28T20:49:29.000+00:00","updated_at":"2024-02-29T15:58:22.000+00:00","published_at":"2024-02-23T22:24:00.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab’s platform to build an AI model to accelerate high-volume invoice and document processing from PDF documents using OCR.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa608375d13000123d7fa","name":"Industry: Finance \u0026 insurance","slug":"industry-finance-insurance","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-finance-insurance/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},"url":"https://labelbox-guides.ghost.io/how-to-use-ai-to-automate-invoice-and-document-processing/","excerpt":"Learn how to leverage Quantumworks Lab’s platform to build an AI model to accelerate high-volume invoice and document processing from PDF documents using OCR.","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65e754531ac5200001373f08","uuid":"bec8483a-e1a5-4aed-aefb-77189e33fdea","title":"How to accelerate and automate data labeling with labeling functions","slug":"how-to-speed-up-labeling-with-labeling-functions","html":"\u003cp\u003eWhen it comes to data labeling and annotation, an approach that teams like to evaluate is the efficacy of using programmatic labeling via labeling functions to speed up their labeling operations. You can think of labeling functions as a set of rules or instructions that you follow in order to help automatically assign labels or categories to your data. This is especially useful when working with large datasets where manually labeling each piece of data could be resource or time-intensive. In this guide, we'll cover two specific workflows around how teams can perform labeling functions within Labelbox.\u003c/p\u003e\u003cp\u003eWith recent advances in foundation models, teams can now incorporate models such as GPT, Gemini, Claude, etc to kickstart a zero-shot or rules-based approach for labeling at scale. This can work well for regular expressions to identify phone numbers, zip codes, currencies, etc. Without having to train any models from scratch, you're able to simply call an API and have it complete many of these tasks. However, the nature of these generative AI models is that there may still be prone to hallucinations or there may be a desire to include custom business level logic that you may want to supplement in your labeling workflows. Generative AI models are also not inherently meant to address rules-based approaches as typically found in custom business level logic or reg expression. In these situations, having the ability to leverage custom labeling functions is needed.\u003c/p\u003e\u003cp\u003eLet's get started and you can follow along using the Colab notebook \u003ca href=\"https://colab.research.google.com/drive/1pdtB2kuUypYWu1Q8Ot89C-HR1sGkPwN4?usp=sharing\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"part-1-an-sdk-approach-to-creating-labeling-functions\"\u003ePart 1: An SDK-Approach to Creating Labeling Functions\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/5kud55wo9p\" title=\"Kushal labeling functions vid 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou can follow this guide via text or watch the video walkthrough above. To get started, imagine you have a dataset of customer comments for product reviews, and your goal is to run sentiment analysis in order to identify positive, negative or neutral emotions across different comments.  \u003c/p\u003e\u003cp\u003e1) With an SDK-approach, you can first navigate over to your coding environment, and install the appropriate set up steps shown in the notebook \u003ca href=\"https://colab.research.google.com/drive/1pdtB2kuUypYWu1Q8Ot89C-HR1sGkPwN4?usp=sharing\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eprovided\u003c/a\u003e. The first step is to come up with several different rules or labeling functions based on keywords, phrases, heuristics, or knowledge sources that are commonly associated or attributed to your different sentiment categories. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.00.35-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1956\" height=\"928\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.00.35-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.00.35-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.00.35-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.00.35-AM.png 1956w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e2)  An example of a rule that you could use is if your comments data set contains the words \"happy\" or \"excellent\" or \"delighted\". You can use these words as the anchor for your keywords and assign a positive label. Similarly, if the text contains any of the negative words that serves as the basis for negative keywords, then you can  assign this as a negative label. \u003c/p\u003e\u003cp\u003e3) Note that there may be times when a comment is ambiguous or contains both positive and negative words. As an example, this could be something like \"I love how this product looks but I hate how it works\". In such cases, labeling functions can be designed with more nuanced rules to decide the overall sentiment such as considering the context or the number of positive versus negative words. In the example shown, we've used a library like \u003ca href=\"https://textblob.readthedocs.io/en/dev/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eTextBlob\u003c/a\u003e to help with defining several of these labeling functions and returning true if the text meets the criteria and false otherwise. \u003c/p\u003e\u003cp\u003e4) Now that you have all of your initial labeling functions, the next step is to aggregate the outputs from these labeling functions in order to make a more accurate and reliable prediction about the text sentiment. You can choose to use  a variety of aggregation approaches, whether that's majority voting, weighted voting or any other voting based technique.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.00.12-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"871\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.00.12-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.00.12-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.00.12-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.00.12-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe process whereby you're taking several noisy or weak labels and aggregating them to a strong label using an aggregation technique is commonly referred to as \"weak supervision\". Each labeling function can be thought of as an \"expert\" that provides their opinion on how a data point should be labeled, but these experts may not always be right. \u003c/p\u003e\u003cp\u003e5) The goal is to amortize the cost of these potentially noisy or weaker labels by coming up with a strong label. In the example shown, if you were to go with a majority voting-based approach, the idea is that we can define our classes and our aggregation function through a tally up score for your different labels. By going ahead and doing that for positive, negative, or neutral sentiments, you're taking the max score and the max label associated with the max score and assigning that as your final sentiment value. By applying this on you sample piece of text, it returns positive as expected. \u003c/p\u003e\u003cp\u003e6) The next step is to iterate through all of your text assets in your datasets. Using your aggregation function turns your predictions for the sentiment using the rules-based functions that we had seen above, reconstructing the Python annotation and you can use this as the sentiment schema in your ontology, collect your labels, and import it as a label import job to upcert all of these labels into Quantumworks Lab . \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.55.36-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1758\" height=\"1130\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-10.55.36-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-10.55.36-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-10.55.36-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.55.36-AM.png 1758w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this example shown above, we are defining metadata fields for whether or not the text contains a phone number, whether or not the text contains a hashtag, and define your labeling functions. If it meets that criteria then you'll want to set the metadata value to be whatever is returned from that labeling function.  Go ahead and bulk upcert those metadata fields. And once this is all done, you can navigate over to the Quantumworks Lab UI. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.54.08-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"759\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-10.54.08-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-10.54.08-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-10.54.08-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.54.08-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e7) By clicking on the Analytics tab,  you can now see all of your sentiments labels. These are the labels that are a result of running the labeling functions in the notebook. You'll see that roughly 46% of the text has been labeled as neutral,  36% percent is positive, 17% percent is negative. If you want to click into one of the specific classes, you can see the sentiment for negative and there's roughly 10,000 records. Similarly, if you wanted to observe what the metadata distribution looks like, you can see that for the phone numbers and for the hashtags. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.27.32-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1178\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-10.27.32-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-10.27.32-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-10.27.32-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.27.32-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e8) By selecting all the text that contains hashtags, you see around one hundred and sixty four results. This SDK-approach enables teams to quickly labels large datasets, saving time and effort. You can easily add, remove, or adjust rules as you discover new patterns in how sentiments are expressed, especially in instances where over time, you'll observe that there's variations in the way that vernacular or jargon or slang is being used across your text. \u003c/p\u003e\u003cp\u003eThis wraps up the first part of the guide on how you can use labeling functions within the SDK for bulk labeling data and bring that into the Quantumworks Lab UI. In the next part, let's cover how you can perform rules-based processes within the Quantumworks Lab UI for faster annotation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.53.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"830\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-10.53.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-10.53.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-10.53.34-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-10.53.34-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"part-2-using-the-labelbox-ui-to-create-labeling-functions\"\u003ePart 2: Using the Quantumworks Lab UI to Create Labeling Functions\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/d618qv9z47\" title=\"Kushal Labeling functions vid 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eNow that you've seen how to leverage labeling functions by using the SDK to bulk label your data, let's walk through you can accomplish a similar workflow directly from the Quantumworks Lab UI.\u003c/p\u003e\u003cp\u003e1) One of the first features you can take advantage of in Quantumworks Lab \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eCatalog\u003c/a\u003e is the find text feature, which allows a Quantumworks Lab user to search raw text occurrences of a specific word or sequence of words across their data set. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.42.45-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1256\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.42.45-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.42.45-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.42.45-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.42.45-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAs an example, if you want to find all occurrences of the word \"happy\", you can use this feature to see all of your data rows that contain the words \"happy\". From here, you can save this as a slice and name this as a \"happy\" raw text search and hit save. Any time new data gets added that meets the filtering criteria, it will automatically get added to the slice. This is equivalent to writing a similar Python function that does a substring match or raw text search to match the specific word. \u003c/p\u003e\u003cp\u003e2) You can also take advantage of Similarity Search and select a subset of these data rows and click \"similar to selection\". Quantumworks Lab leverages built-in embeddings that get automatically generated. Instead of looking just for the raw text of \"happy\", you can look for text that has the overall theme or structure of happiness. You can select all of these data rows and toggle by confidence level to filter by an even finer granularity.\u003c/p\u003e\u003cp\u003e3) Next, you can add a pre label or metadata. As an example, let's add metadata by selecting that option, and selecting the emotion category for \"happy\" and hit save. This will apply that metadata to all of my data rows in bulk. Similarly, if you wanted to add additional classifications, you could do that as well. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUse Aggregation Functions to Group Weak Labels into a Strong Label\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOne of the other things shown in Part 1 of the SDK demo was how to leverage aggregation functions to group together several weaker labels into a strong label. Let's cover how you can do this all directly within the Quantumworks Lab UI.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.47.27-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"828\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.47.27-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.47.27-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.47.27-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.47.27-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) First, curate a subset of text records that contain hashtags and emojis with the goal of getting labels for whether or not each of these text records contain hashtags and emojis. Select all of these data rows and hit \"Predict with Foundry\". \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.44.46-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"731\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.44.46-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.44.46-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.44.46-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.44.46-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e2) Next, choose a foundation model of choice (e.g., GPT-4 in this case) and connect that to an ontology, and hit generate preview. Once this is done, GPT-4 will come back with predictions on whether or not the text contains hashtags and emojis. This approach will work with many advanced foundation models including Google  Gemini so let's compare, and you will see that Gemini also has returned with a different set of predictions. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.47.05-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-05-at-11.47.05-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-05-at-11.47.05-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/03/Screenshot-2024-03-05-at-11.47.05-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-11.47.05-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e3) Finally, navigate over to your data set and deselect all of your data rows, and select the different models that you just ran (GPT-4, Gemini, etc). If either of these two models have predicted that it contains hashtag as true. If Gemini has said yes and a GPT four has said yes, and if you notice that your metadata value that was imported earlier from the SDK, also says that it has a hashtag, then you'll have 3 different signals that are telling us this contains a hashtag. By providing 3 different weak labels that you feel pretty confident about, you are now able to aggregate that to be your strong label and use these signals to improve your overall data annotation process. To complete the project, go ahead and select your data, hit classification, choose your project, and set the \"has hashtag\" as \"yes\" and then hit \"Submit\". To wrap up, this approach shows how you can leverage weak labels and weak supervision to come up with a strong label for your data directly by using the Quantumworks Lab UI.\u003c/p\u003e\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\u003cp\u003eIn this guide, we walked through two approaches for creating labeling functions with Quantumworks Lab for speeding up data labeling and annotation. By embracing programmatic labeling approaches through the utilization of labeling functions, teams can enhance the efficiency of their overall labeling operations for a variety of verticals especially in domains such as retail/e-commerce, media and internet, and more.  Give the solution a try using the \u003ca href=\"https://colab.research.google.com/drive/1pdtB2kuUypYWu1Q8Ot89C-HR1sGkPwN4?usp=sharing\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003enotebook\u003c/a\u003e provided, and we'd love to hear your feedback or ideas on how we can help you improve your data annotation workflows via labeling functions.\u003c/p\u003e","comment_id":"65e754531ac5200001373f08","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-05-at-2.12.03-PM.png","featured":false,"visibility":"public","created_at":"2024-03-05T17:20:19.000+00:00","updated_at":"2024-03-06T19:30:08.000+00:00","published_at":"2024-02-21T19:54:00.000+00:00","custom_excerpt":"Learn how teams can accelerate and automate data labeling by using labeling functions with Labelbox.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/how-to-speed-up-labeling-with-labeling-functions/","excerpt":"Learn how teams can accelerate and automate data labeling by using labeling functions with Labelbox.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65c97ca417c84d0001395ccb","uuid":"ed465aa9-c5cf-4c1e-bf18-fa38cc037a18","title":"Distilling a faster and smaller custom LLM using Google Gemini","slug":"end-to-end-workflow-for-knowledge-distillation-with-nlp","html":"\u003cp\u003eThe race to both mimic and create competitor models to OpenAI’s GPT3.5 energized the interest in model compression and quantization techniques.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKnowledge distillation, \u003c/strong\u003ealso known as \u003cstrong\u003emodel distillation,\u003c/strong\u003e is one of many techniques that have grown in popularity and importance in enabling small teams to leverage foundation models in developing small (but mighty) custom models, used in intelligent applications.\u003c/p\u003e\u003cp\u003eIn “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, we illustrated some of the conceptual foundations for how model distillation works as well as why we even need smaller models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe also provided an in-depth guide with a worked example in the second part of our series,\u0026nbsp;“\u003ca href=\"https://labelbox.com/guides/end-to-end-workflow-with-model-distillation-for-computer-vision/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnd-to-end workflow with model distillation for computer vision\u003c/u\u003e\u003c/a\u003e”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eNow we turn our attention to demonstrating the flexibility and power of model distillation in another domain and use case, where increased efficiency through supervised training of a smaller model by a foundation model is necessary.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll demonstrate an end-to-end workflow for natural language processing, using model distillation to fine-tune a BERT model with labels created in Model Foundry using \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGoogle Gemini\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll show how easy it is to go from raw data to cutting-edge models, customized to your use case, using a \u003ca href=\"https://huggingface.co/datasets/SetFit/emotion?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esentiment dataset\u003c/u\u003e\u003c/a\u003e (additional public datasets can be found \u003ca href=\"https://labelbox.com/datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e or on sites like Kaggle or HuggingFace).\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn less than 30 min you’ll learn how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIngest\u003c/strong\u003e, \u003cstrong\u003eexplore\u003c/strong\u003e and \u003cstrong\u003eprepare\u003c/strong\u003e the sentiment dataset;\u003c/li\u003e\u003cli\u003ePick and configure \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany text-based foundation model\u003c/u\u003e\u003c/a\u003e to \u003cstrong\u003eautomatically label\u003c/strong\u003e data with just a few clicks;\u003c/li\u003e\u003cli\u003eExport the labeled predictions to a project as a potential set-up for \u003cstrong\u003emanual evaluation\u003c/strong\u003e;\u003c/li\u003e\u003cli\u003eUse the labeled predictions dataset to \u003cstrong\u003efine-tune a student model\u003c/strong\u003e using a cloud-based notebook provider;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEvaluate performance\u003c/strong\u003e of the fine-tuned student model.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAt the end of the tutorial we’ll also discuss advanced considerations in scaling your models up and out, such as automating data ingestion and labeling, and resources for incorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e\u0026nbsp; into your workflows.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-use-model-distillation-to-fine-tune-a-small-task-specific-language-model\"\u003eSee it in action: How to use model distillation to fine-tune a small, task-specific Language Model\u003c/h3\u003e\u003cp\u003e\u003cem\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab\u0026gclid=Cj0KCQiA7OqrBhD9ARIsAK3UXh1OL7LeJadsNvbP_43MnwLSpLlRPD0IkilIwyvf3VwKOzxQ5ioGon8aApXREALw_wcB\u0026referrer_url=https://www.google.com/\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial. You’ll also need to \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003ecreate API keys for accessing the SDK\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eNotebook: \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=1FMXvdze28AZ\"\u003e\u003cu\u003eText Bert Model Distillation\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003ch3 id=\"the-model-distillation-workflow-for-nlp\"\u003eThe Model Distillation Workflow for NLP\u003c/h3\u003e\u003cp\u003eIn our prior post on model distillation concepts, we discussed the different model distillation patterns, based on the following criteria:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe type of \u003cstrong\u003eTeacher\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eStudent\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eknowledge\u003c/strong\u003e being transferred from teacher to student model(s) (\u003cstrong\u003eresponse-based\u003c/strong\u003e,\u003cstrong\u003e feature-based\u003c/strong\u003e, \u003cstrong\u003erelation-based\u003c/strong\u003e knowledge);\u003c/li\u003e\u003cli\u003eThe cadence and scheme for how the student mode is trained (\u003cstrong\u003eoffline\u003c/strong\u003e, \u003cstrong\u003eonline\u003c/strong\u003e, \u003cstrong\u003eself\u003c/strong\u003e).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e“Benefits of Using Model Distillation”\u003c/strong\u003e, Source: “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, Fig \u003cstrong\u003e3.1\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll be demonstrating the most popular and easiest pattern to get started with: \u003cstrong\u003eoffline, response-based model\u003c/strong\u003e (or \u003cstrong\u003eknowledge\u003c/strong\u003e) \u003cstrong\u003edistillation\u003c/strong\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe teacher model we’ll be using to produce the responses is \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGoogle \u003c/u\u003eGemini\u003c/a\u003e and the student model is \u003ca href=\"https://huggingface.co/docs/transformers/en/model_doc/bert?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBERT\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://huggingface.co/distilbert/distilbert-base-uncased?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edistilbert-base-uncased\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs you’ll see, we could have chosen \u003ca href=\"https://labelbox.com/guides/how-to-fine-tune-large-language-models-with-labelbox/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany combination of teacher or student models\u003c/u\u003e\u003c/a\u003e, because the \u003cstrong\u003eoffline, response-based\u003c/strong\u003e pattern of model distillation is incredibly flexible.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen implementing this process for your own use case, it’s important to understand the relative strengths and weaknesses of each model and match them according to your requirements and use cases (whether it’s \u003ca href=\"https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edetecting and removing PII to be GDPR compliant\u003c/u\u003e\u003c/a\u003e or \u003ca href=\"https://labelbox.com/guides/how-to-build-a-content-moderation-model-to-detect-disinformation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003edetecting unsavory content\u003c/u\u003e\u003c/a\u003e).\u003c/p\u003e\u003ch3 id=\"using-the-labelbox-platform-to-automate-model-distillation\"\u003eUsing The Quantumworks Lab Platform To Automate Model Distillation\u0026nbsp;\u003c/h3\u003e\u003cp\u003eLabelbox is the data factory for genAI, providing an end-to-end solution for curating, transforming, annotating, evaluating and orchestrating unstructured data for data science and machine learning.\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab platform supports the development of intelligent applications using the model distillation and fine-tuning workflow enabling AI developers to easily:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImport, curate, filter and eventually select the text that will be labeled for use in student model training and fine-tuning with \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e. Additionally you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/model-foundry-automating-data-tasks-with-foundation-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomate the labeling and annotation of the original text dataset using foundation models\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eor any pre-trained model, including custom models\u003c/u\u003e\u003c/a\u003e) in \u003ca href=\"https://labelbox.com/guides/guide-to-using-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e. The ability to leverage a variety of open source or third-party models to accelerate pre-labeling can cut labeling costs by up to 90% for teams with existing labeling initiatives (and kickstart new and budget constrained AI developers operating without\u0026nbsp; manual labeling capabilities).\u003c/li\u003e\u003cli\u003eLeverage \u003ca href=\"https://labelbox.com/customers/intuitive-surgical-customer-story/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman-in-the-loop\u003c/u\u003e\u003c/a\u003e evaluation (and \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e) through the Foundry to Annotate integration, so labels can be reviewed before being used for training or fine-tuning.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIntegrate with some of the most common cloud providers like \u003ca href=\"https://learn.labelbox.com/accelerate-retail-and-ecommerce-ai-roi/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Cloud Platform\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://labelbox.com/company/partnerships/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eDatabricks\u003c/u\u003e\u003c/a\u003e for additional MLOps services.\u003c/li\u003e\u003cli\u003eOrchestrate and schedule future automated labeling and model runs as \u003ca href=\"https://labelbox.com/blog/10x-faster-uploads-with-labelboxs-new-ingestion-upgrades/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enew data flows into the Quantumworks Lab platform\u003c/u\u003e\u003c/a\u003e through Model Apps (including \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emultimodal data labeling\u003c/u\u003e\u003c/a\u003e). \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"introduction-to-data-preparation-for-natural-language-processing-with-catalog\"\u003eIntroduction To Data Preparation for Natural Language Processing With Catalog\u003c/h2\u003e\u003cp\u003eBefore beginning the tutorial:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCreate a free HuggingFace account (in order to access the \u003ca href=\"https://huggingface.co/datasets/SetFit/emotion?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003e\"Setfit/emotion\" dataset\u003c/u\u003e\u003c/a\u003e)\u003cul\u003e\u003cli\u003eDownload the dataset locally\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.32.27-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab\u0026gclid=Cj0KCQiA7OqrBhD9ARIsAK3UXh1OL7LeJadsNvbP_43MnwLSpLlRPD0IkilIwyvf3VwKOzxQ5ioGon8aApXREALw_wcB\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003eCreate a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e (in order to \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecreate a free API key\u003c/u\u003e\u003c/a\u003e)\u003cul\u003e\u003cli\u003eNavigate to “\u003cstrong\u003eCatalog\u003c/strong\u003e” in the sidebar\u003c/li\u003e\u003cli\u003eSelect “\u003cstrong\u003e+New\u003c/strong\u003e”\u003c/li\u003e\u003cli\u003eUpload the dataset from Kaggle\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNote\u003c/strong\u003e: The easiest method is to use \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#create-a-dataset-via-the-app-ui\"\u003e\u003cu\u003ethe Quantumworks Lab Web UI\u003c/u\u003e\u003c/a\u003e\u0026nbsp; to manually upload your dataset.\u0026nbsp;\u003cul\u003e\u003cli\u003eIf your goal is to scale the data ingestion process for future labeling or data refreshes, check out our SDK.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’re able to see your dataset in Quantumworks Lab Catalog, you’ll be able to do the following:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e for more granular data curation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003eFor additional details on how to use Catalog to enable data selection for downstream data-centric workflows (such as data labeling, model training, model evaluation, error analysis, and active learning), check out \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour documentation\u003c/u\u003e\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"using-a-large-nlp-model-or-llm-to-generate-and-distill-predictions-for-fine-tuning\"\u003eUsing A Large NLP Model Or LLM To Generate And Distill Predictions For Fine-Tuning\u003c/h2\u003e\u003cp\u003eThe first step of model distillation is to \u003cstrong\u003eidentify an appropriate teacher model\u003c/strong\u003e, which will be used to produce responses that, when combined with the original text, will serve as the \u003cstrong\u003efine-tuning dataset for the student model\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResponse-based\u003c/strong\u003e model distillation is powerful because it can be used even when access to the original model weights is limited (or the model is so big that downloading a copy of the model would take a really long time). Response-based distillation also doesn’t require the user to have trained the model themselves; just that the model was pre-trained.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox allows you to pick any of the \u003ca href=\"https://labelbox.com/blog/6-key-llms-to-power-your-text-based-ai-applications/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecurrently hosted, state-of-the-art models to use\u003c/u\u003e\u003c/a\u003e (as well as upload your own custom models) to use as the teacher model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor now, let’s get started with preparing the text we’ll be labeling, or generating predictions with, using Google Gemini. The combination of text and label pairs will be used for BERT.\u003c/p\u003e\u003ch3 id=\"step-1-select-text-assets-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select text assets and choose a foundation model of interest\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1002\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003ePreview of text in Quantumworks Lab Catalog\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to your uploaded Emotions dataset in Catalog.\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the text on which the predictions should be made.e.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “\u003cstrong\u003ePredict with Model Foundry\u003c/strong\u003e”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run (in this case \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eGemini\u003c/u\u003e\u003c/a\u003e).\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task -\u0026nbsp; such as \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003etext classification, summarization, and text generation\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eTo locate a \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003especific model\u003c/u\u003e\u003c/a\u003e, you can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003eStep 2: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eWhen developing ML based applications, developers need to quickly and iteratively prepare and version training data, launch model experiments, and use the performance metrics to further refine the input data sources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe performance of a model can vary wildly depending on the data used, the quality of the annotations, and even the model architecture itself. A necessary requirement for replicability is being able to see the exact version of all the artifacts used or generated as a result of an experiment.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox will snapshot the experiment, the data artifacts as well as the trained model, as a saved process known as a \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis includes the types of items the model is supposed to identify and label, known as an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eontology\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of settings, which you can find in the Advanced model setting.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce you’ve located Gemini, you can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIn this case, we will enter the following prompt:\u0026nbsp;\u003cul\u003e\u003cli\u003e\u003cem\u003eFor the given text, answer the following. Classify emotions, pick one of the options: [sadness, joy, love, anger, fear, surprise]. Return the result as a JSON object. {\"emotions\" : \"\"}.\u0026nbsp;\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis prompt is designed to facilitate responses from the model with one of the following: \u003cem\u003esadness, joy, love, anger, fear, surprise\u003c/em\u003e.\u003c/p\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-review-predictions-in-the-model-tab\"\u003eStep 3: Review predictions in the Model tab \u003c/h3\u003e\u003cp\u003eBecause each \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run is submitted with a unique name\u003c/u\u003e\u003c/a\u003e, it’s easy to distinguish between each subsequent model run.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/foundry-view-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eView prediction results\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results \u003ca href=\"https://docs.labelbox.com/docs/comparing-model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eacross a variety of model runs\u003c/u\u003e\u003c/a\u003e different models\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-2.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-2.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-2.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eView predictions in Model tab for the model run\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"999\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-4.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-4.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-4.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eIn this case, Gemini Pro predicted this text to be \"joy\"\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eThese generated labels are now ready to be used for fine-tuning the BERT student model.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-enriching-and-evaluating-predictions-using-human-in-the-loop-and-annotate\"\u003eStep 4: Enriching and evaluating predictions using human-in-the-loop and Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAlthough fine-tuning a foundation model requires less data than pre-training a large foundation model from scratch, the data (specifically the labels) need to be high-quality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEven big, powerful foundation models make mistakes or miss edge cases.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou might also find that there are additional categories that the parent model didn’t identify correctly because the ontology was incomplete.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce a parent model like Gemini has been used for the initial model-assisted labeling run, those predictions \u003ca href=\"https://docs.labelbox.com/docs/foundry-annotate-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then be sent\u003c/u\u003e\u003c/a\u003e to a \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eproject\u003c/u\u003e\u003c/a\u003e, a container \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhere all your labeling processes happen\u003c/u\u003e\u003c/a\u003e. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bpsrmes4da\" title=\"Distilling a faster and smaller LLM using BERT and Gemini_SendToAnnotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eIn this case, we feel fairly confident\u003cstrong\u003e \u003c/strong\u003ein how well Gemini performed so we’ll send the inferences to the corresponding Quantumworks Lab project and treat them as the ground truth that the student model will be fine-tuned on. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"fine-tuning-the-student-model-bert\"\u003eFine-Tuning The Student Model (BERT)\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWe’ve shown the first half of the model distillation to fine-tuning workflow.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe identified the categories we wanted the parent model (Gemini) to detect and label in the form of an ontology.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe used Gemini to automatically label the texts as \u003cem\u003e\"sadness\"\u003c/em\u003e or \u003cem\u003e\"fear\"\u003c/em\u003e (for example).\u003c/li\u003e\u003cli\u003eWe exported the generated labels to a Quantumworks Lab project, at which point we could review the labels manually and enrich them further using the Quantumworks Lab editor.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe next step is to use the generated labels, along with the original texts, to fine-tune a student model in Colab.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: You’ll now need the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=cclwx_KmJr3Z\u0026line=3\u0026uniqifier=1\"\u003e\u003cem\u003e\u003cu\u003eAPI keys from earlier\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to follow along with the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=1FMXvdze28AZ\"\u003e\u003cem\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003ch3 id=\"step-5-fetch-the-ground-truth-labels-from-the-project-via-labelbox-sdk\"\u003eStep 5: Fetch the ground truth labels from the project via Quantumworks Lab SDK\u003c/h3\u003e\u003cp\u003eFor brevity, we’ve omitted the surrounding code samples but you can copy or run the corresponding blocks in the provided notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our documentation to find out all the ways you can automate the model lifecycle (\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eincluding labeling\u003c/u\u003e\u003c/a\u003e) using our SDK. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore fine-tuning the BERT student model, we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=Jq-1tQs2QBrj\"\u003e\u003cu\u003eneed to fetch the generated labels\u003c/u\u003e\u003c/a\u003e from the Quantumworks Lab project using the Quantumworks Lab SDK as well as the \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=jKMysiHvUI1D\"\u003e\u003cu\u003eground truth\u003c/u\u003e\u003c/a\u003e.\u0026nbsp; \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-6-additionally-text-data-processing\"\u003eStep 6: Additionally Text Data Processing \u003c/h3\u003e\u003cp\u003eThere's additional processing that needs to happen, which we walk through below.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNext we’ll ensure the labels are \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=HFiQMbcSQSks\"\u003e\u003cu\u003eexported into a .csv file\u003c/u\u003e\u003c/a\u003e that contains two columns, the original ‘text’ and the generated ‘label’.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=B2_znE_XHbvG\u0026line=2\u0026uniqifier=1\"\u003e\u003cu\u003eread the csv file into a pandas dataframe\u003c/u\u003e\u003c/a\u003e, perform a series of aggregation operations to help us splits the text into train and test sets based on the category count.\u003c/li\u003e\u003cli\u003eWe’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=k9eNSPuLRAJx\"\u003e\u003cu\u003einitialize a tokenizer\u003c/u\u003e\u003c/a\u003e and encode the train and test texts.\u003c/li\u003e\u003cli\u003eFinally we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=PnHQrghbQ7tD\" rel=\"noreferrer\"\u003efinish creating the training \u0026amp; validation dataset\u003c/a\u003e. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNote\u003c/strong\u003e: Expand the “Export labels into .CSV file” block in the Colab notebook for the full code sample.\u0026nbsp; \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-7-fine-tune-student-bert-model-using-labels-generated-by-google-gemini\"\u003eStep 7: Fine tune student BERT model using labels generated by Google Gemini\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn Colab we’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=gjMPaBgAIAGd\u0026line=2\u0026uniqifier=1\" rel=\"noreferrer\"\u003einstantiate a BERT model\u003c/a\u003e and \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=cmbMl5wXIB3I\u0026line=6\u0026uniqifier=1\" rel=\"noreferrer\"\u003etrain it using the data\u003c/a\u003e, which includes both text and labels. Specifically we'll fine-tune a text classifier model called \u003cem\u003e“distilbert-base-uncased”\u003c/em\u003e to classify text as one of the following categories in the ontology: \u003cem\u003esadness, joy, love, anger, fear, surprise\u003c/em\u003e. \u003c/li\u003e\u003cli\u003eWe’ll also \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=VzlNe83KRh98\"\u003esave the model\u003c/a\u003e and \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=RbPplpJdRuXL\"\u003etest the prediction\u003c/a\u003e.\u003cul\u003e\u003cli\u003eBy saving the model (or every model we create) we have the option of A/B testing models and using the models for downstream use cases (as well as share the models with other key stakeholders through a model registry, like MLFlow). \u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1339\" height=\"716\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/FbgM_K2_-b5t42LscwQfIJwhMqxKE9cJ2Fcr69nIuGzIoPN_rRPldvTJHoohtqUw8aiMn10Y0IUsa2WzRMhEmbkLrZDkdrKEVO3j9oCc1P2xjOknPq_PebfTMPu3dEtYY70WTmVOGejvOunwmlG8MNY.png 1339w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1262\" height=\"344\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-16-at-12.38.08-AM.png 1262w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-8-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 8: Create a model run with predictions and ground truth\u003c/h3\u003e\u003cp\u003eOftentimes the initial training or fine-tuning step isn’t the final stop on the journey of developing a model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOne of the biggest differences between the traditional method of training models in the classroom versus the real-world is how much control you have over the quality of your data, and consequently the quality of the model produced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we mentioned earlier, developers can upload predictions and use the Model product to \u003ca href=\"https://labelbox.com/blog/how-to-confidently-compare-test-and-evaluate-models-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ediagnose performance issues with models and compare them across multiple experiments\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDoing so automatically populates model metrics that make it easy to evaluate the model’s performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e \u003c/p\u003e\u003cul\u003e\u003cli\u003eFirst, you’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=mjDaoyV3L3l1\"\u003egrab the model’s ID\u003c/a\u003e to \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=HsDr_g3qqUFo\"\u003ecreate a new model run\u003c/a\u003e (if needed).\u003c/li\u003e\u003cli\u003eThen you’ll \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=K8D7lE9nrOLG\"\u003eget the ground truth from your project\u003c/a\u003e via the export as well as the \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=-cv2GJ6zrU5A\"\u003elabel IDs from ground truth\u003c/a\u003e. \u003c/li\u003e\u003cli\u003eNext you’ll create the predictions by \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=qxwq7kKZrfzL\"\u003erunning the fine-tuned BERT model on the original text assets\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eYou \u003ca href=\"https://docs.labelbox.com/docs/import-ground-truth?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then upload the ground truth labels\u003c/u\u003e\u003c/a\u003e (the labels generated by Gemini and used as the fine-tuning dataset) and the \u003ca href=\"https://docs.labelbox.com/docs/upload-model-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003epredictions\u003c/u\u003e\u003c/a\u003e from the fine-tuned BERT model \u003ca href=\"https://colab.research.google.com/drive/1U_1YS9m7Ut6W4OdjUOXYVdw-0Uy30SwN?ref=labelbox-guides.ghost.io#scrollTo=i7Xq-fldsiRY\"\u003eto the corresponding Quantumworks Lab model\u003c/a\u003e. \u003c/li\u003e\u003cli\u003eYou can see an example of how model metrics are automatically populated by Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"997\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-10.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-10.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-10.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-10.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAuto generated metrics\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"evaluating-model-performance\"\u003eEvaluating Model Performance\u003c/h2\u003e\u003cp\u003eThere’s no single metric to rule them all \u003ca href=\"https://labelbox.com/blog/gpt4-vs-palm-assessing-performance-of-llm-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhen evaluating how your fine-tuned LLM performs\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBoth qualitative and quantitative measures must be considered, combined with sampling and manual review.\u003c/p\u003e\u003cp\u003eWith that being said, Model offers a number of the most common out-of-the-box. With the \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io#metrics-view\"\u003e\u003cu\u003e‘Metrics view’\u003c/u\u003e\u003c/a\u003e users can drill into crucial model metrics, such as \u003ca href=\"https://docs.labelbox.com/docs/upload-model-metrics-1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econfusion matrix, precision, recall, F1 score, false positive, and more\u003c/u\u003e\u003c/a\u003e, to surface model errors.\u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive, which means you can click on any chart or metric to immediately open up the gallery view of the model run and see corresponding examples, as well as visually compare model predictions between multiple model runs. \u003c/p\u003e\u003ch3 id=\"step-9-evaluate-predictions-from-different-bert-model-runs-in-labelbox-model\"\u003eStep 9: Evaluate predictions from different BERT model runs in Quantumworks Lab Model \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.29.47-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBERT fine tuned on labels created by Gemini Pro vs ground truth labels \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to “\u003cstrong\u003eModel\u003c/strong\u003e”\u003c/li\u003e\u003cli\u003eIn this case, we fine-tuned two models, one using 1000 ground truth labels and the other with 1000 labels generated by the Gemini model. We see very similar results and leveraging an off the shelf model is almost as good as using ground truth labels.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"examples-of-predictions-from-fine-tuned-bert-model\"\u003e\u003cstrong\u003eExamples of predictions from fine tuned BERT model\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eHow does our fine-tuned model perform? \u003c/p\u003e\u003cp\u003eLet's manually inspect a few examples of predictions from the fine-tuned BERT model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-12.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-12.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-12.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-12.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “anger”.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/image-11.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/image-11.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/image-11.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/image-11.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “joy”.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1116\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-15-at-6.33.02-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe model correctly classified this text as “fear”.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"advanced-considerations\"\u003eAdvanced Considerations\u003c/h2\u003e\u003cp\u003eIn this step-by-step walkthrough, we’ve shown how anyone with any text-based dataset can leverage an LLM to label, fine-tune and analyze a smaller but mighty custom model.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"610\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/11/Fig-5.2.1_-Foundation-Model-Operations.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e“Leveraging FMOps To Develop intelligent Applications”, Source: “\u003c/span\u003e\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eA pragmatic introduction to model distillation for AI developers\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e”, Fig 5.2.4\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAdditional considerations users should address for scaling similar projects include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCollecting feedback \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efrom users \u0026amp; human SME’s to improve\u003c/u\u003e\u003c/a\u003e the fine-tuning dataset quality on a continuous basis, including error analysis and \u003ca href=\"https://docs.labelbox.com/docs/llm-human-preference?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman preference modeling\u003c/u\u003e\u003c/a\u003e;\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003eStrategic planning for incorporating multiple data modalities besides text, including image, audio, and video;\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomating future data ingestion, curation, enrichment, and labeling when the fine-tuned model needs to be retrained due to drift via a \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erobust SDK\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eEasy-to-use, user interface that can be customized for various modalities of data when multiple users are involved (as well as \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM data generation\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eIn this tutorial we demonstrated an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language processing\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a BERT model with labels created in Model Foundry using \u003ca href=\"https://labelbox.com/product/model/foundry-models/google-gemini/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Gemini\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHopefully you were able to see how easy it is to go from raw data to cutting-edge custom models in less than 30 min.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou learned how the Quantumworks Lab platform enables model distillation by allowing developers to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIngest, explore and prepare text-based datasets using \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eUse any \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM\u003c/u\u003e\u003c/a\u003e to automatically label data using \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e as well as how to incorporate human-in-the-loop evaluation using \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eExport these labeled predictions to a cloud-based training environment for fine-tuning;\u003c/li\u003e\u003cli\u003eAutomate the various workflows using the \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox SDK\u003c/u\u003e\u003c/a\u003e;\u003c/li\u003e\u003cli\u003eEvaluate model performance and analyze model errors using \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox Model\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you’re interested in learning more about model distillation, check out the previous posts in this series: “\u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA pragmatic introduction to model distillation for AI developers\u003c/u\u003e\u003c/a\u003e”, “\u003ca href=\"https://labelbox.com/guides/end-to-end-workflow-with-model-distillation-for-computer-vision/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eEnd-to-end workflow with model distillation for computer vision\u003c/u\u003e\u003c/a\u003e”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLooking to implement a production-ready model distillation and fine-tuning in your organization but not sure how to get started leveraging your unstructured data?\u0026nbsp;\u003c/p\u003e\u003cp\u003eAsk \u003ca href=\"https://community.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour community\u003c/u\u003e\u003c/a\u003e or reach out to \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour solutions engineers\u003c/u\u003e\u003c/a\u003e!\u003c/p\u003e","comment_id":"65c97ca417c84d0001395ccb","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/distilling.png","featured":false,"visibility":"public","created_at":"2024-02-12T02:04:20.000+00:00","updated_at":"2024-11-20T23:09:07.000+00:00","published_at":"2024-02-15T19:25:02.000+00:00","custom_excerpt":"Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"681516a56e02120001d5ab84","name":"#mikiko","slug":"hash-mikiko","description":null,"feature_image":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/404/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"65bd4ff882e5680001e078f9","name":"Model Distillation","slug":"model-distillation","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/tag/model-distillation/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/end-to-end-workflow-for-knowledge-distillation-with-nlp/","excerpt":"Learn how to perform knowledge distillation and fine-tuning to efficiently leverage LLMs for NLP, like text classification with Gemini and BERT. ","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65cd1695643f030001d2239a","uuid":"ab48a216-971c-438e-bd52-110bab1dfe04","title":"How to enhance RAG chatbot performance by refining a reranking model","slug":"how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models","html":"\u003cp\u003eWhen building customized chatbots and similar LLM applications, teams may start off with RAG (Retrieval Augmented Generation) approaches. A RAG-centric application relies on context to formulate an appropriate response when given a user query.\u0026nbsp;Because the context is retrieved from internal documents, RAG based approaches are adept at handling organization specific chatbots while minimizing hallucinations. A good example of how organizations are leveraging RAG centric applications is \u003ca href=\"https://observer.com/2023/06/metamate-foreshadows-ai-workplace-trend/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eMetamate\u003c/u\u003e\u003c/a\u003e, an employee agent used at Meta which pulls internal information at request for answering employee questions, producing meeting summaries, writing code and debugging features.\u003c/p\u003e\u003cp\u003eWhile easy to prototype, optimizing RAG-based applications for relevant retrieval require complex considerations due to the potential semantic information that is lost when embedding internal documents within vector databases. Therefore, a useful RAG based chatbot requires careful strategies for preprocessing document metadata enrichment (e.g,. metadata filtering, domain specific embedding models, chunking, etc) as well as model inferencing LLM, prompt engineering, and re-ranking retrieved documents.\t\t\t\u003c/p\u003e\u003cp\u003eIn this guide, we’ll cover how Quantumworks Lab can be a useful tool to help you preprocess documents (e.g., entity detection, document classification, etc) and improve retrieval relevance by fine-tuning a reranking model. Feel free to walk through the full guide or watch the step-by-step video below.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/32xu4rxe64\" title=\"RAG Fine Tune Reranking Model Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"556\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"why-reranking\"\u003e\u003cstrong\u003eWhy reranking?\u003c/strong\u003e\t\t\t\u003c/h3\u003e\u003cp\u003eWhen building RAG-based applications, thereʼs a limit to the amount of text that can be passed to an LLM and therefore the context given to an LLM is limited to the top k documents pulled from the vector space using a similarity search. Because semantic information may be lost during the embedding process, relevant context that may be useful to answering the user query may fall outside of the top k responses.\u003c/p\u003e\u003cp\u003eWith reranking embedded within a RAG application, we now have the ability to consider a larger corpus of retrieved text and pass the most relevant documents into our LLM to generate a better answer.\t\u003c/p\u003e\u003cp\u003eUsing the Quantumworks Lab Platform, we will fine tune an reranker model to improve chatbot performance. Holding other considerations consistent (i.e., token limit, embedding model, chunking strategy, LLM choice), we will compare baseline responses without reranking to the output of a fine-tuned reranker workflow.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/AjLZJu0SYcbijxfYEg_CdQZeZYWdtqt9AxwwChx9C7hLBPs-h7pv0Zzg7DcmB5yZhSgu9ro9TNBwTnLzrDT8GDc2zUFMseJndTsQJMWBjFWEXYzZ9nLtkHihx3kFoPMIksq-3qH9b96b6Zc9l2gwKc8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1570\" height=\"1290\"\u003e\u003c/figure\u003e\u003ch2 id=\"part-1-review-initial-baseline-responses\"\u003e\u003cstrong\u003ePart 1. Review initial baseline responses\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo illustrate this approach, we will attempt to improve responses to the 2022 NFL Rulebook 🏈.\u003c/p\u003e\u003cp\u003eNote: From the document, the document is divided into different sections. You can leverage the Quantumworks Lab Platform to label metadata for each page (i.e. Contents / Scenarios / Rules) to improve efficient and relevant retrieval.\u003c/p\u003e\u003cp\u003eThe PDF is embedded using the HuggingFace \u003cem\u003esentence- transformers/distiluse-base-multilingual-cased-v2\u003c/em\u003e\u003cstrong\u003e \u003c/strong\u003emodel. We will keep the same vector embeddings for both workflows.\t\t\t\t\u003c/p\u003e\u003cp\u003eAfter extracting the top 2 documents (via similarity search), we will add them as context to the flan-t5-base LLM model to generate initial responses to queries about NFL rules.\t\u003c/p\u003e\u003cp\u003eAs you can see from the sample of results below, the initial responses are spotty at best.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/RZYA6u7ntf_wRWy6bbFQvC-ZydBQ6uHn8tWJF-Jo-R7rCshIEOgR8z051OXL3Hez4NwNLvP5eJjHViBwcYHQDxU9-y214LeD1kLNgULt6TwLL3pleQNCvKAYWuEwRxbL2kzeAki1OJpsgHQVJCq9Zxc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"688\"\u003e\u003c/figure\u003e\u003ch2 id=\"part-2-human-in-the-loop-hitl-and-semi-supervised-approaches-to-fine-tune-reranker-model\"\u003e\u003cstrong\u003ePart 2. Human in the Loop (HITL) and Semi-Supervised Approaches to Fine Tune Reranker Model\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo fine tune our reranker model, let's extract 5 responses for each query and upload the prompt / responses(s) pairs to Quantumworks Lab \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eNote: We can leverage unsupervised techniques such as semantic search, similarity search, cluster search, etc to enrich data rows with relevant metadata and/or bulk classification. The highlighted cluster shown below contains search queries related to flagrant penalties, which we can use to attach metadata \u0026amp; classifications.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/O_bvl7k6r6laf7g4s0cYrXgpzj5CM-_V-ksxn1UyS7-GJN07nzljZ-ql2pFLczwQyM3y3eVyhZYftRbPeDabkopOb3v89KUTWnp91QBw4p5SEOnHDWrKlQTQfB2bB2yWad8k82Z7HJUpb97XkBoI9Fg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1276\" height=\"864\"\u003e\u003c/figure\u003e\u003cp\u003eUsing \u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, we can apply semi-supervised learning with foundation models to generate pre labels. Let’s select GPT4 and attach a customized prompt to generate responses to match the target ontology, in this case, whether the output is relevant or not relevant to the query.\t\t\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/y0K_1Q3dxs-mPxMFolWTnex_3HcGgXN5fWWBgzKUteoKVWUn6Kjuq20vVxJdC8WjsEYrUnQZGm4_LxM-FiC7n4hG-ommExevr9t3P7s2ofvR5PmkEhgKxx5q9qc4nJgJLH0oW_mqYxfJm2P_mdcV0qs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"774\"\u003e\u003c/figure\u003e\u003cp\u003eBy sending the query response pairs with pre-labels to our annotation project, human labelers can now leverage pre-labels (and/or metadata \u0026amp; attachments) to speed up the annotation process. Predictions are then converted to ground truth once the human-in-the loop process is completed.\t\t\t\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/FdUCET_nQQ4dREWVgomYDI2BQfEBDhZxVytEmCWt7NPJTTiradOcxYgXnZhcuWnxZnrirSR3KLrJQ3kG54Ab_75DAr87x0yovDek6Wa1TAF7X-E229wd32MWsfxBkU2njrQ-dYm1tPXYitJfbv21Lo4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"639\"\u003e\u003c/figure\u003e\u003cp\u003eTo track model performance, we can set dataset versions and enable data splits before exporting the data rows via the Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ePython SDK\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/xZmuBG-L4HutIDp8jJXPFhggWul_1hQxXGfdWPH0MlKRzS2zuMmfuB6QYcIMjW4LeX68fUxTkrsp5Tex0A0PCUj5H955vltUwgVSWElgoQ0zYmKy4FJ7ozcvIm7xiLW8-fpZIxkU1sRooXDeSjzP7Po\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"794\"\u003e\u003c/figure\u003e\u003ch2 id=\"part-3-fine-tuning-the-reranker-model\"\u003e\u003cstrong\u003ePart 3. Fine Tuning the Reranker Model\u003c/strong\u003e\t\t\t\u003c/h2\u003e\u003cp\u003eFor our workflow, we will fine tune the \u003cem\u003eBAAI/bge-reranker-large\u003c/em\u003e open source model on HuggingFace 😊. Companies such as Cohere also provides reranking models as well for fine tuning.\t\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/0sj2Uim6XsXN-4kAAzRAI4vUJrKjfjygn54MiDApgcxa53ENt-4XNgDQVIiCALj-G15SrRNCN-gis-UPLw0p26P7EwU1icADJNz2DqUZ3wfh1LApA_-zGmXjUC2WjuW8EN4pUWne5A7SIsKLKleKUU4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"217\"\u003e\u003c/figure\u003e\u003cp\u003eTo fine tune our model, we’ll need to format the ground truth labels into JSON line format.\t\t\t\t\t\u003c/p\u003e\u003cp\u003eNote: that the model was fine-tuned with AWS Sagemaker \u003cem\u003eml.g4dn.xlarge \u003c/em\u003emachine with T4 GPUs. The Google Colab environment also provides free GPUs for model fine tuning.\t\t\t\u003c/p\u003e\u003cp\u003eThe model returns relevancy scores for each chunk. We can see that the relative order of the scores and not the absolute value is what matters. Applying this on a data row in the testing set yields the following:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/7ETMUv_vDBG2rpsO2YOf8a-aACQQBtrEvickIWyzFrvoRxrWsRNC7pF8_bdCWcMn-6I-pkXAobCt0Me474smxdcctrkUwCqFo1fAWNUjybeQXDWEOKsdq5d2RrvA2-VAKE8WfFbNbaWlfkgwGIvAOaQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"151\"\u003e\u003c/figure\u003e\u003cp\u003eWe can see from the histogram distribution of relevancy scores for the testing set that relevant examples tend to have higher scores.\t\t\t\t\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/DGjI4LQsIC774mWeYpQnCVi-jSOGi8ufd1t1VBwaPquBoCggrNWivQqr0f6t9DAJ-YwwSeweqiJj2ulbGE1MEjf92O7EyoUMsZf4OxxVZKw1eaz4OJn1kI5r-HWBaS4rrvhGJ2jInxi8R3j0e20OAVM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"818\"\u003e\u003c/figure\u003e\u003cp\u003e\t\t\t\t\t\u003c/p\u003e\u003ch2 id=\"part-4-inference-with-an-updated-rag-workflow\"\u003ePart 4. Inference with an updated RAG workflow\t\t\t\t\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eUsing the same document embeddings \u003cem\u003e(sentence-transformers/distiluse- base-multilingual-cased-v2\u003c/em\u003e) and LLM (\u003cem\u003eflan-t5-base LLM model\u003c/em\u003e), let's retrieve the top 20 document chunks for each query.\t\t\t\t\u003c/p\u003e\u003cp\u003eUsing the fine tuned reranker model, let's rerank the top 3 documents which will be sent to the LLM as context.\u003c/p\u003e\u003cp\u003eApplying inferencing on the testing set, we can now compare the baseline response with the responses with reranking model. From here, we will see some markedly improved responses.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/6ViKEgEsjzeiU5zdHBVr7qDUISy-qmnQVNbigQmaUfS_ic41envcosrvtL7VYlomVHE7gC8YMNbDYz_rYVgTpe5XRSbdeq95Oy9o7p7GgI1iXH-4pD59fz6LNSox1Tfbf8NcshrYKnEPFY7HmUh3pTA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"763\"\u003e\u003c/figure\u003e\u003ch2 id=\"conclusion\"\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003cbr\u003e\t\t\u003c/h2\u003e\u003cp\u003eAs we’ve shown in this guide, model performance for customized chatbots rely on both the quality and quantity of labeled data. By incorporating Quantumworks Lab Catalog and Foundry to apply pre-labels with unsupervised \u0026amp; semi-supervised techniques, we were able to maximize labeling throughput. Afterwards, by using Quantumworks Lab Annotate, we were able to leverage a human-in-the-loop workflow to ensure that our training labels are of the highest quality. Teams should consider adopting this approach when building RAG-based applications to better preprocess documents and improve retrieval relevance by fine-tuning a reranking model.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful chatbots that fuel knowledge sharing and foster deeper customer interactions. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab%2520pricing\u0026utm_source=house\u0026utm_medium=email\u0026utm_campaign=020824\u0026\u0026landingPageAnonymousId=%22b04806c5-ff2b-4a09-9379-a9837b1801a9%22\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"65cd1695643f030001d2239a","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-14-at-11.54.36-AM.png","featured":false,"visibility":"public","created_at":"2024-02-14T19:37:57.000+00:00","updated_at":"2024-05-09T18:43:05.000+00:00","published_at":"2024-02-14T19:55:15.000+00:00","custom_excerpt":"Learn how to use Quantumworks Lab to preprocess documents (e.g., entity detection, document classification, etc) and improve retrieval relevance by fine-tuning a reranking model. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-enhance-rag-chatbot-performance-through-by-refining-reranking-models/","excerpt":"Learn how to use Quantumworks Lab to preprocess documents (e.g., entity detection, document classification, etc) and improve retrieval relevance by fine-tuning a reranking model. ","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65cbb3b7643f030001d22310","uuid":"2de9eae7-61eb-4935-94ab-91dedf3f59be","title":"How to build equipment detection models to improve worker safety and efficiency","slug":"how-to-build-equipment-detection-models","html":"\u003cp\u003eWith AI-powered object detection, you can now seamlessly integrate the latest advances in foundation models into your warehouse and construction site safety operations. As the demand for better safety monitoring continues to rise, it's essential for teams to maximize protective equipment use to mitigate potential hazards. Quantumworks Lab empowers the world’s largest organizations to leverage AI solutions tailored to their unique safety detection challenges.\u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for large-scale safety detection. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving safety detection requires a vast amount of data in the form of images and videos. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e\u0026nbsp;The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e\u0026nbsp;Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their safety detection through advanced computer vision techniques. Instead of relying on time-consuming manual human review, companies can leverage Quantumworks Lab’s AI-assisted data enrichment and flexible training frameworks to quickly build task-specific models that uncover actionable insights for the prevention of supply chain mistakes.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-13-at-11.11.42-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1778\" height=\"996\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-13-at-11.11.42-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-13-at-11.11.42-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-13-at-11.11.42-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-13-at-11.11.42-AM.png 1778w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through an end-to-end workflow on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve safety detection using personal protective equipment as an example. Specifically, this guide will walk through how you can explore and better understand your visual assets to make more data-driven business decisions for worker safety.\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-build-equipment-detection-models-to-improve-worker-safety-and-efficiency\"\u003eSee it in action: How to build equipment detection models to improve worker safety and efficiency\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/97k6ufmyas\" title=\"WD 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across\u0026nbsp;\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data with Catalog and Foundry\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below via:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/195NPiEDxpzYxuzBNI3aJ-Z-MzzgIo0Ds?ref=labelbox-guides.ghost.io#scrollTo=SBBJzrCYQ9gd\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003e\u003cstrong\u003ePart 1: Explore and prepare your data\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in the \u003ca href=\"https://colab.research.google.com/drive/195NPiEDxpzYxuzBNI3aJ-Z-MzzgIo0Ds?ref=labelbox-guides.ghost.io#scrollTo=SBBJzrCYQ9gd\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, please make a copy of the notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ll be working with a dataset that detects workers wearing safety equipment\u0026nbsp; – with the goal of quickly curating data and finding protective equipment (e..g, helmets, goggles, reflective vests, gloves, masks, etc) from high-volumes of images. \u003c/p\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003ePlease download the \u003ca href=\"https://colab.research.google.com/drive/1W_d3Gq_-2o3gOT8gvF7Zomamg7eSiGYj?ref=labelbox-guides.ghost.io#scrollTo=dOHormwXvTwB\" rel=\"noreferrer\"\u003edataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your data as a CSV.\u003c/li\u003e\u003cli\u003eIf your images sit as individual files in cloud storage, you can reference the URL of these files through our\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your image data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this demo, we'll be using Catalog to find relevant images of equipment for our dataset with the goal of annotating bounding boxes for the personal protective equipment using foundation models.\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"using-foundry-to-pre-label-bounding-boxes\"\u003eUsing Foundry to pre-label bounding boxes\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/o9k033g1ii\" title=\"WD 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn this next step, we'll walk through how you can take a human-in-the-loop approach to iterate or modify pre-labels and speed up the annotation process.  \u003c/p\u003e\u003cp\u003eModel Foundry enables teams to choose from a library of models and in this case, we'll be using GroundingDINO to generate previews and attach them as pre-labels.\u003c/p\u003e\u003cp\u003eWith\u0026nbsp;\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, you can automate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/6jofnqbzb3\" title=\"WD - 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eReview initial inference results and send to annotate\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/vldocd1w58\" title=\"WD 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eCreate model experiment and create a model run\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"part-2-train-your-model-and-generate-predictions\"\u003e\u003cstrong\u003ePart 2: \u003c/strong\u003eTrain your model and generate predictions\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/fewiasx1he\" title=\"WD - 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSee predictions overlayed on top of annotations\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe next step is to take all of our labeled data and train our model on it. This allows us to make predictions on this data and for Quantumworks Lab to calculate evaluation metrics so that we can see where the model is going wrong and improve model performance.\u003c/p\u003e\u003ch3 id=\"view-model-predictions-within-the-labelbox-ui-to-evaluate-and-diagnose-model-effectiveness\"\u003eView model predictions within the Quantumworks Lab UI to evaluate and diagnose model effectiveness\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/o7um6x4md1\" title=\"WD 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a last step, let's compare model inferences with ground-truth annotations to see where the model may be underperforming. A disagreement between model predictions and ground truth labels is typically due to either a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eAfter running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/li\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model for 10x faster corner-case detection – detect and visualize corner-cases where the model is underperforming. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection and additional labeling.\u003c/p\u003e\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\u003cp\u003eBy analyzing high-volumes of images and videos using foundation models and human alignment, Quantumworks Lab provides teams with the ability to inject valuable  insights for delivering better protective equipment detection models for warehouses and construction sites that allow you to improve operational efficiency, compliance and overall worker safety.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful task-specific models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"65cbb3b7643f030001d22310","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-13-at-11.58.37-AM.png","featured":false,"visibility":"public","created_at":"2024-02-13T18:23:51.000+00:00","updated_at":"2024-02-14T17:46:29.000+00:00","published_at":"2024-02-13T18:54:05.000+00:00","custom_excerpt":"Learn how you can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve safety detection for personal protective equipment using the latest advances in foundation models to automate labeling. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},"url":"https://labelbox-guides.ghost.io/how-to-build-equipment-detection-models/","excerpt":"Learn how you can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve safety detection for personal protective equipment using the latest advances in foundation models to automate labeling. ","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66997612e017de000190bd91","uuid":"376155c7-7f33-45df-9636-18c16e3967b6","title":"What is Human-in-the-Loop?","slug":"human-in-the-loop","html":"\u003cp\u003eEvery time a new AI tool is rolled out, the topic ultimately shifts to: Will AI replace humans? A satisfying answer to this question is given by the title of \u003ca href=\"https://hbr.org/2023/08/ai-wont-replace-humans-but-humans-with-ai-will-replace-humans-without-ai?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ea report by Harvard Business Review\u003c/u\u003e\u003c/a\u003e: “AI Won’t Replace Humans — But Humans With AI Will Replace Humans Without AI”. It summarizes how AI is nothing without the intervention of humans.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn one way or another, humans are involved in developing AI models, integrating natural human intelligence at various points of the machine-learning loop, resulting in more human-like AI systems that exhibit helpfulness, empathy, ethics, and reason.\u003c/p\u003e\u003cp\u003eHuman-in-the-loop is a fundamental concept in \u003ca href=\"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ereinforcement learning in the RLHF process\u003c/u\u003e\u003c/a\u003e. Human evaluators and annotators are introduced into the model training cycle to prepare datasets (preference pairs), monitor performance, and annotate feedback. Although that is just a single use case; human-in-the-loop goes beyond dataset preparation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this article, we break down human-in-the-loop as an AI model training concept, explaining what it entails, its bottlenecks, and how we can overcome such challenges.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"understanding-the-concept-of-humans-in-the-loop\"\u003e\u003cstrong\u003eUnderstanding the concept of humans-in-the-loop\u003c/strong\u003e\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXfxSttw09pvIW-vJV0LGkEAcyMo22UmWjqfQEF5OCIKC_kVN5z_iQgLDQgHXpCiu35uHmXLzCHVaNuz75e9-wTXcwjhBSx8B7zoFvWUKdx-hOwJNLM2v65KBA9L5EX2pWs7aHkmQN--_z4i_JzeZFAdP0Np?key=DmbAShmqigs8qkeGzGylXA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1280\" height=\"720\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA basic representation of the human-in-the-loop workflow. Image from \u003c/span\u003e\u003ca href=\"https://www.linkedin.com/pulse/responsible-ai-use-human-in-the-loop-hitl-anderson-anthony/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eLinkedIn\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn the context of AI and machine learning (ML) models, human-in-the-loop is an autonomous system that allows humans to give direct feedback to the model, refining it further. Human-in-the-loop is a system of continuous processes where various “humans,” such as data annotators and evaluators, data scientists, QAs, and engineers, interact with the model differently to guide it. In this context, a human refers to any person participating in the algorithmic decision-making and steering the model toward human-like performance.\u003c/p\u003e\u003cp\u003eUnlike monolithic model training approaches, where AI systems are built, tested, and rolled out without further modifications, human-in-the-loop ensures we keep a close eye on the model and guide it appropriately. In doing so, we beat the risk of obsolescence, scaling constraints, and model degradation, which are common challenges in fully automated model training workflows. Introducing human agents rather than end-to-end automation of the machine learning process guarantees accurate, fast, and human-like AI systems.\u003c/p\u003e\u003cp\u003eHuman-in-the-loop combines human interventions with machine learning algorithms, the goal being to achieve what these two cannot achieve by themselves. In most cases, human-in-the-loop is implemented using either supervised or unsupervised learning methodologies.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn supervised learning, human agents prepare input objects and desired output values to train the model.\u0026nbsp;\u003c/li\u003e\u003cli\u003eConversely, the unlabelled dataset is fed into the algorithm in unsupervised learning, leaving the model to learn independently.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHuman-in-the-loop also blends different machine learning approaches, such as \u003ca href=\"https://labelbox.com/guides/the-guide-to-getting-started-with-active-learning/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003eactive learning\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e,\u003cstrong\u003e interactive learning\u003c/strong\u003e, and\u003cstrong\u003e machine teaching\u003c/strong\u003e. During training, humans and machines take turns controlling the learning process. Depending on who is in control at a particular time, the machine learning approaches mentioned above come into play differently in the light of HITL.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"active-learning\"\u003e\u003cstrong\u003eActive learning\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIn active learning, the system is in control. It interactively queries humans to label the data points used in training. Therefore, humans are merely introduced into the training loops as annotators of the unlabeled data, and their interaction with the model is limited to that.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"interactive-machine-learning\"\u003e\u003cstrong\u003eInteractive machine learning\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIn this approach, the role of human-in-the-loop transcends just data labeling. As its name suggests, interactive machine learning involves a closer interaction between the learning system and the human agents (trainers, annotators, and evaluators). These human agents interact with the models in natural language that the model can observe and imitate. In this case, the human agents supply the model with information that supports learning in a more focused, frequent, and incremental manner.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"machine-teaching\"\u003e\u003cstrong\u003eMachine teaching\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eMachine teaching is the opposite of active learning. In this context, the human agent, usually a domain expert, controls the learning process. Through transfer learning, this knowledge expert guides the model in a contextually specific manner. This way, the model learns better and faster than on labeled training data.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe underlying working principle of human-in-the-loop is that humans should intervene when a machine can't solve a problem, telling them what to do and how to do it. Also, automatable training episodes should be left for the machine but under human supervision to ensure faster and more accurate learning. Combining ML algorithms with human intervention creates a continuous feedback loop that iteratively helps the AI models learn better and faster each time.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"what-are-the-benefits-of-human-in-the-loop\"\u003e\u003cstrong\u003eWhat are the benefits of human-in-the-loop?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eMost \u003ca href=\"https://labelbox.com/foundation-models/what-are-foundation-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efoundational models\u003c/u\u003e\u003c/a\u003e trained on large-scale data lack context-specificity, accuracy, and human preferences. These shortcomings are easily remedied by adding human-in-the-loop to the process. Other human-in-the-loop benefits in training performant AI models include:\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"improving-dataset-quality-and-model-accuracy\"\u003e\u003cstrong\u003eImproving dataset quality and model accuracy\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIntroducing human agents in various model training instances, especially in dataset preparation, creates a ripple effect on the quality of the model attained at the end of the ML pipeline, as each instance of human intervention is considered additional training.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHuman-in-the-loop helps generate accurate custom datasets that can be used to improve the quality of the model's output incrementally. For instance, when building a sentimental analysis AI solution, the algorithm might not understand non-direct language elements like context, cultural lingo, and multilingual texts. Humans, therefore, step in to guide the model in learning some of these elements that cannot be captured in the dataset. In helping improve the utility and accuracy of the datasets, human-in-the-loop guarantees accurate models that align with human values and preferences.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"reducing-errors-and-biases\"\u003e\u003cstrong\u003eReducing errors and biases\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eErrors and biases will always happen in the model training process, whether in the training data or the model’s output. Thus, the goal is to reduce the frequency of such errors and biases. Human agents can always spot the mistakes, skewness, and blank spots in the training data that the algorithm cannot uncover. When the model makes assumptions and generates misleading outputs, in the case of \u003ca href=\"https://labelbox.com/blog/what-does-it-mean-when-an-llm-hallucinates/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAI hallucination\u003c/u\u003e\u003c/a\u003e, human agents are always on standby to troubleshoot and rectify such errors.\u003c/p\u003e\u003cp\u003eDuring training, the data preprocessing phase focuses more on data qualities like size and relevance, ignoring possible biases. Besides, historical data used in model training will likely contain hidden biases that the algorithm might ignore. Adding humans into the loop helps identify such biases as soon as they manifest and eliminate them as early as possible to avoid misleading the model.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"challenges-when-implementing-human-in-the-loop\"\u003e\u003cstrong\u003eChallenges when implementing human-in-the-loop\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWhile there are many benefits in introducing humans into the model training process, there are also challenges that come with it. For example, scalability and cost emerge as the two most profound challenges confronting humans-in-the-loop systems.\u003c/p\u003e\u003ch2 id=\"scalability\"\u003e\u003cstrong\u003eScalability\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCombining humans with automatable systems usually slows down the model training process. A notable setback of most human-in-the-loop systems is that they are often not scalable. Human resources constraints and performance limitations are some of the factors reducing human-in-the-loop systems' scalability. These systems require more and more human resources as the task complexity and the training data volume increase. The need for human agents like annotators, trainers, and QA often becomes a bottleneck to scalability efforts.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAlso, as the number of humans involved in the model training increases, coordinating their efforts efficiently becomes challenging. Having many people interact differently with the model during training increases the chances of errors and generally slows the training process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, there are ways to eliminate the scalability challenges in human-in-the-loop systems. One way to do so is by enhancing machine learning algorithms used in model training. A common technique involves using an interpretable machine learning algorithm that provides a high-level training data summary.\u0026nbsp;\u003c/p\u003e\u003cp\u003eSuch algorithms handle more tasks independently, reducing the frequency of human intervention. Routine tasks performed by human agents can be automated to overcome scalability issues. An example of such a solution is \u003ca href=\"https://labelbox.com/guides/reinforcement-learning-from-ai-feedback-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ereinforcement learning from AI feedback (RLAIF)\u003c/u\u003e\u003c/a\u003e, which involves training LLMs using rewards provided by a preference model as guided by an AI feedback agent.\u003c/p\u003e\u003ch2 id=\"cost\"\u003e\u003cstrong\u003eCost\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAdditional cost is incurred whenever there is a need to increase resource input in the training process. Cost implications as challenges of human-in-the-loop systems often impede the training process. The more humans are in the training loop, the more we have to spend on wages and costs accrued from longer training.\u003c/p\u003e\u003cp\u003eAny workaround eliminating the frequency of human agents in the training lifecycle saves on cost. Algorithm enhancement and automation of routine tasks already mentioned also come in handy in eliminating the cost-related challenges of human-in-the-loop.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"final-thoughts-on-human-in-the-loop\"\u003e\u003cstrong\u003eFinal thoughts on human-in-the-loop\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eHuman-in-the-loop emerges as the additive of human touch and feel to AI. Most AI solutions we appreciate today were trained and tested by humans at some point. As the panic of 'AI is taking over' grows, it is important to note that human expertise remains indispensable in AI. Despite increased automation, data annotation and feedback modeling still require human-in-the-loop systems, making it an indispensable element in the AI model training lifecycle.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox facilitates human-in-the-loop workflows for machine learning by providing tools for data annotation, quality control, active learning, iterative improvement, and integration with automation. \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eTry it for free today.\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e","comment_id":"66997612e017de000190bd91","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/human-in-the-loop.png","featured":false,"visibility":"public","created_at":"2024-07-18T20:07:46.000+00:00","updated_at":"2024-07-18T20:11:05.000+00:00","published_at":"2024-02-12T20:08:00.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/human-in-the-loop/","authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"tags":[],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/human-in-the-loop/","excerpt":"Every time a new AI tool is rolled out, the topic ultimately shifts to: Will AI replace humans? A satisfying answer to this question is given by the title of a report by Harvard Business Review: “AI Won’t Replace Humans — But Humans With AI Will Replace Humans Without AI”. It summarizes how AI is nothing without the intervention of humans. \n\nIn one way or another, humans are involved in developing AI models, integrating natural human intelligence at various points of the machine-learning loop, r","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"What is Human-in-the-Loop? | Quantumworks Lab","meta_description":"Human-in-the-loop integrates human feedback into AI training, enhancing accuracy and mitigating biases, despite scalability and cost challenges.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66985965e017de000190bd7e","uuid":"2a198140-129f-4f95-9f4a-ae4ce2a34cf4","title":"What is multimodal data labeling?","slug":"multimodal-data-labeling","html":"\u003cp\u003eThe rise of models like \u003ca href=\"https://openai.com/index/hello-gpt-4o/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGPT-4o\u003c/u\u003e\u003c/a\u003e by OpenAI and \u003ca href=\"https://blog.google/products/gemini/google-gemini-update-may-2024/?ref=labelbox-guides.ghost.io#context-window\"\u003e\u003cu\u003eGemini\u003c/u\u003e\u003c/a\u003e by Google have made multimodal models more and more commonplace, and subsequently made multimodal data labeling fundamental to the AI development process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eMultimodal models capture and process different data modalities, becoming the bridge to AI that understands and interacts with the real world. A critical question that developers and consumers of multimodal models should be asking themselves (and aren’t) is: “How do we ensure these multimodal models are aligned with human preferences while still being performant?” The answer: multimodal data labeling.\u003c/p\u003e\u003cp\u003eMultimodal models leverage different data modalities and use them to make predictions. In this article we will dive deeper into the process of multimodal data labeling, the technologies used, and the contribution of multimodal data labeling to AI advancements.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"understanding-data-modalities\"\u003e\u003cstrong\u003eUnderstanding data modalities\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eUnderstanding the concept of data modalities is a prerequisite for understanding multimodal data labeling. Modality refers to the various data types that a system can combine and process for particular tasks. Some of the common data modalities are:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImage:\u003c/strong\u003e This modality contains visual data in the form of photographs, sketches, paintings, and drawings.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eText:\u003c/strong\u003e Text modality includes documents written in natural language.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVideo:\u003c/strong\u003e This modality combines visual and auditory data, mostly moving images accompanied by sounds.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAudio:\u003c/strong\u003e Audio modality comprises sound data in the form of spoken words or music.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSensor Data:\u003c/strong\u003e This modality consists of data collected from sensors like GPs or environment sensors.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo achieve multimodal models, we simply combine a subset or all of the listed modalities above during training.\u0026nbsp;\u003c/p\u003e\u003cp\u003eUnlike traditional foundation models focused on singular modalities, multimodal models integrate different modalities to ensure we capture all the perspectives of the problem at hand. For instance, if we were to develop a patient diagnosis system, a multimodal model would be the best fit than a unimodal one focusing on only one data source, like text data from the patient's record. We can capture and use text data from the records, image data from X-rays, audio data from stethoscopes, and sensor data from wearables like smartwatches. This model would have a more holistic view of the patient's health than if we trained only on a particular modality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eSo how can we efficiently combine data from diverse sources in a way that can be used to train a single model? The answer is proper multimodal data labeling that supports cross-modal relationship building.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"tools-and-technologies-for-multimodal-data-labeling\"\u003e\u003cstrong\u003eTools and technologies for multimodal data labeling\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eMultimodal data labeling is a \u003ca href=\"https://labelbox.com/ai-glossary/supervised-learning/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esupervised learning\u003c/u\u003e\u003c/a\u003e technique in which human labelers prepare the dataset by assigning labels that guide the model during training. This process was initially done manually, meaning human labelers would assign labels to the data by hand. However manual approaches are time-consuming and resource-intensive. For example, it would take many annotators several days to annotate enough data to train a mid-sized model of 5 billion parameters.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe whole point of AI is automation and problem-solving, so researchers found a workaround for manual data labeling. Automated tools using machine learning algorithms and can guarantee faster and more accurate annotation under human supervision were introduced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAn example of such a tool is the Quantumworks Lab's\u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e Label Blocks\u003c/u\u003e\u003c/a\u003e. This all-in-one labeling studio is designed to handle different data types, providing a unified annotation platform. Label Blocks supports labeling text, image, audio, geospatial, video, and sensor data. The platform supports multimodal labeling with project management, segmentation, quality control, and collaboration functionalities.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"how-to-label-multimodal-data\"\u003e\u003cstrong\u003eHow to label multimodal data\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTraining multimodal models starts with identifying the project's scope and modalities involved, then collecting data. Once we have consolidated diverse datasets from all modalities, we move straight to labeling. The labeling process is quite extensive for multimodal models as labeling techniques differ for each modality.\u003c/p\u003e\u003cp\u003eFor text, we label data with tags, while for images, we annotate with bounding boxes and segmentation masks. On the other hand, sensor data is labeled through temporal alignment and labeling specific event instances. Conversely, audio data might first be transcribed and then labeled by classification.\u003c/p\u003e\u003cp\u003eMultimodal labeling requires an integrated platform like Label Blocks to handle different modalities. This tool offers editor interfaces that accept different attachment formats. The Label Blocks editor has global settings and enhancements, such as attachments, data row information panels, and instructions.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe labeling process is quite simplified while using third-party tools like \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabel Blocks\u003c/u\u003e\u003c/a\u003e. After importing the multimodal datasets, we simply tweak the data row information panel and supplementary features to align with the labeling goals. The editor offers the superior functionality of attaching a labeling instruction document for various modalities. Alongside this instruction document, the labeler can attach additional context and information such as metadata, curated tags, and media attributes to expedite the labeling process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eSimply put, labeling multimodal data entails understanding the data format and its elements. Platforms like \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabel Blocks\u003c/u\u003e\u003c/a\u003e provide highly customizable editors, making multimodal data labeling less hassle.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"real-world-applications-of-multimodal-data-labeling\"\u003e\u003cstrong\u003eReal world applications of multimodal data labeling\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eMultimodal models are the new normal in AI development, and so is the significance of multimodal data labeling. Developing these models requires us to have a large-scale annotated multimodal dataset. Various application areas need multimodal data labeling. Besides OpenAI’s \u003ca href=\"https://openai.com/index/hello-gpt-4o/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGPT-4o model\u003c/u\u003e\u003c/a\u003e with natural language processing, video generation, and voice assistance capabilities, other industries also need multimodal data labeling.\u003c/p\u003e\u003ch2 id=\"autonomous-automobiles\"\u003e\u003cstrong\u003eAutonomous automobiles\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIn the manufacturing of autonomous self-driving automobiles, multimodal data labeling is critical. Since these systems leverage machine learning techniques, multimodal data from LIDAR, GPS, camera, and radar are consolidated to train the model. These data from diverse modalities must be accurately labeled to guarantee a navigation system that can handle object detection, decision-making in complex environments, and path planning.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"augmented-reality-and-virtual-reality-vrar\"\u003e\u003cstrong\u003eAugmented reality and virtual reality (VR/AR)\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eMultimodal data labeling is also applied in developing augmented reality (AR) and virtual reality (VR) systems. Annotating and labeling visual data, haptic feedback, and visual data makes it easier to develop models that capture various modalities. Such multimodal models result in AR/VR systems that give users immersive and interactive experiences.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"challenges-of-multimodal-data-labeling\"\u003e\u003cstrong\u003eChallenges of multimodal data labeling\u003c/strong\u003e\u0026nbsp;\u003c/h1\u003e\u003cp\u003eAlthough multimodal data labeling leads to the advancement of AI, its implementation in model development is not without challenges. Some examples encountered during multimodal data annotation include:\u003c/p\u003e\u003ch2 id=\"data-synchronization-and-alignment\"\u003e\u003cstrong\u003eData synchronization and alignment\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eMultimodal data labeling is sometimes difficult as aligning all the modalities during labeling to achieve a visible relationship during model training is daunting. This challenge stems from the consolidated modalities operating at different resolutions and time scales.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo address this challenge, it is crucial to align datasets from various sources in a uniform format that the models can take in during training.\u003c/p\u003e\u003ch2 id=\"scalability\"\u003e\u003cstrong\u003eScalability\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eGiven the enormous volume of multimodal data consolidated during labeling, scalability emerges as a significant challenge. Efficient multimodal data labeling and processing require optimized workflows and powerful computational resources. Most organizations may not have the technical and resource capacity to run such systems and annotate this data in-house.\u0026nbsp;\u003c/p\u003e\u003cp\u003eA solution to the scalability issue is using automated and semi-automated labeling platforms like \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabel Blocks\u003c/u\u003e\u003c/a\u003e. Such tools bridge the resource gap and manual effort required in labeling multimodal data for model development.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"a-recap-of-multimodal-data-labeling\"\u003e\u003cstrong\u003eA recap of multimodal data labeling\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eMultimodal data labeling is propelling the development of next-generation multimodal models. It promises intelligent and adaptable systems that can make predictions based on data from language, vision, and sensory modalities. This process advances AI development, and as the field evolves, it will only expand. With extensive research around emerging trends like deep learning for automatic feature extraction during labeling, the process will get even better. As a result, we are set to witness groundbreaking AI solutions built from this technique.\u003c/p\u003e\u003cp\u003eLabelbox supports multimodal data labeling by offering a unified platform for images, videos, text, etc. It provides customizable workflows, diverse annotation tools, collaboration features, and seamless integration with AI pipelines, ensuring efficient and accurate annotation across various data types. Experience the benefits firsthand by\u003ca href=\"https://app.labelbox.com/signup?_r=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_content=model_product_page\u0026utm_campaign=modelfoundry\u0026utm_source=linkedin\u0026utm_medium=organic_social\u0026\u0026landingPageAnonymousId=%22583ce60e-4c83-4821-ada7-bfdc420a7a2b%22\u0026referrer_url=https://www.google.com/\"\u003e \u003cu\u003etrying Quantumworks Lab for free\u003c/u\u003e\u003c/a\u003e.\u0026nbsp; \u003c/p\u003e","comment_id":"66985965e017de000190bd7e","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/_multimodal-data-labeling.png","featured":false,"visibility":"public","created_at":"2024-07-17T23:53:09.000+00:00","updated_at":"2024-07-17T23:56:47.000+00:00","published_at":"2024-02-05T23:55:00.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/multimodal-data-labeling/","authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"tags":[],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/multimodal-data-labeling/","excerpt":"The rise of models like GPT-4o by OpenAI and Gemini by Google have made multimodal models more and more commonplace, and subsequently made multimodal data labeling fundamental to the AI development process. \n\nMultimodal models capture and process different data modalities, becoming the bridge to AI that understands and interacts with the real world. A critical question that developers and consumers of multimodal models should be asking themselves (and aren’t) is: “How do we ensure these multimod","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"What is multimodal data labeling? | Quantumworks Lab","meta_description":"Learn about the process of multimodal data labeling, the technologies used, and the contribution of multimodal data labeling to AI advancements. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65bd22d782e5680001e07877","uuid":"3e5a16b6-f3f5-494e-bb8b-4162abb3e830","title":"End-to-end workflow with model distillation for computer vision","slug":"end-to-end-workflow-with-model-distillation-for-computer-vision","html":"\u003cp\u003eModel distillation, also known as \u003cstrong\u003eknowledge distillation\u003c/strong\u003e, is a technique that focuses on creating efficient models by transferring knowledge from large, complex models to smaller, deployable ones.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel distillation is one of many techniques that have grown in popularity and importance in enabling small teams to leverage foundation models in developing small (but mighty) custom models, used in intelligent applications.\u003c/p\u003e\u003cp\u003eIn \u003ca href=\"https://labelbox.com/blog/a-pragmatic-introduction-to-model-distillation-for-ai-developers/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e“A Pragmatic Introduction to Model Distillation for AI Developers\u003c/u\u003e\u003c/a\u003e”, we illustrated some of the conceptual foundations for how model distillation works.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe described how distillation can be leveraged in any domain or data modality requiring efficiency and model optimization, whether the use case is \u003cu\u003ecomputer vision or NLP related\u003c/u\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this tutorial we’ll demonstrate an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003ecomputer vision\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a YOLO model with labels created in Model Foundry using Amazon Rekognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll show how easy it is to go from raw data to cutting-edge models, customized to your use case, using a \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efashion products dataset\u003c/u\u003e\u003c/a\u003e (additional public datasets can be found \u003ca href=\"https://labelbox.com/datasets/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e or on sites like Kaggle).\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn less than 30 min you’ll learn how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIngest\u003c/strong\u003e, \u003cstrong\u003eexplore\u003c/strong\u003e and \u003cstrong\u003eprepare\u003c/strong\u003e the products fashion dataset;\u003c/li\u003e\u003cli\u003ePick and configure\u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e any image based foundation model\u003c/u\u003e\u003c/a\u003e to \u003cstrong\u003eautomatically label\u003c/strong\u003e data with just a few clicks;\u003c/li\u003e\u003cli\u003eExport the labeled predictions to a project as a potential set-up for \u003cstrong\u003emanual evaluation\u003c/strong\u003e;\u003c/li\u003e\u003cli\u003eUse the labeled predictions dataset to \u003cstrong\u003efine-tune a student model\u003c/strong\u003e using a cloud-based notebook provider;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEvaluate performance\u003c/strong\u003e of the fine-tuned student model.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAt the end of the tutorial we’ll also discuss advanced considerations in scaling your models up and out, such as automating data ingestion and labeling, and resources for incorporating \u003ca href=\"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e\u0026nbsp; into your workflows.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-use-model-distillation-to-fine-tune-a-small-task-specific-computer-vision-model\"\u003eSee it in action: How to use model distillation to fine-tune a small, task-specific Computer Vision Model\u003c/h3\u003e\u003cp\u003e\u003cem\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial. You’ll also need to \u003c/em\u003e\u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003ecreate API keys for accessing the SDK\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNotebook\u003c/strong\u003e: \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cu\u003eCV YOLO Model Distillation\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003ch3 id=\"the-model-distillation-workflow-for-computer-vision\"\u003eThe Model Distillation Workflow for Computer Vision\u003c/h3\u003e\u003cp\u003eIn our prior post on model distillation concepts, we discussed the different model distillation patterns, based on the following criteria:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe type of \u003cstrong\u003eTeacher\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eStudent\u003c/strong\u003e model architecture used;\u003c/li\u003e\u003cli\u003eThe type of \u003cstrong\u003eknowledge\u003c/strong\u003e being transferred from teacher to student model(s) (\u003cstrong\u003eresponse-based\u003c/strong\u003e,\u003cstrong\u003e feature-based\u003c/strong\u003e, \u003cstrong\u003erelation-based\u003c/strong\u003e knowledge);\u003c/li\u003e\u003cli\u003eThe cadence and scheme for how the student mode is trained (\u003cstrong\u003eoffline\u003c/strong\u003e, \u003cstrong\u003eonline\u003c/strong\u003e, \u003cstrong\u003eself\u003c/strong\u003e).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this tutorial we’ll be demonstrating the most popular and easiest pattern to get started with: \u003cstrong\u003eoffline, response-based model\u003c/strong\u003e (or \u003cstrong\u003eknowledge\u003c/strong\u003e) \u003cstrong\u003edistillation\u003c/strong\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe teacher model we’ll be using to produce the responses is Amazon Rekognition and the student model is \u003ca href=\"https://labelbox.com/product/model/foundry-models/yolov8-object-detection/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eYOLOv8 Object Detection\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs you’ll see, we could have chosen \u003ca href=\"https://labelbox.com/blog/6-cutting-edge-foundation-models-for-computer-vision-and-how-to-use-them/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eany combination of teacher or student models\u003c/u\u003e\u003c/a\u003e, because the \u003cstrong\u003eoffline, response-based\u003c/strong\u003e pattern of model distillation is incredibly flexible.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen implementing this process for your own use case, it’s important to understand the relative strengths and weaknesses of each model and match them according to your requirements.\u003c/p\u003e\u003ch3 id=\"using-the-labelbox-platform-to-automate-model-distillation\"\u003eUsing The Quantumworks Lab Platform To Automate Model Distillation\u0026nbsp;\u003c/h3\u003e\u003cp\u003eLabelbox is the leading data-centric AI platform, providing an end-to-end platform for curating, transforming, annotating, evaluating and orchestrating unstructured data for data science, machine learning, and generative AI.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab platform supports the development of intelligent applications using the model distillation and fine-tuning workflow. \u003c/p\u003e\u003cp\u003eAI developers are enabled to easily:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImport, curate, filter and eventually select the images that will be labeled for use in student model training and fine-tuning with \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/a\u003e. Additionally you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/blog/model-foundry-automating-data-tasks-with-foundation-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAutomate the labeling and annotation of the original image dataset using foundation models\u003c/u\u003e\u003c/a\u003e (\u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eor any pre-trained model, including custom models\u003c/u\u003e\u003c/a\u003e) in \u003ca href=\"https://labelbox.com/guides/guide-to-using-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e. The ability to leverage a variety of open source or third-party models to accelerate pre-labeling can cut labeling costs by up to 90% for teams with existing labeling initiatives (and kickstart new and budget constrained AI developers operating without\u0026nbsp; manual labeling capabilities).\u003c/li\u003e\u003cli\u003eLeverage \u003ca href=\"https://labelbox.com/customers/intuitive-surgical-customer-story/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman-in-the-loop\u003c/u\u003e\u003c/a\u003e evaluation (and \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRLHF\u003c/u\u003e\u003c/a\u003e) through the native Foundry to Annotate integration, so labels can be reviewed before being used for training or fine-tuning.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIntegrate with some of the most common cloud providers like \u003ca href=\"https://learn.labelbox.com/accelerate-retail-and-ecommerce-ai-roi/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Cloud Platform\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://labelbox.com/company/partnerships/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eDatabricks\u003c/u\u003e\u003c/a\u003e for additional MLOps services.\u003c/li\u003e\u003cli\u003eOrchestrate and schedule future automated labeling and model runs as \u003ca href=\"https://labelbox.com/blog/10x-faster-uploads-with-labelboxs-new-ingestion-upgrades/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enew data flows into the Quantumworks Lab platform\u003c/u\u003e\u003c/a\u003e through Model Apps (including \u003ca href=\"https://labelbox.com/blog/multimodal-data-labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emultimodal data labeling\u003c/u\u003e\u003c/a\u003e).\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"introduction-to-data-preparation-for-computer-vision-with-catalog\"\u003eIntroduction To Data Preparation for Computer Vision With Catalog\u003c/h2\u003e\u003cp\u003eBefore beginning the tutorial:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eCreate a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e (in order to \u003ca href=\"https://docs.labelbox.com/reference/create-api-key?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecreate a free API key\u003c/u\u003e\u003c/a\u003e)\u003c/li\u003e\u003cli\u003eCheck that you can access an \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eexisting fashion products dataset\u003c/u\u003e\u003c/a\u003e in Catalog\u003cul\u003e\u003cli\u003eIf not, check out sites like Kaggle for similar datasets.\u003c/li\u003e\u003cli\u003eDownload the images and their metadata.\u003c/li\u003e\u003cli\u003eChoose whether to upload data via \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#create-a-dataset-via-the-app-ui\"\u003e\u003cu\u003ethe Quantumworks Lab Web UI\u003c/u\u003e\u003c/a\u003e (preferred method) or the SDK.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’re able to see your dataset in Quantumworks Lab Catalog, you’ll be able to do the following:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e for more granular data curation.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor additional details on how to use Catalog to enable data selection for downstream data-centric workflows (such as data labeling, model training, model evaluation, error analysis, and active learning), check out \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour documentation\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"using-a-large-computer-vision-model-to-generate-and-distill-predictions\"\u003eUsing A Large Computer Vision Model To Generate And Distill Predictions\u003c/h2\u003e\u003cp\u003eThe first step of model distillation is to \u003cstrong\u003eidentify an appropriate teacher model\u003c/strong\u003e, which will be used to produce responses that, when combined with the original images, will serve as the \u003cstrong\u003efine-tuning dataset for the student model\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResponse-based\u003c/strong\u003e model distillation is powerful because it can be used even when access to the original model weights is limited (or the model is so big that downloading a copy of the model would take a really long time). Response-based distillation also doesn’t require the user to have trained the model themselves; just that the model was pre-trained.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox allows you to pick any of the \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecurrently hosted, state-of-the-art models to use\u003c/u\u003e\u003c/a\u003e (as well as upload your own custom models) to use as the teacher model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor now, let’s get started with preparing the images we’ll be labeling, or generating predictions with, using Amazon Rekognition. The combination of image and label pairs will be used for YOLO.\u003c/p\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select images and choose a foundation model of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to the \u003ca href=\"https://labelbox.com/datasets/fashion-product-images-dataset/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efashion products dataset\u003c/u\u003e\u003c/a\u003e in Catalog.\u003c/li\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the images on which the predictions should be made.\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “\u003cstrong\u003ePredict with Model Foundry\u003c/strong\u003e”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run (in this case Rekognition).\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task - such as \u003ca href=\"https://labelbox.com/usecases/computer-vision/image-classification/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eimage classification\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/usecases/computer-vision/object-detection/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eobject detection\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/guides/how-to-build-generative-captioning-using-foundation-models-for-product-listings/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage captioning\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-settings-and-submit-a-model-run\"\u003eStep 2: Configure model settings and submit a model run\u003c/h3\u003e\u003cp\u003eWhen developing ML based applications, developers need to quickly and iteratively prepare and version training data, launch model experiments, and use the performance metrics to further refine the input data sources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe performance of a model can vary wildly depending on the data used, the quality of the annotations, and even the model architecture itself. A necessary requirement for replicability is being able to see the exact version of all the artifacts used or generated as a result of an experiment.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox will snapshot the experiment, the data artifacts as well as the trained model, as a saved process known as a \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis includes the types of items the model is supposed to identify and label, known as an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eontology\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of settings, which you can find in the Advanced model setting.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce you’ve located Rekognition, you can click on the model to view and set the model and ontology settings or prompt.\u0026nbsp;\u003c/li\u003e\u003cli\u003eIn this case, we set the ontology to detect “Jacket” and we can see a preview of running the model on this ontology above.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo get an idea of how your current model settings affect the final predictions, you can generate preview predictions on up to five data rows. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-review-predictions-in-the-model-tab\"\u003eStep 3: Review predictions in the Model tab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eBecause each \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel run is submitted with a unique name\u003c/u\u003e\u003c/a\u003e, it’s easy to distinguish between each subsequent model run.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhen the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/foundry-view-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eView prediction results\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results \u003ca href=\"https://docs.labelbox.com/docs/comparing-model-runs?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eacross a variety of model runs\u003c/u\u003e\u003c/a\u003e different models\u003c/li\u003e\u003cli\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe can see that for the most part, “jackets” were correctly identified and labeled.\u003c/li\u003e\u003cli\u003eThese generated labels are now ready to be used for fine-tuning the student model YOLO.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-enriching-and-evaluating-predictions-using-human-in-the-loop-and-annotate\"\u003eStep 4: Enriching and evaluating predictions using human-in-the-loop and Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAlthough fine-tuning a foundation model requires less data than pre-training a large foundation model from scratch, the data (specifically the labels) need to be high-quality.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEven big, powerful foundation models make mistakes or miss edge cases.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou might also find that there are additional objects that the parent model didn’t identify because the items in the image weren’t initially identified as being important in the ontology (for example, “boots” or “ski hats”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce a parent model like Rekognition has been used for the initial model-assisted labeling run, those predictions \u003ca href=\"https://docs.labelbox.com/docs/foundry-annotate-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan then be sent\u003c/u\u003e\u003c/a\u003e to a Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eproject\u003c/u\u003e\u003c/a\u003e, a container \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewhere all your labeling processes happen\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/la022y1i78\" title=\"End-to-End Workflow for Model Distillation with Computer Vision - Send to annotate Jacket YOLO Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"484\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn this case, we feel fairly confident\u003cstrong\u003e \u003c/strong\u003ein how well Rekognition performed so we’ll send the inferences to the corresponding Quantumworks Lab project and treat them as the ground truth that the student model will be fine-tuned on. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"fine-tuning-the-student-model-yolo\"\u003eFine-Tuning The Student Model (YOLO)\u0026nbsp;\u003c/h2\u003e\u003cp\u003eWe’ve shown the first half of the model distillation to fine-tuning workflow.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe identified the items we wanted the parent model (Rekognition) to detect and label in the form of an ontology.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe used Rekognition to automatically label items like “jackets”.\u003c/li\u003e\u003cli\u003eWe exported the generated labels to a Quantumworks Lab project, at which point we could review the labels manually and enrich them further using the Quantumworks Lab editor.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe next step is to use the generated labels, along with the original image, to fine-tune a student model in Colab.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: You’ll now need the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=PgdwI9SR5HHd\"\u003e\u003cem\u003e\u003cu\u003eAPI keys from earlier\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to follow along with the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cem\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003ch3 id=\"step-5-fetch-the-ground-truth-labels-from-the-project-via-labelbox-sdk-and-convert-the-images-into-the-relevant-format\"\u003eStep 5: Fetch the ground truth labels from the project via Quantumworks Lab SDK and convert the images into the relevant format.\u003c/h3\u003e\u003cp\u003eFor brevity, we’ve omitted the relevant code samples but you can copy or run the corresponding blocks in the provided notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out our documentation to find out all the ways you can automate the model lifecycle (\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eincluding labeling\u003c/u\u003e\u003c/a\u003e) using our SDK.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"977\" height=\"542\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.09.13-PM.png 977w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore fine-tuning the YOLO student model, we’ll \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=7hs-oTEXOQKx\"\u003e\u003cu\u003eneed to fetch the generated labels\u003c/u\u003e\u003c/a\u003e from the Quantumworks Lab project using the Quantumworks Lab SDK as well as the \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=jKMysiHvUI1D\"\u003e\u003cu\u003eground truth\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003cli\u003eWe’ll also need to \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=cqmNeTFwTZR0\"\u003e\u003cu\u003econvert the images and ensure they’re in the right format\u003c/u\u003e\u003c/a\u003e for fine-tuning, specifically the COCO format.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-6-fine-tune-student-yolo-model-using-labels-generated-by-amazon-rekognition\"\u003eStep 6: Fine-tune student YOLO model using labels generated by Amazon Rekognition\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"608\" height=\"109\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.10.41-PM.png 608w\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn Colab we’ll \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=Becr0BZQO_Ze\u0026line=1\u0026uniqifier=1\" rel=\"noreferrer\"\u003einstantiate a YOLO model and train it using the data\u003c/a\u003e, which includes both images and labels.\u003c/li\u003e\u003cli\u003eWe’ll then \u003ca href=\"https://colab.research.google.com/drive/1Fsn5inTMCOx-PN45yciuxQX3qe1uUvbN?ref=labelbox-guides.ghost.io#scrollTo=_quHpcfihY7s\" rel=\"noreferrer\"\u003erun the fine-tuned student YOLO model\u003c/a\u003e on the images to generate the predictions for analysis.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSee the example notebook for omitted code.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"923\" height=\"307\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.14.28-PM.png 923w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-7-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 7: Create a model run with predictions and ground truth\u0026nbsp;\u003c/h3\u003e\u003cp\u003eOftentimes the initial training or fine-tuning step isn’t the final stop on the journey of developing a model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOne of the biggest differences between the traditional method of training models in the classroom versus the real-world is how much control you have over the quality of your data, and consequently the quality of the model produced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we mentioned earlier, developers can upload predictions and use the Model product to \u003ca href=\"https://labelbox.com/blog/how-to-confidently-compare-test-and-evaluate-models-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ediagnose performance issues with models and compare them across multiple experiments\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDoing so automatically populates model metrics that make it easy to evaluate the model’s performance.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"857\" height=\"266\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-12.15.46-PM.png 857w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eHere we show how you \u003ca href=\"https://docs.labelbox.com/docs/import-ground-truth?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecan upload the ground truth labels\u003c/u\u003e\u003c/a\u003e (the labels generated by Rekognition and used as the fine-tuning dataset) and the \u003ca href=\"https://docs.labelbox.com/docs/upload-model-predictions?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003epredictions\u003c/u\u003e\u003c/a\u003e from the fine-tuned YOLO model to the corresponding Quantumworks Lab model. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"evaluating-model-performance\"\u003eEvaluating Model Performance\u003c/h2\u003e\u003cp\u003eThere’s no single metric to rule them all when evaluating your \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecomputer vision model performance\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith that being said, Model offers a number of the most common out-of-the-box. With the \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io#metrics-view\"\u003e\u003cu\u003e‘Metrics view’\u003c/u\u003e\u003c/a\u003e users can drill into crucial model metrics, such as \u003ca href=\"https://docs.labelbox.com/docs/upload-model-metrics-1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econfusion matrix, precision, recall, F1 score, false positive, and more\u003c/u\u003e\u003c/a\u003e, to surface model errors.\u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive, which means you can click on any chart or metric to immediately open up the gallery view of the model run and see corresponding examples, as well as visually compare model predictions between multiple model runs.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-7-evaluate-predictions-from-different-yolo-model-runs-in-labelbox-model\"\u003eStep 7: Evaluate predictions from different YOLO model runs in Quantumworks Lab Model\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eSteps:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNavigate to “Model”\u003c/li\u003e\u003cli\u003eBy fine-tuning the Yolo v8 model with approximately 1000 images annotated using Amazon Rekognition, we can achieve performance similar to the Rekognition model within roughly one hour.\u003c/li\u003e\u003cli\u003eWe can now manually inspect examples of predictions from the fine-tuned YOLO model\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"advanced-considerations\"\u003eAdvanced Considerations\u003c/h2\u003e\u003cp\u003eIn this step-by-step walkthrough, we’ve shown how anyone with any image-based dataset can leverage an image-based foundation model to label, fine-tune and analyze a smaller but mighty custom model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAdditional considerations users should address for scaling similar projects include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAutomating future data ingestion, curation, enrichment, and labeling when the fine-tuned model needs to be retrained due to drift.\u003c/li\u003e\u003cli\u003eIncorporating \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehuman evaluators and human-in-the-loop\u003c/u\u003e\u003c/a\u003e, as well as error analysis for identifying and addressing edge cases.\u0026nbsp;\u003c/li\u003e\u003cli\u003eEasy-to-use, user interface that can be customized for various modalities of data when multiple users are involved.\u0026nbsp;\u003c/li\u003e\u003cli\u003eA \u003ca href=\"https://docs.labelbox.com/reference/install-python-sdk?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erobust SDK\u003c/u\u003e\u003c/a\u003e for integrating with any MLOps solutions provider, especially when incorporating model monitoring and complex deployment patterns.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEven with fine-tuned models, there’s no such thing as “setting and forgetting”.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAll models eventually need to be retrained, with the data refreshed to account for changes.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eIn this tutorial, we demonstrated an end-to-end workflow for \u003ca href=\"https://labelbox.com/usecases/computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003ecomputer vision\u003c/u\u003e\u003c/a\u003e, using model distillation to fine-tune a YOLO model with labels created in Model Foundry using Amazon Rekognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHopefully you were able to see how easy it is to go from raw data to cutting-edge custom models in less than 30 min.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou learned how the Quantumworks Lab platform enables model distillation by allowing developers to:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIngest, explore and prepare image-based datasets using Catalog;\u003c/li\u003e\u003cli\u003eUse any \u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eimage based foundation model\u003c/u\u003e\u003c/a\u003e to automatically label data using Model Foundry as well as how to incorporate human-in-the-loop evaluation using Annotate;\u003c/li\u003e\u003cli\u003eExport these labeled predictions to a cloud-based training environment for fine-tuning;\u003c/li\u003e\u003cli\u003eAutomate the various workflows using the Quantumworks Lab SDK;\u003c/li\u003e\u003cli\u003eEvaluate model performance and analyze model errors using Quantumworks Lab Model.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn the next part of this series we replicate a very similar workflow for NLP, using model distillation to fine-tune a BERT model with labels created in Model Foundry using PaLM2. \u003c/p\u003e","comment_id":"65bd22d782e5680001e07877","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/02/Social-Cards-Example--1-.jpg","featured":false,"visibility":"public","created_at":"2024-02-02T17:13:59.000+00:00","updated_at":"2024-10-02T00:02:14.000+00:00","published_at":"2024-02-01T20:23:00.000+00:00","custom_excerpt":"Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},{"id":"6330dc6516e912003d39b321","name":"Rahul Sharma","slug":"rahul","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/rahul/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"681516a56e02120001d5ab84","name":"#mikiko","slug":"hash-mikiko","description":null,"feature_image":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/404/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"65bd4ff882e5680001e078f9","name":"Model Distillation","slug":"model-distillation","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://labelbox-guides.ghost.io/tag/model-distillation/"}],"primary_author":{"id":"1","name":"Manu Sharma","slug":"manu","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/manu/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/end-to-end-workflow-with-model-distillation-for-computer-vision/","excerpt":"Learn how to perform model distillation and fine-tuning to efficiently leverage foundation models for computer vision, like object detection with Amazon Rekognition and YOLOv8. ","reading_time":11,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"66985287e017de000190bd62","uuid":"f1ed906d-cefe-436a-a545-ef4047f0c14f","title":"How to do knowledge distillation","slug":"knowledge-distillation","html":"\u003cp\u003eKnowledge Distillation, which compresses large, powerful AI models into smaller, faster versions without losing performance, vital for efficient deployment on less powerful devices, has become an important technique for AI development and streamline the process of building intelligent applications.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs advancements in artificial intelligence continue, \u003ca href=\"https://labelbox.com/usecases/large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003elarge language models (LLMs)\u003c/u\u003e\u003c/a\u003e and deep neural networks (DNNs) are becoming increasingly capable. The latest iterations outperform their predecessors, partly due to \u003ca href=\"https://deepchecks.com/question/how-does-the-size-of-the-training-data-affect-the-accuracy/?ref=labelbox-guides.ghost.io#:~:text=A%20machine%20learning%20model's%20accuracy,and%20are%20expensive%20to%20store.\"\u003e\u003cu\u003eexpanded datasets used during training\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese sophisticated models are invaluable across various sectors including marketing, cybersecurity, logistics, and medical diagnostics. However, their deployment is often limited by the significant computing resources they require.\u003c/p\u003e\u003cp\u003eAs these models grow in complexity with increased data and parameters, they also become larger and slower, which complicates deployment on less powerful user devices such as office computers, embedded systems, and mobile devices. Knowledge distillation is the ideal solution to this problem, allowing models to maintain similar accuracy and performance in a much smaller, more deployable format.\u003c/p\u003e\u003cp\u003eIn this article, we will learn how knowledge is distilled from large language models to smaller models that can be deployed in downstream edge devices and user systems.\u003c/p\u003e\u003ch1 id=\"what-is-knowledge-distillation\"\u003e\u003cstrong\u003eWhat is knowledge distillation?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eKnowledge distillation is a technique where a smaller, simpler model—referred to as the \"student\"—learns from a larger, more complex model, known as the \"teacher.\" This process goes beyond just mimicking the final decision outputs (hard targets) of the teacher; it crucially involves the student model learning from the soft output distributions (soft labels) provided by the teacher.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThese soft labels represent the probabilities the teacher model assigns to each class, conveying not just the decision but also the confidence levels across potential outcomes. The goal of this method is to transfer the comprehensive knowledge of the teacher model to a student model that retains much of the teacher’s accuracy and performance but with significantly fewer parameters.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThis allows the student model to be deployed on user devices where computational resources are more limited. Knowledge distillation thus offers a strategic trade-off between the robust training capabilities of large models and the deployment needs of smaller, more efficient ones. The outcome is a compact model that meets the latency, throughput, and performance benchmarks of its larger counterpart but is more suitable for environments with resource constraints.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"what-is-the-knowledge-distillation-process\"\u003e\u003cstrong\u003eWhat is the knowledge distillation process\u003c/strong\u003e?\u003c/h1\u003e\u003cp\u003eThe student-teacher architecture is the basis for knowledge distillation. It ensures that the teacher model can be compressed into a simpler student model deployed on low-grade devices. The student model can learn as much as possible from the teacher model through this architecture, capturing all the knowledge with minimal computing resources. This model compression technique banks on three processes: pre-training the teacher model, knowledge transfer, and refining the student model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcZthO0ui4PEtOFEY3UBxRpbNdJxRicYT-CXmt9FiV1CiDJVWqF7a1Mfogq3_jqxeRt9-KKxL725-BJnw9Ag5xgbQem_rPBX4jZaFZff7SJ5f6xCMYnwuBNG_2-qjTdK3mC1DbB28COgRJSRWm3MAjmDmAt?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"298\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA generic representation of the Knowledge Distillation process. Image from \u003c/span\u003e\u003ca href=\"https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0285901\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003ePlos One\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe \u003cstrong\u003eteacher model pre-training\u003c/strong\u003e phase builds the knowledge to be transferred during the distillation process. In this step of the knowledge distillation process, a complex model is trained on large datasets using standard machine learning procedures.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe pre-training phase is the most expensive and resource-intensive, as the teacher model training requires extensive datasets and computing resources. The teacher model can take the form of existing legacy models like GPT, Llama, or BERT, with billions of parameters and gigabytes of data.\u003c/p\u003e\u003cp\u003eOnce trained, the teacher model generates soft labels (logits) for the training data, which are later used to supervise the student model. In simple terms, these soft labels are the output probability distributions provided by the teacher model for training the student model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eUnlike hard (binary) labels, soft labels provide more information on the prediction probabilities over the classes. The teacher model pre-training must be thorough enough to capture all the nuances of prediction or the input-output correlations to be transferred to the student model.\u003c/p\u003e\u003cp\u003eNext, the \u003cstrong\u003estudent model is trained\u003c/strong\u003e on the teacher model-generated soft labels (output), features, or relations, depending on the chosen type of distillation. \u003cstrong\u003eKnowledge transfer\u003c/strong\u003e happens in this phase of knowledge distillation. The training of the student model aims to distill the knowledge of the teacher model by matching its soft targets to the student model. In doing so, the difference between the student model predictions and the teacher model's soft labels is minimized.\u003c/p\u003e\u003cp\u003eBesides soft labels, the student model is also trained to learn the teacher model's pairwise relation between data points. It is also primed to mimic the feature representation of the teacher model's layers. These processes steer the student model to capture almost all the knowledge of the teacher model and obtain similar or higher accuracy while using minimal computing resources.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo enhance the performance of the student model, it is refined through further training. Once the knowledge has been distilled from the teacher to the student model, the \u003cstrong\u003estudent model may undergo additional training\u003c/strong\u003e with the original dataset and hyperparameter tuning. Refining the student model augments the knowledge distillation process, ensuring we achieve an optimal model.\u003c/p\u003e\u003ch1 id=\"knowledge-categories\"\u003e\u003cstrong\u003eKnowledge categories\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAs discussed in the knowledge distillation process above, the student model can learn from different knowledge categories of the teacher model. This knowledge includes soft labels, intermediate layer features, and the relationship between various layers and data points. Various types of knowledge distillation emerge from these knowledge categories. As a result, we have three known knowledge distillation types:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eResponse-based knowledge\u003c/strong\u003e: The student model learns from the soft outputs of the teacher model.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFeature-based knowledge\u003c/strong\u003e: The student model mimics intermediate feature representations from the teacher.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRelation-based knowledge\u003c/strong\u003e: Focuses on the relationships between different layers and data points in the teacher model.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXecgNwozETH0rMVZLeQ6e-k9K6_Rt2PXv7owTcjrhRYOsF78p9SLKKr92XfkN-nLlP3UmdUgg08P5R-VKO6sT5Ps7b1MjS3pfSBVGFCn6tQ_0UYoPydWdlmT-mt3kblmW458puG1WFm6q2Blgb-Q4-Hrz42?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"685\" height=\"491\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA representation of the different knowledge categories in the knowledge distillation process for a neural network. Image from \u003c/span\u003e\u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-031-32095-8_1?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eSpringer Link\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"response-based-knowledge\"\u003e\u003cstrong\u003eResponse-based knowledge\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eResponse-based distillation systems capture the knowledge from the teacher model's output layer and transfer it to the student model. The student model is trained to directly mimic the teacher model's output probabilities (soft targets). However, not all knowledge will be distilled to the student model as certain underlying factors might result in divergence. In this case, the Kullback-Leibler Divergence is used to compute the divergence metrics between the teacher and student predictions and minimize the loss function.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcYOX2dO4c72xjisaADT2FpoDMYu74qlf924WrxJtZZUx8vFuWaA_UusjEPZYUqRfMZt-OPgrCAn6fNLB6fsl8i2hqVmWehdr6qBMZUBtoIYHl1sx0tjeL3CFPt7WhlJzSKJ4L3UeGx3n-on0LqHxFBMFcr?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"753\" height=\"263\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA representation of the Response-based Distillation process. Image by \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Response-based-knowledge-distillation_fig4_371616469?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eAbdelaziz Abohamama\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"feature-based-knowledge\"\u003e\u003cstrong\u003eFeature-based knowledge\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFeature-based knowledge represents the intermediate levels of feature representations of the teacher model. When distilling knowledge based on the features, the intermediate layers of the teacher model that contain feature activations are transferred to the student model. Instead of relying solely on the teacher model's output, this approach goes a step higher by training the student model to mimic the feature maps of the various layers of the teacher model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXfkuU4ha3M5L-8rMMfS4N8N6I3nq1lUhnKCr8ZiGLl5O0n5LYgutD9jOTup4CqUBiFfBWzXFDuREN4Gv3oWTTQgYQgrxBQqH_Uu97aQZxvnFdTg12lcNKii65GlW5yOHt-O0eBFSwEejHmLpSQOXhqffZQ?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"722\" height=\"320\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFeature-based Knowledge Distillation lifecycle. Image by \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-generic-feature-based-knowledge-distillation_fig9_342094012?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eJianping Gou\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"relation-based-knowledge\"\u003e\u003cstrong\u003eRelation-based knowledge\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eRelation-based knowledge goes beyond the output of the teacher model. It covers the relationships between the various layers from which the output is drawn. Relation-based knowledge also includes the data samples learned by the teacher model. In relation-based distillation, we focus on distilling the relationship between the data sample and different layers of the teacher model into the student model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/07/The-generic-instance-relation-based-knowledge-distillation.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"715\" height=\"351\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/07/The-generic-instance-relation-based-knowledge-distillation.png 600w, https://labelbox-guides.ghost.io/content/images/2024/07/The-generic-instance-relation-based-knowledge-distillation.png 715w\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRelation-based Knowledge Distillation lifecycle. Image by \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-generic-feature-based-knowledge-distillation_fig9_342094012?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eJianping Gou\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"knowledge-distillation-algorithms\"\u003e\u003cstrong\u003eKnowledge distillation algorithms\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eDifferent algorithms are employed to facilitate the transfer of knowledge, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAdversarial learning\u003c/strong\u003e: The student model learns to perform tasks that the teacher model finds challenging, improving robustness.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCross-modal distillation\u003c/strong\u003e: Knowledge is transferred between different modalities, such as from text to images.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMulti-teacher distillation\u003c/strong\u003e: Knowledge from multiple teacher models is distilled into a single student model.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGraph-based distillation\u003c/strong\u003e: Uses graphs to map and transfer intra-data relationships, enriching the student model's learning process.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"adversarial-learning-distillation-algorithm\"\u003e\u003cstrong\u003eAdversarial learning distillation algorithm\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe adversarial learning algorithm primes the student model to mimic the teacher model's output but generates samples that the teacher model cannot classify correctly. It is inspired by \u003ca href=\"https://arxiv.org/abs/1406.2661?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGenerative Adversarial Networks (GANs)\u003c/u\u003e\u003c/a\u003e as it generates synthetic (adversarial) data, which it uses to train the student model alongside the training set. In doing so, the algorithm gives the student model a better understanding of the true data distribution.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXeuGBfS-LsgI8theaNCxu_b_G_sATswJlU4sJDNcLTS42TyLEBN6p-cJSwlwGcidm3kiF1M3v8wruFClMUAkWY-U1c4uMaAWRuD_LBeMMm94SgXtEP-5-qbAS-UxdTt2I0a1pX_XG6qdeApBjy_9dgRvCZB?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"550\" height=\"336\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA generic architecture of the Adversarial Learning Algorithm. Image from \u003c/span\u003e\u003ca href=\"https://www.mdpi.com/1999-4893/15/8/283?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eMDPI\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"cross-modal-distillation-algorithm\"\u003e\u003cstrong\u003eCross-modal distillation algorithm\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eCross-modal algorithms facilitate knowledge transfer between different modalities. Sometimes, data or labels are available in one modality but not another. So, we invoke the cross-modal distillation algorithm to transfer this data from this modality to the missing one during distillation. The algorithm is sequential; the teacher model is trained on the source modality, and then the student model is trained on the target modality. The knowledge transfer is, therefore, achieved across different modalities of the teacher and the student models.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXcU2Of9W8ndKErF3ARaGX8Qe1LmsG3ALNQnetN3zhYuYirw_0_LKE7fhwUJ_2mZlIFENyUj7357w92nQap2yaQPrPkisGpHEs34Ep-iWR-zz6PxP6FwyQC_GGDUTaJbcpraulkBC3Ox25zCvftzG6hT84P-?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"850\" height=\"387\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA representation of Cross-Modal Distillation Algorithm. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-generic-framework-for-cross-modal-distillation-For-simplicity-only-two-modalities_fig4_350293589?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eInternational Journal of Computer Vision\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"multi-teacher-distillation-algorithm\"\u003e\u003cstrong\u003eMulti-teacher distillation algorithm\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe multi-teacher algorithm transfers knowledge from multiple teacher models to a single student model during the distillation process. The best way to achieve this knowledge transfer is by averaging all the teacher models' soft label outputs and then distilling the averaged output into the student model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXdV42MlSL5yT5TtP2dRznLC06R01BwSjvT6OudzK8LLduw7vkMMqI43Y8MglPa6OENJFERPqlaGlpgw5jSSTuig3viAIDDdynCYqz1otboynJyaDtt9wiOwqCvXLpB-5dBlJvWuSAK5Pw4git3a4hYm-qDN?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"850\" height=\"419\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBasic framework for the Multi-teacher Distillation Algorithm. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-generic-framework-for-multi-teacher-distillation_fig3_350293589?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eInternational Journal of Computer Vision\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"graph-based-distillation-algorithm\"\u003e\u003cstrong\u003eGraph-based distillation algorithm\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhile most distillation algorithms focus on transferring individual knowledge instances from the teacher to the student model, graph-based algorithms use graphs to map intra-data relationships instead. These graphs carry the teacher's knowledge and transfer several instances of this knowledge to the student model.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/docsz/AD_4nXfquhhA_EB9hn2JD_QsdyMV1asev_Wyg74xXYzDWbr47SqpDKgcVRqUtA3g9mnJQmL6f8oSDbn7LdkGEoY5ZTBK50-zvubLf7IRZQvhyrS-ZeNq8iWKaTLBCT66-VGOD2yDGrLLmyM2IBPC3UTiDUm-plEu?key=DqY3bu6KydZu1cDgHMknJQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"774\" height=\"327\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA generic framework for Graph-based Distillation Algorithm. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-framework-of-Graph-based-Knowledge-Distillation-for-Graph-Neural-Networks-GKD_fig2_368877191?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eResearchGate\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"a-recap-of-knowledge-distillation\"\u003e\u003cstrong\u003eA recap of knowledge distillation\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eKnowledge distillation is a highly effective technique for compressing large, resource-intensive AI models like LLMs and DNNs, enabling their deployment on edge devices with limited computational resources. This process begins with the comprehensive training of a large-scale \"teacher\" model, followed by the distillation of its essential knowledge into a more compact \"student\" model. The student model can then be efficiently deployed on a variety of downstream devices and computing systems.\u003c/p\u003e\u003cp\u003eKnowledge distillation encompasses various methodologies depending on the type of knowledge transferred, including response-based, feature-based, and relation-based distillation. Each type employs specific algorithms tailored to optimize the knowledge transfer, ensuring that the student model achieves comparable performance to the teacher model but with far fewer parameters.\u003c/p\u003e\u003cp\u003eWhile the computational demands of deploying full-scale LLMs and DNNs are beyond the reach of many devices, knowledge distillation allows us to create smaller, efficient replicas that perform similarly. This technique bridges the gap between the substantial resource requirements of advanced AI models and the performance expectations of user devices. Critical to the success of knowledge distillation is the quality of the teacher model's training, which heavily depends on the precision of data annotations. \u003c/p\u003e\u003cp\u003eLabelbox addresses this need by providing powerful, customizable tools for creating high-quality annotated datasets, thereby enhancing both the efficiency and effectiveness of the distillation process. By improving data annotation, Quantumworks Lab not only boosts the learning efficiency of the student model but also streamlines the overall approach to model training. Experience the benefits firsthand by \u003ca href=\"https://app.labelbox.com/signup?_r=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_content=model_product_page\u0026utm_campaign=modelfoundry\u0026utm_source=linkedin\u0026utm_medium=organic_social\u0026\u0026landingPageAnonymousId=%22583ce60e-4c83-4821-ada7-bfdc420a7a2b%22\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003etrying Quantumworks Lab for free\u003c/u\u003e\u003c/a\u003e and enhance your model training initiatives.\u003c/p\u003e","comment_id":"66985287e017de000190bd62","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/07/knowledge-distillation.png","featured":false,"visibility":"public","created_at":"2024-07-17T23:23:51.000+00:00","updated_at":"2024-07-17T23:56:29.000+00:00","published_at":"2024-01-29T23:27:00.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/knowledge-distillation/","authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"tags":[],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/knowledge-distillation/","excerpt":"Knowledge Distillation, which compresses large, powerful AI models into smaller, faster versions without losing performance, vital for efficient deployment on less powerful devices, has become an important technique for AI development and streamline the process of building intelligent applications. \n\nAs advancements in artificial intelligence continue, large language models (LLMs) and deep neural networks (DNNs) are becoming increasingly capable. The latest iterations outperform their predecesso","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to do knowledge distillation | Quantumworks Lab","meta_description":"Knowledge distillation compresses large, powerful AI models into smaller, faster versions without losing performance. Learn how it works.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65b14b577aa1d10001dfcda3","uuid":"2189684b-4093-4d42-87ea-9edaae64f147","title":"How to build generative captioning and enrich product listings faster with foundation models","slug":"how-to-build-generative-captioning-using-foundation-models-for-product-listings","html":"\u003cp\u003eThe rise of foundation models has enabled companies to seamlessly enrich all of their products and services with rich captions and descriptions in minimal time and with little human effort required. Organizations can now use AI taught to automatically generate descriptions for product listings based on a wide range of images or product specifications. By incorporating a powerful generative captioning system, companies that span retail and internet \u0026amp; media can now readily enhance customer assets and foster stronger connections to boost customer loyalty and increase key metrics such as conversion rate, engagement, and average order value.\u003c/p\u003e\u003cp\u003eHowever, building a robust and effective AI-powered captioning system can be challenging for many teams. Some key challenges include: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eBuilding a strong captioning system that makes accurate predictions requires a vast amount of high-quality data. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights. Furthermore,  there are situations where the volume and speed of text generation tasks required means it is not efficiently achieved through human input alone. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability: \u003c/strong\u003eAs a business grows and their catalog expands, the system should be able to handle new and incoming data. Additionally, it can be a costly process when allocating significant portions of an individual’s or team’s time to repetitively generating text outputs. Ensuring scalability and maintaining model performance with new data can be particularly challenging for teams relying on in-house solutions or disparate ML tools.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrivacy and Security: \u003c/strong\u003eWhen it comes to customer data and specific product information, ensuring user privacy and safeguarding against potential security violations is critical to maintain trust with customers and maintaining a relevant website/app. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that can help automate generative captioning systems. Rather than spending valuable time building in-house or relying on disparate systems and applications, teams can leverage Quantumworks Lab’s platform to seamlessly build an end-to-end workflow that integrates with your existing tech stack and helps teams build AI systems faster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1730\" height=\"970\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.17.30-AM.png 1730w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how you can leverage Quantumworks Lab’s platform to build a powerful generative captioning system, ensuring your customers get deeper personalization from LLMs and for your internal teams to derive insights faster from your website product listings.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-build-a-powerful-generative-captioning-system-in-labelbox\"\u003eSee it in action: How to build a powerful generative captioning system in Quantumworks Lab\u003c/h2\u003e\u003cp\u003e\u003cem\u003eThe  walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eCatalog\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eAnnotate\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eModel\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003ecreate a Quantumworks Lab account\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial.\u003c/em\u003e\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003ch3 id=\"painlessly-consolidate-all-your-product-listing-data\"\u003ePainlessly consolidate all your product listing data\u003c/h3\u003e\u003cp\u003eBuilding a generative captioning system requires consolidating data of different types from various sources. Such data can include product, business, and customer information that might be siloed or stored in different databases. To holistically browse and visualize your entire product catalog, leverage Quantumworks Lab Catalog to bring and view all of your data in a single place.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/u35epr8pyd\" title=\"Solution Accelerator - Description Generation - Image Intro Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"492\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"accelerate-product-discovery-across-your-entire-catalog\"\u003eAccelerate product discovery across your entire catalog\u003c/h3\u003e\u003cp\u003eAn effective captioning system for product listings relies on training a model with a thorough understanding of your product data, encompassing product tags, categories, and more. However, organizations often have an ever-growing product catalog with hundreds or thousands of products. Dealing with this volume of data at scale and effectively searching, organizing, and managing data for machine learning tasks can be a challenge.\u003c/p\u003e\u003cp\u003eYou can leverage Quantumworks Lab Catalog to visualize, browse, and curate your product listings.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.30.38.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1450\" height=\"661\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2023-12-14-at-17.30.38.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2023-12-14-at-17.30.38.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.30.38.png 1450w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExplore \u0026amp; curate product listings by creating slices\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSearch and curate data\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith Catalog, you will be able to see the sample your product listings dataset and query specific items such as shoes, tops, trousers, and more. Try searching across key product-specific metadata such as category, the year the item was released, season, type, and more. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context. \u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003eSearch across datasets\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003efind similar data\u003c/a\u003e in seconds with off-the-shelf embeddings \u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003enatural language\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003elayer structured and unstructured filters\u003c/a\u003e for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"categorize-and-curate-product-listings-faster\"\u003eCategorize and curate product listings faster\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/nm4oggk5cv\" title=\"Solution Accelerator - Description Generation - Image Main Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"488\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce we have appropriately explored and curated our data, we're now in a position to begin generating product descriptions for the images that we have available to us. In some cases, it may be beneficial to create a product description for all images at once. However, in a real-world setting, it may be the case that specific teams are responsible for creating product listings for various departments such as shoe wear tops or dresses. \u003c/p\u003e\u003cp\u003eTo replicate this scenario, we can navigate to the slices that we created earlier. For example, one team may be responsible for generating product descriptions for the tops department. In this instance, you can select a subset of the data or you can select all data rows available within the slice. Having done so, you can then predict with Model Foundry which allows API connectivity to state-of-the-art foundational models  as well as the option to integrate custom models that you may have trained within your own organization.\u003c/p\u003e\u003ch2 id=\"part-2-streamline-captioning-product-listings-and-labeling-automation-with-foundry\"\u003e\u003cstrong\u003ePart 2: Streamline captioning product listings and labeling automation with Foundry\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIn this example, we're interested in generating a text description for the images provided and will therefore be working with a multi-modal model (i.e., OpenAI's GPT-4 Vision). \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.32.29.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1872\" height=\"817\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2023-12-14-at-17.32.29.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2023-12-14-at-17.32.29.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2023-12-14-at-17.32.29.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2023-12-14-at-17.32.29.png 1872w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eChoose from a variety of foundation models to enrich your product listing data.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAs a next step, you'll have the option to consider model run parameters and provide a prompt. The prompt helps you describe to the model what it is you want it to execute during inference.\u003c/p\u003e\u003cp\u003eYou can generate a prompt in a conversational manner specifying various characteristics such as the type of tone that you want your generated response to have, the aspects of the picture that you're interested in or the desired length of the response.\u003c/p\u003e\u003cp\u003eHere, we asked the model to create a description for the main item of clothing shown in the picture and asked it to exclude words such as \"the item\", \"these are\" or \"this shows\", and also to speak in certainties by ignoring such phrases as \"it appears\". We also ask for a specific prompt response length. \u003c/p\u003e\u003cp\u003eHaving done so, we can configure any of the additional parameters as required, and then we have the option to generate preview. Generating preview allows you to run inference on just a sample of the data. In this case, the maximum is five however, if of interest, we can decrease this as required.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"910\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.21.01-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eGPT-4 Vision outputting the preview for a specific product listing\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAfter completing your first model run, we can navigate back to your model to explore the outputs. We now have the option to explore each data row and observe the outputted description that GPT4-Vision has generated. Here we can see a remarkable level of detail and accuracy based on the image provided. \u003c/p\u003e\u003cp\u003eAt this point, the descriptions may be appropriate to pass straight to our website for the product listing. However, in some instances, we may wish to have a human in the loop to either tweak or validate the product descriptions that our model outputs.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSend a subset of data to the labeling project for human-in-the-loop validation\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"892\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.26.30-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSet up your ontology and create a human-in-the-loop review process.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e1) When it comes to setting up an Annotate project that enables human in the loop validation for our model outputs, the first thing to do is to set up our ontology. When doing so, it may also be of interest to have a binary classification, such as whether or not we needed to tweak the overall description, such as validation and we can have an option edited or accepted.\u003c/p\u003e\u003cp\u003e2) Once we've established this ontology, you can create it, and we're now in a position to set up our annotate project. You can navigate to Annotate, create a new project and name it something appropriate such as \"product description validation\".\u003c/p\u003e\u003cp\u003e3) Next, attach the ontology that we've just created and we're now in a position where our project has set up and the last thing we need to do is to add our data. For this, let's navigate back to the model and select the appropriate model run as before and select all our data rows. \u003c/p\u003e\u003cp\u003e4) We now want to \"Send to Annotate\" and we can include our model predictions. An option will appear on whether we want to to map the text output that the model is created to our description free text classification. Select \"Map\", and select description, and we can then select text, and then \"Save\" to ensure that we have correctly mapped the text output from the model to the description option within our ontology.\u003c/p\u003e\u003cp\u003e5) Next, let's set our batch settings. In this case we can set it to \"1\" given this is the highest priority within our project, and dictate what step of the workflow we want to put it in. In our case, we'll select \"Initial labeling task\". Having done so, we can navigate back to our Annotate project where we'll see that the data rules have been successfully added and we're now in a position for the our subject matter experts to validate all of the outputs coming from the model, and to start our labeling process (as shown below).\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"923\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.34.22-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample product description that can be acceped\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe can see above that the description provided, \"a pale blue cable knit sweater with a high, ribbed turtleneck collar and long sleeves\" appears to be appropriate so we may not need to edit it and can simply hit \"Accept\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"840\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.20.54-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample product description that needs to be edited.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn\u0026nbsp;an\u0026nbsp;instance\u0026nbsp;such\u0026nbsp;as\u0026nbsp;this\u0026nbsp;one above,\u0026nbsp;we\u0026nbsp;may\u0026nbsp;wish\u0026nbsp;to\u0026nbsp;edit\u0026nbsp;the\u0026nbsp;description\u0026nbsp;slightly. In\u0026nbsp;this\u0026nbsp;case,\u0026nbsp;the\u0026nbsp;sleeves\u0026nbsp;are\u0026nbsp;\"long\",\u0026nbsp;but\u0026nbsp;not\u0026nbsp;\"full\u0026nbsp;length\", so let's describe them as \"quarter length\" sleeves and keep everything else as consistent with what the model outputted, but mark it as \"edited\".\u003c/p\u003e\u003cp\u003eWe can continue with this workflow until all of our data rows have been processed with a human-in-the-loop review. The advantage of this approach is that it allows you to validate whether or not the model is performing is expected, offering a certain level of security by having domain experts assess each of the product descriptions before they go live on your website or app. \u003c/p\u003e\u003ch2 id=\"part-3-using-text-inputs-to-generate-and-compare-detailed-product-descriptions-with-foundry\"\u003e\u003cstrong\u003ePart 3: Using text inputs to generate and compare detailed product descriptions with Foundry\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ztmzjm7ps1\" title=\"Solution Accelerator - Description Generation - Text Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"542\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn the walkthrough above, we looked at how we could create product listing descriptions based on an image input. Next, let's look at how we can generate  detailed product descriptions based on text input containing rough product specifications, such as those found on online marketplaces.\u003c/p\u003e\u003cp\u003e1) The first step is to set up your ontology again by navigating to the schema tab, creating a new scheme and selecting the \"text\" media type as we're dealing with text inputs this time. We recommend naming it something descriptive such as \"multi description\", and then add the overall text classifications for our descriptions, with free text responses. In this example, we'll be generating 3 product descriptions. We will then need some indicator to say which of the preferred descriptions has been identified as the best by the human labeler (e.g., description 1, description 2, description 3, preferred description) as shown below .\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1058\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.54.24-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSet up a multi-description comparison workflow.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e2) We're now in a position to execute our Model Foundry run, which we can do by selecting all the data rows within this data set and by predicting with Foundry. Since we're interested in generating an output text based on the input provided, let's use OpenAI's GPT-4 again in this instance. \u003c/p\u003e\u003cp\u003e3) As a next step, let's select an ontology. We're going to want to edit this ontology because we're not interested in asking the model to provide the preferred description but simply to output each of the three descriptions. To do so, we can click \"edit\", then we can ignore the preferred description option. Clicking \"save\" will update the automatically generated prompt before and we can add some additional information via prompt to direct the model in our desired manner.\u003c/p\u003e\u003cp\u003e4) As shown above, we'll be asking the model to provide three different output descriptions based on the input specifications provided. We can then edit any of the parameters available to us as required and once we're ready, we can generate the preview as shown below. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1305\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-10.59.49-AM-1.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eGenerate description previews.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e5) After we've generated our preview, we can begin to explore each of the outputted results for the sample of five data rows. To see the full response, we can navigate to to \"view log\" and see the outputted response from our model which has provided three different descriptions. As it appears the model is performing as expected and we can now execute the full model run (as shown below).\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1173\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.00.55-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eView log to see model output.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e6) Once our model run has been complete, we're in a position to navigate to the Model Run of interest. We can explore the three different descriptions that the model has generated for each of our data rows. Let's pass these to an Annotate project for a human labeler to select which of the three descriptions are preferable. To do so, we'll set up an Annotate project again, making sure to select \"text\", add our ontology that we used earlier, and then pass our model run outputs, and then \"Send to Annotate\".\u003c/p\u003e\u003cp\u003e7) Finally, we can view the 3 descriptions that the model has outputted. We can now have a human labeler review each of these descriptions, and select which one of the descriptions is most appropriate to include in the website listing as shown below.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"986\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.07.21-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHuman labelers can review each description for the preferred description.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe can continue with this workflow until labelers have reviewed and assessed all data row descriptions. This allows us to build a dataset ready to extract and identify preferred descriptions and pass these description downstream for whatever use case is required such as inclusion on the product listing of our website. \u003c/p\u003e\u003cp\u003eWe hope that this walkthrough gives you an idea of how you can leverage Quantumworks Lab's Foundry capabilities to create automatically generated product descriptions for a wide range of use cases. This should dramatically reduce the time taken and costs associated with generating outputted text (from product descriptions, to alt-text, to articles), while leveraging foundation models to automate the first pass. \u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAs consumer businesses in media, retail, and internet strive to distinguish themselves in a competitive market, the power of AI-driven captioning systems for automating product listings serves as a powerful lever for speeding up manual tagging processes. Companies can tap into their vast data stores and harness the capabilities of advanced algorithms to enrich the customer experience. \u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to more quickly build intelligent applications. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..\u0026ref=labelbox-guides.ghost.io\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e. \u003c/p\u003e","comment_id":"65b14b577aa1d10001dfcda3","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-24-at-11.16.33-AM.png","featured":false,"visibility":"public","created_at":"2024-01-24T17:39:35.000+00:00","updated_at":"2024-01-25T21:17:14.000+00:00","published_at":"2024-01-25T17:06:09.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab’s platform and Foundry to build a powerful generative captioning system, ensuring your customers get deeper personalization from LLMs and for your internal teams to derive insights faster from your website product listings.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/how-to-build-generative-captioning-using-foundation-models-for-product-listings/","excerpt":"Learn how to leverage Quantumworks Lab’s platform and Foundry to build a powerful generative captioning system, ensuring your customers get deeper personalization from LLMs and for your internal teams to derive insights faster from your website product listings.","reading_time":11,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65aadec23c5a9d00013a702d","uuid":"3b49686d-0ba6-4e2e-ba83-03a007d34b03","title":"How to build defect detection models to automate visual quality inspection","slug":"how-to-build-defect-detection-models-to-improve-visual-quality-inspection","html":"\u003cp\u003eWith AI-powered defect detection, you can now easily harness the latest advances in automation and computer vision into your quality inspection models. As the demand for defect-free manufacturing continues to rise, it's important for teams to deliver the highest-quality products for their assembly lines and minimize operational and quality-related costs. Quantumworks Lab empowers the world’s largest manufacturers to leverage AI solutions tailored to their unique defect detection challenges in order to more quickly build intelligent applications.\u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for large-scale defect detection for manufacturing use cases. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving defect detection requires a vast amount of data in the form of images and videos. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e\u0026nbsp;The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e\u0026nbsp;Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to automate their visual inspection through advanced computer vision techniques. Instead of relying on time-consuming manual human review, companies can now leverage AI-assisted data enrichment and flexible training frameworks to quickly build task-specific models that uncover actionable insights for visual inspection faster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1654\" height=\"912\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1654w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through a workflow on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific models to improve defect detection using image segmentation. Specifically, this guide will walk through how you can explore and better understand your visual assets to make more data-driven business decisions for minimizing defects.\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-build-defect-detection-models-to-automate-visual-quality-inspection\"\u003eSee it in action: How to build defect detection models to automate visual quality inspection\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/er234bmdq6\" title=\"Solution Accelerator - Defect Detection - Exploration \u0026amp; Annotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across\u0026nbsp;\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data with Foundry\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below via:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://drive.google.com/file/d/1B1qPR8u9Wp2AkRdWjo-WSM0nsW_nQzdP/view?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://drive.google.com/file/d/1nw_dQGQI-bhYdyZCX_49DwVSDvt4DkWu/view?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eDatabricks Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"explore-and-prepare-your-data\"\u003e\u003cstrong\u003eExplore and prepare your data\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in the \u003ca href=\"https://drive.google.com/file/d/1B1qPR8u9Wp2AkRdWjo-WSM0nsW_nQzdP/view?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, please make a copy of the notebook.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ll be working with a dataset that shows submersible pump impeller images for a defect detection use case\u0026nbsp; – with the goal of quickly curating data and segmenting for chips and frayed edges in order to detect broken parts.\u003c/p\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003ePlease download the dataset and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your data as a CSV.\u003c/li\u003e\u003cli\u003eIf your images sit as individual files in cloud storage, you can reference the URL of these files through our\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your image data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/er234bmdq6\" title=\"Solution Accelerator - Defect Detection - Exploration \u0026amp; Annotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this demo, we'll be using Catalog to find relevant images of submersible pump impellers for our dataset with the goal of annotating broken parts using foundation models.\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"manually-label-a-subset-of-data-to-train-a-first-model-with-auto-segment\"\u003eManually label a subset of data to train a first model with Auto-Segment \u003c/h3\u003e\u003cp\u003eThe next step is to manually label a subset of our data using the Segment Anything Model within Annotate. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-9.13.12-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1033\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-25-at-9.13.12-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-25-at-9.13.12-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-25-at-9.13.12-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-25-at-9.13.12-AM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Let's first start off with two hundred images and include the project that we've just set up within Annotate. Once the data rows have been loaded into our Annotate project, we can begin the labeling process. \u003c/p\u003e\u003cp\u003e2) By navigating to the labeling editor, we can set up and use the ontology that we defined earlier with each of the objects of interest, in this case, chips and frayed edged. \u003c/p\u003e\u003cp\u003e3) Next, let's use the auto-segment tool powered by Facebook's Segment Anything Model to accelerate the overall labeling. This makes use of powerful AI in the back end of the editor and allows the user to simply draw a bounding box around objects of interest before the model automatically identifies the segment mask for chips and edges. \u003c/p\u003e\u003cp\u003e4) An additional aspect you can make use here are keyboard hotkeys to save us time, which is shown as the number next to our object of interest. By simply selecting one of the keys, this allows us to activate the chip annotation. Hovering over the auto-segment tool, we can see the key \"R\" is shown. By selecting \"R\" on our keyboard, we can activate the auto-segment tool on and off. So having tagged this one chip, we may want to select the second one. \u003c/p\u003e\u003cp\u003e5) You can continue through in this manner until you reach an appropriate data set for training your first iteration of your model. In the example shown, once we reach two hundred images, we're now in a position to train the first version of our model and upload our model predictions.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/enbva2ynh1\" title=\"Solution Accelerator - Defect Detection - Model Evaluation (1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"640\" height=\"412\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eHaving uploaded our model predictions, we can step back into the Quantumworks Lab platform to begin to explore our model run and understand how we performed in terms of predictions versus ground truth. \u003c/p\u003e\u003cp\u003eTo do so, you can navigate to the Model tab, find the experiment of interest, in this case defect detection and make sure that we're selecting the run that we have just created. We can begin to explore visually how our predictions compare to the ground truth data.\u003c/p\u003e\u003cp\u003eWe can see that there's some good overlap when it comes to chips, but the frayed edges are performing poorer in this case. Aside from just simply visualizing and exploring in this manner here, we can also step into the Analytics tab which provides various views of performance. You'll see that in the overall object metrics, (which provides a traditional precision recall F1 score) is performing fairly well in terms of where there is an instance of a object be that afraid edge or a chip. \u003c/p\u003e\u003cp\u003eHowever, the intersection over union (IoU) score is still low, which suggests that while we're detecting some objects, we are not correctly detecting the full segmentation area of the objects. This is where you can deep dive into the confusion matrix and see that on the whole chip detection performing significantly better than frayed edges. \u003c/p\u003e\u003cp\u003eNow that we have our first iteration of the model, we can see that there's room for improvement. In the next step, let's use our initial model's predictions as part of a model assisted labeling pipeline to accelerate the overall annotations for our second iteration of the training data.\u003c/p\u003e\u003ch3 id=\"leveraging-model-assisted-labelling-to-accelerate-labeling-and-improve-model-performance\"\u003eLeveraging Model Assisted Labelling to accelerate labeling and improve model\u0026nbsp;performance\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/0vbn9n2n0t\" title=\"Solution Accelerator - Defect Detection - Model Assisted Labelling Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"554\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eHaving trained the first iteration of our model, we can now run inference over the remaining dataset and upload these predictions into our Annotate project as part of a model-assisted labeling pipeline. You can do this by following through the code provided in the \u003ca href=\"https://drive.google.com/file/d/1B1qPR8u9Wp2AkRdWjo-WSM0nsW_nQzdP/view?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e that accompanies this demo.\u003c/p\u003e\u003cp\u003eOnce you've uploaded your predictions to your Annotate project, you can step back into Quantumworks Lab, find the appropriate project and iterate or modify pre-labels and speed up the annotation process.  \u003c/p\u003e\u003cp\u003eYou can continue through with this process, accelerating the time it takes for labeling until you arrive at a suitable sized dataset for training your second iteration of your model. This process will continue until you have reached a model that is of production quality as assessed through the Model tab we have seen previously.\u003c/p\u003e\u003ch3 id=\"set-up-a-second-model-run-and-train-an-improved-model\"\u003eSet up a second Model run and train an improved model\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kufuomb75p\" title=\"Solution Accelerator - Defect Detection - Model Run 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter having progressed through a second round of labeling, we'll arrive at a point where the labeled dataset is sufficiently large for training a second iteration of our model. \u003c/p\u003e\u003cp\u003eIn this case, we have a thousand datarows, and we're now in a position to create our second model run. To do so, we navigate to the model experiment that we created previously.\u003c/p\u003e\u003cp\u003eLet's keep the splits between the train, validate, and test set as they were before and create a new Model run. Once this is done, so we can select Model run and see the splits between training, validation, and test sets. By refreshing this view, we will be able to visualize our labeled data, and continue with the second training of a new model.\u003c/p\u003e\u003ch3 id=\"upload-second-model-results-and-compare-performance\"\u003eUpload second model results and compare performance\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/y31qcxz9rm\" title=\"Solution Accelerator - Defect Detection - Model Evaluate 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"552\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a next step, after having uploaded our second model runs predictions is  the to  evaluate whether or not we have seen the expected improvement between our two models. \u003c/p\u003e\u003cp\u003eLet's navigate back to our Experiments tab within Labelbox. For our first model we had a particularly low intersection over union (IoU) score. If we're interested in understanding whether or not we've seen improvement in this metric, we can scroll down to the appropriate graphic that demonstrates our intersection over union performance and in the top left, we can select the option to compare against our second model run. \u003c/p\u003e\u003cp\u003eLooking at this graph allows us to compare the intersection over union (IoU) scores between our two classes across both model runs. We can see that while there's been a small improvement in performance for a chip class, there's been a larger performance improvement for frayed edges. This shows that we're heading in the right direction with the overall performance of our second model run being better than that of our first. \u003c/p\u003e\u003cp\u003eIn a real-world setting, a decision point would have to come as to whether or not our second model is ready for production. In the event that it is not, we recommend training a third iteration using our second model to perform model-assisted labeling and accelerate the time for labeling a larger proportion of our dataset. \u003c/p\u003e\u003cp\u003eTeams can repeat this iterative process at each stage, evaluating whether or not your model performance is improving with each iteration. In conclusion, this workflow should be able to help you leverage Quantumworks Lab at each stage from evaluating models to helping speed up labeling and accelerating your time to value.\u003c/p\u003e\u003ch2 id=\"summary\"\u003eSummary\u003cbr\u003e\u003c/h2\u003e\u003cp\u003eBy analyzing high-volumes of images using automation and model-assisted labeling, Quantumworks Lab provides teams with the ability to inject valuable human-in-the-loop insights for delivering better models that allow you to detect defects faster that can help improve throughput, quality assurance and overall visual inspection.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful task-specific models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"65aadec23c5a9d00013a702d","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-19-at-3.34.17-PM.png","featured":false,"visibility":"public","created_at":"2024-01-19T20:42:42.000+00:00","updated_at":"2024-03-01T21:14:26.000+00:00","published_at":"2024-01-22T17:38:30.000+00:00","custom_excerpt":"Learn how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection using image segmentation for visual inspection.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},"url":"https://labelbox-guides.ghost.io/how-to-build-defect-detection-models-to-improve-visual-quality-inspection/","excerpt":"Learn how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection using image segmentation for visual inspection.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65a881b23c5a9d00013a6f62","uuid":"8ae52e1c-9e63-40ed-8b21-ef8098acb112","title":"How to build defect detection models to improve predictive maintenance","slug":"how-to-build-defect-detection-models-to-improve-preventative-maintenance-2","html":"\u003cp\u003eWith AI-powered defect detection, you can now seamlessly integrate the latest advances in foundation models into your equipment maintenance and QA operations. As the demand for better monitoring continues to rise, it's essential for teams to maximize the lifespan of their critical assets and minimize operational and quality-related costs. Quantumworks Lab empowers the world’s largest organizations to leverage AI solutions tailored to their unique object detection challenges.\u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for large-scale defect detection. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving defect detection requires a vast amount of data in the form of images and videos. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e\u0026nbsp;The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e\u0026nbsp;Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their predictive maintenance through advanced computer vision techniques. Instead of relying on time-consuming manual human review, companies can leverage Quantumworks Lab’s AI-assisted data enrichment and flexible training frameworks to quickly build task-specific models that uncover actionable insights for defects faster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1654\" height=\"912\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-17-at-5.47.44-PM.png 1654w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through an end-to-end workflow on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection on pipes. Specifically, this guide will walk through how you can explore and better understand your assets to make more data-driven business decisions for predictive maintenance.\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-build-defect-detection-models-to-improve-predictive-maintenance\"\u003eSee it in action: How to build defect detection models to improve predictive maintenance\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xofopzl9cm\" title=\"Set up project - 1 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across\u0026nbsp;\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data with Foundry\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below via:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1s4UZoQfrxrB_WlrQVmuc068sWQst6aSM?usp=sharing\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003e\u003cstrong\u003ePart 1: Explore and prepare your data\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in the \u003ca href=\"https://colab.research.google.com/drive/1s4UZoQfrxrB_WlrQVmuc068sWQst6aSM?usp=sharing\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, please make a copy of the notebook.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ocyptoa2t2\" title=\"Add data - 2 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ll be working with a dataset that shows multiple parts of equipment for pipes for a defect detection use case\u0026nbsp; – with the goal of quickly curating data and finding 3 specific parts (i.e., pipe, flange, elbow) from high-volumes of images to detect corrosion and broken parts.\u003c/p\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003ePlease download the dataset and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your data as a CSV.\u003c/li\u003e\u003cli\u003eIf your images sit as individual files in cloud storage, you can reference the URL of these files through our\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your image data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/c10qgqqpoq\" title=\"Curate and prioritize - 3 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this demo, we'll be using Catalog to find relevant images of pipes for our dataset with the goal of annotating bounding boxes for the pipe parts using foundation models.\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"using-foundry-to-pre-label-bounding-boxes\"\u003eUsing Foundry to pre-label bounding boxes\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/8b0nlr5afz\" title=\"Human in the loop - 4 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn this next step, we'll walk through how you can take a human-in-the-loop approach to iterate or modify pre-labels and speed up the annotation process.  \u003c/p\u003e\u003cp\u003eModel Foundry enables teams to choose from a library of models and in this case, we'll be comparing the effectiveness of two object detection models (Grounding DINO vs. OWL-VT) to generate previews and attach them as pre-labels.\u003c/p\u003e\u003cp\u003eWith\u0026nbsp;\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, you can automate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/p\u003e\u003ch2 id=\"part-2-train-a-yolov8-model-and-generate-predictions\"\u003e\u003cstrong\u003ePart 2: \u003c/strong\u003eTrain a YOLOv8 model and generate predictions\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/8cl9s78sj4\" title=\"Experiments - 5 - Defect Detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe next step is to take all of this labeled data and train our model on it, and then  make predictions on this data which allows Quantumworks Lab to calculate evaluation metrics, and see where the model is going wrong.\u003c/p\u003e\u003cp\u003eAs an additional visual method, you can navigate\u0026nbsp;to\u0026nbsp;the\u0026nbsp;Quantumworks Lab projector\u0026nbsp;view to visualize\u0026nbsp;different\u0026nbsp;groups\u0026nbsp;or\u0026nbsp;clusters\u0026nbsp;of\u0026nbsp;different\u0026nbsp;classes.\u0026nbsp;You'll see that there\u0026nbsp;are\u0026nbsp;three\u0026nbsp;different\u0026nbsp;clusters,\u0026nbsp;which\u0026nbsp;aligns\u0026nbsp;with\u0026nbsp;our\u0026nbsp;expectations because\u0026nbsp;we\u0026nbsp;have\u0026nbsp;three\u0026nbsp;different\u0026nbsp;classes\u0026nbsp;across\u0026nbsp;pipes,\u0026nbsp;elbows,\u0026nbsp;and\u0026nbsp;flanges. This allows you to  find outliers in the clusters to provide an initial review.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/8akbbpbmpb\" title=\"Train Yolo v8 model - 7- defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/lr4zqg10ax\" title=\"Run inference - 8 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eRun inference from trained model on unlabeled data from your Databricks notebook\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/feuc47q3b6\" title=\"alternative - 9 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOne alternative method to running the inference workflow in the previously shown step is to take model weights and deploy them directly within Quantumworks Lab Foundry as a custom model. The benefits of this is that it will allow you to run predictions using your custom model as an end-to-end workflow and more quickly classify parts of interest (i.e., elbows, pipes, and flanges). \u003c/p\u003e\u003cp\u003e\u003cu\u003eNote\u003c/u\u003e: If this is interesting and you're looking to adopt this method within Quantumworks Lab, please reach out to our support team as we would be happy to assist with deploying your custom model within Foundry.\u003c/p\u003e\u003ch3 id=\"view-model-predictions-within-the-labelbox-ui-to-evaluate-and-diagnose-model-effectiveness\"\u003eView model predictions within the Quantumworks Lab UI to evaluate and diagnose model effectiveness\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jxi7miwgs6\" title=\"final step - 10 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a last step, let's compare model inferences with ground-truth annotations to see where the model may be underperforming. A disagreement between model predictions and ground truth labels can be due to a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eAfter running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/li\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model for 10x faster corner-case detection – detect and visualize corner-cases where the model is underperforming. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection and additional labeling.\u003c/p\u003e\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/f78ql0djrc\" title=\"Summary - 11 - defect detection Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBy analyzing high-volumes of images and videos using foundation models and human alignment, Quantumworks Lab provides teams with the ability to inject valuable human-in-the-loop insights for delivering better models that allow you to detect defects that can help improve operational efficiency, quality assurance and overall equipment lifespan.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful task-specific models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"65a881b23c5a9d00013a6f62","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-19-at-12.44.25-PM.png","featured":false,"visibility":"public","created_at":"2024-01-18T01:41:06.000+00:00","updated_at":"2024-03-01T21:10:16.000+00:00","published_at":"2024-01-18T18:06:14.000+00:00","custom_excerpt":"In this guide, we’ll walk through an end-to-end tutorial on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection on pipes.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-build-defect-detection-models-to-improve-preventative-maintenance-2/","excerpt":"In this guide, we’ll walk through an end-to-end tutorial on how your team can leverage Quantumworks Lab’s platform to build a powerful task-specific model to improve defect detection on pipes.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65a6bf0b3c5a9d00013a6ea2","uuid":"e42b9d02-59c8-4914-9b59-88f6c01c9ff4","title":"How to build damage classification models with aerial imagery to improve claims automation","slug":"how-to-visually-assess-damage-detection-and-improve-claims-automation-with-ai","html":"\u003cp\u003e\u003cbr\u003eWith AI-powered claims automation, you can now seamlessly integrate the latest advances in foundation models into your damage detection and disaster assessment models. As the demand for real-time intelligence into understanding residential and commercial properties grows, it's essential for teams to maximize compliance and minimize operational costs. \u003c/p\u003e\u003cp\u003eHowever, teams\u0026nbsp;can face multiple challenges when implementing AI for damage detection. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity\u003c/strong\u003e: Improving damage detection requires a vast amount of data in the form of images and videos. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape:\u003c/strong\u003e The changing nature and format data from multiple sources poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability:\u003c/strong\u003e Developing accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification and active learning, can help accelerate model development by automating the labeling process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their claims automation through advanced computer vision techniques. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s AI-assisted data enrichment and flexible training frameworks to quickly build task-specific models that uncover actionable insights from damage assessment.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-16-at-3.48.51-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1714\" height=\"936\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-16-at-3.48.51-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-16-at-3.48.51-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-16-at-3.48.51-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-16-at-3.48.51-PM.png 1714w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build a task-specific model to improve building damage detection via aerial imagery. Specifically, this guide will walk through how you can explore and better understand unstructured data to make more data-driven business decisions around damage detection initiatives.\u003c/p\u003e\u003ch3 id=\"see-it-in-action-how-to-visually-assess-damage-and-improve-claims-automation-with-ai\"\u003eSee it in action: How to visually assess damage and improve claims automation with AI\u003c/h3\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below via:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1v9_arRsnr0od8VlLSk6fnTcTRsaVgOu_?usp=drive_link\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003e\u003cstrong\u003ePart 1: Explore and prepare your data\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1v9_arRsnr0od8VlLSk6fnTcTRsaVgOu_?usp=drive_link\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, please make a copy of the notebook.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/b3hi000zyz\" title=\"SA Imagery -- Part 1. Data Ingestion Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eFor this use case, we’ll be working with an open dataset from the Hurricane Maria aerial assessment dataset\u0026nbsp; – with the goal of quickly curating data and finding building damage from high-volumes of images.\u003c/p\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003ePlease download the dataset and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your data as a CSV.\u003c/li\u003e\u003cli\u003eIf your images sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your image data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mk64mj4bw4\" title=\"SA Imagery -- Part 2. Catalog Data Discovery Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this demo, we'll be using Catalog to find relevant images of buildings for our dataset with the goal of annotating footprints using foundation models. \u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically\u0026nbsp;\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e\u0026nbsp;in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e\u0026nbsp;and flexibly\u0026nbsp;\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e\u0026nbsp;for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"use-foundry-to-generate-building-footprints\"\u003eUse Foundry to Generate Building Footprints\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/0lnyh3ue3z\" title=\"SA Imagery Part 3. Generate Building Footprints w_ Model Foundry Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe next step is to use foundation models to detect as many building footprints as possible.\u003c/p\u003e\u003cp\u003eModel Foundry enables teams to choose from a library of models and in this case, we'll be using an object detection model (Grounding DINO) to generate previews and attach them as pre-labels.\u003c/p\u003e\u003cp\u003eWith\u0026nbsp;\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, you can automate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/p\u003e\u003ch3 id=\"set-up-your-annotation-project\"\u003eSet up your annotation project\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/i05gqcznas\" title=\"SA Imagery Part 4. Setting Up Annotation Project Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBy pre-labeling building footprints, we have significantly sped up our labeling efficiency and the next step is to set up our annotation project.\u003c/p\u003e\u003cp\u003e1) Set up annotation project and ontology.\u003c/p\u003e\u003cp\u003e2) Ensure ontology includes sub classification for damage severity (e.g., Low, Medium, High).\u003c/p\u003e\u003cp\u003e3) Include bounding box model predictions from Foundry pre-labels.\u003c/p\u003e\u003cp\u003e4) Zoom-in to determine the level of damage and select the appropriate level of damage or draw additional bounding boxes as needed.\u003c/p\u003e\u003ch3 id=\"optional-send-ground-truth-data-to-annotate\"\u003e[Optional] Send ground-truth data to Annotate\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/achdmpcq69\" title=\"SA Imagery Part 5. MAL Upload (Optional) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThese next series of steps are optional and will help you bypass the manual component by uploading ground-truth directly by using the Annotation project ID.\u003c/p\u003e\u003cp\u003e1) Reference a cloud bucket that references annotations and corresponding damage classifications.\u003c/p\u003e\u003cp\u003e2) Send annotation data type of annotation project so that you have ground-truth data.\u003c/p\u003e\u003cp\u003e3) By uploading annotation data directly via the Python SDK, you can access ground-truth data to send directly to your models for fine-tuning and refinement.\u003c/p\u003e\u003ch2 id=\"part-2-create-a-model-run-and-and-evaluate-model-performance\"\u003ePart 2: Create a model run and and evaluate model performance\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this\u0026nbsp;\u003ca href=\"https://colab.research.google.com/drive/1NSxJRK_I8TZFSDRlp_b8cHMdyx0kJjn1?usp=drive_link\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along,\u0026nbsp;\u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCreate a model run\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/pkyquongd8\" title=\"SA Imagery Part 6. Model Run _ Model Tuning _ Active Learning Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"550\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you have your labeled data in your project in Annotate, you’re ready to move on to creating a model run in\u0026nbsp;\u003ca href=\"https://app.labelbox.com/mea?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\" rel=\"noreferrer\"\u003eLabelbox Model\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eWe’ll be using this\u0026nbsp;\u003ca href=\"https://colab.research.google.com/drive/1NSxJRK_I8TZFSDRlp_b8cHMdyx0kJjn1?usp=drive_link\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/a\u003e\u0026nbsp;to train a model on the training dataset and bring back inferences from the trained model for evaluation and diagnosis.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou’ll be able to view the model in the ‘Experiments’ tab in Quantumworks Lab Model – you can view ground truth predictions in green and predictions in red.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness\"\u003eEvaluate and diagnose model effectiveness\u003c/h3\u003e\u003cp\u003eA disagreement between model predictions and ground truth labels can be due to a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eAfter running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/li\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model for 10x faster corner-case detection – detect and visualize corner-cases where the model is underperforming. For example, you can drill into cases where ‘empty’ objects are not predicted, where the model might have difficulty identifying empty spaces on shelves where there is a wire mesh material present.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select ‘Find similar in Catalog’ from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled images that you can send to your model for labeling, you can filter on the ‘Annotation is’ filter and select ‘none’. This will only show unlabeled images that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all images that apply and send them as a batch to your original labeling project. Labeling these in priority will help improve model performance.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCompare model runs across iterations\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eImprove model development by up to 90% by leveraging Quantumworks Lab Model to compare model runs across iterations to track and quantify how model performance has improved over time.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model using the same steps with the Colab notebook on this improved data set.\u0026nbsp;You can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003cp\u003eBy analyzing high-volumes of images, videos and text, Quantumworks Lab provides valuable human-in-the-loop insights for damage detection processes to ensure underwriting risks, enabling insurance companies to make data-driven decisions that improve operational efficiency, compliance and revenue.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful task-specific models. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520platform\u0026utm_source=linkedin_labelbox\u0026utm_medium=organic_social\u0026\u0026referrer_url=https://www.google.com/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"65a6bf0b3c5a9d00013a6ea2","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/01/Screenshot-2024-01-16-at-2.48.33-PM.png","featured":false,"visibility":"public","created_at":"2024-01-16T17:38:19.000+00:00","updated_at":"2024-02-28T22:32:48.000+00:00","published_at":"2024-01-16T18:30:45.000+00:00","custom_excerpt":"Learn how your team can leverage Quantumworks Lab’s platform to build a task-specific model to improve building damage detection. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa608375d13000123d7fa","name":"Industry: Finance \u0026 insurance","slug":"industry-finance-insurance","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-finance-insurance/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-visually-assess-damage-detection-and-improve-claims-automation-with-ai/","excerpt":"Learn how your team can leverage Quantumworks Lab’s platform to build a task-specific model to improve building damage detection. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6628249bd733f0000145b209","uuid":"016e4a90-2a89-43a0-97fb-16a7d6cfd91c","title":"What is Model Distillation?","slug":"model-distillation","html":"\u003cp\u003eAI models are increasingly getting bigger with the increase in training data and the number of parameters. For instance, the latest \u003ca href=\"https://labelbox.com/product/model/foundry-models/gpt4/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eOpen AI’s GPT-4 \u003c/u\u003e\u003c/a\u003emodel is estimated to have about 1.76 trillion parameters and terabytes of training corpus. Whether training \u003ca href=\"https://labelbox.com/blog/6-key-llms-to-power-your-text-based-ai-applications/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003elarge language models (LLMs)\u003c/u\u003e\u003c/a\u003e or neural networks, the main goal remains: to train using as much data as possible.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile training from diverse data and increasing the number of parameters produces powerful models, real-world application becomes challenging. Deploying larger and larger models to edge devices like mobile phones and smart devices becomes difficult because of their intensive computational requirements and costs.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo solve this challenge, model distillation comes in handy. Model distillation, also known as knowledge distillation, is a machine learning technique that involves transferring knowledge from a large model to a smaller one that can be deployed to production. It bridges the gap between computational demand and the cost of enormous models in the training lab and the real-world application of these models while maintaining performance.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"why-model-distillation\"\u003e\u003cstrong\u003eWhy Model Distillation?\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eBefore diving into the steps taken to achieve model distillation, let’s discuss why it’s needed and the technical challenges it addresses in model training and deployment. Significant differences between requirements for model training and model inference stem from the infrastructure differences in training and deployment environments.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile LLMs like \u003cu\u003eLlama\u003c/u\u003e and GPT-4 have incredible power, their applications suffer drawbacks caused by hardware requirements, speed, and cost. Hosting these models or directly accessing them through APIs would be expensive. The infrastructure cost and carbon footprints would be astronomical, even when run in the cloud. Large models also tend to be slow, which affects performance.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith model distillation, the deployment and real-world application of AI models can be realized. An inference-optimized model that keeps all the qualities and behaviors of the larger training model can be achieved. Using the teacher-student architecture, this supervised learning approach ensures knowledge transfer, enabling AI teams to develop secondary models that are responsive and cheaper to host and run.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"how-does-the-model-distillation-process-work\"\u003e\u003cstrong\u003eHow Does the Model Distillation Process Work?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe model distillation process can be complex, depending on the complexity of the base and target models. The key proponents are the teacher model, the knowledge, the student model, and the distillation algorithm. This model compression technique starts with the teacher model and ends with the optimization of the resultant model using the distillation loss function, as described in the steps below.\u003c/p\u003e\u003ch2 id=\"teacher-model-training\"\u003e\u003cstrong\u003eTeacher Model Training\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe model distillation process starts with a pre-trained model, like an LLM trained on a large corpus. This phase involves selecting, fine-tuning, and/or training an expensive AI model with billions of parameters and gigabytes of data in a lab environment. The teacher model would be the base model in this case, serving as the knowledgeable expert system from which the knowledge is distilled. The student model would then inherit the behaviors and functionalities of this teacher model during knowledge transfer.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eSoft targets are also generated during the training of the teacher model. Unlike hard labels, which are binary encoded, soft targets provide a more precise classification of the data points during the teacher training phase. The soft target generated offers more information on the teacher model’s decision-making, which will guide the student model's behavior during distillation.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"student-model-initialization\"\u003e\u003cstrong\u003eStudent Model Initialization\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIn this phase, a smaller and lightweight student model is introduced. The model is initialized with random weights and set ready for knowledge transfer, targeting deployability on edge devices. With a simpler architecture and shrank computational demands, this student model is trained to replicate the teacher model’s output probabilities.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"knowledge-transfer\"\u003e\u003cstrong\u003eKnowledge Transfer\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAfter the student model is initialized, it is trained using the outputs of the teacher model in a process known as knowledge transfer; this is the distillation phase. Soft targets generated by the teacher model are combined with the original training dataset to train the student model. With the aim of matching the student and teacher’s predictions, the student model is steered towards mimicking the feature representation of the teacher model’s intermediate layers. In doing so, the student model learns the pairwise relation between data points captured by the base model. A distillation algorithm is applied in the knowledge transfer phase to ensure student models can acquire knowledge from the teacher model efficiently. Such algorithms include \u003cstrong\u003eadversarial distillation, multi-teacher distillation, graph-based, and cross-modal distillation. \u003c/strong\u003eThe choice of these algorithms depends on the work at hand, the model’s features, and data points. Either way, applying a distillation knowledge algorithm is a highly encouraged practice during model distillation as it reduces complexity and improves the performance of the student model.\u0026nbsp; \u0026nbsp; \u003cstrong\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch2 id=\"optimization-with-distillation-loss-function\"\u003e\u003cstrong\u003eOptimization with Distillation Loss Function\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAs already highlighted, the learning curve is often steep for the student model, considering its minimal computation power and performance expectations. As a result, it might sometimes drift from the training domain, resulting in distillation loss. To address this challenge, a distillation loss function is applied to guide the knowledge transfer process. This loss function helps the student model to steadily acquire knowledge by quantifying the discrepancies between the teacher model’s soft targets and the student model’s predictions. These discrepancies provide a blueprint for minimizing feature activation differences between the teacher and student models, helping the student model to adapt gradually.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"fine-tuning-the-student-model\"\u003e\u003cstrong\u003eFine-Tuning the Student Model\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAs much as the knowledge transfer process is expected to be seamless and the resultant model a replica of the teacher model, the student model might not achieve perfection. Therefore, it is a good practice to fine-tune the student model further on the original dataset after the model distillation process. This optional process employs supervised learning methodologies to improve the performance and accuracy of the student model.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/gRn1yl70cPIULEAEZvNYJ2V4z3ScNA7IfLRzz3OiPde7DGuvKRaywdOY3ke44dNW7s3eDeRvq5BwZrGT0qgFQN8vApHRZWqbnvbIEGudIQbKAm6uWlFLpswl-04ATweV2StKuSgqtOm7M3BSZs00Wr0\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"850\" height=\"316\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eModel distillation lifecycle. Image from\u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Fig-2-Generic-architecture-of-knowledge-distillation-using-a-teacher-student-model_fig2_355180688?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003e ResearchGate\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"what-are-the-different-types-of-model-distillation\"\u003e\u003cstrong\u003eWhat are the different types of Model Distillation?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eModel distillation can be classified into three types:\u003cstrong\u003e response-based, feature-based, and relation-based\u003c/strong\u003e. Each type has a unique approach to knowledge transfer from the teacher model to the student model, as discussed below.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"response-based-model-distillation\"\u003e\u003cstrong\u003eResponse-based Model Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThis is the most common and easiest-to-implement type of model distillation that relies on the teacher-model’s outputs. Instead of making primary predictions, the student model is trained to mimic the prediction of the teacher model. During the distillation process, the teacher model is prompted to generate soft labels. An algorithm is then applied to train the student model to predict the same soft labels as the teacher model and minimize the differences in their outputs (also know as distillation loss).\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/k7jfzbrR_B2AruVBbtCBOLK5wARtxwmcMyCrhk_O-feR1TONPKtWDHzd7kZ6VZ0GXkkOo_IJsLxYj472bx_fJy4hXI-y60aP4FJ4N4JjK36lxftY20L15XZWKr0EYpuvkjYui7GSfuHyFRoxDvYXJxc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"850\" height=\"317\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eResponse-based distillation process. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Response-based-knowledge-distillation_fig3_356869657?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eResearchGate\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"feature-based-model-distillation\"\u003e\u003cstrong\u003eFeature-based Model Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAs the name suggests, feature-based distillation involves the student model learning from the teacher model’s internal features. In this type of model distillation, the teacher model is trained on task-specific features. The intermediate layers of these features are then extracted and used as targets when training student models. The student model is also trained to incrementally minimize the difference between the features it learns and those learned by the teacher model.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J0szaw1p4EGVavZkPaiQQZYPZancpacn4WZTcparpZx7JFR8Lzv4Mh0rpmSjGS0XvZkq8W4UWPAgnLmMuy_RFzP2tYQyhyhkJwA5k-w5iWNvsj8Ju7p6AdIqA_P8mOP1Vwih606Vnz8GlBNrOh9f8hI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"753\" height=\"290\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eKnowledge transfer over feature layers in feature-based distillation. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/Feature-based-knowledge-distillation_fig5_371616469?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eResearchGate\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"relation-based-model-distillation\"\u003e\u003cstrong\u003eRelation-based Model Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThis is the most sophisticated yet robust type of model distillation. It focuses on transferring the underlying relationships between the inputs and outputs from the teacher model to the student model. The teacher model generates sets of relationship matrices for the input examples and output labels. While the goal is to minimize the loss function, the student model is trained on these relationship matrices progressively.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/BGscvBD1uJJN9F1Fs7nkbZsx48IQQd0hcxOh7OZD4hRtJUW4UHgwpzP7AE0EP8F3VgCiD9unM6IM-4i-lx6regEDqFZOatI0fyLW7h_k6CULklRDuulT6La2X2p8R2lkVaYSLJomy2bw5cwrCZ97Ix4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"715\" height=\"351\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eDiagrammatic representation of relation-based model/ knowledge distillation. Image from \u003c/span\u003e\u003ca href=\"https://www.researchgate.net/figure/The-generic-instance-relation-based-knowledge-distillation_fig10_342094012?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003eResearchGate\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"model-distillation-schemes\"\u003e\u003cstrong\u003eModel Distillation Schemes\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAside from the model distillation process and types, the technique applied during the student model training is also crucial. Depending on whether the teacher model is modified concurrently with the student model, there exist three primary model distillation schemes: \u003cstrong\u003eoffline, online, and self-distillation\u0026nbsp;.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch2 id=\"offline-distillation\"\u003e\u003cstrong\u003eOffline Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThis is the most popular model distillation method, which involves pre-training a teacher model, freezing it, and later using it to train a student model. During the model distillation process, the training focuses on the student model as the teacher model remains unmodified.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/lZglb4gNoQCFjcAtPDgBvL0KiCfEZooH84oYVvYFNLZWZdOJi7qV12m1e8tevOTG8Hlmij6GNIflCOO4TTHa2SF6ejlKdCvZvDjV66JRXnpfl3MNXiTB99rSUDbCD_2La2fXslrWyJRbv8xBj-YNca8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"352\" height=\"143\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOffline Distillation. Image from \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2206.12005.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003earXiv\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"online-distillation\"\u003e\u003cstrong\u003eOnline Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eOnline distillation is an end-to-end technique in which the student and teacher models are trained simultaneously. In other words, the teacher model is continuously updated with new datasets, and the student model is trained to reflect these changes.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/7frHJRuPtc3-RaxfVjJ9xbcwQYKJU4cQPfxy5bVy-HAtRD41bnGLVyU7Lavq2w-zf-q5DnXUmG9AJYE5C9bJHIqxb7VyalJSSGs53kRn3OCzyC4v0GswSqPRbFCBuVcMEw0w2s5gENLvfXjaAWAIJ5w\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"382\" height=\"132\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOnline Distillation. Image from \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2206.12005.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003earXiv\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"self-distillation\"\u003e\u003cstrong\u003eSelf-Distillation\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eSelf-distillation involves using the same networks for the teacher and student models. In this case, a model learns from itself, meaning knowledge from the deeper layers of the model is continuously distilled into its shallow section. This method solves the conventional limitations experienced when either online or offline techniques are used—the discrepancy in accuracy between teacher and student models.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Rh58RzWLGycQSarK3JqZjpQJ15vIqOVTf-MnpG4bd7b3ISaoQwIQZIntnkQss7FpRt-hqdVpovwD-GkpPaNwgMcMmMIxOBW3i5dUUxLDZZVa6xibarSOUA2d8LIUFnCKjl-ceq94w7DOTpNXUgfm-ec\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"264\" height=\"191\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSelf-Distillation. Image from \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2206.12005.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cspan class=\"underline\" style=\"white-space: pre-wrap;\"\u003earXiv\u003c/span\u003e\u003c/u\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"limitations-of-model-distillation\"\u003e\u003cstrong\u003eLimitations of Model Distillation\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eHaving looked at the benefits of model distillation, it’s important to discuss its limitations. While model distillation is a powerful knowledge transfer approach between models in different computational environments, it still suffers from various challenges:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe technical complexities of the distillation process\u003c/li\u003e\u003cli\u003eDifficulty in multi-task learning\u003c/li\u003e\u003cli\u003eThe student model is limited by the teacher\u0026nbsp;\u003c/li\u003e\u003cli\u003ePotential loss of information during the distillation process\u003c/li\u003e\u003cli\u003eLimited applicability on existing proprietary models\u003c/li\u003e\u003c/ul\u003e\u003ch1 id=\"final-thoughts-on-model-distillation\"\u003e\u003cstrong\u003eFinal Thoughts on Model Distillation\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe AI models currently being rolled out are cumbersome and certainly not ideal for deployment straight from the lab. Model distillation provides an alternative model compression mechanism to train lightweight models with comparably lesser computational demands from these heavyweight teacher models. In the realm of artificial intelligence, this model training approach has proven helpful in bridging the gap between limited resources and high-performance capabilities of various AI models. \u003ca href=\"https://labelbox.com/guides/end-to-end-workflow-with-model-distillation-for-computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModel distillation has limitless potential ahead as AI model training advances.\u003c/a\u003e\u003c/p\u003e\u003cp\u003eLabelbox aids in model distillation by providing a robust platform for generating high-quality labeled datasets, which are essential for both training the teacher model and fine-tuning the student model. With support for various data types and comprehensive management features, Quantumworks Lab also helps organize and version-control datasets, streamlining the process of integrating labeled data into the distillation pipeline. \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eTry Quantumworks Lab for free\u003c/u\u003e\u003c/a\u003e to enhance the efficiency and effectiveness of the distillation process.\u003c/p\u003e","comment_id":"6628249bd733f0000145b209","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/Model-Distillation.png","featured":false,"visibility":"public","created_at":"2024-04-23T21:14:03.000+00:00","updated_at":"2024-10-02T23:00:01.000+00:00","published_at":"2024-01-15T21:15:00.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"tags":[],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/model-distillation/","excerpt":"AI models are increasingly getting bigger with the increase in training data and the number of parameters. For instance, the latest Open AI’s GPT-4 model is estimated to have about 1.76 trillion parameters and terabytes of training corpus. Whether training large language models (LLMs) or neural networks, the main goal remains: to train using as much data as possible. \n\nWhile training from diverse data and increasing the number of parameters produces powerful models, real-world application become","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"What is Model Distillation?","meta_description":"Model distillation compresses large models for deployment, transferring knowledge from a teacher to a smaller student model efficiently.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"661ece5e8d5c4a00014062bb","uuid":"71559364-03fe-4b31-8828-727996c3e49a","title":"How to Implement Reinforcement Learning from AI Feedback (RLAIF)","slug":"reinforcement-learning-from-ai-feedback-rlaif","html":"\u003cp\u003eThe AI revolution is an unstoppable wave, with unique and more capable solutions being rolled out almost every month. These rapid developments, especially around \u003ca href=\"https://labelbox.com/usecases/large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003elarge language models (LLMs)\u003c/u\u003e\u003c/a\u003e, have been made possible by aligning models with human preferences. Reinforcement Learning from AI Feedback (RLAIF) is one of the ways of ensuring such alignments. User feedback has been incorporated into emerging AI models to improve their quality and usefulness. As a result, we have seen more capable models and AI solutions, particularly now that LLM research and development is at its peak. \u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eReinforcement Learning from Human Feedback (RLHF)\u003c/u\u003e\u003c/a\u003e emerged as a powerful method of improving the safety and objectivity of these language models. However, as highlighted in our \u003ca href=\"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eprevious article\u003c/u\u003e\u003c/a\u003e, this approach is confronted with the challenges of bias and subjectivity, cost, and delays. RLAIF offers a solution for these challenges without compromising the quality of these models.\u003c/p\u003e\u003cp\u003eAt a time when AI industry leaders are moving with speed to train and roll out superior language models, you cannot afford to be slow or stuck with costly training methodologies. This hassle, fortunately, has become a thing of the past, thanks to RLAIF. The RLAIF process is similar to RLHF except for the replacement of the human agents with an AI agent in the training and fine-tuning phases. In doing so, we can achieve performant and unbiased models within the shortest time, while saving on cost.\u0026nbsp;\u003c/p\u003e\u003cp\u003eRead on to understand how the AI agent is introduced into the loop to supplement human agents and expedite LLM training using the RLAIF process.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"what-is-reinforcement-learning-from-ai-feedback-rlaif\"\u003e\u003cstrong\u003eWhat is Reinforcement learning from AI feedback (RLAIF)?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eReinforcement learning from AI feedback (RLAIF) is a recent LLM training approach that integrates feedback from other AI models with Reinforcement Learning (RL) algorithms. It augments the RLHF process by addressing challenges introduced by human agents in the modeling lifecycle.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://openreview.net/forum?id=AAxIs3D2ZZ\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eA study by Google\u003c/u\u003e\u003c/a\u003e identified the need to scale RLHF with AI feedback as gathering the dataset of human preferences was not only resource-intensive but also time-consuming. Also, human preference data could be problematic due to its narrowed scope and biased sampling. In a situation where, let's say, 50 human annotators review and label the feedback data used in RLHF, the model is likely to be biased. This bias emerges from using just a subset of preferences to mirror a diverse global population. As a result, the model will behave as dictated by these 50 individuals. RLAIF solves these challenges by ensuring we train our model with diverse preference datasets whose annotation is free from human bias.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eRLAIF starts by generating a reward model from another off-the-shelf AI model. The responses of this reward model are primed to be helpful and harmless by following various principles of \u003ca href=\"https://arxiv.org/abs/2212.08073?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eConstitutional AI\u003c/u\u003e\u003c/a\u003e. In training a performant model using AI feedback, the synergy between safety requirements and elimination of constraints becomes not just desirable but indispensable. Therefore, as a contribution to the progression of RLAIF, \u003ca href=\"https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAnthropic came up with the concept of Constitutional AI\u003c/u\u003e\u003c/a\u003e, which has been widely adopted across the industry. The Constitution complements the RLAIF process by ensuring that the AI feedback model adheres to ethical and safety standards. Generating a dataset of ranked preferences for training a preference model (the reward model) in the RLAIF life cycle is guided by these standards.\u003c/p\u003e\u003cp\u003eIn short, RLAIF involves training LLMs using rewards provided by a preference model as guided by an AI feedback agent. Unlike RLHF, RLAIF takes in non-binary feedback pairs generated autonomously by a Feedback Model with reference to a Constitution. In doing so, it eliminates the complexities caused by human feedback in the model training process.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"what-are-the-challenges-that-rlaif-solves\"\u003e\u003cstrong\u003eWhat are the challenges that RLAIF solves?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eRLAIF came to light in response to the shortcomings of RLHF. Although RLHF was a great development towards aligning AI models with human needs and preferences, introducing human evaluators into the training loop came at a cost. The training and fine-tuning processes got slower, costly, and riddled with biases. The most critical question to answer is, does RLAIF sufficiently solve these problems? The answer is yes. Whether training large language models commercially or for research, RLAIF emerges as an outstanding technique. The Google study terms RLAIF as a game changer for eliminating human involvement challenges and maintaining the model’s integrity and accuracy.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eRLAIF solves the challenge of absolute reliance on human judgment as learning signals by training a preference model from AI-generated feedback. In the RLHF life cycle, the resultant AI model could adopt the human evaluator’s bias during training. This challenge has been eliminated by replacing human feedback with AI feedback in the RLAIF process. Since RLAIF involves an AI model training another AI model, the process bottlenecks associated with feedback collection and dataset labeling have been eliminated. Instead, AI feedback agents have guaranteed the autonomous generation of large and high-quality training datasets. Due to the size and accuracy of these datasets and the autonomy of the RLAIF process, the resultant AI models have achieved unmatched scalability and higher performances.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eFinally, the challenge of model drifting as a result of the Proximal Policy Optimizer algorithm exploiting a human-generated reward model is eliminated in RLAIF. In any supervised learning scenario, a fundamental training goal is for the resultant LLM to stay within the base model while improving its responses. As \u003ca href=\"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ewe covered in our RLHF post\u003c/u\u003e\u003c/a\u003e, a model’s stability is enforced by PPO fine-tuning and the Kullback-Leibler penalty. Drifts from the base model are limited in RLAIF, leading to better-performing AI models. Therefore, we can say that most of the model training challenges stemming from human feedback in the RL process have significantly reduced.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"how-is-rlaif-implemented\"\u003e\u003cstrong\u003eHow is RLAIF Implemented?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe RLAIF process is an iterative reinforcement learning approach that starts with a skewed RLHF model and ends up with a functional Supervised Learning for Constitutional AI (SL-CAI) model that is harmless, ethical, and helpful. Depending on the scope of the task, RLAIF can be achieved through a series of LLM fine-tuning and monitoring processes. The process can be divided into five main steps:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cu\u003eGenerating Revisions from Biased RLHF Mode\u003c/u\u003e\u003c/li\u003e\u003cli\u003e\u003cu\u003eFine-tuning a SL-CAI Model with Revisions\u003c/u\u003e\u003c/li\u003e\u003cli\u003e\u003cu\u003eGenerating Harmlessness Preference Dataset\u003c/u\u003e\u003c/li\u003e\u003cli\u003e\u003cu\u003eTraining Preference Model\u003c/u\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cu\u003eReinforcement Learning With Proximal Policy Optimization\u0026nbsp;\u003c/u\u003e\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"step-1-generating-revisions-from-helpful-rlhf-model\"\u003e\u003cstrong\u003eStep 1: Generating Revisions from Helpful RLHF Model\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe RLAIF process starts with prompting a shrink-wrapped and helpful RLHF model, keeping in mind that it would generate harmful or biased responses. The RHLF model is then made aware of the AI Constitution and asked to critique the harmful responses following various principles of the Constitution. This process is iterated using random principles of the Constitution till a harmless and non-evasive final revision is obtained. The set of prompts and final revisions are then carried to the next phase of the RLAIF process as referential data points.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"step-2-fine-tuning-a-sl-cai-model-with-revisions\"\u003e\u003cstrong\u003eStep 2: Fine-tuning a SL-CAI Model with Revisions\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIn the second step of the RLAIF process, a model called Supervised Learning for Constitutional AI (SL-CAI) is created and fine-tuned. The process starts with a pre-trained language model, which is then passed through a fine-tuning process using the final revision and prompt dataset from the previous step. Fine-tuning in this stage is done conventionally using the final revisions generated from the helpful RLHF following constitutional principles rather than human-generated feedback. The fine-tuning process should be thorough since the resultant SL-CAI model serves two purposes: as the Response Model for the training Preference Model and as the RL policy trained during the final stage of RLAIF.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/sRiwfifteox2bHvNv7awn86tKcbID8Z6SPcuvdr07baB8UAI6opcgh4mpYQXmZtXf5mMCuc36bfZtQKp9aaM_u2w3oJRHnXV7xqhYtQ-HQYXd6uR_Ghz_WajJadznTYquiIswJlM0rU1qmpxzm_bgWI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"720\" height=\"184\"\u003e\u003c/figure\u003e\u003cp\u003eFine-tuning an SL-CAI model with Response, Critique, and Revision from \u003ca href=\"https://arxiv.org/abs/2212.08073?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003earXiv\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"step-3-generating-harmlessness-dataset\"\u003e\u003cstrong\u003eStep 3: Generating Harmlessness Dataset\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe harmlessness dataset is generated in this phase through a combination of prompting and fine-tuning the SL-CAI model obtained in step 2 above. Instead of human feedback, AI agents' feedback is used to fine-tune the SL-CAI model. This step is the basis of the differences between RLAIF and RLHF. The fine-tuned SL-CAI model takes the harmful prompts from step 1 above and generates two responses for each. It then invokes the principles of Constitutional AI to come up with a Feedback Model from the model fine-tuned in step 2 above. This Feedback model evaluates the response pairs generated to create a dataset of preferences. It then calculates the probabilities and normalizes the dataset to assign each response a score. The responses with the highest score are then filtered and bundled as the harmlessness dataset.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/_SJMy7d03kEjkGqjc-VseOoW_pyFx8WmyYTul28WuY4gyox5gozcanIPiX1WxtZeJumqmLc1O1AA0juDqyDwkUxcYLTRouxHSUzOh9SHL07hHSsR3l49Bq7KtTQOTMShEz5hCaWzviOvkBZZS9VeF7E\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"844\" height=\"540\"\u003e\u003c/figure\u003e\u003cp\u003eA step-by-step process of generating a harmlessness dataset from \u003ca href=\"https://www.assemblyai.com/blog/how-reinforcement-learning-from-ai-feedback-works/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAssemblyAI\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"step-4-training-a-preference-model\"\u003e\u003cstrong\u003eStep 4: Training a Preference Model\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eA Preference Model (PM) similar to a reward model in RLHF is trained in this step of RLAIF using the harmlessness dataset obtained in step 3 above. After this point, the RLAIF processes are pretty similar to those of RLHF. For instance, like the reward model in RLHF, Preference Model training starts with pretraining and undergoes incremental fine-tuning till it is deemed fit for use. This approach is considered efficient as it uses less data to produce a performant model. A stable Preference Model that uses comparison data from the harmlessness dataset to assign a preference score to any prompt-response pair is obtained from this step of RLAIF.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"step-5-reinforcement-learning\"\u003e\u003cstrong\u003eStep 5: Reinforcement Learning\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWith the Preference Model at hand, the last step of RLAIF involves Reinforcement Learning on the SL-CAI model from step 1. A Proximal Policy Optimization (PPO) algorithm is applied to train and stabilize the SL-CAI model. The algorithm optimizes the RL policy’s mappings and limits the exploitation of the Preference Model as a reward signal. Any unusual model drift during RL is handled by the Kullback-Leibler (KL) Divergence penalty. In the end, we obtain a reinforcement learning AI model that is not only harmless and helpful but also scalable.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/m6dpBXj4vBNfG7gStsoVFDg0QMrtnFiY-upSyX6Exowa8we3eG2vI2FlVKUyyxH9KLvpVnSVh9_XOKYrYyxMk8V0yQ6YymLNZSZiYOVHwHB9SL5hEUKVTnDMsJ5UB7j1oPxi5oa-mKYkf-uFIIl7b78\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1180\" height=\"658\"\u003e\u003c/figure\u003e\u003cp\u003eA side-by-side diagrammatic comparison of the phases of RLAIF and RLHF from \u003ca href=\"https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eDeci AI\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"final-thoughts-on-rlaif\"\u003e\u003cstrong\u003eFinal thoughts on RLAIF\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eRLAIF is a game changing RL methodology with notable impacts on the quality and utility of large language models. It is an industry best practice that continues to mature. The paradigm shift to AI-generated feedback as proponents of reinforcement learning has enhanced the accuracy and speed of training AI models while solving the cost complexities. The Constitution – a set of principles for ensuring ethics, safety, and quality of models, is undoubtedly the cornerstone of RLAIF. It guides all decision-making processes, including the assignment of preference scores. In short, RLAIF is a promising RL process with limitless ethical and technical potential in the evolution of large language models. Quantumworks Lab is a complete solution combining the best tools and fully managed services for reinforcement learning from human feedback (RLHF) and LLM evaluation. \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eTry the platform for free\u003c/u\u003e\u003c/a\u003e to see it for yourself.\u003c/p\u003e","comment_id":"661ece5e8d5c4a00014062bb","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/RLAIF.png","featured":false,"visibility":"public","created_at":"2024-04-16T19:15:42.000+00:00","updated_at":"2024-04-16T19:46:50.000+00:00","published_at":"2024-01-15T19:25:00.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/reinforcement-learning-from-ai-feedback-rlaif/","authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"tags":[],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/reinforcement-learning-from-ai-feedback-rlaif/","excerpt":"The AI revolution is an unstoppable wave, with unique and more capable solutions being rolled out almost every month. These rapid developments, especially around large language models (LLMs), have been made possible by aligning models with human preferences. Reinforcement Learning from AI Feedback (RLAIF) is one of the ways of ensuring such alignments. User feedback has been incorporated into emerging AI models to improve their quality and usefulness. As a result, we have seen more capable model","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to Implement Reinforcement Learning from AI Feedback (RLAIF)","meta_description":"Learn how AI is integrated to supplement human agents and expedite LLM training using the Reinforcement Learning from AI Feedback (RLAIF) process.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"661577554f0096000190d872","uuid":"724b64ec-ebdb-4c3f-a6a3-dc485e867fa2","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","slug":"how-to-implement-reinforcement-learning-from-human-feedback-rlhf","html":"\u003cp\u003eThe Artificial Intelligence (AI) revolution has been brought to reality with the development of systems and solutions that align with human values and preferences. Reinforcement Learning from Human Feedback (RLHF) is one such example of a system that has transformed model training and improved the accuracy and applicability of AI applications.\u0026nbsp;\u003c/p\u003e\u003cp\u003eImplementing RLHF presents a promising avenue for enhancing AI systems with human guidance. RLHF has been used to develop impressive, human-like conversational bots, such as OpenAI’s ChatGPT. While this model training technique is still under development, its application is widespread, and is the cornerstone of large language models (LLMs).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eRLHF is an extension of \u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eReinforcement Learning (RL)\u003c/u\u003e\u003c/a\u003e, a reward and punishment-based training technique for AI models. The only difference from the other RL techniques lies in the introduction of human feedback to ensure that the resultant models behave in safe, ethical, and desirable ways. Instead of relying on predefined rewards, RLHF allows human users to interactively provide feedback to the model in the form of corrections, ratings, and preferences. The feedback is taken to train a reward model, which is then used to fine-tune the target model using a reinforcement learning algorithm.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"human-in-the-loop-as-rlhf-backbone\"\u003e\u003cstrong\u003eHuman-in-the-Loop as RLHF Backbone\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eHuman feedback is fundamental to RLHF and distinguishes RLHF from other supervised RL techniques. Since most LLMs are trained on a large corpus with diverse contexts and domains, their applicability to individual users are limited. To make these models helpful, harmless, and context-specific, a supervised machine learning technique called human-in-the-loop (HITL) is applied. HITL involves introducing human evaluators in the model-training lifecycle to show the system how to generate human-preferred content. It is considered the backbone of RLHF as it creates a continuous feedback loop where human input is integrated into the AI system to improve its usability in various contexts. The goal of human feedback is to augment a reward system for fine-tuning LLMs beyond sub-optimal generalized performance. This feedback data aligns LLMs with complex human values and preferences.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eAlthough RLHF is a powerful model training technique, it can be slow and costly to implement and maintain because of its reliance on human feedback. Without the right tools and systems in-place, collecting and annotating human feedback for pre-training and fine-tuning datasets can be an expensive and time-consuming proposition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLet’s dive into the current approach being used by AI leaders developing new LLMs, leveraging RLHF to incrementally fine-tune base LLMs using human-provided reward signals and automated inputs.\u003c/p\u003e\u003ch1 id=\"the-rlhf-process-a-step-by-step-guide\"\u003e\u003cstrong\u003eThe RLHF Process: A Step-by-Step Guide\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWhile RLHF is a complex concept coupled with multiple model-training processes and tools, its implementation can be broken down into four straightforward steps:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePre-training a language model\u003c/li\u003e\u003cli\u003eSupervised Fine-tuning\u003c/li\u003e\u003cli\u003eTraining a reward model using Human Feedback\u003c/li\u003e\u003cli\u003eFine-tuning the RL policy with the Reward Model\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"step-1-pre-training-a-language-model\"\u003e\u003cstrong\u003eStep 1: Pre-training a Language Model\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003ePre-training a language model is the foundation of the RLHF process. It involves coming up with a base model through an end-to-end training or simply selecting a pre-trained language model to begin with. Depending on the approach taken, pretraining is the most tedious, time-consuming, and resource-intensive phase of RLHF. Since training a language model from scratch complicates the RLHF process even more, choosing one of the many pre-trained models is recommended.\u003c/p\u003e\u003cp\u003eSimply put, RLHF can be seen as a way of unlocking the capabilities of the existing pre-trained models. For example, a \u003ca href=\"https://labelbox.com/guides/how-to-train-a-chatbot/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econversational AI chatbot\u003c/u\u003e\u003c/a\u003e can be developed from existing mid-sized models like\u003ca href=\"https://labelbox.com/product/model/foundry-models/llama-3-1-405b/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e \u003cu\u003eLLaMA\u003c/u\u003e\u003c/a\u003e, which has 65 billion parameters, or the extensive \u003ca href=\"https://labelbox.com/product/model/foundry-models/gpt4/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGPT-4 model\u003c/u\u003e\u003c/a\u003e, which has 1.76 trillion parameters. Selecting or pre-training the right base model depends on the available resources and task at hand, as there is no universally best model to kickstart RLHF training.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/ohLISJBxZIz4eLtcCos26LxM9P_IQY8X0gWQJWUbvu9PQkBpIBULSsGTNaZ5YYt6zwePlXcVCHODTiUKDh2A1bBj1TVI_Mi_8QRMUDhmy_CQYEHAjWheRnoqENVkIvdtPRqiR06-lwUuZdwYmygyr7o\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1400\" height=\"1046\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003ePretrained LM as a starting point: Image from \u003c/em\u003e\u003ca href=\"https://huggingface.co/blog/rlhf?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eHuggingface\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"step-2-supervised-fine-tuning\"\u003e\u003cstrong\u003eStep 2: Supervised Fine-tuning\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe base model pre-trained or selected in step 1 above has the responses that users may want, but lacks the context and capability to generate them in formats expected by users. Therefore, before reinforcement learning, supervised fine-tuning (SFT) is applied on the pre-trained model. The goal of SFT is to prime the model to respond appropriately to different user prompts. It uses \u003ca href=\"https://labelbox.com/solutions/supervised-fine-tuning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003esupervised learning\u003c/u\u003e\u003c/a\u003e in which human annotators point the existing model to specific desired patterns through prompting. It is a significant starting point for RLHF implementation. Prompting guides the model towards the desired output per training data. It is important to note that the SFT phase focuses on optimizing the base model’s parameters by distilling the original language model on context clues for the target criteria. For that reason, having a model that responds well to diverse instructions is foundational to the RLHF process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe goal of the SFT phase of the RLHF process is to prime the base model to understand user goals, language patterns, and contexts. It exposes the model to diverse linguistic patterns that enable it to generate coherent and contextually appropriate text. The human trainer guides the base model to iterate numerous examples of human-preferred outputs. Throughout this process, the model learns various relationships between words and concepts and their appropriate usage. This text-based machine learning approach is the building block of \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eNatural Language Processing (NLP)\u003c/u\u003e\u003c/a\u003e. However, at this point, the model still lacks the human touch and preferences. Additional data is needed to bring this human-like feel to the model. This is where human feedback comes in.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn the next phase of RLHF implementation, a reward model is developed from the pre-trained language model blueprint to integrate human preferences into the system.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"step-3-training-a-reward-model-using-human-feedback\"\u003e\u003cstrong\u003eStep 3: Training a Reward Model Using Human Feedback\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eA reward model (RM) is key for implementing RLHF. In an ideal scenario, we could just fine-tune the base model using RLHF techniques until we achieve a domain-specific model. However, this approach would need large chunks of training samples directly fed into the base model by the human annotator. As a result, this could be slow, expensive, and counterproductive. The best way to overcome these shortcomings is by training a reward model and introducing it in the RL loop.\u003c/p\u003e\u003cp\u003eA reward model’s precept is to map the input text with a scalar reward value in ways humans would. It is an alignment tool that evaluates the base model’s output and returns a reward signal, which is then used by the main LLM to optimize its parameters.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHuman annotators do the heavy lifting in this phase of RLHF implementation. They generate the training dataset (prompt answer pairs) and rank them according to their preferences before feeding them to the model. The reward model then has to align its ‘rewarding’ system with the patterns of such samples. Nonetheless, this process is subjective since the human perception reinforced by annotators could be biased. As such, there is a need for diversity when creating prompt and reward pairs.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn practice, annotation prediction would be the most straightforward way to build a reward model. The model could be curated to provide a rating score and determine which output aligns more closely with human preferences. It then rewards the more appropriate output. The reward and prompt pairs are then used to train the reward model to associate specific outputs with reward values.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHuman feedback is then used to refine the reward model as it matures. For instance, users can rank the AI output with either a thumbs-up or thumbs-down feedback. This feedback data gives the reward model insights into human preference. It can then automatically rank the RL agent’s output without human intervention while iteratively learning from such feedback to better imitate humans.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Jn7XN-gMuM5s2KrzOOTokotEXcNXArdG48Lk1N_f6_4OWQ5EKyK_Dn1WtPcIxX7e3ybQFe5xlgMPmlG48G5o8WmUyH4fB9t4lsDgk2OplIPlnFqsdqPXyCn1AeufgT35-oZUxgl3jTcSCwEpPmXiQD0\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1000\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eReward model training lifecycle \u003c/em\u003e\u003ca href=\"https://bdtechtalks.com/2023/01/16/what-is-rlhf/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eTechTalks\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"step-4-fine-tuning-the-rl-policy-with-the-reward-model\"\u003e\u003cstrong\u003eStep 4: Fine-Tuning the RL Policy with the Reward Model\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/blog/using-reinforcement-learning-from-human-feedback-to-fine-tune-large-language-models/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eFine-tuning\u003c/u\u003e\u003c/a\u003e is one of the ways to unlock the potential of LLMs. It involves upskilling the base model for specific tasks and adapting it to more specialized domains. This is the last phase of RLHF implementation. It involves creating a feedback loop to train and fine-tune the RL policy (a copy of the original LLM) with the reward model trained in step 2 above.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe RL policy interacts with the reward model by taking reward signals to adjust its behavior. It then sends its output to the reward model for evaluation. The output is evaluated, and a reward score is sent back to the RL policy. Through the RM’s reward score, the RL policy can generate responses it deems human-preferable.\u003c/p\u003e\u003cp\u003eA policy-gradient RL algorithm called \u003ca href=\"https://arxiv.org/abs/1707.06347?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eProximal Policy Optimization (PPO)\u003c/u\u003e\u003c/a\u003e and the \u003ca href=\"https://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eKullback-Leibler (KL) Divergence\u003c/u\u003e\u003c/a\u003e are the basis of this RLHF phase. The RL policy is optimized using PPO, which balances exploitation and exploration during training. At this point, some base LLM parameters are frozen because fine-tuning, let’s say, 65 billion parameters would be practically slow.\u0026nbsp;\u003c/p\u003e\u003cp\u003ePPO fine-tuning improves training stability by limiting changes made to the policy at each training epoch. However, given the chance, the PPO algorithm might exploit the imperfections of a reward model and generate hogwash output. To counter such exploits, the KL penalty is introduced.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn RLHF, Kullback-Leibler (KL) Divergence measures the difference between a reference distribution representing the most human-aligned response and the RL policy’s current responses. Simply put, it penalizes the RL policy from substantially veering off the base model with each training batch.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/5hUDv7nrLlXTVmj5JzKfa3BS-kXNq6_IG2hg1i2MCeUxcKlqLAvTAjNpE1x63KoFIZHT81J14XVeyNgfS7QgAfimPPF9ZTKgAHd7t2UogMUogdG93_OJUm--3XOb4IN3rOor1_12Bz7qczWUwPYtCLE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1400\" height=\"695\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eRL policy fine-tuning using the reward model and the PPO from \u003c/em\u003e\u003ca href=\"https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093?permalink_comment_id=4519136\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eGitHub Gist\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003eFine-tuning with the reward model discourages inappropriate responses by punishing those with low rewards. Since such low-reward outputs are unlikely to be repeated, the language model iteratively learns and produces outputs that align closely with human expectations– this is the prime of RLHF.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"rlhf-use-cases\"\u003e\u003cstrong\u003eRLHF Use Cases\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eRLHF has proven useful in developing models used in healthcare, technology, banking, and finance, among other fields, from pre-trained models like GPT-4 and LLaMA. OpenAI’s InstructGPT, Anthropic’s Claude and Google’s Gemini are some of the successful applications of RLHF.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI’s InstructGPT –\u003c/strong\u003e In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. \u003ca href=\"https://openai.com/research/instruction-following?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eOpenAI used the prompts submitted by its customers to the API as human feedback\u003c/u\u003e\u003c/a\u003e. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAnthropic’s Claude –\u003c/strong\u003e RLHF has also been applied in training\u003ca href=\"https://www.anthropic.com/claude?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003cu\u003e Claude,\u003c/u\u003e\u003c/a\u003e Anthropic’s next-gen AI, to be more helpful. The AI assistant depends on human feedback and AI Constitution to align its responses with human values and preferences.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle’s Gemini –\u003c/strong\u003e Google Deepmind introduced Gemini Ultra, a model enhanced through RLHF. This powerful model \u003ca href=\"https://deepmind.google/technologies/gemini/?ref=labelbox-guides.ghost.io#gemini-1.0\"\u003e\u003cu\u003eoutperformed GPT-4 on several benchmarks, including reasoning and math\u003c/u\u003e\u003c/a\u003e, because it was primed to generate helpful and harmless responses using RLHF.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003ch1 id=\"challenges-of-rlhf\"\u003e\u003cstrong\u003eChallenges of RLHF\u0026nbsp;\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWhile RLHF emerged as a groundbreaking AI model training technique, its implementation is not always a straightforward process. Some of the limitations of RLHF are:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eShortcomings of the human agents –\u003c/strong\u003e Introducing human agents into the training cycle comes with issues of reliance, scalability, and bias (divergence from the expected outcome). Ineffective human feedback may lead to suboptimal performance and create biases, leading to skewed learning.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTime and resource complexities –\u003c/strong\u003e Scaling the models to handle more complex tasks could also be time-consuming and resource-intensive with the introduction of human agents in the training cycle.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHowever, the benefits outweigh the setup and maintenance costs of implementing RHF. The challenges can be mitigated by balancing feedback, diversifying the perspectives of human annotators, and evaluating performance of the model from time to time. Another method used by companies like Anthropic and Google to mitigate the cost and timeframe of gathering the necessary feedback is RLAIF, or reinforcement learning from AI feedback. By using another LLM to augment or replace the volume of human feedback needed during the supervised fine-tuning stage, teams can move faster and even choose to focus human efforts on more expert-level, domain-specific evaluation tasks.\u003c/p\u003e\u003ch1 id=\"final-thoughts-on-reinforcement-learning-from-human-feedback-rlhf\"\u003e\u003cstrong\u003eFinal Thoughts on Reinforcement Learning from Human Feedback (RLHF)\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAs AI advances, RLHF ensures that the LLMs’ capabilities are aligned with complex human preferences, goals, and environments. RLHF has significantly revolutionized the subfield of NLP, specifically downstream LLM applications.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIt has pioneered the humanization of AI solutions, following feedback from users and the alignment of human annotators. We have seen that implementing RLHF takes a three-step process that starts with a pre-trained model and ends with fine-tuning the base model with a reward model trained from human feedback.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIntroducing humans in the training loop is the cornerstone of RLHF, although a balance is needed as slight inefficiencies could lead to biases and skew the model learning process. It is important to note that RLHF performance is only as good as the quality of human annotators and the human-generated text used for fine-tuning.\u0026nbsp;\u003c/p\u003e\u003ch1 id=\"get-started-with-rlhf-today\"\u003e\u003cstrong\u003eGet started with RLHF today \u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eLabelbox is a complete solution combining the best tools and fully managed services for RLHF and LLM evaluation. We ensure high quality, trustworthy, safe outputs with highly accurate datasets for instruction tuning, RLHF, and supervised fine-tuning. \u003cbr\u003e\u003cbr\u003eReady to experiment with implementing humans-in-the loop for your generative AI projects? Try out our \u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eon-demand expert labeling services\u003c/a\u003e with SMEs for your projects in any expert domain or popular languages.\u0026nbsp;\u003cbr\u003e\u003cbr\u003e Or want to directly find the right experts yourself? Quickly search and select your own team of expert AI labelers with \u003ca href=\"https://labelbox.com/services/alignerr-connect/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox's Alignerr Connect\u003c/a\u003e. \u003c/p\u003e","comment_id":"661577554f0096000190d872","feature_image":"https://labelbox-guides.ghost.io/content/images/2024/04/Frame-3697.png","featured":false,"visibility":"public","created_at":"2024-04-09T17:13:57.000+00:00","updated_at":"2024-11-20T23:41:32.000+00:00","published_at":"2024-01-08T17:33:00.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/","authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"tags":[],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":null,"url":"https://labelbox-guides.ghost.io/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/","excerpt":"The Artificial Intelligence (AI) revolution has been brought to reality with the development of systems and solutions that align with human values and preferences. Reinforcement Learning from Human Feedback (RLHF) is one such example of a system that has transformed model training and improved the accuracy and applicability of AI applications. \n\nImplementing RLHF presents a promising avenue for enhancing AI systems with human guidance. RLHF has been used to develop impressive, human-like convers","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to Implement Reinforcement Learning from Human Feedback","meta_description":"RLHF allows users to interactively provide model feedback with corrections, ratings, and preferences. Learn how to implement it with this guide.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"656e6d5472e557000141cf9d","uuid":"44473dbc-7bfa-43a0-a5d6-7e061877d134","title":"How to automatically ingest data from Databricks into Quantumworks Lab","slug":"how-to-automatically-ingest-data-from-databricks-into-labelbox","html":"\u003cp\u003eMoving data seamlessly through your MLOps pipeline is essential to building successful AI products. In this guide, learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cjp13xg9no\" title=\"Databricks Ingestion Pipeline Demo Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"506\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1.\u0026nbsp; Navigate to the \u003ca href=\"https://huggingface.co/spaces/Quantumworks Lab/databricks_upload?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003epipeline creator webpage\u003c/u\u003e\u003c/a\u003e and enter your Databricks domain. You can find this by going to your Databricks environment. The domain will be in the URL, so you can copy and paste it into the pipeline creator.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_1_Domain.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1734\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_1_Domain.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_1_Domain.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_1_Domain.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_1_Domain.png 1734w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eYour Databricks domain is in the URL whenever you access your Databricks environment.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e2. Now select the cloud environment that your Databricks workspace runs in. This information is usually also in the Databricks domain that you just copy/pasted.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3. If you don’t already have a Databricks API, create one by going to your Databricks domain. Go to the user tab in the top right, then go to \u003cstrong\u003eUser settings \u0026gt; Developer \u0026gt; Access Tokens\u003c/strong\u003e, and create an access token. Paste your Databricks API key into the pipeline creator.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_2_AccessToken.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1734\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_2_AccessToken.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_2_AccessToken.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_2_AccessToken.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_2_AccessToken.png 1734w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eYou can create an access token for the Databricks API by going to the user tab, then to User Settings \u0026gt; Developer \u0026gt; Access tokens.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e4. Next, you’ll need your Quantumworks Lab API key. If you don’t already have one, you can create one from within your Quantumworks Lab environment by going to \u003cstrong\u003eWorkspace Settings \u0026gt; API \u0026gt; Create a new key\u003c/strong\u003e. Paste this key into the creator pipeline. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_3_LBAPIKey.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1734\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_3_LBAPIKey.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_3_LBAPIKey.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_3_LBAPIKey.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_3_LBAPIKey.png 1734w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eCreate an API key in Quantumworks Lab from the Workspace Settings section of your environment.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e5. Next, the pipeline creator will give you the option of making a new dataset or appending an existing one. Be sure to give the dataset a relevant name, as it will appear under than name within Quantumworks Lab Catalog once the dataset has been created and the data ingested from Databricks. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_4_NameDataset.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1738\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_4_NameDataset.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_4_NameDataset.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_4_NameDataset.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_4_NameDataset.png 1738w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eName your dataset within the pipeline creator webpage.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e6. Select a cluster from within your Databricks environment on the pipeline creator page.\u0026nbsp;\u003c/p\u003e\u003cp\u003e7. Once the cluster is ready, you’ll see the option to select a run frequency, or the cadence with which the workflow is going to execute. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_5_RunFrequency.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"713\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_5_RunFrequency.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_5_RunFrequency.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_5_RunFrequency.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eChoose how often you want this workflow to run.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e8. Next, choose the relevant table and database in Databricks from which you want to pull data. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_6_SelectTable.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"713\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_6_SelectTable.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_6_SelectTable.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_6_SelectTable.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eChoose the database and table you want to pull data from.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e9. The page will show you a sample of data rows within the selected table. The row data column signifies the URL by which you want to pool the data from. This can either be a public URL or objects hosted within your cloud storage.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_7_RowData.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1738\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_7_RowData.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_7_RowData.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_7_RowData.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_7_RowData.png 1738w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eA sample of the data rows in the selected dataset will appear on the pipeline creator page.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e10. Choose the data row column pointing to the object that you want to render. You also have an additional option of choosing the global key, which will specify the unique identifier assigned to each data row once they’re ingested into Labelbox.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_8_SelectRowDataGlobalKey.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"753\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_8_SelectRowDataGlobalKey.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_8_SelectRowDataGlobalKey.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_8_SelectRowDataGlobalKey.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eSelect your row data column and if needed, select global key.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e11. Click on \u003cstrong\u003eDeploy pipeline. \u003c/strong\u003eAfter executing for a few seconds, you’ll see a confirmation that your pipeline has been deployed. Now you can navigate to your Databricks environment, go to the workflows tab on the left, and see the new upload workflow that you’ve just created. Once the workflow has run, you’ll be able to see the ingested data in Quantumworks Lab Catalog.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_8_NewUploadWorkflow.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1738\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/DBIngestionPipeline_8_NewUploadWorkflow.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/DBIngestionPipeline_8_NewUploadWorkflow.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/DBIngestionPipeline_8_NewUploadWorkflow.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/DBIngestionPipeline_8_NewUploadWorkflow.png 1738w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eSee the new upload workflow within your Databricks environment by navigating to the Workflows tab.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYour new workflow will now ingest the specified data into Quantumworks Lab at the cadence you chose. \u003ca href=\"https://labelbox.com/blog/seamlessly-integrate-databricks-data-pipelines-with-labelbox/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eRead this blog post\u003c/u\u003e\u003c/a\u003e to learn more about how you can integrate Databricks and Quantumworks Lab into a seamless data engine for AI.\u0026nbsp;\u003c/p\u003e","comment_id":"656e6d5472e557000141cf9d","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/12/DatabricksLabelboxIngestionPipeline_Guide_Header.png","featured":false,"visibility":"public","created_at":"2023-12-05T00:22:44.000+00:00","updated_at":"2023-12-05T17:38:22.000+00:00","published_at":"2023-12-05T17:38:22.000+00:00","custom_excerpt":"Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-automatically-ingest-data-from-databricks-into-labelbox/","excerpt":"Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"How to automatically ingest data from Databricks into Quantumworks Lab","og_description":"Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.","twitter_image":null,"twitter_title":"How to automatically ingest data from Databricks into Quantumworks Lab","twitter_description":"Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.","meta_title":"How to automatically ingest data from Databricks into Quantumworks Lab","meta_description":"Learn how you can leverage Quantumworks Lab’s Databricks pipeline creator to automatically ingest data from your Databricks domain into Quantumworks Lab for data exploration, curation, labeling, and much more.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6567b04d72e557000141cf3c","uuid":"fba5c2d3-75df-445a-87ea-ff02a1a28db0","title":"How to generate data for model comparison and RLHF","slug":"how-to-generate-data-for-model-comparison-and-rlhf","html":"\u003cp\u003eLarge language models (LLMs) have disrupted the way teams build and train intelligent applications. Trained on massive text datasets\u0026nbsp; containing millions or even billions of data points, LLMs have become increasingly important for various AI applications such as chatbots, personalized recommendation systems, and more. These models have shattered the barriers of what was once thought possible in natural language understanding and generation.\u003c/p\u003e\u003cp\u003eWhile large language models have enormous potential to revolutionize intelligent applications, effectively deploying them requires adapting LLMs to align with human preferences to mitigate risks. Successfully adopting LLMs involves continually fine-tuning and evaluating the model’s performance. This requires assessing subjective qualities like tone, fluency, and propriety through a combination of automated techniques and human-in-the-loop validation. \u003c/p\u003e\u003ch2 id=\"the-importance-of-human-preference-datasets-and-rlhf\"\u003eThe importance of human preference datasets and RLHF\u0026nbsp;\u003c/h2\u003e\u003cp\u003eAs LLMs rapidly advance in capability, properly directing them for the benefit of humanity should be a top priority. Human preference datasets and reinforcement learning from human feedback (RLHF) provide critical approaches for promoting the safe and ethical alignment of these influential AI systems.\u003c/p\u003e\u003cp\u003eA human preference dataset comprises numerous examples where humans indicate their preferences between model outputs, such as judging which content could be more harmful or which summary appears more accurate. By training models with this data, we can instill more ethical and beneficial behaviors aligned to human values.\u0026nbsp;\u003c/p\u003e\u003cp\u003eReinforcement learning from human feedback (RLHF) goes one step further by actively querying humans within a feedback loop to continuously improve models by correcting mistakes and reinforcing positive behaviors. You can learn more about the importance of RLHF in \u003ca href=\"https://labelbox.com/blog/rlhf-vs-rlaif/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ethis blog post.\u003c/u\u003e\u003c/a\u003e \u003c/p\u003e\u003ch2 id=\"where-does-labelbox-come-in\"\u003eWhere does Quantumworks Lab come in? \u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform that can help your team navigate the future of large language models with human-centric evaluation. To ensure trustworthy, reliable, and safe AI aligned with human values, Quantumworks Lab allows teams to generate high-quality data for alignment and confidently ship LLMs with human experts to validate model outcomes.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith the new LLM human preference editor, you can create human preference data for model comparison or RLHF (reinforcement learning with human feedback). You can compare model outputs side-by-side and select the most favorable model output on a conversational text thread by assigning the model output a classification. This editor solves two important problems that are critical to ensuring responsible AI aligned with human preferences:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eModel comparison: \u003c/strong\u003eConduct the evaluation and comparison of model configurations for a given use case and decide which one to pick.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRLHF: \u003c/strong\u003eCreate preference data for training a reward model for RLHF based on multiple outputs from a single model.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThe \u003ca href=\"https://docs.labelbox.com/docs/llm-human-preference?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003enew LLM human preference editor\u003c/a\u003e and the previously released \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM data generation editor\u003c/u\u003e\u003c/a\u003e support:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eMarkdown rendering and markdown editing capabilities\u0026nbsp;\u003c/li\u003e\u003cli\u003eThe ability to import model predictions and ground truth through model-assisted labeling\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWith Quantumworks Lab, you can tap into both reinforcement learning from human feedback (RLHF) with your own internal team and skillful data labeling services with expertise in RLHF, evaluation, and red teaming. This human-centered approach is the key to developing reliable, responsible AI systems scaled for your business.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-generate-data-for-model-comparison-and-rlhf\"\u003eSee it in action: How to generate data for model comparison and RLHF\u003c/h2\u003e\u003cp\u003e\u003cem\u003eWe recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?utm_keyword=Quantumworks Lab\u0026utm_source=organic\u0026utm_medium=website\u0026utm_campaign=boost\u0026\u0026attr=intercom\u0026referrer_url=https://connect.labelbox.co/\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/wckevgl1sc\" title=\"How to generate data for model comparison and RLHF - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eFor this tutorial, we will utilize conversational text data in Quantumworks Lab to train an AI assistant for a shopping app. This assistant will be designed to aid customers at various stages, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProduct discovery:\u003c/strong\u003e Help browse items and make recommendations based on the customer's needs and interests\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePre-purchase guidance\u003c/strong\u003e: Answer questions and provide details to assist in the buying decision process\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePost-purchase support:\u003c/strong\u003e Provide helpful information about orders, shipping, returns, or other purchase-related needs\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-1-data-upload-and-setup\"\u003eStep 1: Data upload and setup\u003c/h3\u003e\u003cp\u003eThe data that we will be using for this tutorial are three conversational text datasets based on the above scenarios:\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eConversation #1:\u003c/strong\u003e The customer has a budget in mind and is searching for an affordable vacuum cleaner option that meets their needs.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eConversation #2:\u003c/strong\u003e A customer's existing vacuum cleaner is having technical issues, so they require assistance troubleshooting the problems or finding a suitable replacement.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eConversation #3: \u003c/strong\u003eA customer wants gift recommendations for a family member and asks the shopping assistant bot what it would suggest based on the situation.\u003c/p\u003e\u003cp\u003eThe first step is to upload these conversations into Labelbox. To follow along with this tutorial, we’ve provided the 3 sample conversational text datasets in this \u003ca href=\"https://console.cloud.google.com/storage/browser/labelbox-developer-testing-assets/Pranoy;tab=objects?authuser=0\u0026prefix=\u0026forceOnObjectsSortingFiltering=false\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Cloud folder\u003c/u\u003e\u003c/a\u003e available for download.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dasjgw2aat\" title=\"How to generate data for model comparison and RLHF - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWhen inspecting the conversation JSON in VS code, you’ll notice it has 2 parts:\u0026nbsp;\u003c/p\u003e\u003col\u003e\u003cli\u003eThe conversation itself\u0026nbsp;\u003c/li\u003e\u003cli\u003eModel outputs - containing:\u0026nbsp;\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli\u003eTitle of the content (e.g “Response A”)\u0026nbsp;\u003c/li\u003e\u003cli\u003eThe actual content (e.g \"I have 2 options for you…”)\u003c/li\u003e\u003cli\u003eModel configuration (e.g “modelConfigName\": \"GPT-3.5 with temperature 0\") - this allows you to save the metadata around the models that are saved\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this JSON, the content is formatted in markdown since it is an important part of the experience when comparing detailed, nuanced conversations. \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e{\n      \"title\": \"Response A\",\n      \"content\": \"I have 2 options for you:\\n- The Dyson V15 [Product page](https://www.dyson.com/vacuum-cleaners/cordless/v15)\\n- The Shark Stratos [Product page](https://www.sharkclean.com/products/shark-stratos-cordless-vacuum-with-free-steam-mop-zidIZ862HB)\",\n      \"modelConfigName\": \"GPT-3.5 with temperature 0\"\n    }\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSince these data rows are stored in a cloud bucket, we can import them as a pairwise comparison dataset to Labelbox. You can learn more about how to upload a JSON to Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ethrough the UI\u003c/u\u003e\u003c/a\u003e or through a \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecloud storage integration\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-2-explore-and-curate-data-in-catalog\"\u003eStep 2: Explore and curate data in Catalog\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dm1sca15sk\" title=\"How to generate data for model comparison and RLHF - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve successfully uploaded the conversations to Quantumworks Lab, you’ll be able to view them in Catalog. Since we’ve imported them as a pairwise comparison dataset, you can click into each conversation and view the conversation and outputs. Filtering by metadata makes it easy to locate these datasets – for example, you can set up \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ea slice\u003c/u\u003e\u003c/a\u003e to capture all incoming comparison data in a single place.\u0026nbsp;\u003c/p\u003e\u003cp\u003eNot only can you click in to view the outputs, you can switch to \"markdown mode\" and see all of the information with necessary links and formatting in markdown.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-3-create-a-labeling-project-with-the-llm-human-preference-editor\"\u003eStep 3: Create a labeling project with the LLM Human Preference editor\u0026nbsp;\u003c/h3\u003e\u003cp\u003eNow that we have our conversation data in Catalog, we can create an ontology and a labeling project with the LLM Human Preference editor.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cwhf2v5o1p\" title=\"How to generate data for model comparison and RLHF - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eCreate an ontology\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can create our ontology. \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eOntologies\u003c/u\u003e\u003c/a\u003e can be reused across different projects and they are required for data labeling, model training, and evaluation.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cu\u003eTo create a new ontology:\u003c/u\u003e\u003c/p\u003e\u003cp\u003e1) Navigate to the \"Schema\" tab\u003c/p\u003e\u003cp\u003e2) Hit \"Create new ontology\"\u003c/p\u003e\u003cp\u003e3) Select the media type that you wish to work with — for this use case it would be \"Conversational text\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Give your ontology a name\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Add objects and classifications based on you use case\u003c/p\u003e\u003cp\u003eFor this use case, we’ll create two classifications:\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Choose the best response (Radio classification) with options as:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eResponse A\u0026nbsp;\u003c/li\u003e\u003cli\u003eResponse B\u0026nbsp;\u003c/li\u003e\u003cli\u003eTie\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe strongly recommend doing pairwise comparison, especially for more nuanced use cases where alignment using RLHF or robust evaluation can have a significant impact on the quality of the model.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Provide a reason for your choice (Free form text) \u003c/p\u003e\u003cp\u003eAfter this, you can save your ontology. \u003c/p\u003e\u003cp\u003eAfter creating an ontology, you can create a labeling project and begin labeling your data.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCreate a labeling project\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e1) Navigate to the Annotate tab\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Click \"Create new project\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Select the \"LLM human preference\" under LLM alignment\u003c/p\u003e\u003cp\u003e4) Name your project\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Select your quality mode between \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ebenchmark\u003c/u\u003e\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econsensus\u003c/u\u003e\u003c/a\u003e\u0026nbsp;\u003c/p\u003e\u003cp\u003e6) Save your project \u003c/p\u003e\u003cp\u003eAttach your previously created ontology to your new project to complete project setup.\u003c/p\u003e\u003ch3 id=\"step-4-import-model-predictions\"\u003eStep 4: Import model predictions\u0026nbsp;\u003c/h3\u003e\u003cp\u003eTo accelerate the labeling process, oftentimes you might want to leverage pre-labels or model predictions to automate the labeling process. This allows your team of labelers or an external team of experts to focus their valuable time on human review rather than spending time starting to label from scratch.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor this use case, we can import model predictions through Quantumworks Lab’s model-assisted labeling (MAL) to do exactly just that. With \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel-assisted labeling\u003c/u\u003e\u003c/a\u003e, you can import computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo demonstrate, we’ve uploaded a choice of Response B as a model-generated label on one of these data rows, which will show up in the editor as a default response. From there, in the editor, a human could verify whether Response B is in fact the best option and provide an explanation for it. \u003c/p\u003e\u003cp\u003eYou can follow this \u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/llm_asset_import/conversational_MAL_GT.ipynb?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e to upload MAL predictions.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-5-label-data\"\u003eStep 5: Label data\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/o279ezt54h\" title=\"How to generate data for model comparison and RLHF - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWith our data, ontology, and labeling project setup, we can begin reviewing the pre-labels and create ground truth data. You can select \"Start labeling\" and see both the conversation and the model outputs for comparison. The model outputs can be viewed both in markdown formatting and as raw text. \u003c/p\u003e\u003cp\u003eDepending on the conversation, you can select the appropriate response and provide a reason for your choice. You can continue to do this for all conversations and compare outputs to decide the best output based on human preference.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can learn more about the new LLM human preference editor in our \u003ca href=\"https://docs.labelbox.com/docs/llm-human-preference?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAs large language models continue to rapidly advance, maintaining reliable and ethical systems aligned to human values is essential. Techniques like human preference learning and reinforcement learning from human feedback are mechanisms for this.\u0026nbsp;\u003c/p\u003e\u003cp\u003eAs we build increasingly capable AI systems, maintaining human oversight is key. With Quantumworks Lab, you can enable reinforcement learning from human feedback (RLHF) by leveraging your internal team or external labeling services with specialized expertise in areas like evaluation and red teaming. This human-centered approach provides the ability to create reliable, responsible AI aligned with human values and tailored to your business use cases at scale. \u003c/p\u003e","comment_id":"6567b04d72e557000141cf3c","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-3091.png","featured":false,"visibility":"public","created_at":"2023-11-29T21:42:37.000+00:00","updated_at":"2024-01-17T19:06:23.000+00:00","published_at":"2023-12-01T14:36:04.000+00:00","custom_excerpt":"Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-generate-data-for-model-comparison-and-rlhf","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-generate-data-for-model-comparison-and-rlhf/","excerpt":"Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. ","reading_time":7,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-3091-1.png","og_title":"How to generate data for model comparison and RLHF","og_description":"Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-3091-2.png","twitter_title":"How to generate data for model comparison and RLHF","twitter_description":"Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. ","meta_title":"How to generate data for model comparison and RLHF","meta_description":"Learn how to generate human preference data for model comparison or RLHF (reinforcement learning with human feedback) with the new LLM human preference editor. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65523b444a21b900018ee9ba","uuid":"4e760591-7da7-43c2-af80-554fdb2ed601","title":"Detecting swimming pools with GPT4 Visual","slug":"detecting-swimming-pools-with-gpt4-visual","html":"\u003cp\u003eThe rise of off-the-shelf and foundation models has enabled AI teams to fine-tune existing models and pre-label data faster and more accurately — in short, significantly accelerating AI development. However, using these models at scale for building AI can quickly become expensive. One way to mitigate these costs and reduce waste is to add a model comparison process to your workflow, ensuring that any model you choose to integrate for AI development is the best choice for your requirements.\u003c/p\u003e\u003cp\u003eA comprehensive model comparison process evaluates models on various metrics, such as performance, robustness, and business fit. The results will enable teams to quickly kickstart model development, decrease time to value, and ensure the best results with less time and costs for their specific use case.\u003c/p\u003e\u003cp\u003eEmbedding model comparison into the AI development workflow, however, comes with its own unique challenges, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConfidently assessing the potential and limitations of pre-trained models\u003c/li\u003e\u003cli\u003eVisualizing models’ performance for comparison\u003c/li\u003e\u003cli\u003eEffectively sharing experiment results\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this blog post, we’ll explore how you can tackle these challenges for a computer vision use case with \u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003ethe Foundry add-on for Quantumworks Lab Model\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"why-is-selecting-the-right-model-critical\"\u003eWhy is selecting the right model critical?\u0026nbsp;\u003c/h3\u003e\u003cp\u003eSelecting the most suitable off-the-shelf model is pivotal for ensuring accurate and reliable predictions tailored to your specific business use case, often leading to accelerated AI development. As different models exhibit diverse performance characteristics, diligently comparing the models’ predictions on your data can help distinguish which model excels in metrics such as accuracy, precision, recall, and more. This systematic approach to model evaluation and comparison enables you to refine the model’s performance with a “store of record” for future reference to continuously improve model performance.\u003c/p\u003e\u003cp\u003eChoosing the best off-the-shelf model provides a quick and efficient pathway to production, ensuring that the model aligns well with the business objectives. This alignment is crucial for the model's immediate performance and sets the stage for future improvements and adaptability to evolving requirements. The most suitable model for your use case also enables you to reduce the time and money spent on labeling a project. For instance, when pre-labels generated by a high-performing model are sent for annotation, less editing is required, making the labeling project quicker and more cost-effective. This is due to better Intersection Over Union (IOU) for tasks like Bounding Box, resulting in higher quality pre-labels and, therefore, fewer corrections. Furthermore, utilizing the best model can make your trove of data more queryable by enriching your data, thereby enhancing its searchability.\u003c/p\u003e\u003ch2 id=\"comparing-vision-llms-models-using-labelbox-model-foundry\"\u003eComparing vision LLMs models using Quantumworks Lab Model Foundry\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/h2VyJaeRIswlqFJEn0IhiQumboC5OqF_mOj8KjCCcFT5nvztUALIucquzxtsYtX47PvvhHXtWP1_xDr2dllNebQf-FCFxu9YLQr184snn_y-SiKVUyB9ONmkttllzI1p6htDg_Nny5GDCCjFg7AF3RA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"239\"\u003e\u003c/figure\u003e\u003cp\u003eThis flow chart provides a high-level overview of the model comparison process when using Quantumworks Lab Model Foundry.\u003c/p\u003e\u003cp\u003eWith the Foundry add-on for Quantumworks Lab Model, you can evaluate a range of models for computer vision tasks to select the best model to perform pre-labeling or data enrichment on your data.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003e\u003cstrong\u003eStep 1: Select images and choose a foundation model of interest\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/L1IEgc57cHhpmPUlxyw7kqGv_RZoYOEII6Vr2iBfYUMe6iDFWViUlmHpXHWM5yfy8ZLj3WN4wZjrMUSTpPF5R5Ra17eeT7ggEoukjvqeh7rJ-vsVWiMWASsURgs-8_uxvuk2rkBN3arCrJB0RSZa9Cs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data, leverage Catalog’s filters including media attribute, a natural language search, and more, to refine the images on which the predictions should be made on.\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “Predict with Model Foundry”.\u003c/li\u003e\u003cli\u003eYou will then be prompted to choose a model that you wish to use in the model run.\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task - such as image classification, object detection, and image captioning.\u003c/li\u003e\u003cli\u003eTo locate a specific model, you can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the appropriate models available for the machine learning task.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-hyperparameters-and-submit-a-model-run\"\u003eStep 2: \u003cstrong\u003eConfigure model hyperparameters and submit a model run\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/NnyPrWdiuwQmxl8y1IMTUWEFGZ2pdyoS4RmKme8-S9hoxvVjZscwQwAN03TMizRD8M70z1CvySOFEShP_4KXZpUjC3Z7cg6peF-T-ceKxlSi5ys432EP_AMPe1NQi8ip0-VHV1FLMT-0n0rmJS5XKxc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve located a specific model of interest, you can click on the model to view and set the model and ontology settings or prompt. In this case, we will enter the following prompt “Is there an entire swimming pool clearly visible in this image? Reply with yes or no only, without period.” This prompt is designed to facilitate responses from the model in a simple 'Yes' or 'No' format.\"\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/li\u003e\u003cli\u003eEach model will also have its own set of hyperparameters, which you can find in the Advanced model setting.\u003c/li\u003e\u003cli\u003eTo get an idea of how your current model settings affect the final predictions, you can generate preview predictions on up to five data rows.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;\u003c/li\u003e\u003cli\u003eOnce you’re satisfied with the predictions, you can submit your model run.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-predictions-will-appear-in-the-model-tab\"\u003eStep 3: Predictions will appear in the Model tab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/image.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/11/image.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/11/image.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eEach model run is submitted with a unique name, allowing you to distinguish between each subsequent model run\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eWhen the model run completes, you can:\u003cul\u003e\u003cli\u003eView prediction results\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results across a variety of model runs different models\u003c/li\u003e\u003cli\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-repeat-steps-1-5-for-another-model-from-model-foundry\"\u003eStep 4: Repeat steps 1-5 for another model from Model Foundry\u003c/h3\u003e\u003cul\u003e\u003cli\u003eYou can repeat steps 1-5 with a different model, on the same data and for the same desired machine learning task, to evaluate and compare model performance\u003c/li\u003e\u003cli\u003eBy comparing the predictions and outputs from different models, you can assess and determine which one would be the most valuable in helping automate your data labeling tasks\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-5-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 5: Create a model run with predictions and ground truth\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"898\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/JLY41AWwbtclBA-ktFdU2RBRd1kmQYpPs5KHHUlJp9ULH_lFlNdE3zalG1UKZSpt0oQf_Pc8GBq7eD9b4M348Ha-E_VEiCGBo359Dxz29ceU16E9fRbreJJgIU7EvGxq953IrGRv5xmcY6CoeJ2GggI.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eTo create a model run with model predictions and ground truth, users currently have to use a \u003ca href=\"https://colab.research.google.com/drive/1f0WkWtsDh2LYiy35pc3JF16srj6SxeTA?ref=labelbox-guides.ghost.io#scrollTo=W8tE-H7VmKea\"\u003escript\u003c/a\u003e to import the predictions from the Foundry add-on for Quantumworks Lab Model and ground truth from a project into a new model run. \u003c/p\u003e\u003cp\u003eIn the near future, this will be possible via the UI, and the script will be optional.\u003c/p\u003e\u003ch3 id=\"step-6-evaluate-predictions-from-different-models-from-model-foundry-in-labelbox-model\"\u003eStep 6: Evaluate predictions from different models from Model foundry in Quantumworks Lab Model\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003cp\u003eAfter running the notebook, you'll be able to visually compare model predictions between two models.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-7-send-model-predictions-as-pre-labels-to-a-labeling-project-in-annotate\"\u003eStep 7: Send model predictions as pre-labels to a labeling project in Annotate\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/MZHoMXGtxeXRYWIH41mENw-0Uz26ydqJBOkE-wtsY-1BYzGdyABhi5UXN6s5QUxrkx-ZHzwuhirnUDB4tNrDrSOEVyvlI7aHcCDMI7zekND5QRDEkBh48DfbxHrzgtpHGKxsRSVcxz6W8AikGbgZRdg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"349\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eSelect the best performing model and leverage the model predictions as pre-labels.\u003c/li\u003e\u003cli\u003eRather than manually labeling data rows, select and send a subset of data to your labeling project with pre-labels to automate the process.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"model-comparison-in-practice\"\u003eModel comparison in-practice:\u003c/h2\u003e\u003ch3 id=\"llava-vs-gpt-4v\"\u003eLLaVA vs GPT-4V\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"893\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/sf5tXNORNESblbwfj9WWrRLLNR0RfDA488oe2WPqd74QCki5o9Jt3wV3JXt0K2j_KV0678WftHgNjyro2__a59RM-KEYrb-Xdw9JlcGwcFx9rz54pxFjBMSewAsUwL8eSk_o8XiXU2nQlTi0UThwPIQ.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eMetrics view \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn this example, LLaVA has 25 true positives, while GPT-4V has 30. Additionally, LLaVA has 10 more false positives. Therefore, GPT-4V is the superior model.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLet’s now take a look at how the predictions appear on the images below:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Jh89URRISL3Qy_D8JCfPEN2KiksoCYJGSaLEulXsh-ZzztLaKoiOEPaGMdatag8YGCCPFrGTP6E2GCY4380KliNWyPXkAlPq-GzP9l0wr44cT81MG4xq2_fUm72YsS1g8KiNnpAXYpv22pLp-RvDxe0\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/4xlsujbFfyNcvACvwcAXIYVeL5E13FYt0RZC0EtsGdkVbrU_542h-EjEr3vGGcnpDManAjF6YcbaWBqaxHoCqOd9Pe31vhHGkGWyK11HGjBHy74iJt3PeuaMoY4p68RbeStGWzGEY3_bPwVge257les\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/zewKkLUtDyBrb0nu6Ci_LgW_mkCPcg6s_TIMFUHwMzvudwilvTP19XWwevbdrCbXx2nKNIWwsNe1ZrH05WN0St36HcnkJ-2mPxBXxeiCEWaVlhVJvxlWF2ptxlPvyycNpg-nlLDe5b8lMI1xdQMQsDw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"332\"\u003e\u003c/figure\u003e\u003cp\u003eIn the examples above, GPT-4V and LLaVA both correctly predicted there to be a swimming pool.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/DLBp8vaTB5ixPdI7mvxOkPM5eWQKzwfyrWHZnTIRnUqO4uycb8wPSdwWrqLzgR6Yp2rTqgIXvc9uBLPLuOwmW8hlWKVdcnelWGVuF4ajkV_bfvp064NWuNMTsJ4Tj_Kbgup4qmgFtuRtMKUXWk3Okt4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"345\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Qxzm1UsZc6osUix_3cmT9AQVhWVll5FOVe2_Svd1ArO7pZj0pxQFbAk_dsUF57rnhtO_iMYpBNGYBWMRwvo4Mw9BxhyvE_wXvbbrbA5sEiXsOXZ6j7dRCryJWDZ4NL4IcaC3mD4rblY57A1P7umfuiU\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"345\"\u003e\u003c/figure\u003e\u003cp\u003eIn the two instances above, GPT-4V correctly identified the presence of a swimming pool, while LLaVA incorrectly predicted its absence. It's possible that LLaVA was misled by the shadows over the pool.\u003c/p\u003e\u003ch3 id=\"imagen-vs-gpt-4v\"\u003eImagen vs GPT-4V\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/11/image-1.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/11/image-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eMetrics view \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eImagen has 6 more true positives and 1 less false positive than GPT-4V, so Imagen is the best model in this test of 40 images based on the metrics.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/tCGt5rcYTHcjW0fkwL2ay4J-jkXni3TDU12eUjVOTwg7X4wzieQ3zodx3gTI02pE2YZsi6R9bcBhslM_69Ixb0T3zDJsY00ISbyNYL4aI0RmusF6pxVdxucBUDVaRWfl400cTYfQEwRlW6cqU7zDFlw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"335\"\u003e\u003c/figure\u003e\u003cp\u003eIn the above example, GPT-4V and Imagen both correctly predicted there to be a swimming pool.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Rl9GXiF4XURZlNr5QreUz7fYaVr9sKrAEv94XI7ciYRGjvCPAxajuBOsmmMb2EoOO2Vq3_aTGyC-_iPniCuCgL7w4YAjKJ8zp6zRm866nNjojMeigw4sMVrwSBb1-oxuXvyGPc3CfsLbyApnidR1S1w\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"335\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Kd4aif9Oa08qnFziD5dr8a3L96H0bEsMeuxsqeMqnjAPpyP9OxqNoemqpxiTYSeVt504k-SVMyTqGr5ZL83fT2ra2FwWsWC2pbC2i27nGhSclbgXFBoJ0wEGl0k_V7W7FZdtAgrouHsTx1Xd_kmlzv8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"345\"\u003e\u003c/figure\u003e\u003cp\u003eIn the two instances above, GPT-4V and Imagen both correctly predicted there to be no obvious swimming pool.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u0026nbsp;Send the predictions as pre-labels to Quantumworks Lab Annotate for labeling\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eSince we've evaluated that Imagen is the best model from our test, we can send model predictions as pre-labels to our labeling project by highlighting all data rows and selecting \"Send to annotate\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/MZHoMXGtxeXRYWIH41mENw-0Uz26ydqJBOkE-wtsY-1BYzGdyABhi5UXN6s5QUxrkx-ZHzwuhirnUDB4tNrDrSOEVyvlI7aHcCDMI7zekND5QRDEkBh48DfbxHrzgtpHGKxsRSVcxz6W8AikGbgZRdg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"349\"\u003e\u003c/figure\u003e\u003cp\u003eIn conclusion, you can leverage Model Foundry to not only select the most appropriate model to accelerate data labeling, but to automate data labeling workflows. Use quantitative and qualitative analysis, along with model metrics, to surface the strengths and limitations of each model and select the best performing model for your use case. Doing so can help reveal detailed insights, such as seen in the above comparison between Imagen and GPT-4V. In the above model comparison example, we can see that Imagen emerged as the superior model that will allow us to rapidly automate data tasks for our given use case.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003eModel Foundry\u003c/a\u003e streamlines the process of comparing model predictions, ensuring teams are leveraging the most optimal model for data enrichment and automation tasks. With the right model, teams can easily create pre-labels in Quantumworks Lab Annotate – rather than starting from scratch, teams can boost labeling productivity by correcting the existing pre-labels.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"65523b444a21b900018ee9ba","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-3060--1-.png","featured":false,"visibility":"public","created_at":"2023-11-13T15:05:40.000+00:00","updated_at":"2023-12-06T22:27:12.000+00:00","published_at":"2023-11-13T15:11:32.000+00:00","custom_excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart LLM development, decreasing costs and accelerating time-to-value. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},"url":"https://labelbox-guides.ghost.io/detecting-swimming-pools-with-gpt4-visual/","excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart LLM development, decreasing costs and accelerating time-to-value. ","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"654bbab4016f5100016579c3","uuid":"6c33e4d1-fcaa-44de-b994-0fe65fce3dcc","title":"How to analyze customer reviews and improve customer care with NLP","slug":"how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp","html":"\u003cp\u003eCustomer reviews have become a critical tool for businesses looking to improve their products, services, and customer satisfaction. In today’s digital world, review sites like Yelp and social media make it easier than ever for customers to share their experiences with the world. Customer care can range in the services and support that businesses provide to their customers before, during, and after purchase. Great customer care can create positive brand experiences that lead to greater loyalty and customer satisfaction. In the ever-evolving world of retail, it also helps keep your business competitive and at the forefront of your customer’s sentiment and desires.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile companies now have access to a wealth of customer feedback data, sifting through all of these reviews can be incredibly time-consuming and manual. By leveraging AI, teams can analyze \u003ca href=\"https://birdeye.com/blog/review-management/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ecustomer reviews and feedback\u003c/a\u003e at scale, to gain insights into common review topics or customer sentiment. This allows businesses to identify common themes and pinpoint areas of improvement to enhance\u0026nbsp; the customer experience.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for customer care. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eImproving customer care requires a vast amount of data in the form of customer reviews. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic review landscape: \u003c/strong\u003eThe changing nature and format of customer review data from multiple sources (e.g webpages, apps, social media, etc.) poses the challenge for businesses to account for continuous data updates and re-training needs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers businesses to transform their customer care through advanced natural language processing. Instead of relying on time-consuming manual reviews, companies can leverage Quantumworks Lab’s assisted data enrichment and flexible training frameworks to quickly build AI systems that uncover actionable insights from customer reviews. Tackle unique customer care challenges with AI-driven insights to create more thoughtful and strategic customer interactions. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1444\" height=\"784\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.35.34-AM.png 1444w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build an NLP model to improve customer care. Specifically, this guide will walk through how you can explore and better understand review topics and classify review sentiment to make more data-driven business decisions around customer care initiatives.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-accelerate-and-train-an-nlp-model-to-improve-customer-care\"\u003eSee it in action: How to accelerate and train an NLP model to improve customer care\u0026nbsp;\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Create a model run and evaluate model performance\u003c/p\u003e\u003cp\u003eYou can follow along with both parts of the tutorial below in either: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e or \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs customer reviews and feedback across channels proliferate, brands want to learn from customer feedback to foster positive experiences. For this use case, we’ll be working with a dataset of customer hotel reviews – with the goal of analyzing the reviews to demonstrate how a hospitality company could gain insight into how their customers feel about the quality of service they receive.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/q4dqjyg9xf\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"498\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to gather data:\u003c/p\u003e\u003cp\u003eFor the purpose of this tutorial, we’ve provided a sample open-source \u003ca href=\"https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe?ref=labelbox-guides.ghost.io\"\u003eKaggle dataset\u003c/a\u003e that can be downloaded.  \u003c/p\u003e\u003cp\u003ePlease \u003ca href=\"https://drive.google.com/file/d/1hh2MYzol6-4PSsqX7mbX2YSLLGA4jAYZ/view?usp=drivesdk\u0026ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edownload the dataset\u003c/a\u003e and store it in an appropriate location on your environment. You'll also need to update the read/write file paths throughout the notebook to reflect relevant locations on your environment. You'll also need to update all references to API keys, and Quantumworks Lab ontology, project, and model run IDs\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you wish to follow along and work with your own data, you can import your text data as a CSV. \u003c/li\u003e\u003cli\u003eIf your text snippets sit as individual files in cloud storage, you can reference the URL of these files through our \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eIAM delegated access integration\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve uploaded your dataset, you should see your text data rendered in Quantumworks Lab Catalog. You can browse through the dataset and visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xn3sj0uc8j\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExplore topics of interest\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith your data in Quantumworks Lab, you can begin to leverage Catalog to uncover interesting topics to get a sense of what customers are talking about from hotel reviews.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize your data –\u0026nbsp; you can click through individual data rows to get a sense for what customers are writing reviews on\u0026nbsp;\u003c/li\u003e\u003cli\u003eDrill into specific topics of interest\u003cstrong\u003e \u003c/strong\u003e– leverage a natural language search, for example searching “interior design,” to bring up all related reviews related to interior design. You can adjust the confidence threshold of your searches accordingly (this can be helpful in gauging the volume of data related to the topic of interest)\u0026nbsp;\u003c/li\u003e\u003cli\u003eBegin to surface subtopics or trends within your initial search – for example is the interior design review related to the style of design, attention to detail, or the type of environment created from the interior design\u003c/li\u003e\u003cli\u003eEasily find all instances of similar examples to data of interest\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate and save data slices\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf you have a search query that you’re interested in saving or reusing in the future, you can save it as \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003ea slice\u003c/a\u003e. You can construct a slice by using one or more filters to curate a collection of data rows. Users often combine filters to surface high-impact data and then save the results as a slice.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this example, we’ve surfaced reviews on the topic of breakfast that all talk about the value and price of the hotel’s breakfast. We can save this as a slice for future reference (“Breakfast_value”) and as we ingest more data that matches the slice’s criteria, they will automatically get filed into the slice.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-an-ontology\"\u003eCreate an ontology\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAfter we’ve explored our data, we now have a better understanding of what topics exist in our dataset and can create our ontology. \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eOntologies\u003c/a\u003e can be reused across different projects and they are required for data labeling, model training, and evaluation.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/gdczmynqjt\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a new ontology:\u003c/p\u003e\u003cp\u003e1) Navigate to the ‘Schema’ tab\u003c/p\u003e\u003cp\u003e2) Hit ‘Create new ontology’\u003c/p\u003e\u003cp\u003e3) Select the media type that you wish to work with – for this use case ‘Text’\u003c/p\u003e\u003cp\u003e4) Give your ontology a name\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Add objects and classifications based on you use case\u003c/p\u003e\u003cp\u003e6) Objects are named entities\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003ePerson’s name\u0026nbsp;\u003c/li\u003e\u003cli\u003eLocation\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003e7) Classifications\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eReview sentiment such as positive or negative (radio)\u0026nbsp;\u003c/li\u003e\u003cli\u003eReview topics such as breakfast, dinner, location, staff, interior design (checklist)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAdd sub-classifications as desired\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003e8) Save and create your ontology\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter creating an ontology, you can begin labeling your data to fine-tune or train a model.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"label-data-of-interest\"\u003eLabel data of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003eWith Quantumworks Lab, you can label your data in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e: leverage our global network of\u0026nbsp; specialized labelers for a variety of tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWorkforce Boost provides a collaborative platform for labeling services in a self-serve manner — this is great for teams that don’t have the technical expertise to build a machine learning system yet are looking for an easy-to-use technology to get a quick turnaround on quality training data. You can learn more about our Boost offerings \u003ca href=\"https://docs.labelbox.com/docs/using-data-labeling-service?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create pre-labels with foundation models\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn addition to creating pre-labels for classification projects, you have the ability to send model predictions as pre-labels to your labeling project. This can be done in one of two ways:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel-assisted labeling\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eImport computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel Foundry\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eAutomate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePre-label data with Model Foundry\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModel Foundry\u003c/a\u003e acts as the copilot to create your training data –\u0026nbsp; instead of going into unstructured text datasets blindly, you can use pre-existing LLMs to pre-label data or pre-tag parts of it, reducing manual labeling efforts and cost.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9zspjgoau7\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1) Select data you wish to label in Catalog\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Hit \"Predict with Model Foundry\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Choose a foundation model\u003c/p\u003e\u003cul\u003e\u003cli\u003eYou can select a foundation model based on your use case to have the model take a first pass at labeling your data\u003c/li\u003e\u003cli\u003eThese pre-labels can be verified with human-in-the-loop review in Quantumworks Lab Annotate\u003c/li\u003e\u003cli\u003eFor this use case, we’ve selected the GPT-4 model\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e4) Configure the model’s settings\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect the previously created ontology in the earlier part of the tutorial\u0026nbsp;\u003c/li\u003e\u003cli\u003eLabelbox will auto-generate a prompt based on your ontology and use case – in this case we wish to classify the sentiment (positive or negative) and classify a topic with one or more options (breakfast, dinner, location, staff, room, facilities, value for money, or interior design)\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e5) Generate preview predictions\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eBefore submitting the model run, you can generate prediction previews to understand how the model will perform\u003c/li\u003e\u003cli\u003eIt is recommended that you preview some predictions to confirm the model parameters are configured as desired\u003c/li\u003e\u003cli\u003eBased on the preview, you can then make any adjustments to the settings or choose to submit the model run as-is\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e6) Name and submit the model run\u0026nbsp;\u003c/p\u003e\u003cp\u003e7) View the model run in the Model tab to explore results\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eOnce your model run is complete, you navigate to the Model tab\u003c/li\u003e\u003cli\u003eExplore the model’s results and click into each data row to dig deeper into the model’s predictions\u003c/li\u003e\u003cli\u003eFor this example, we can see that there are instances where GPT-4 has correctly tagged named entities and identified sentiment\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve evaluated and are satisfied with GPT-4’s predictions, you can send them to a labeling project in Quantumworks Lab Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAdd a batch to a labeling project as pre-labels\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eBefore you can send these model predictions to a labeling project as pre-labels, you need to create a labeling project. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7zgl76n76w\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eCreate a new labeling project\u003c/em\u003e\u003c/p\u003e\u003cp\u003e1) Navigate to the Annotate tab\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Create a ‘New project’\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Select the project type – in this case we want to create a ‘Text’ project\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Name your project\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Attach your model’s ontology (created in a previous step)\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce you’ve created your labeling project and configured the ontology, head back to the Model tab to send your batch of data with pre-labels to that labeling project.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Highlight all data rows of interest\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Select ‘Manage selection’ \u0026gt; ‘Add batch to project’\u003c/p\u003e\u003cp\u003e3) Select the appropriate project that you created in the above step\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) You can give the batch a priority (from 1-5)\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Select the appropriate model run of the predictions you wish to send\u0026nbsp;\u003c/p\u003e\u003cp\u003e6) You can explore and select the various tags that have been applied and uncheck those that aren’t of interest\u0026nbsp;\u003c/p\u003e\u003cp\u003e7) Submit the batch\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can now navigate back to your project in Annotate and hit ‘Start labeling’.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"verify-data-quality-with-custom-workflows\"\u003eVerify data quality with custom workflows\u003c/h3\u003e\u003cp\u003eRather than starting from scratch, your internal or external team of labelers can now see predictions from the Model Foundry run. From here, you can validate or edit predictions as necessary and submit data rows to create ground truth labels.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/56bratjais\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs you begin to progress through your data rows, you’ll notice data rows that are initially marked up and reviewed by labelers in the ‘Initial review’ task (for your reviewers to verify and approve), with all submitted data rows falling into ‘Done’.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can create customizable, multi-step review and rework pipelines to drive efficiency and automation for your review tasks. Set a review task based on specific parameters that are unique to your labeling team or desired outcome.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eInitial labeling task: reserved for all data rows that have been queued for labeling\u003c/li\u003e\u003cli\u003eInitial review task: first review task for data rows with submitted labels\u003c/li\u003e\u003cli\u003eRework task: reserved for data rows that have been rejected\u003c/li\u003e\u003cli\u003eDone task: reserved for data rows that have a) moved through their qualified tasks in the workflow or b) did not qualify for any of the tasks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce all data rows have been reviewed and moved to the ‘Done’ step, you can begin the model training process.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn Part 1 of this tutorial, we have looked at how we can leverage Catalog to understand the topics that exist within your dataset and construct an appropriate ontology. To accelerate our initial labeling job, we leveraged Model Foundry as part of our model-assisted labeling pipeline to use pre-labels from GPT-4 to our labeling workforce for validation. Those initial annotations can be exported via a model run and can be used to train or fine-tune a model outside of Labelbox.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"part-2-train-or-fine-tune-a-model-and-evaluate-model-performance\"\u003ePart 2: Train or fine-tune a model and evaluate model performance\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in either the \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eGoogle Colab Notebook\u003c/a\u003e or \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eDatabricks Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"train-a-custom-model-on-a-subset-of-data-outside-of-labelbox\"\u003eTrain a custom model on a subset of data outside of Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/07yc0p652w\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIn the previous step, we leveraged Model Foundry to create pre-labels that were passed through Annotate for review with human-in-the-loop validation. Now that we have our appropriate annotation data, we can train a series of initial models on sentiment, topic classification, and named entity recognition.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eYou can reference \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io#scrollTo=EDmBNjYP_0u7\"\u003ethis step\u003c/a\u003e (Databricks) or \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io#scrollTo=EDmBNjYP_0u7\" rel=\"noreferrer\"\u003ethis step\u003c/a\u003e (Google Colab) in either notebook.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBring the trained model’s predictions back into a model run\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce the model has been trained, you can create an inference pipeline that leverages each model to classify different attributes for review. We can then leverage this for two things:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun inference on the model run dataset and upload it to Quantumworks Lab for evaluation\u003c/li\u003e\u003cli\u003eRun inference on our remaining dataset and use the predictions for model-assisted labeling, to be refined in the platform and used to accelerate labeling efforts\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ePlease follow \u003ca href=\"https://colab.research.google.com/drive/1ppNgzhP12Ph1xyIws3xE5ChVThZda0q2?ref=labelbox-guides.ghost.io#scrollTo=B9_dYR_D_0vE\"\u003ethis step\u003c/a\u003e (Databricks) or \u003ca href=\"https://colab.research.google.com/drive/1quSxnKhSNffw0GMhyxGYxFFFNaf34OQp?ref=labelbox-guides.ghost.io#scrollTo=B9_dYR_D_0vE\" rel=\"noreferrer\"\u003ethis step\u003c/a\u003e (Google Colab) to create an inference pipeline and to upload predictions to the model run and evaluate it against ground truth.\u003c/p\u003e\u003cp\u003eAfter following the notebook, you’ll be able to compare ground truth (green) versus the model’s predictions (red) for sentiment and topic.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness\"\u003eEvaluate and diagnose model effectiveness\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/dqzdzj1seb\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 8 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eDiagnose model performance with model metrics\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn addition to visualizing the difference between model predictions and ground truth, you can click into the ‘Metrics’ view to get a better sense of how your model is performing.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the \"Metrics view\" to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor example, we can click into false negatives or false positives to narrow down situations where there might be false positives – where ‘negative’ sentiment is predicted whereas ground truth sentiment is ‘positive’.\u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select \"Find similar in Catalog\" from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled reviews that you can send to your model for labeling, you can filter on the \"Annotation is\" filter and select \"none.\" This will only show unlabeled text reviews that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all reviews that apply and select \"Add batch to project\"\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eUse model predictions as model-assisted labeling pipeline\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/r4p2h6iklg\" title=\"How to analyze customer reviews and improve customer care with NLP - Part 9 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve filtered for and have selected reviews that you wish to label you can \"Add batch to project\" to send them to your labeling project in Annotate.\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Name your batch\u003c/p\u003e\u003cp\u003e2) Select your labeling project from the dropdown\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Include model predictions (from your model run) – this will perform better than the initial GPT-4 run with Model Foundry since it has been trained on your custom data\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Select or uncheck any predictions as desired\u0026nbsp;\u003c/p\u003e\u003cp\u003e5) Submit the batch\u003c/p\u003e\u003cp\u003eWhen you return to Quantumworks Lab Annotate, you will now see the original batch that we added at the start of the project, as well as the newly added batch ready for labeling.\u0026nbsp;\u003c/p\u003e\u003cp\u003eRather than starting from scratch, similar to the predictions created by GPT-4 in Model Foundry, your labelers will now see the custom model predictions and validate them with human-in-the-loop review in the same manner. This workflow helps accelerate model iterations, allowing your team to bring in the latest model prediction as pre-labels for your project to reduce the amount of human labeling effort required to create ground truth labels.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model and can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003eCustomer reviews and feedback data represent an invaluable yet untapped opportunity for businesses. Manually analyzing this growing mountain of data is no longer practical. Instead, forward-thinking companies are turning to AI to efficiently sift through and extract actionable insights from reviews.\u003c/p\u003e\u003cp\u003eNatural language processing can help identify customer sentiment, pain points, and unmet needs. By leveraging AI to tap into this feedback treasure trove, businesses can drive measurable improvements in customer satisfaction, retention, and advocacy. They can refine products, enhance user experiences, and preemptively address concerns.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"654bbab4016f5100016579c3","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-2457--1-.png","featured":false,"visibility":"public","created_at":"2023-11-08T16:43:32.000+00:00","updated_at":"2024-06-25T16:28:21.000+00:00","published_at":"2023-11-08T21:47:13.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-analyze-customer-reviews-and-improve-customer-care-with-nlp/","excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","reading_time":12,"access":true,"comments":false,"og_image":null,"og_title":"How to analyze customer reviews and improve customer care with NLP","og_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Group-2457--1--1.png","twitter_title":"How to analyze customer reviews and improve customer care with NLP","twitter_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","meta_title":"How to analyze customer reviews and improve customer care with NLP","meta_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to redefine customer care with AI and create solutions tailored to unique customer care challenges.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"654407cdd96ee80001d8c876","uuid":"e29733c1-b966-4d1e-858d-f96deab4850e","title":"How to build a content moderation model to detect disinformation","slug":"how-to-build-a-content-moderation-model-to-detect-disinformation","html":"\u003cp\u003eAs user-generated content increases and the amount of data grows, trust and safety on digital platforms is becoming increasingly critical. Content that goes unmoderated can not only directly hurt brand reputation, but it can directly impact a businesses bottom line through lost users, advertisers, and revenue. Regulators worldwide are also implementing \u003ca href=\"https://insightplus.bakermckenzie.com/bm/data-technology/united-states-now-is-the-time-to-evaluate-your-online-content-moderation-program?ref=labelbox-guides.ghost.io\"\u003estricter rules\u003c/a\u003e around content moderation, online safety, misinformation, and disinformation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo address these growing risks, more businesses are looking to AI and machine learning as part of robust trust and safety strategies. State-of-the-art AI solutions enable unprecedented scale, nuance, consistency, and efficiency in identifying and taking action on high-risk user content.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, businesses can face multiple challenges when implementing AI for trust and safety. This includes:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDynamic content landscape: \u003c/strong\u003eModels are only as good as the data they are trained on. As new trends or content emerges, AI models need constant retraining on compelling diverse, unbiased, and large labeled datasets to reinforce content moderation.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEthical risks \u0026amp; biases: \u003c/strong\u003eWithout careful design, machine learning models risk exacerbating biases and are prone to \u003ca href=\"https://labelbox.com/blog/what-does-it-mean-when-an-llm-hallucinates/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ehallucination\u003c/a\u003e. Teams need a way to monitor and evaluate model training with ethical oversight.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeveloping accurate custom AI can be expensive in data, tools, and expertise. Leveraging foundation models, with human-in-the-loop verification, can help accelerate model development by automating the labeling process.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that enables businesses to build state-of-the-art AI solutions for enhanced controls, transparency, efficiency in content moderation, and greater brand safety. Rather than spending valuable time building an in-house solution or relying on disparate systems, businesses can explore data, use foundation models for assisted-enrichment, and evaluate models to quickly build more accurate AI systems for analyzing user behavior, detecting disinformation, and enhancing ad-targeting. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1430\" height=\"786\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.36.31-AM.png 1430w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build a model for content moderation, such as detecting and classifying disinformation, allowing you to elevate brand trust and improve the trust and safety of your applications.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-build-a-content-moderation-model-to-detect-disinformation\"\u003eSee it in action: How to build a content moderation model to detect disinformation\u003c/h2\u003e\u003cp\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. We recommend that you \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003ecreate a free Quantumworks Lab account\u003c/a\u003e to best follow along with this tutorial.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1:\u003c/strong\u003e Explore and enhance your data: \u003ca href=\"https://colab.research.google.com/drive/1bTIKkQUHiccIy1adgKqQ_CVCm2KAxiHP?ref=labelbox-guides.ghost.io\"\u003eGoogle Colab Notebook\u0026nbsp;\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2: \u003c/strong\u003eCreate a model run, fine-tune an LLM, and evaluate model performance: \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1bTIKkQUHiccIy1adgKqQ_CVCm2KAxiHP?ref=labelbox-guides.ghost.io\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab\u0026nbsp;\u003c/h3\u003e\u003cp\u003eWith the growing amount of user-generated content, businesses want to ensure that there is no inappropriate content or disinformation happening on their platform. To implement content moderation at scale, teams can leverage AI to analyze and detect harmful content and classify disinformation from existing data stored in a cloud bucket or a local folder.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9e76g8llbu\" title=\"How to enhance brand safety and content moderation with AI - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo upload a sample of your content to Quantumworks Lab for labeling, you have a few options:\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUpload a dataset through the SDK\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eUsing the \u003ca href=\"https://colab.research.google.com/drive/1bTIKkQUHiccIy1adgKqQ_CVCm2KAxiHP?ref=labelbox-guides.ghost.io\"\u003eGoogle Colab notebook\u003c/a\u003e, upload the sample dataset into Quantumworks Lab or use it to import data from various sources like Bigquery, Databricks, or Snowflake.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this notebook, we’re going to bring in two libraries of interest:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/sdk-fundamental-concepts-1?ref=labelbox-guides.ghost.io\"\u003eLabelbox SDK\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/Quantumworks Lab/labelpandas?ref=labelbox-guides.ghost.io\"\u003eLabelpandas\u003c/a\u003e (for bringing tabular data into Quantumworks Lab)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou’ll need your Quantumworks Lab API key to initiate the Quantumworks Lab Client and create a dataset. For this guide, we’ll be using a dataset stored in a Google Cloud bucket as a CSV and we can use Labelpandas to bring this data in.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe provided sample dataset includes:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAn article with a corresponding headline\u003c/li\u003e\u003cli\u003eWhen it was retrieved\u003c/li\u003e\u003cli\u003eMetadata (sorted by source)\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003cli\u003ePre-labels based on if the article contains “disinformation” or not\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eUpload a dataset through the UI \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf you have a dataset from your local file, you can upload it through the Quantumworks Lab UI by clicking \"new dataset\" in Catalog.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce you’ve successfully uploaded your text, you can browse the dataset in Catalog — along its metadata. You can visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/vl9jr5463n\" title=\"How to enhance brand safety and content moderation with AI - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003eSearch across datasets\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003efind similar data\u003c/a\u003e in seconds with off-the-shelf embeddings\u0026nbsp;\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003enatural language\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003elayer structured and unstructured filters\u003c/a\u003e for more granular data curation\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate and save data slices\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ksmc9w7acz\" title=\"How to enhance brand safety and content moderation with AI - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eIf you have a search query that you’re interested in saving or reusing in the future, you can save it as \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003ea slice\u003c/a\u003e. You can construct a slice by using one or more filters to curate a collection of data rows. Users often combine filters to surface high-impact data and then save the results as a slice.\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn this example, we are interested in saving the surfaced data rows as “Climate Articles” so that this filtered dataset can easily be surfaced later on for annotation or data discovery purposes.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-a-labeling-project-in-annotate\"\u003eCreate a labeling project in Annotate\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uxspsczkpn\" title=\"How to enhance brand safety and content moderation with AI - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1) Create a text project in \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Sample and send your uploaded dataset as a \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003ebatch\u003c/a\u003e to your newly created project. In this case we can send the two dataset slices that we created: “Climate related articles” and “Non-climate related articles”\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eontology\u003c/a\u003e to determine how to structure your data. If you have a previous ontology you’d like to use, you can do so. If not, you’ll need to create a new ontology. For this use case, our ontology consists of two classifications:\u003c/p\u003e\u003cul\u003e\u003cli\u003e“Does the article contain disinformation?” with two options\u003c/li\u003e\u003cli\u003e“Is the article climate related?” with two options\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e4) If you’re relying on an external team of labelers or want to provide your internal labeling team with more instructions, you can upload instructions as a PDF for your labelers during the ontology creation process.\u003c/p\u003e\u003ch3 id=\"label-the-data-of-interest\"\u003eLabel the data of interest\u0026nbsp;\u003c/h3\u003e\u003cp\u003eNow that we have a project with our data set up in Annotate, we’ll need to label this training data.\u0026nbsp;\u003c/p\u003e\u003cp\u003eSince this project is a classification use case, we can also leverage \u003ca href=\"https://docs.labelbox.com/docs/bulk-classification?ref=labelbox-guides.ghost.io\"\u003ebulk classification\u003c/a\u003e to speed up our labeling process and maximize labeling efficiency. Teams who have used bulk classification in Quantumworks Lab have seen labeling time decrease from a full quarter to a few days. Since we’ve leveraged filters in Catalog to identify “Climate related articles,” we can send these articles to our newly created labeling project with pre-labels.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/eh5mwisy6q\" title=\"How to enhance brand safety and content moderation with AI - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo bulk classify and pre-label data rows, you can:\u003c/p\u003e\u003cp\u003e1) Highlight any data rows of interest, in our use case these would be data rows in the slice \"Climate related articles\",\u0026nbsp; and select \"Manage selection\" \u0026gt; \"Add classifications\"\u003c/p\u003e\u003cp\u003e2) Select the labeling project that you made in the previous step and determine a step of the project’s review workflow that you would like to send the classifications to. In the above demo, we are sending these to the \"Initial labeling task\" because we want to have a labeler verify that these are indeed all climate related articles\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Select the desired classification — in this case it would be \"Climate related\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) You can determine the batch’s data row priority (from 1-5) and submit the bulk classification job \u003c/p\u003e\u003cp\u003eRather than labeling from scratch, a team of labelers can now simply verify or correct the pre-labels used during this bulk classification step.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can label your data in the following ways:\u003c/p\u003e\u003cp\u003e1) Internal team of labelers: your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) External team of expert labelers with \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e: leverage our global network of\u0026nbsp; specialized labelers for a variety of tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWorkforce Boost provides a collaborative platform for labeling services in a self-serve manner — this is great for teams that don’t have the technical expertise to build a machine learning system yet are looking for an easy-to-use technology to get a quick turnaround on quality training data. You can learn more about our Boost offerings \u003ca href=\"https://docs.labelbox.com/docs/using-data-labeling-service?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create pre-labels with foundation models\u0026nbsp;\u003c/p\u003e\u003cp\u003eIn addition to creating pre-labels for classification projects, you have the ability to send model predictions as pre-labels to your labeling project. This can be done in one of two ways:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel-assisted labeling\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eImport computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eModel Foundry\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eAutomate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"verify-data-quality-with-custom-workflows\"\u003eVerify data quality with custom workflows\u003c/h3\u003e\u003cp\u003eContent moderation relies heavily on training the model on accurate and verified data. To ensure that you’re producing the most reliable and high-quality training datasets, you can customize your \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003elabeling review workflow\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/lqa22gzj7o\" title=\"How to enhance brand safety and content moderation with AI - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou can create customizable, multi-step review and rework pipelines to drive efficiency and automation for your review tasks. Set a review task based on specific parameters that are unique to your labeling team or desired outcome.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eInitial labeling task: reserved for all data rows that have been queued for labeling\u003c/li\u003e\u003cli\u003eInitial review task: first review task for data rows with submitted labels\u003c/li\u003e\u003cli\u003eRework task: reserved for data rows that have been rejected\u003c/li\u003e\u003cli\u003eDone task: reserved for data rows that have a) moved through their qualified tasks in the workflow or b) did not qualify for any of the tasks\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-2-create-a-model-run-fine-tune-an-llm-and-evaluate-model-performance\"\u003ePart 2: Create a model run, fine-tune an LLM, and evaluate model performance\u0026nbsp;\u0026nbsp;\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this part of the tutorial, we’ll be taking the ground truth labels created in Part 1 to fine-tune a large language model (LLM). From there, we’ll evaluate model performance in Quantumworks Lab Model to diagnose model strengths and weaknesses and look to continuously boost and improve model performance.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-a-new-model\"\u003eCreate a new model\u0026nbsp;\u003c/h3\u003e\u003cp\u003eOnce you have your labeled data in your project in Annotate, you’re ready to move on to creating a model run in \u003ca href=\"https://app.labelbox.com/mea?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003eLabelbox Model\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/z31zew9hbd\" title=\"How to enhance brand safety and content moderation with AI - Part 7 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a new model, you’ll need to:\u003c/p\u003e\u003cp\u003e1) Navigate to the \"Experiments\" tab in Model. The \"Experiments\" tab will be where you can find all model experiments across iterations.\u003c/p\u003e\u003cp\u003e2) Create a new model by selecting the \"New model\" button.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eProvide a model name\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect the model ontology — in this case we will select the same ontology we used to create our labeling project that contains the corresponding ground truth data.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSubmit and create a model — before creating a model run, you will also be able to see and verify the number of data rows that are being submitted.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCreate a model run\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve created a new model, we will need to create a new \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003emodel run\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eA model run is a model training experiment — each model run provides a versioned data snapshot of the data rows, annotations, and \u003ca href=\"https://docs.labelbox.com/docs/curate-data-splits?ref=labelbox-guides.ghost.io\"\u003edata splits\u003c/a\u003e for that model run. You can upload predictions to the model run and compare its performance against other model runs in a model directory.\u003c/p\u003e\u003cp\u003eThe model run we create will be the initial model run for our LLM fine-tuning experiment. To add a new model run:\u003c/p\u003e\u003cp\u003e1) Select \"New model run\"\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) Give the model run a name (e.g “model run #1”)\u003c/p\u003e\u003cp\u003e3) Set data splits for the model run (for train, validate, and test)\u0026nbsp;\u003c/p\u003e\u003cp\u003e4) Create the model run\u0026nbsp;\u003c/p\u003e\u003cp\u003eAfter creating a model run, you’ll be able to see the corresponding data rows with ground truth populated into the appropriate train, validate, and test splits. This model run will be the gateway for us to export ground truth data to fine-tune a large language model.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/bObh0Kje6BWPfrIPWcd3V0TFt09svuX0-7Wka4IQI9j-bKdmhJAEjTWsWPWOmdFUg-CgU9fLQC-p_vFdafFXv4nYhMZupffw7Bl6TN8Z-2j771nF4riavmSL-xiDAmjU8E32deblRc4eEmNjptF3GpI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"327\"\u003e\u003c/figure\u003e\u003ch3 id=\"export-ground-truth-from-the-model-run-experiment-for-fine-tuning\"\u003eExport ground truth from the model run experiment for fine-tuning\u0026nbsp;\u003c/h3\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/s2gzlnehyy\" title=\"How to enhance brand safety and content moderation with AI - Part 8 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWe’ll be using this \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e to fine-tune a model and bring back inferences from the fine-tuned model for evaluation and diagnosis.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor this step, you will need:\u003c/p\u003e\u003cul\u003e\u003cli\u003eYour API Key\u0026nbsp;\u003c/li\u003e\u003cli\u003eYour Model Run ID to export the corresponding ground truth and articles from the model run\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eExport ground truth from the model run experiment\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLabelbox will return the ground truth export in a JSON format. With the provided \u003ca href=\"https://colab.research.google.com/drive/1p7d3UGBu0x4lGwLB06iGjCiS71HYoueU?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e, we can visualize the exported JSON into a DataFrame format for us to view corresponding ground truth for each article.\u0026nbsp;\u003c/p\u003e\u003cp\u003eGiven that we want to fine-tune a Google Vertex model with this data, we’ll need to convert the ground truth export to a GCP vertex tuning format (JSONL):\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# build LLM prompt and convert to GCP vertex tuning format (jsonl)\n\nprompt = 'Given the following headline and content, determine whether the article is related to climate change or similar topics. Also determine whether the article contains inaccurate or disinformation. Answer in the following format with Yes/No Answers: [climate related? / disinformation?]'\ndf['input_text'] = prompt + df['content']\ndf['output_text'] = 'climate related: ' + df['climate_related'] + ' disinformation: ' + df['disinformation_flag']\n\n\nwith open('modelPrompt_GCP.jsonl', 'w') as file:\nfor _, row in df[['input_text', 'output_text']].iterrows():\njson_line = row.to_json()\nfile.write(json_line + '\\n')\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"fine-tune-an-llm-with-google-vertex-ai\"\u003eFine-tune an LLM with Google Vertex AI \u003c/h3\u003e\u003cp\u003eFine-tuning is a technique whereby we take an off-the-shelf open-source or proprietary model and retrain it on a variety of concrete examples, and save the updated weights as a new model checkpoint. You can learn more about other techniques to leverage LLMs \u003ca href=\"https://labelbox.com/guides/zero-shot-learning-few-shot-learning-fine-tuning/?ref=labelbox-guides.ghost.io#zero-shot-learning-few-shot-learning-and-fine-tuning-in-action\"\u003ein this guide\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/um0f8w1rzn\" title=\"How to enhance brand safety and content moderation with AI - Part 9 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eFor this use case, we’ll be using \u003ca href=\"https://cloud.google.com/vertex-ai?ref=labelbox-guides.ghost.io\"\u003eGoogle Vertex AI \u003c/a\u003eto fine-tune an LLM with the ground truth from Part 1 of this tutorial. Once in the Vertex AI console, we’ll want to create a tuned model:\u003c/p\u003e\u003cul\u003e\u003cli\u003eChoose a supervised learning task\u0026nbsp;\u003c/li\u003e\u003cli\u003eEnter additional model parameters (e.g model name)\u0026nbsp;\u003c/li\u003e\u003cli\u003eUpload the JSONL file from the previous step\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, we can start the model tuning process. Once the model fine-tuning job has been completed, we can head over to the Google Vertex sandbox and give the newly tuned model a prompt.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor example, we can ask if the article is climate related and if it contains disinformation and it will provide a response based on the training dataset we provided.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"create-inferences-with-the-tuned-model-and-evaluate-model-effectiveness-in-labelbox\"\u003eCreate inferences with the tuned model and evaluate model effectiveness in Quantumworks Lab\u003c/h3\u003e\u003cp\u003eNow that we’ve fine-tuned a model, we can use it to make predictions on the initial dataset and compare it with our ground truth data to assess the fine-tuned model’s performance.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cjbrcpktcr\" title=\"How to enhance brand safety and content moderation with AI - Part 10 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eCreate inferences with the tuned model\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWe’ll need to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eInstall Google Vertex and Google Cloud SDK\u0026nbsp;\u003c/li\u003e\u003cli\u003eProvide the endpoint ID for the tuned model\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe can then start creating model inferences and predictions from the tuned model on our news articles. Use Pandas to clean up the responses, to remove corresponding prompts, and save them as a DataFrame — this will return the model’s initial headline and the client’s response if the data row is climate related or contains disinformation. \u003c/p\u003e\u003cp\u003eOnce we have model inferences, we can send the inferences back to a model run in Quantumworks Lab for further evaluation and analysis.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eEvaluate and diagnose model effectiveness\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo evaluate the effectiveness of the fine-tuned model in Quantumworks Lab, we’ll need to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSpecify the model run ID\u0026nbsp;\u003c/li\u003e\u003cli\u003eUpload the list of model inferences for each specific data row\u0026nbsp;\u003c/li\u003e\u003cli\u003eAttach each list of data rows and submit it to a model run in Quantumworks Lab as an upload job via the SDK\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce that’s complete, you can hop back to the original Quantumworks Lab model run and view the corresponding ground truth data and model inferences on each data row. You can visually compare the effectiveness of the fine-tuned model predictions (in red) with ground truth (in green).\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse the \"Metrics view\" to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor this use case, our goal is to minimize the spread of disinformation, so we can take a look at the metric that shows corresponding articles that are considered \"disinformation\" by labelers, but where the model incorrectly predicted articles as \"not disinformation\". \u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003ch3 id=\"curate-high-impact-data-to-drastically-improve-model-performance\"\u003eCurate high-impact data to drastically improve model performance\u003c/h3\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7podbew17q\" title=\"How to enhance brand safety and content moderation with AI - Part 11 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select \"Find similar in Catalog\" from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled articles that you can send to your model for labeling, you can filter on the \"Annotation is\" filter and select \"none\". This will only show unlabeled text articles that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all articles that apply and send them as a batch to your original labeling project. Labeling these in priority will help improve model performance. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model and can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eUnmoderated content poses mounting risks to businesses with the risk of spreading misinformation, disinformation, and an unsafe online environment. With responsible implementation, businesses can leverage AI for trust and safety to efficiently and consistently identify high-risk content at scale. This not only helps create an online environment that is safe for users, but also helps protect brand reputation.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"654407cdd96ee80001d8c876","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Frame-2299--2-.png","featured":false,"visibility":"public","created_at":"2023-11-02T20:34:21.000+00:00","updated_at":"2024-07-17T20:55:32.000+00:00","published_at":"2023-11-02T21:18:18.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-build-a-content-moderation-model-to-detect-disinformation","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},"url":"https://labelbox-guides.ghost.io/how-to-build-a-content-moderation-model-to-detect-disinformation/","excerpt":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","reading_time":12,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Frame-2299--2--2.png","og_title":"How to build a content moderation model to detect disinformation","og_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/11/Frame-2299--2--1.png","twitter_title":"How to build a content moderation model to detect disinformation","twitter_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","meta_title":"How to build a content moderation model to detect disinformation","meta_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a model for content moderation for trust \u0026 safety applications. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"653feb16375d13000123da16","uuid":"62c9c370-85df-4a74-a997-f0f25da4b1ee","title":"How to evaluate object detection models with Quantumworks Lab Model Foundry","slug":"how-to-evaluate-object-detection-models-using-labelbox-model-foundry","html":"\u003cp\u003eThe rise of off-the-shelf and foundation models has enabled AI teams to fine-tune existing models and pre-label data faster and more accurately — in short, significantly accelerating AI development. However, using these models at scale for building AI can quickly become expensive. One way to mitigate these costs and reduce waste is to add a model comparison process to your workflow, ensuring that any model you choose to integrate for AI development is the best choice for your requirements.\u003c/p\u003e\u003cp\u003eA comprehensive model comparison process evaluates models on various metrics, such as performance, robustness, and business fit. The results will enable teams to quickly kickstart model development, decrease time to value, and ensure the best results with less time and costs for their specific use case.\u003c/p\u003e\u003cp\u003eEmbedding model comparison into the AI development workflow, however, comes with its own unique challenges, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConfidently assessing the potential and limitations of pre-trained models\u003c/li\u003e\u003cli\u003eVisualizing models’ performance for comparison\u003c/li\u003e\u003cli\u003eEffectively sharing experiment results\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this blog post, we’ll explore how you can tackle these challenges for a computer vision use case with \u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003ethe Foundry add-on for Quantumworks Lab Model\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"why-is-selecting-the-right-model-critical\"\u003eWhy is selecting the right model critical?\u0026nbsp;\u003c/h3\u003e\u003cp\u003eSelecting the most suitable off-the-shelf model is pivotal for ensuring accurate and reliable predictions tailored to your specific business use case, often leading to accelerated AI development. As different models exhibit diverse performance characteristics, diligently comparing the models’ predictions on your data can help distinguish which model excels in metrics such as accuracy, precision, recall, and more. This systematic approach to model evaluation and comparison enables you to refine the model’s performance with a “store of record” for future reference to continuously improve model performance.\u003c/p\u003e\u003cp\u003eChoosing the best off-the-shelf model provides a quick and efficient pathway to production, ensuring that the model aligns well with the business objectives. This alignment is crucial for the model's immediate performance and sets the stage for future improvements and adaptability to evolving requirements. The most suitable model for your use case also enables you to reduce the time and money spent on labeling a project. For instance, when pre-labels generated by a high-performing model are sent for annotation, less editing is required, making the labeling project quicker and more cost-effective. This is due to better Intersection Over Union (IOU) for tasks like Bounding Box, resulting in higher quality pre-labels and, therefore, fewer corrections. Furthermore, utilizing the best model can make your trove of data more queryable by enriching your data, thereby enhancing its searchability.\u003c/p\u003e\u003ch2 id=\"comparing-computer-vision-models-with-labelbox-model-foundry\"\u003eComparing computer vision models with Quantumworks Lab Model Foundry\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"748\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-01-at-3.10.43-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThis flow chart provides a high-level overview of the model comparison process when using the Foundry add-on for Quantumworks Lab Model. \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWith Quantumworks Lab Model Foundry, you can evaluate a range of models for computer vision tasks to select the best model to perform pre-labeling or data enrichment on your data.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"step-1-select-images-and-choose-a-foundation-model-of-interest\"\u003eStep 1: Select images and choose a foundation model of interest \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jrgrl20l0x\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 1 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eTo narrow in on a subset of data and refine the images on which the predictions should be made, leverage filters in Catalog, including media attribute, natural language search, and more\u003c/li\u003e\u003cli\u003eOnce you’ve surfaced data of interest, click “Predict with Model Foundry.” You will then be prompted to choose a foundation model that you wish to use in the model run\u003c/li\u003e\u003cli\u003eSelect a model from the ‘model gallery’ based on the type of task — such as image classification, object detection, and/or image captioning\u003c/li\u003e\u003cli\u003eTo locate a specific model, you can browse the models displayed in the list, search for a specific model by name, or select individual scenario tags to show the models available for this machine learning task\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-configure-model-hyperparameters-and-submit-a-model-run\"\u003e\u003cstrong\u003eStep 2: Configure model hyperparameters and submit a model run\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/tqszjqj6l1\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 2 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve located a specific model of interest, you can click into the model to view and set the model and ontology settings.\u003c/p\u003e\u003cp\u003eEach model has an ontology defined to describe what it should predict from the data. Based on the model, there are specific options depending on the selected model and your scenario. For example, you can edit a model ontology to ignore specific features or map the model ontology to features in your own (pre-existing) ontology.\u003c/p\u003e\u003cp\u003eEach model will also have its own set of hyperparameters, which you can find in the Advanced model setting. To get an idea of how your current model settings affect the final predictions, you can generate preview predictions on up to five data rows.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWhile this step is optional, generating preview predictions allows you to confidently confirm your configuration settings. If you’re unhappy with the generated preview predictions, you can make edits to the model settings and continue to generate preview predictions until you’re satisfied with the results.\u0026nbsp;Once you’re satisfied with the predictions, you can submit your model run.\u003c/p\u003e\u003ch3 id=\"step-3-predictions-will-appear-in-the-model-tab\"\u003eStep 3: Predictions will appear in the Model tab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ro3vxhcagr\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 3 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eEach model run is submitted with a unique name, allowing you to distinguish between each subsequent model run. When the model run completes, you can:\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli\u003eView prediction results\u0026nbsp;\u003c/li\u003e\u003cli\u003eCompare prediction results across a variety of model runs different models\u003c/li\u003e\u003cli\u003eUse the prediction results to pre-label your data for a project in Quantumworks Lab Annotate \u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-repeat-steps-1-5-for-another-model-from-labelbox-model-foundry\"\u003eStep 4: Repeat steps 1-5 for another model from Quantumworks Lab Model Foundry\u003c/h3\u003e\u003cp\u003eYou can repeat steps 1-5 with a different model, on the same data and for the same desired machine learning task, to evaluate and compare model performance. By comparing the predictions and outputs from different models, you can assess and determine which one would be the most valuable in helping automate your data labeling tasks.\u003c/p\u003e\u003ch3 id=\"step-5-create-a-model-run-with-predictions-and-ground-truth\"\u003eStep 5: Create a model run with predictions and ground truth\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rzqnbhasuz\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTo create a model run with model predictions and ground truth, users currently have to use a \u003ca href=\"https://colab.research.google.com/drive/1f0WkWtsDh2LYiy35pc3JF16srj6SxeTA?ref=labelbox-guides.ghost.io#scrollTo=W8tE-H7VmKea\"\u003escript\u003c/a\u003e to import the predictions from the Foundry add-on for Quantumworks Lab Model and ground truth from a project into a new model run. \u003c/p\u003e\u003cp\u003eIn the near future, this will be possible via the UI, and the script will be optional.\u003c/p\u003e\u003ch3 id=\"step-6-evaluate-predictions-from-different-models-from-model-foundry-in-labelbox-model\"\u003eStep 6: Evaluate predictions from different models from Model foundry in Quantumworks Lab Model\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qbx1mvx878\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 5 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter running the notebook, you'll be able to visually compare model predictions between two models. Use the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors. \u003c/p\u003e\u003cp\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/p\u003e\u003ch3 id=\"step-7-send-model-predictions-as-pre-labels-to-a-labeling-project-in-annotate\"\u003eStep 7: Send model predictions as pre-labels to a labeling project in Annotate\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/d3rxjzg111\" title=\"[Guide] How to evaluate object detection models with Model Foundry - Part 6 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"640\" height=\"360\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eSelect the best performing model and leverage the model predictions as pre-labels. Rather than manually labeling data rows, select and send a subset of data to your labeling project with pre-labels to automate the process.\u003c/p\u003e\u003ch2 id=\"model-comparison-in-practice\"\u003eModel comparison in-practice:\u003c/h2\u003e\u003ch3 id=\"google-cloud-vision-vs-microsoft-azure-ai\"\u003eGoogle Cloud Vision vs Microsoft Azure AI\u0026nbsp;\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/hms66_aBLd_Kj7jsKaYhr30ChoP5kflDM6nDmlqseCR63P-8uwSriJ9FqVf-biUS-uIQelFbtSvxq7Dq-Us-tq8qy3vkyxvs_--3CSShlVLZkbF3uS2-eHEaEV0SVugIfAVxB_xmDA4kL1ZFV2Z77hA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"880\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the metrics overview, we see that Google Cloud Vision outperforms Microsoft Azure AI for recall, f1 score, intersection over union and false negatives.\u003c/p\u003e\u003cp\u003eMicrosoft Azure AI boasts a precision score of 0.8633, which outperforms the 0.7948 score of Google Cloud Vision.\u0026nbsp;Microsoft Azure AI has an intersection over union score of 0.4034, an F-1 score of 0.7805, and a recall of 0.3852. In contrast, Google Cloud Vision exhibits a superior intersection over union score of 0.4187, an F-1 score of 0.7832, and a recall of 0.4149.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe can also see that the Microsoft Azure AI model has 12,665 more false negatives than Google Cloud Vision, and for our use case, we want the model with the least false negatives.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/xWXS7lRpI1UxITFAcG50Yq3RJMdKsSqK26uRudhilYHn_LvlSdyd7WMnUv8gtaj3C-GTJq2_e_4v0ZcxjVduI-VVui0AF57ZcL-2WUQURwHPRzO7ER2rGPNOKpY8YIW0NYqgz9-SWuMfMYRjgU0Z_cc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"589\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eF1 scores for both Microsoft Azure AI and Google Cloud Vision Model are generally comparable, with a few instances showcasing superior performance by the Google Cloud Vision Model. Here are the specific results for each category:\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Microsoft Azure AI scored 0.857, while Google Cloud Vision Model scored higher with 0.932\u003c/li\u003e\u003cli\u003eFor the boat category, Microsoft Azure AI achieved a score of 0.619, compared to the slightly higher 0.656 of Google Cloud Vision Model\u003c/li\u003e\u003cli\u003eFor the person category, Microsoft Azure AI obtained a score of 0.773, whereas Google Cloud Vision Model marginally outperformed with a score of 0.785\u003c/li\u003e\u003cli\u003eFor the airplane, Microsoft Azure AI scored 0.868, with Google Cloud Vision Model again performing better with a score of 0.893\u003c/li\u003e\u003cli\u003eFor the bus category, Microsoft Azure AI had a score of 0.705, significantly lower than the 0.925 achieved by Google Cloud Vision Model\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/SC9uJlITaaDLqRvZqSjBoVGIgHdQiQHgrLiiBBovLzjHK-Yiwt-URHAGyy9Z7l5WETRaFlS2Gukx4_UbeyQrazgJiljGnP5qv-wzJgMfTfW5yCUhnvvgUPEKz870g1FEmh5pe2zpnJHxX_QMYMYXul0\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"573\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eGenerally, the Google Cloud Vision Model exhibits superior performance in terms of intersection over union for classes such as train, boat, person, airplane, and bus.\u0026nbsp; Intersection over union (IOU) is crucial as it dictates the accuracy of the bounding box prediction area.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Microsoft Azure AI scored 0.304, while Google Cloud Vision Model significantly outperformed with a score of 0.745\u003c/li\u003e\u003cli\u003eFor the boat category, Microsoft Azure AI achieved a score of 0.251, compared to a higher 0.394 by Google Cloud Vision Model\u003c/li\u003e\u003cli\u003eFor the person category, Microsoft Azure AI secured a score of 0.697, with Google Cloud Vision Model slightly ahead at 0.704\u003c/li\u003e\u003cli\u003eFor the airplane category, Microsoft Azure AI scored 0.603, whereas Google Cloud Vision Model again demonstrated superior performance with a score of 0.738\u003c/li\u003e\u003cli\u003eFor the bus category, Microsoft Azure AI recorded a low score of 0.05, markedly lower than the 0.637 achieved by Google Cloud Vision Model\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/65NRl_i7lXdcTqhIrn968_0t_p_d4nqYsEKkEDAT_j1GVt3snxpTXrNmEwB-pQmeXD4sbOPxN3_arHpEgA-iD50iEDSTXC-ZSR1nc6JzI4BIJMYBBrxrr0hUTuhIM2t94PKY6AenEBgwCYQYUnSb18M\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"573\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eIn summary, Google Cloud Vision Model exhibits superior recall values across the categories of train, boat, person, airplane, and bus.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Microsoft Azure AI exhibited a recall of 0.331, while Google Cloud Vision Model showcased a considerably higher recall of 0.769\u003c/li\u003e\u003cli\u003eFor the boat category, Microsoft Azure AI demonstrated a recall of 0.203, compared to a higher 0.308 by Google Cloud Vision Model\u003c/li\u003e\u003cli\u003eFor the person category, Microsoft Azure AI achieved a recall of 0.618, with Google Cloud Vision Model slightly ahead at 0.622\u003c/li\u003e\u003cli\u003eFor the airplane category, Microsoft Azure AI registered a recall of 0.719, whereas Google Cloud Vision Model marginally outperformed with a recall of 0.747\u003c/li\u003e\u003cli\u003eFor the bus category, Microsoft Azure AI posted a low recall of 0.04, significantly lower than the 0.652 achieved by Google Cloud Vision Model\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLet’s now take a look at how the predictions appear on the images below\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/J9PexX_zah5-eoCAgFdV6g6PftInYqzHoupESkHtdLeYzz47V0bsp5upVVrNmd5R6EXuqoKU-PrWnRa_JgwqQy0W6-vwFS-1kjoqzO3E_80WWYxaHSEcPsmpEnDbh7MCWNobGK2R5nEVqcvVSCpF0ZE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"741\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eHere we can see that the model from Google Cloud Vision has a blue bounding box that properly captures the dimension of the airplane. Whereas Azure’s orange bounding box only covers ~3/4 of the airplane.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Uy4LGTk8uhGha-Rkw6qvZaUpLQ9Zo7-EIJvPTrS0fcygRWRIJvIPi04HQ9zjTpWC4y3-A2sSy9OirucS20JObzVgfWVCVXKYpfmCItE-M7PEajJX6XalsmNKRnAO5I8MHL30r8gJHs0EurUwtNBnUGw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"763\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAnother example where Google Cloud Vision in blue bounding box has better IOU than Azure model’s orange bounding box. Based on the qualitative and quantitative analysis above, Google Cloud Vision is the superior model compared to Microsoft Azure AI.\u003c/p\u003e\u003ch3 id=\"google-cloud-vision-vs-amazon-rekognition\"\u003eGoogle Cloud Vision vs Amazon Rekognition\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eQuantitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/5Qsk1VepZUnxPhjfAGB33fhejnnJn5rOwQ0rrqQjEScKS38ep9vAnlViHSV1MynCeEVWfkOjcnZbF19tqFeJFXnYZ2SpLuZp5CR8x3QJ4-w8Z-XDeW-NiMKMOogsnbdeBHxvCc1wmNEaCXMG05GaY0s\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"655\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eFrom the metrics overview, Amazon Rekognition generally demonstrates better performance in false negatives, true positives, recall, and IOU against Google Cloud Vision.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor false negatives, Amazon Rekognition reported 21,935, whereas Google Cloud Vision had a slightly higher count of 22,868\u003c/li\u003e\u003cli\u003eFor true positives, Amazon Rekognition significantly outperformed with 25,953, compared to 10,112 recorded by Google Cloud Vision\u003c/li\u003e\u003cli\u003eFor recall, Amazon Rekognition showcased a higher value of 0.5419, while Google Cloud Vision exhibited a lower recall of 0.3913\u003c/li\u003e\u003cli\u003eFor Intersection over Union (IOU), Amazon Rekognition achieved a score of 0.4596, surpassing the 0.4212 scored by Google Cloud Vision\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/B5_bkD0aYI0lRIoUJZpX7m-J5_BzgJbzrCKhWtbzhOcw4EjWt2NY858HNCFV1duFbBwTpuHUZnMl8rggUcMQwWc6-7QhlKIgikUMKFZocxCeNiD0auUQLHSEd8xEVuSLUpIVtXU2HgEfO9Nm0TxzmTM\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"518\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAmazon Rekognition outperformed Google Cloud Vision in the train, airplane, and bus categories\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Amazon Rekognition achieved an F-1 score of 0.969, outperforming Google Cloud Vision, which scored 0.932\u003c/li\u003e\u003cli\u003eFor the boat category, Amazon Rekognition scored 0.747, while Google Cloud Vision significantly outshone with a score of 0.956\u003c/li\u003e\u003cli\u003eFor the person category, Amazon Rekognition registered an F-1 score of 0.566, with Google Cloud Vision achieving a higher score of 0.773\u003c/li\u003e\u003cli\u003eFor the airplane category, Amazon Rekognition had an F-1 score of 0.919, slightly higher than the 0.893 scored by Google Cloud Vision\u003c/li\u003e\u003cli\u003eFor the bus category, Amazon Rekognition secured an F-1 score of 0.929, marginally outperforming Google Cloud Vision, which scored 0.925\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/XbeoxMVEBEttqsPy4S3P9gRbofKkTYbuQT_UeFdMmkCMWAt_YmKRHdXU6ltUSe7c1B4ov2PhYdnpJC4LS8XTj07tVu6HSqfnhDXDjA0-oObUG76juz61tgutFGdKRU84LGc18j6yc5NI4Ip8nzrF03c\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"518\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAmazon Rekognition led in the bus, boat and person categories,\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Amazon Rekognition has an IOU score of 0.741, closely competing with Google Cloud Vision which scored 0.745\u003c/li\u003e\u003cli\u003eFor the boat category, Amazon Rekognition outperformed with an IOU score of 0.454, compared to Google Cloud Vision's 0.394\u003c/li\u003e\u003cli\u003eFor the person category, Amazon Rekognition significantly led with a score of 0.813, while Google Cloud Vision scored 0.704\u003c/li\u003e\u003cli\u003eFor the airplane category, Amazon Rekognition recorded an IOU of 0.677, slightly below Google Cloud Vision's 0.738\u003c/li\u003e\u003cli\u003eFor the bus category, Amazon Rekognition and Google Cloud Vision scored closely with IOU scores of 0.65 and 0.637 respectively\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/tCrtU-Xg3sv5kX3oNYLbqafdToRY9VJ78cWeF9UwQBOAe7fvBBIV_cJFQbaXDMiLxxOlxMk1bU3DSzvNbsjvS8mIzwn22L85p4xnqsr1zIYUfUAhP-8j2onpPqDJSVgPxfFlEi1eTomuh4uzaBBBt6o\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"518\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eRecall for Amazon Rekognition for boat, person, airplane, and bus, is better than Google Cloud Vision.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor the train category, Amazon Rekognition demonstrated a recall score of 0.893, while Google Cloud Vision significantly outperformed with a score of 0.983\u003c/li\u003e\u003cli\u003eFor the boat category, Amazon Rekognition led with a recall score of 0.432, compared to Google Cloud Vision's lower score of 0.308\u003c/li\u003e\u003cli\u003eFor the person category, Amazon Rekognition achieved a higher recall score of 0.787, surpassing Google Cloud Vision's 0.622\u003c/li\u003e\u003cli\u003eFor the airplane category, Amazon Rekognition outperformed with a recall score of 0.851, as opposed to Google Cloud Vision's 0.743\u003c/li\u003e\u003cli\u003eFor the bus category, Amazon Rekognition showcased a recall score of 0.836, significantly higher than Google Cloud Vision's 0.652\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eQualitative comparison\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/rgrVd-plFthhvoZgeUDJgc8BXH4KwkKK-wQMBmrAY-tkhr5lh-PP3gyyGIRVIWWL9_sO1OBgCqhrGAdr9gmPy4mwXkGOnlOXItmSicvvPQ1H5hExjipwUVye2Ep970YX33rzEYTRlH4WIIVUXVrhybI\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"810\"\u003e\u003c/figure\u003e\u003cp\u003eHere, Google Cloud Vision in the orange bounding box has detected only one boat, but Amazon Rekognition has detected 5 more boats, a person, and a car.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/JycIqkuvst751-UDITIYiwj-xH-3WDp_5fE3I8KKFvBOicjJ80dfPXt4wjDbfbsN1fqYwPA4JbHDBGyI6E8sjS1J-yd3e52S67093113NvjpmKDQ4Worn0_-_nLDR9tiVpfC6PjL-1hcEmrttzVApHs\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"810\"\u003e\u003c/figure\u003e\u003cp\u003eAmazon Rekognition in the blue bounding box has detected more boats and a clock tower.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/oOtbtSHmfi3Ob6pf7hLRSk1JrSlO6zWUlM2m92ILaa20Oc0HL4GZYcqzYh6mi-kVbuU2l8oyObhbB36EgrvLJOkpPEyBke7hZ6BlMWwHb08bUhln0mFMAry9EBAczeRuLFCN9Dz24BlTA3lLGdZQypA\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"860\"\u003e\u003c/figure\u003e\u003cp\u003eAmazon Rekognition in the blue bounding box has an IOU of the truck with full coverage, whereas the IOU for Google Cloud Vision is around 90%.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/l87bsRsULhAXMrcUMtDbW6k2p3xGA4E7TpxuuaiVzjZGlWO4WjHhtbAMwomLKXf2BD58DXi6BznC-2zSmKvPbuki11Y5F3deYJGYC9tsfpLgqShOwS2IuhI0DWb1NHJufQDnhx7BuaVqgs3JQMss4hg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"810\"\u003e\u003c/figure\u003e\u003cp\u003eThe IOU for person is also better in addition to being able to detect cars in the background for Amazon Rekognition in the blue bounding box compared to Google Cloud Vision. Based on the analysis above, Amazon Rekognition is the best model for our use case.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSend the predictions as pre-labels to Quantumworks Lab Annotate for labeling\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eSince we've evaluated that Amazon Rekognition is the best model for our use case, we can send model predictions as pre-labels to our labeling project by highlighting all data rows and selecting \"Add batch to project.\" \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Yxobsiulkb3gR3KREdjS2x0m_aSOTVV_Mx_XXnGrRk1Q-8YueaAw33Y_uoZxJb6rDhJsF2PqWWb2yc2H5P6vEvfKW6qDoGwHFiJQ1VqeD5COxfagUNORNuzco1n6CXnKqJAv67UAGH8Yw_dQsdi6fHc\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"484\"\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eIn conclusion, you can leverage the Foundry add-on for Quantumworks Lab Model to not only select the most appropriate model to accelerate data labeling, but to automate data labeling workflows. Use quantitative and qualitative analysis, along with model metrics, to surface the strengths and limitations of each model and select the best performing model for your use case. Doing so can help reveal detailed insights, such as seen in the above comparison between Google Cloud Vision and Amazon Rekognition. In the above model comparison example, we can see that Amazon Rekognition emerged as particularly well-suited for our project’s requirements and allows us to rapidly automate data tasks for our given use case.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Model Foundry\u003c/a\u003e streamlines the process of comparing model predictions, ensuring teams are leveraging the most optimal model for data enrichment and automation tasks. With the right model, teams can easily create pre-labels in Quantumworks Lab Annotate – rather than starting from scratch, teams can boost labeling productivity by correcting the existing pre-labels.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started,\u0026nbsp;\u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=guide103123\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e\u0026nbsp;or\u0026nbsp;\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"653feb16375d13000123da16","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3084.png","featured":false,"visibility":"public","created_at":"2023-10-30T17:42:46.000+00:00","updated_at":"2024-02-02T18:32:57.000+00:00","published_at":"2023-11-01T15:46:26.000+00:00","custom_excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-evaluate-object-detection-models-with-model-foundry","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},"url":"https://labelbox-guides.ghost.io/how-to-evaluate-object-detection-models-using-labelbox-model-foundry/","excerpt":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","reading_time":12,"access":true,"comments":false,"og_image":null,"og_title":"How to evaluate object detection models with Model Foundry","og_description":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3084-1.png","twitter_title":"How to evaluate object detection models with Model Foundry","twitter_description":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","meta_title":"How to evaluate object detection models with Model Foundry","meta_description":"Explore how Model Foundry enables teams to efficiently compare and select the right foundation model to kickstart computer vision development, decreasing costs and accelerating time-to-value. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6537fb91375d13000123d799","uuid":"6609e3d3-fc0e-4183-91ef-a83c801c7c6d","title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine-Tuning: A technical walkthrough using OpenAI's APIs \u0026 models","slug":"zero-shot-learning-few-shot-learning-fine-tuning","html":"\u003cp\u003eWith large language models (LLMs) gaining popularity, new techniques have emerged for applying them to NLP tasks. Three techniques in particular — zero-shot learning, few-shot learning, and fine-tuning — take different approaches to leveraging LLMs. In this guide, we’ll walk through the key difference between these techniques and how to implement them.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWe’ll walk through a case study of extracting airline names from tweets to compare the techniques. Using an entity extraction dataset, we’ll benchmark performance starting with zero-shot prompts, then experiment with few-shot learning, and finally fine-tune a model. By analyzing the results, we can better understand when to use zero-shot, few-shot, or fine-tuning with LLMs. You’ll also pick up tips for constructing effective prompts and setting up LLM experiments.\u003c/p\u003e\u003cp\u003eThe goal of this guide is to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompare the quantitative results among zero-shot learning, few-shot learning, and fine-tuning on an NER use case\u003c/li\u003e\u003cli\u003eExplore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026amp; Quantumworks Lab Model\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"zero-shot-learning-few-shot-learning-and-fine-tuning-in-action\"\u003eZero-shot learning, few-shot learning, and fine-tuning in action\u003c/h2\u003e\u003cp\u003eFor the purposes of this case study, we will be walking through an example of entity extraction featured in \u003ca href=\"https://colab.research.google.com/drive/1OCD8ivCtPS84cEhtjXkIWNAQX9oyKfFe?usp=sharing\u0026ref=labelbox-guides.ghost.io\"\u003ethis Google Colab Notebook\u003c/a\u003e. Specifically, we have a dataset of tweets about major airlines, and the task is to use an API to extract all airlines names that appear in the tweets.\u0026nbsp;\u003c/p\u003e\u003cp\u003eThe full dataset can be found on Kaggle \u003ca href=\"https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eHere is an example data row:\u003c/p\u003e\u003cp\u003e\"@AmericanAir do you have the phone number of a supervisor i can speak to regarding my travels today,['American Airlines’]\u003c/p\u003e\u003cp\u003ewhere the:\u003c/p\u003e\u003cp\u003eTWEET: @AmericanAir do you have the phone number of a supervisor i can speak to regarding my travels today\u003c/p\u003e\u003cp\u003eLABEL: [‘American Airlines']\"\u003c/p\u003e\u003cp\u003eBefore delving into the details, a few key technical definitions to keep in mind:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eZero-shot learning\u003c/strong\u003e — a technique whereby we prompt an LLM without any examples, attempting to take advantage of the reasoning patterns it has gleaned (i.e. a generalist LLM)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFew-shot learning\u003c/strong\u003e — a technique whereby we prompt an LLM with several concrete examples of task performance\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-tuning\u003c/strong\u003e — a technique whereby we take an off-the-shelf open-source or proprietary model, re-train it on a variety of concrete examples, and save the updated weights as a new model checkpoint\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"establishing-a-benchmark-baseline-with-zero-shot-learning\"\u003eEstablishing a benchmark baseline with zero-shot learning\u003c/h3\u003e\u003cp\u003eAs with any scientific or machine learning experiment, it is important to establish a benchmark baseline. For this case study, we will use zero-shot learning as the baseline by experimenting with various zero-shot prompts and evaluating the performance (precision, recall, f1-score, accuracy) of these prompts against the test set.\u003c/p\u003e\u003cp\u003eThe demo video below shows how we can leverage the \u003cstrong\u003e‘Humans Generate Prompt’ \u003c/strong\u003eoption of the \u003ca href=\"https://www.google.com/url?q=https://docs.labelbox.com/docs/llm-data-generation\u0026sa=D\u0026source=docs\u0026ust=1732675975664445\u0026usg=AOvVaw1fM_zT8IUOw2SSFmqUnUcd\" rel=\"noreferrer\"\u003ePrompt and Response Editor\u003c/a\u003e within Quantumworks Lab Annotate to create various zero-shot prompts using in-house and/or external teams to scale out our annotation operations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/44tuzazslq\" title=\"[Guide] Create Annotate Project \u0026amp; Airline Prompts in LLM Editor Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"534\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce we have constructed our dataset of zero-shot prompts within Quantumworks Lab using a prompt engineering workforce, we can export them from the Quantumworks Lab UI  the Quantumworks Lab Python SDK and use them in our script, as shown in the video below.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/kryaxyhdx3\" title=\"[Guide] Exporting Prompts from LB Annotate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"620\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eUsing gpt-3.5-turbo, we see the results of the prompts, along with their performance metrics.\u003c/p\u003e\u003cp\u003eFrom these results, we observe that prompts that fared best provided a clear and structured instruction to the model. These prompts explicitly mention the expected format for identifying airline names. Furthermore, these prompt structures introduce a pattern that the model can recognize and follow. In the case of \"Detect airline references...\" the model is prompted to look for references in a specific format (e.g. hashtags), which may be a common pattern in tweets mentioning airlines.\u003c/p\u003e\u003cp\u003ePrompts that fared worst had a theme: ambiguity in the prompt response format. Examples like \"What are the airlines in this tweet?\" and \"Find all airline mentions in the tweet\" are open- ended and do not provide a specific structure, making it harder for the model to interpret the task.\u003c/p\u003e\u003cp\u003eWhat’s interesting to note is that the prompt “Identify airlines like this - [#AIRLINE_NAME_1]:'{tweet}’” did NOT perform well even though it provided an example response format. This underscores the profound impact that punctuation and grammatical structure can have for prompt engineering. Instead of interpreting “[#AIRLINE_NAME_1]\" as the LLM-output format, the LLM instead interpreted it as a pattern-matching task to identify all airlines within a tweet that contain this specific format [#AIRLINE_NAME_1], of which there are none (hence, 0% across the evaluation metrics).\u003c/p\u003e\u003cp\u003eIn addition to testing different prompts, we can run various experiments to evaluate the efficacy of the task and evaluate the impact of those experiments in Quantumworks Lab Model. One such experiment could be to compare different models (GPT-4 vs. GPT-3.5, etc.). The video below shows how we can compare GPT-4 vs. GPT-3.5 on how each model performs on extracting airlines from the prompts we created above.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/13yquhzp4o\" title=\"[Guide] Airline extraction model comparison Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"534\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eZero-shot learning netted us the following baseline benchmark on the test set: 19%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"few-shot-learning\"\u003eFew-shot learning\u0026nbsp;\u003c/h3\u003e\u003cp\u003eTo build upon this benchmark, we used one of the prompts that performed well in zero-shot learning in tandem with few-shot learning — a technique whereby along with the prompt, we also feed an LLM concrete examples of task performance. These concrete examples are chosen from our training dataset (found in \u003cstrong\u003eairline_train.csv\u003c/strong\u003e).\u003c/p\u003e\u003cp\u003eThis is an example few-shot prompt that we passed to the LLM:\u003c/p\u003e\u003cp\u003eGiven the following tweets and their corresponding airlines, separated by new lines:\u003c/p\u003e\u003cp\u003e1) SouthwestAir bags fly free..just not to where you're going.,['Southwest Airlines']\u003c/p\u003e\u003cp\u003e2) Jet Blue I don't know- no one would tell me where they were coming from,['JetBlue Airways']\u003c/p\u003e\u003cp\u003ePlease extract the airline(s) from the following tweet:\u003c/p\u003e\u003cp\u003e\"SouthwestAir Just got companion pass and trying to add companion flg. Help!\"\u003c/p\u003e\u003cp\u003eUsing the following format - ['#AIRLINE_NAME_1] for one airline or ['#AIRLINE_NAME_1, #AIRLINE_NAME_2...] for multiple airlines.\u003c/p\u003e\u003cp\u003eEvaluating our model (gpt-3.5-turbo) on the test set via few-shot learning, we achieved an accuracy of 96.66%! There were 7 total misclassifications. Further inspection into these misclassifications actually reveals that there may be issues within the ground-truth dataset.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFew-shot learning netted us the following accuracy benchmark on the test set: 97%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"fine-tuning-with-a-training-dataset\"\u003eFine-tuning with a training dataset\u003c/h3\u003e\u003cp\u003eLastly, we seek to determine whether fine-tuning would improve our results. To ensure parity across our experiments, we used the same 100 randomly generated examples from the training dataset for few-shot learning as our control variable for the fine-tuning task.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFine-tuning netted us the following accuracy benchmark on the test set: 97%.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"comparing-results\"\u003e\u003cstrong\u003eComparing results\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThese were the final results:\u003c/p\u003e\u003cp\u003e1) Zero-shot learning netted us the following accuracy baseline benchmark on the test set: 19%.\u003c/p\u003e\u003cp\u003e2) Few-shot learning netted us the following accuracy benchmark on the test set: 97%.\u003c/p\u003e\u003cp\u003e3) Fine tuning netted us the following accuracy benchmark on the test set: 91%.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKey takeaways\u003c/strong\u003e\u003c/p\u003e\u003cp\u003ePrompt engineering in tandem with few-shot learning versus fine-tuning both yield similar results for the task of extracting airline references from tweets. The ultimate consideration between the two boils down to achieving economies of scale. If teams find themselves needing to execute a prompt 100,000 times, the effort invested in terms of both human hours and GPU usage (or, token costs, if utilizing an API) can be justified when fine-tuning, as the cumulative savings in prompt tokens and the potential for improved output quality can accumulate significantly. Conversely, if we’re only using the prompt ten times and it's already effective, there's no rationale for fine-tuning it.\u003c/p\u003e\u003cp\u003eRegardless of which option you choose, we’ve seen how \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Annotate\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e can help achieve both outcomes.\u003c/p\u003e\u003cp\u003eWe can also improve results by ensuring variability within the few-shot learning examples. Currently, we are using a naive approach by selecting 100 randomly generated examples from the training dataset, so that it fits within the context window of gpt-3.5-turbo. Despite leveraging ~10% of our training data (100 rows / 900 possible training data rows), it’s less about data quantity and more about data quality. If we can curate a few-shot learning dataset of only 100 rows that has enough variance to be representative of the larger 900 rows, then that would be most ideal, from a token-sizing and, subsequently, cost perspective in addition to an engineering perspective (achieving more with less).\u003c/p\u003e\u003cp\u003eExamples of variance include tweet structure as well as a healthy mix of tweets with multiple airlines referenced. For tweet structure, we can use regular expressions to create patterns to capture mentions (@username), hashtags (#hashtag), airline stock ticker symbols, or emojis. In our use case, hashtags and ticker symbols would be super beneficial, since we see them scattered throughout our training dataset (e.g. #LUVisbetter, #jetblue, #UnitedAirlines, AA, etc.).\u003c/p\u003e\u003cp\u003eUsing \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e, we can version our few-shot learning and fine-tuning experiments by tying each experiment to a corresponding model run. Comparing evaluation metrics across different model runs allows us to either:\u003c/p\u003e\u003cul\u003e\u003cli\u003eChoose the few-shot learning prompt with the best precision, accuracy, and/or recall metric\u003c/li\u003e\u003cli\u003eChoose the fine-tuned model with the best precision, accuracy, and/or recall metric\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"get-started-today\"\u003eGet started today \u003c/h2\u003e\u003cp\u003eThrough our case study extracting airline names from tweets, we've explored the key differences between zero-shot learning, few-shot learning, and fine-tuning for applying large language models to NLP tasks.\u003c/p\u003e\u003cp\u003eThe best technique depends on your use case and available resources. The key is understanding how different data inputs affect an LLM's behavior. With the right approach, you can take advantage of LLMs' reasoning skills for a wide range of NLP applications. You can leverage each of these learning techniques with Quantumworks Lab’s LLM data generation editor and Quantumworks Lab Model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLabelbox is a data factory that empowers teams to generate high-quality data and accelerate the development of differentiated genAI solutions. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026attr=intercom\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026gclid=Cj0KCQiA6Ou5BhCrARIsAPoTxrBVxRVcyUPeK8y7Mn2rtcM2M257gsxyxejwhFES91vcP7xYdoij4OQaAjUwEALw_wcB\u0026attr=arcade\u0026landingPageAnonymousId=%22ce322bab-56ea-43b8-a113-000851a3ebaf%22\u0026referrer_url=https://www.google.com/\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"6537fb91375d13000123d799","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083.png","featured":false,"visibility":"public","created_at":"2023-10-24T17:14:57.000+00:00","updated_at":"2024-11-27T02:11:35.000+00:00","published_at":"2023-10-24T20:34:32.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/zero-shot-learning-few-shot-learning-fine-tuning","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},"url":"https://labelbox-guides.ghost.io/zero-shot-learning-few-shot-learning-fine-tuning/","excerpt":"With large language models (LLMs) gaining popularity, new techniques have emerged for applying them to NLP tasks. Three techniques in particular — zero-shot learning, few-shot learning, and fine-tuning — take different approaches to leveraging LLMs. In this guide, we’ll walk through the key difference between these techniques and how to implement them. \n\nWe’ll walk through a case study of extracting airline names from tweets to compare the techniques. Using an entity extraction dataset, we’ll be","reading_time":6,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083-2.png","og_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","og_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3083-1.png","twitter_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","twitter_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","meta_title":"Zero-Shot Learning vs. Few-Shot Learning vs. Fine Tuning","meta_description":"In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Quantumworks Lab’s LLM Editor \u0026 Quantumworks Lab Model","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"65300bc0a6fedb0001417480","uuid":"59641c97-b4ef-4ade-8e83-52f3887883d0","title":"How to use the model Foundry for automated data labeling and enrichment","slug":"guide-to-using-model-foundry","html":"\u003cp\u003eFoundation models, such as GPT-4, are ushering in a new era of AI by outperforming humans on numerous tasks across modalities including images and language. With the introduction of\u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e \u003c/a\u003e\u003ca href=\"https://labelbox.com/product/model/foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ethe Foundry add-on for Quantumworks Lab Model\u003c/u\u003e\u003c/a\u003e, we’re bringing the power of foundation models into the Quantumworks Lab platform.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eMeet your new AI co-pilot: Foundry\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eLabelbox Foundry enables AI teams to use world-class foundation models to enrich datasets and automate tasks. In just a few clicks, AI builders can explore, test and integrate powerful models to build vital workflows for pre-labeling, or other specific data tasks. Kickstart your AI efforts with this AI copilot to build intelligent applications faster than ever.\u003c/p\u003e\u003cp\u003eEarly tests with Fortune 500 companies show up to 88% reductions in human labeling time, with complex tasks going from days to hours. Foundry integrates these state-of-the-art models into every step of the workflow, unlocking the next evolution of AI development. Kickstart your AI and data labeling efforts with this co-pilot to build intelligent applications faster than ever.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re excited to announce that Foundry is available as an add-on for Quantumworks Lab Model!\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith Foundry, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAccess the world’s most cutting-edge models and combine them with human-in-the-loop systems to accelerate model development.\u003c/li\u003e\u003cli\u003ePre-label data in a few clicks to reduce labeling costs by up to 90%.\u003c/li\u003e\u003cli\u003eTailor intelligence to your needs using Quantumworks Lab’s comprehensive end-to-end platform and workflow in Foundry. From fine-tuning to model distillation, you can customize intelligence beyond pre-built AI.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFoundry works across Quantumworks Lab's products — Catalog, Annotate, and Model — to supercharge your labeling and model development.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"how-to-access-foundry\"\u003eHow to access Foundry:\u003c/h2\u003e\u003ch3 id=\"free-tier-organizations\"\u003eFree Tier organizations\u003c/h3\u003e\u003cp\u003eFoundry is only available to our Starter and Enterprise plans, to access Foundry you will need to:\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Upgrade your account to our \u003cstrong\u003eStarter plan\u003c/strong\u003e:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn the \u003ca href=\"https://app.labelbox.com/workspace-settings/billing?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eBilling tab\u003c/u\u003e\u003c/a\u003e, locate “Starter” in the All Plans list and select “Switch to Plan.” \u003cem\u003eThe credit card on file will only be charged when you exceed your existing free 10,000 LBU.\u0026nbsp;\u003c/em\u003e\u003c/li\u003e\u003cli\u003eUpgrades take effect immediately so you'll have access to Foundry right away on the Starter plan. After upgrading, you’ll see the option to activate Foundry for your organization.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-04_16-22-54--1--min-1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1904\" height=\"932\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/2023-12-04_16-22-54--1--min-1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/2023-12-04_16-22-54--1--min-1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/12/2023-12-04_16-22-54--1--min-1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-04_16-22-54--1--min-1.gif 1904w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e2) After upgrading to our Starter plan, please follow the instructions under the Starter plan below to activate Foundry as an add-on for your organization.\u003c/p\u003e\u003ch3 id=\"self-serve-organizations-on-a-starter-or-standard-plan\"\u003eSelf-serve organizations on a Starter or Standard plan\u0026nbsp;\u003c/h3\u003e\u003cp\u003eAs part of the self-serve Starter or Standard plan, you automatically have the ability to opt-in to using Foundry.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eTo generate model predictions, just submit a model run and you’ll be guided to activate Foundry along the way:\u0026nbsp;\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Generate predictions:\u003c/strong\u003e Start by selecting data in Catalog and click “Predict with Foundry.” You’ll be able to select a foundation model and submit a model run to generate predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Activate Foundry:\u003c/strong\u003e The first time you submit a model run, you’ll see the option to activate Foundry for your organization’s \u003ca href=\"https://docs.labelbox.com/docs/members-group-management?ref=labelbox-guides.ghost.io#member-roles\"\u003e\u003cu\u003eAdmins\u003c/u\u003e\u003c/a\u003e. You’ll need to agree to Quantumworks Lab’s Model Foundry add-on service terms and confirm you understand the associated compute fees.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Payment method:\u003c/strong\u003e With Foundry, you will be charged for the Quantumworks Lab units (LBUs) your account consumes, plus any\u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003epass-through compute costs\u003c/u\u003e\u003c/a\u003e for the models you use.\u003c/p\u003e\u003cp\u003eFor self-serve Starter and Standard tier to enable billing for these model compute charges, you'll be asked to confirm the credit card on file for your Quantumworks Lab account. Alternatively, you may add another card if you prefer to keep the Foundry charges separate. By confirming your payment method, you agree to let Quantumworks Lab to bill your card as Foundry model compute fees accrue based on your usage.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/OLBO-X7WF_y9EXTcmQ5nM1-6W75tJyYWojkmQp_IH-bdx2R_i985qA00Tbf3nMqHLGzUIkKo-0MES9-jfKUlv86kFjlmASbzfCo_CZTAau-38nUtH22IYHxUJ4mHgYi_VgfTdf9CaBsT3PK-46FeAF8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"305\"\u003e\u003c/figure\u003e\u003ch3 id=\"enterprise-organizations\"\u003eEnterprise organizations\u003c/h3\u003e\u003cp\u003eAs an organization on our Enterprise plan, you automatically have the ability to opt-in to using Foundry.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cem\u003eTo generate model predictions, just submit a model run and you’ll be guided to activate Foundry along the way:\u0026nbsp;\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Generate predictions:\u003c/strong\u003e Start by selecting data in Catalog and click “Predict with Foundry.” You’ll be able to select a foundation model and submit a model run to generate predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Activate Foundry:\u003c/strong\u003e The first time you submit a model run, you’ll see the option to activate Foundry for your organization’s \u003ca href=\"https://docs.labelbox.com/docs/members-group-management?ref=labelbox-guides.ghost.io#member-roles\"\u003e\u003cu\u003eAdmins\u003c/u\u003e\u003c/a\u003e. You’ll need to agree to Quantumworks Lab’s Model Foundry add-on service terms and confirm you understand the associated compute fees.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Payment method:\u003c/strong\u003e With Foundry, you will be charged for the Quantumworks Lab units (LBUs) your account consumes, plus any\u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003epass-through compute costs\u003c/u\u003e\u003c/a\u003e for the models you use.\u003c/p\u003e\u003cp\u003eIf you are on an annual contract plan, you may add a credit card on file to pay for the associated compute costs of a model or choose to receive a monthly invoice at the end of each month based on your organization’s Foundry usage.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-06_12-19-47.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1016\" height=\"738\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/12/2023-12-06_12-19-47.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/12/2023-12-06_12-19-47.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/12/2023-12-06_12-19-47.png 1016w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"see-it-in-action-how-foundry-works\"\u003eSee it in action: How Foundry works\u003c/h2\u003e\u003cp\u003eIn the below interactive demos, you'll learn how to leverage foundation models to generate model predictions, send predictions as pre-labels to a labeling project in Annotate, and verify the predictions with human-in-the-loop review:\u003c/p\u003e\u003ch3 id=\"step-1-how-to-generate-model-predictions-with-foundry\"\u003eStep 1: How to generate model predictions with Foundry\u003c/h3\u003e\u003cp\u003eAccess a vast range of ready-to-use foundation models that embed advanced AI into your data tasks with ease. Quickly generate predictions to pre-label datasets or to enrich your existing data to extract better insights that boost productivity and increase time-savings.\u003c/p\u003e\u003ch3 id=\"step-2-how-to-send-foundry-predictions-as-pre-labels-to-a-labeling-project\"\u003e\u003cstrong\u003eStep 2: How to send Foundry predictions as pre-labels to a labeling project\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRather than labeling from scratch, combine the power of foundation models with human-in-the-loop review to accelerate your labeling operations.\u003c/p\u003e\u003ch3 id=\"step-3-how-to-verify-pre-labels-with-human-in-the-loop-review\"\u003eStep 3: How to verify pre-labels with human-in-the-loop review\u003c/h3\u003e\u003cp\u003eFocus human intelligence on critical quality assurance instead of on initial labeling efforts. Seamlessly validate model-generated pre-labels in Annotate – approving accurate predictions with a click and easily editing or sending incorrect labels to be corrected.\u0026nbsp;\u003c/p\u003e\u003cp\u003eCheck out the below tutorials to learn how Foundry can accelerate your pre-labeling and data enrichment workflows in Labelbox.\u0026nbsp;\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003cdiv style=\"position: relative; padding-bottom: calc(48.601036269430054% + 41px); height: 0;\"\u003e\u003ciframe src=\"https://demo.arcade.software/HnecUTysIiTR66fMWJ1C?embed\" title=\"Object Detection - Foundry Arcade \" frameborder=\"0\" loading=\"lazy\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;color-scheme: light;\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\u003c!--kg-card-end: html--\u003e\n\n\u003c!--kg-card-begin: html--\u003e\n\u003cdiv style=\"position: relative; padding-bottom: calc(48.57586742620404% + 41px); height: 0;\"\u003e\u003ciframe src=\"https://demo.arcade.software/PqkNFZRaNrGWBPjlotve?embed\" title=\"Named Entity Recognition - Foundry Arcade\" frameborder=\"0\" loading=\"lazy\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;color-scheme: light;\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003ch2 id=\"what-use-cases-is-foundry-available-for\"\u003eWhat use cases is Foundry available for?\u003c/h2\u003e\u003cp\u003eFoundry currently supports a variety of tasks for computer vision and natural language processing. This includes:\u003c/p\u003e\u003ch3 id=\"computer-vision\"\u003eComputer Vision\u003c/h3\u003e\u003cul\u003e\u003cli\u003eObject detection\u003c/li\u003e\u003cli\u003eImage classification\u003c/li\u003e\u003cli\u003eImage segmentation \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWatch the below demo to see how to use Foundry for an object detection use case with Amazon Rekognition and Grounding Dino:\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/78pt3397io\" title=\"Model Foundry - Image Demo with Amazon Rekognition and Grounding Dino Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThis demo uses Amazon Rekognition and Grounding Dino, but you can leverage any other \u003ca href=\"https://labelbox.com/model-foundry/models/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision model available in Foundry\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"natural-language-processing\"\u003eNatural Language Processing\u003c/h3\u003e\u003cul\u003e\u003cli\u003eText generation\u003c/li\u003e\u003cli\u003eTranslation\u003c/li\u003e\u003cli\u003eQuestion answering\u003c/li\u003e\u003cli\u003eZero-shot classification\u003c/li\u003e\u003cli\u003eSummarization\u003c/li\u003e\u003cli\u003eConversational\u003c/li\u003e\u003cli\u003eText classification\u003c/li\u003e\u003cli\u003eNamed entity recognition\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWatch the below demo to see how to use Foundry for a text classification use case with GPT-4:\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jf0g7ou9bk\" title=\"Model Foundry - Text Demo with GPT-4 Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThis demo uses GPT-4, but you can leverage any other \u003ca href=\"https://labelbox.com/model-foundry/models/?ref=labelbox-guides.ghost.io\"\u003elarge language model available in Foundry\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"how-is-foundry-priced\"\u003e\u003cstrong\u003eHow is Foundry priced?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFoundry pricing will be calculated and billed monthly based on the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInference cost\u003c/strong\u003e – Quantumworks Lab will charge customers for inference costs for all models hosted by Labelbox. Inference costs will be bespoke to each model available in Foundry. The inference price is determined based on vendors or our compute costs – these are published publicly on \u003ca href=\"https://labelbox.com/product/model/foundry-pricing/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eour website\u003c/u\u003e\u003c/a\u003e as well as inside the product.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLabelbox's platform cost\u003c/strong\u003e – each asset with predictions generated by Foundry will accrue LBUs.\u0026nbsp;\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eModel LBU\u003c/strong\u003e is consumed for every data row that goes through Foundry. \u003cstrong\u003eAnnotate LBU\u003c/strong\u003e is consumed for every data row that has a prediction (from Foundry) submitted as a label.\u003c/p\u003e\u003cp\u003eThe unit compute costs are listed on the model card for each model found in the Quantumworks Lab app.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh7-us.googleusercontent.com/Iqv7Lg9_XlQFIdXK_NVWx-Az4ig-zIlTZ2H2ZFiCsb-BQ-U94-VdccpvJ17SpayYK4VkS-GbAnGBWu8qpvfiARtORoPagIpr4wcqUSHRWq9LcmLBJ5eNOGCLMpQ-d-_c1_Xo9IrT1iZ9nAmigLk0A_A\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"220\"\u003e\u003c/figure\u003e\u003cp\u003eLearn more about the pricing of the Foundry add-on for Quantumworks Lab Model on our \u003ca href=\"https://labelbox.com/pricing/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003epricing page\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"where-can-i-provide-feedback-or-ask-questions\"\u003e\u003cstrong\u003eWhere can I provide feedback or ask questions?\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf you have questions or feedback related to the Foundry workflow, please\u003ca href=\"https://labelbox.atlassian.net/servicedesk/customer/portal/2/group/3/create/214?ref=labelbox-guides.ghost.io\"\u003e \u003cu\u003esubmit a ticket here\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"additional-resources\"\u003eAdditional Resources\u0026nbsp;\u003c/h2\u003e\u003cp\u003eTo learn more about Foundry and familiarize yourself with the workflow, we recommend checking out the below resources:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox/docs/foundry?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eFoundry Documentation\u003c/a\u003e\u0026nbsp;\u003c/li\u003e\u003cli\u003eDemo videos\u0026nbsp;\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.wistia.com/medias/78pt3397io?ref=labelbox-guides.ghost.io\"\u003eImage \u003c/a\u003e- Amazon Rekognition and Grounding DINO Demo\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.wistia.com/medias/jf0g7ou9bk?ref=labelbox-guides.ghost.io\"\u003eText \u003c/a\u003e- GPT-4 Demo\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/product/model/foundry-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eModels currently available in Foundry\u003c/a\u003e\u003c/li\u003e\u003cli\u003eRecent blog posts exploring the power of foundation models:\u003cul\u003e\u003cli\u003eExplore six of the most powerful foundation models available to AI builders, the use cases and applications they are best suited for. \u003ca href=\"https://labelbox.com/blog/6-cutting-edge-foundation-models-for-computer-vision-and-how-to-use-them/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eGPT-4 offers a versatile toolkit to help enterprises kickstart labeling. Learn how to leverage GPT-4 for your machine learning or specific business use case. \u003ca href=\"https://labelbox.com/blog/how-to-unlock-the-full-power-of-gpt-4-for-enterprise-ai/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eMany businesses have more specialized and domain-specific use cases. Discover the benefits of incorporating foundation models into your specialized AI application development workflow. \u003ca href=\"https://labelbox.com/blog/why-you-should-leverage-foundation-models-for-specialized-ai-applications/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003cli\u003eRemoving personally identifiable information from datasets is crucial for teams building AI. Explore how you can extract PII from your datasets with higher accuracy and speed. \u003ca href=\"https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/?ref=labelbox-guides.ghost.io\"\u003eRead now\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"65300bc0a6fedb0001417480","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/12/Welcome-to-the-Foundry-1--2-.png","featured":false,"visibility":"public","created_at":"2023-10-18T16:45:52.000+00:00","updated_at":"2023-12-12T15:05:47.000+00:00","published_at":"2023-10-18T17:21:36.000+00:00","custom_excerpt":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/guide-to-using-model-foundry","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},"url":"https://labelbox-guides.ghost.io/guide-to-using-model-foundry/","excerpt":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":"How to use the model Foundry for automated data labeling and enrichment","og_description":"Learn how to harness the power of Model Foundry to automate and enrich data workflows in Labelbox.","twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to use the model Foundry for automated data labeling and enrichment","meta_description":"Introducing the model Foundry - Enrich data and automate tasks using foundation models","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"652ddccebe381f00016e8e79","uuid":"6ce142af-2859-45cb-9b97-010b3461c209","title":"How to boost retail profitability with AI-powered shelf object detection","slug":"how-to-boost-retail-profitability-with-ai-powered-shelf-object-detection","html":"\u003cp\u003eFor today’s retailers facing intense competition and thin margins, optimizing shelves is a high-impact yet often overlooked opportunity to directly boost sales, enhance in-store experience, and enable better data-driven decisions across the organization. However, manually monitoring and checking shelves can be an inefficient process that limits productivity and prevents real-time insights across stores nationwide.\u0026nbsp;\u003c/p\u003e\u003cp\u003eBy applying AI and computer vision for automated shelf monitoring, retailers can unlock the benefits of real-time shelf optimization at scale. With data-driven insights into inventory levels, pricing, product placement, and more, they can make better decisions around merchandising, improve customer satisfaction, optimize supply chain operations, and ultimately drive higher productivity and profitability. With real-time insights and efficiency gains, retailers can look to improve sales by millions of dollars per year leveraging AI.\u0026nbsp;\u003c/p\u003e\u003cp\u003eHowever, building and scaling an effective AI-powered shelf detection solution can pose the following key challenges for retailers:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eBuilding accurate computer vision models for a shelf optimization use case requires vast amounts of high-quality labeled images – being able to capture shelves with a diverse product range, from various distances and angles. Collecting, organizing, and labeling this data can be expensive and time-consuming for retailers.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCost \u0026amp; scalability: \u003c/strong\u003eDeploying models across hundreds or thousands of retail locations can require significant investment. As shelf inventory rapidly changes, keeping models accurately trained and scaled can incur costs.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLack of machine learning expertise: \u003c/strong\u003eMany brick and mortar retailers might not have access to engineering or machine learning talent. Without the right expertise, it can be difficult to develop automated shelf-monitoring solutions needed to drive value for shelf management.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform that enables retailers to rapidly develop accurate computer vision models for automated shelf monitoring allowing for real-time monitoring across stores. Rather than spending valuable time building an in-house solution or relying on disparate systems, retailers can leverage Quantumworks Lab's end-to-end platform to label high-impact data, ensure model accuracy, and continue to evaluate the effectiveness of models in production. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.37.14-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1472\" height=\"782\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/Screenshot-2023-11-20-at-11.37.14-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/Screenshot-2023-11-20-at-11.37.14-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/Screenshot-2023-11-20-at-11.37.14-AM.png 1472w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage \u003ca href=\"https://labelbox.com/product/platform/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox’s platform\u003c/a\u003e to build a shelf object detection model, empowering you to make more precise merchandising decisions, enhance customer satisfaction, optimize supply chain operations, and in turn, boost productivity and profitability.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"see-it-in-action-how-to-build-a-shelf-object-detection-model\"\u003eSee it in action: How to build a shelf object detection model\u0026nbsp;\u003c/h2\u003e\u003cp\u003e\u003cem\u003eThe walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eCatalog\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eAnnotate\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003eModel\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003e\u003cu\u003ecreate a free Quantumworks Lab account\u003c/u\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial.\u003c/em\u003e\u003c/p\u003e\u003cp\u003ePart 1: Explore and enhance your data: \u003ca href=\"https://colab.research.google.com/drive/1jfD1bBIyi_o6SUFdCVRYiDHko8GzYHXm?ref=labelbox-guides.ghost.io#scrollTo=7Gd-ZMpf0trE\"\u003e\u003cu\u003eGoogle Colab Notebook\u0026nbsp;\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003ePart 2: Create a model run and evaluate model performance: \u003ca href=\"https://colab.research.google.com/drive/120ar420ZGmrh7LJ-6cvISpfHv5wnGMXG?ref=labelbox-guides.ghost.io#scrollTo=RI-8G2LxbqV2\"\u003e\u003cu\u003eGoogle Colab Notebook\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1jfD1bBIyi_o6SUFdCVRYiDHko8GzYHXm?ref=labelbox-guides.ghost.io#scrollTo=7Gd-ZMpf0trE\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook. \u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"ingest-data-into-labelbox\"\u003eIngest data into Quantumworks Lab \u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/uo2jmabchf\" title=\"How to boost retail profitability with AI-powered shelf object detection (Part 1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAs a retailer, you might have hundreds of images captured on shelves from various stores stored in a cloud bucket or as a local folder of images. In order to upload the desired images of shelves to Quantumworks Lab, you have the option of:\u0026nbsp;\u003c/p\u003e\u003cp\u003e1) Using the \u003ca href=\"https://colab.research.google.com/drive/1jfD1bBIyi_o6SUFdCVRYiDHko8GzYHXm?ref=labelbox-guides.ghost.io#scrollTo=7Gd-ZMpf0trE\"\u003e\u003cu\u003eGoogle Colab notebook\u003c/u\u003e\u003c/a\u003e, upload the CSV of images and metadata to Quantumworks Lab\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eInput your Quantumworks Lab API key into the provided \u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003e\u003cu\u003eGoogle Colab notebook\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eSelect ‘Runtime’ in the navigation bar and hit ‘Run all’ to bring the selected amount of data rows into your \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox Catalog \u003c/u\u003e\u003c/a\u003e— where you can browse, explore, and curate the data for insights and model development.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e2) If you have a dataset from your local file, you can upload it through the Quantumworks Lab UI by clicking ‘new dataset’ in Catalog.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/10/Screenshot-2023-10-16-at-9.03.31-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"436\" height=\"279\"\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve successfully uploaded your images, you can browse them in Catalog - along with image metadata such as Date, Time, Store ID, Camera View, and more. You can visualize your data in a no-code interface to quickly pinpoint and curate data for model training.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"search-and-curate-data\"\u003eSearch and curate data\u003c/h3\u003e\u003cp\u003eYou’ll now be able to see your dataset in Quantumworks Lab Catalog. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ecustom metadata\u003c/u\u003e\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003e\u003cu\u003eattachments\u003c/u\u003e\u003c/a\u003e to each asset for greater context.\u0026nbsp;\u003c/p\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSearch across datasets\u003c/u\u003e\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u0026nbsp;\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003efind similar data\u003c/u\u003e\u003c/a\u003e in seconds with off-the-shelf embeddings\u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003enatural language\u003c/u\u003e\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003e\u003cu\u003elayer structured and unstructured filters\u003c/u\u003e\u003c/a\u003e for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"create-a-labeling-project-in-annotate\"\u003eCreate a labeling project in Annotate\u0026nbsp;\u003c/h3\u003e\u003cp\u003e1) Create an image project in \u003ca href=\"https://app.labelbox.com/projects?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Annotate\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e2) Sample and send your uploaded dataset as a batch to your newly created project.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Create an \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eontology\u003c/a\u003e to determine how to structure your data. If you have a previous ontology you’d like to use, you can do so. If not, you’ll need to create a new ontology. To create a new ontology:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eAdd a class. For this use case we’ll want to add bounding boxes\u003c/li\u003e\u003cli\u003ePick a name for the class. For this use case we’ll be detecting empty spaces on shelves, so we can create two bounding box classes named ‘empty’ and ‘product’ respectively\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e4) If you’re relying on an external team of labelers or want to provide your internal labeling team with more instructions, you can upload instructions as a PDF for your labelers during the ontology creation process. \u003c/p\u003e\u003ch3 id=\"label-the-data-of-interest\"\u003eLabel the data of interest\u0026nbsp;\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jhlkuc8pym\" title=\"How to boost retail profitability with AI-powered shelf object detection (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eNow that we have a project with our data set up in Annotate, we’ll need to label this training data. With Quantumworks Lab, you can label your data in the following ways:\u003c/p\u003e\u003cp\u003e1) \u003cstrong\u003eInternal team of labelers:\u003c/strong\u003e your team can start labeling directly in the Quantumworks Lab editor, utilizing automation tools and maintaining quality with custom workflows to maintain human-in-the-loop review.\u0026nbsp;\u003c/p\u003e\u003cp\u003e2) \u003cstrong\u003eExternal team of expert labelers\u003c/strong\u003e with \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Boost\u003c/a\u003e: leverage our global network of\u0026nbsp; specialized labelers for a variety of tasks.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWorkforce Boost provides a collaborative platform for labeling services in a self-serve manner – this is great for teams that don’t have the technical expertise to build a machine learning system yet are looking for an easy-to-use technology to get a quick turnaround on quality training data. You can learn more about our Boost offerings \u003ca href=\"https://docs.labelbox.com/docs/using-data-labeling-service?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) \u003cstrong\u003eCreate pre-labels with foundation models\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you have the ability to send model predictions as pre-labels to your labeling project. This can be done in one of two ways:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eModel-assisted labeling: \u003c/strong\u003eWith \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003emodel-assisted labeling\u003c/u\u003e\u003c/a\u003e, you can import computer-generated predictions (or simply annotations created outside of Quantumworks Lab) as pre-labels on an asset. The imported annotations will be pre-populated in the labeling editor and a human can correct or verify and submit the prediction as ground truth.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eModel Foundry: \u003c/strong\u003eWith \u003ca href=\"https://labelbox.com/model-foundry/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eModel Foundry\u003c/u\u003e\u003c/a\u003e, you can automate data workflows, including data labeling with world-class foundation models. Leverage a variety of open source or third-party models to accelerate pre-labeling and cut labeling costs by up to 90%. \u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"part-2-create-a-model-run-and-evaluate-model-performance\"\u003ePart 2: Create a model run and evaluate model performance\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/120ar420ZGmrh7LJ-6cvISpfHv5wnGMXG?ref=labelbox-guides.ghost.io#scrollTo=RI-8G2LxbqV2\"\u003e\u003cu\u003eColab Notebook\u003c/u\u003e\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"create-a-model-run\"\u003eCreate a model run\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/hub0lc15ue\" title=\"How to boost retail profitability with AI-powered shelf object detection (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOnce you have your labeled data in your project in Annotate, you’re ready to move on to creating a model run in \u003ca href=\"https://app.labelbox.com/mea?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Model\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eWe’ll be using this \u003ca href=\"https://colab.research.google.com/drive/120ar420ZGmrh7LJ-6cvISpfHv5wnGMXG?ref=labelbox-guides.ghost.io#scrollTo=RI-8G2LxbqV2\"\u003e\u003cu\u003eColab notebook\u003c/u\u003e\u003c/a\u003e to train a model on the training dataset and bring back inferences from the trained model for evaluation and diagnosis.\u0026nbsp;\u003c/p\u003e\u003cp\u003eFor this step, you will need:\u003c/p\u003e\u003cul\u003e\u003cli\u003eYour Project ID (located in the URL)\u0026nbsp;\u003c/li\u003e\u003cli\u003eYour API key\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe Colab notebook will modify the output layer of a YOLOv5 algorithm and train data on the provided project’s labels. To do so, you’ll need to:\u003c/p\u003e\u003cp\u003e1) Input your API key and project ID into the notebook\u003c/p\u003e\u003cp\u003e2) Name your model\u0026nbsp;\u003c/p\u003e\u003cp\u003e3) Ensure that the ‘MEA’ field is ‘true’ and ‘MAL_upload’ and ‘Label_import’ fields are ‘false’\u0026nbsp;\u003c/p\u003e\u003cp\u003eOnce those are inputted, you can select ‘Runtime’ in the navigation bar and hit ‘Run all’ – the notebook will pull all of the labels into this environment, fine-tune YOLOv5 on your use case and import those predictions back into Quantumworks Lab Model.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou’ll be able to view the model in the ‘Experiments’ tab in Quantumworks Lab Model – you can view ground truth predictions in green and predictions in red.\u0026nbsp;\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness\"\u003eEvaluate and diagnose model effectiveness\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qnm905nupl\" title=\"How to boost retail profitability with AI-powered shelf object detection (Part 5) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eDiagnose model performance with model metrics\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eA disagreement between model predictions and ground truth labels can be due to a model error (poor model prediction) or a labeling mistake (ground truth is wrong).\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eAfter running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/li\u003e\u003cli\u003eUse the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/li\u003e\u003cli\u003eModel metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model for 10x faster corner-case detection – detect and visualize corner-cases where the model is underperforming. For example, you can drill into cases where ‘empty’ objects are not predicted, where the model might have difficulty identifying empty spaces on shelves where there is a wire mesh material present. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified an example of a corner-case where the model might be struggling, you can easily leverage Catalog to surface similar unlabeled examples to improve model performance.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect any corner-cases and select ‘Find similar in Catalog’ from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows (both labeled and unlabeled) to the selected example.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo only surface unlabeled images that you can send to your model for labeling, you can filter on the ‘Annotation is’ filter and select ‘none’. This will only show unlabeled images that are similar to the selected corner case.\u0026nbsp;\u003c/li\u003e\u003cli\u003eSelect all images that apply and send them as a batch to your original labeling project. Labeling these in priority will help improve model performance.\u0026nbsp; \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCompare model runs across iterations\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eImprove model development by up to 90% by leveraging Quantumworks Lab Model to compare model runs across iterations to track and quantify how model performance has improved over time.\u0026nbsp;\u003c/p\u003e\u003cp\u003eWith new high-impact data labeled, you can retrain the model using the same steps with the Colab notebook on this improved data set.\u0026nbsp;You can track model improvements across various runs for comparison and how this has affected model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003eAI-powered computer vision enables real-time, scalable shelf optimization for retailers. By automating insights into inventory, product placement, and more, retailers can make smarter merchandising decisions, boost customer satisfaction, and increase sales by millions per year. AI shelf optimization unlocks greater efficiency, agility, and profitability.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..?utm_keyword=Quantumworks Lab\u0026utm_medium=email\u0026utm_source=house\u0026utm_campaign=modelfoundry\u0026\u0026referrer_url=https://connect.labelbox.co/\"\u003e\u003cu\u003esign up for a free Quantumworks Lab account\u003c/u\u003e\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003erequest a demo\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e","comment_id":"652ddccebe381f00016e8e79","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3082.png","featured":false,"visibility":"public","created_at":"2023-10-17T01:01:02.000+00:00","updated_at":"2024-01-17T22:04:19.000+00:00","published_at":"2023-10-17T01:16:44.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-boost-retail-profitability-with-ai-powered-shelf-object-detection","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},"url":"https://labelbox-guides.ghost.io/how-to-boost-retail-profitability-with-ai-powered-shelf-object-detection/","excerpt":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. ","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":"How to boost retail profitability with AI-powered shelf object detection","og_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/10/Group-3082-1.png","twitter_title":"How to boost retail profitability with AI-powered shelf object detection","twitter_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. ","meta_title":"How to boost retail profitability with AI-powered shelf object detection","meta_description":"Learn how to leverage Quantumworks Lab’s data-centric AI platform to build a robust shelf object detection model to boost retail profitability. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64ff69ca935e200001ee0180","uuid":"89463a23-02d8-47e3-98db-49448eb94d96","title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","slug":"how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox","html":"\u003cp\u003eIn machine learning, fine-tuning pre-trained models is a powerful technique that adapts models to new tasks and datasets. Fine-tuning takes a model that has already learned representations on a large dataset, such as a large language model, and leverages prior knowledge to efficiently “teach” the model a new task. \u003c/p\u003e\u003cp\u003eThe key benefit of fine-tuning is that it allows you to take advantage of transfer learning. Rather than training a model from scratch, which requires massive datasets and compute resources, you can start with an existing model and specialize it to your use case with much less data and resources. Fine-tuning allows ML teams to efficiently adapt powerful models to new tasks with limited data and compute. It is essential for applying state-of-the-art models to real-world applications of AI. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning?ref=labelbox-guides.ghost.io\"\u003eOpen AI’s fine-tuning API\u003c/a\u003e allows teams to fine-tune the following models:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGPT-3.5 -turbo-0613 (recommended)\u003c/li\u003e\u003cli\u003eBabbage-002\u003c/li\u003e\u003cli\u003eDavinci-002\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we’ll cover how to leverage \u003ca href=\"https://www.ssw.com.au/rules/what-is-gpt/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eOpen AI’s GPT-3.5\u003c/a\u003e and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eThe goal of model fine-tuning is to improve the model’s performance against a specific task. Over other techniques to optimize model output, such as prompt design, fine-tuning can help achieve:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigher quality results: \u003c/strong\u003eFine-tuning allows the model to learn from a much larger and more diverse dataset than can fit into a prompt. The model can learn more granular patterns and semantics that are relevant to your use case through extensive fine-tuning training. Prompts are limited in how much task-specific context they can provide, while fine-tuning teaches the model your specific domain.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eToken savings: \u003c/strong\u003eFine-tuned models require less prompting to produce quality outputs. With fine-tuning, you can leverage a shorter, more general prompt since the model has learned your domain – saving prompt engineering effort and tokens. Whereas highly-specific prompts can often hit token limits.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLower latency: \u003c/strong\u003eHeavily engineered prompts can increase latency as they require more processing. As fine-tuned models are optimized for your specific task, they allow faster inference and can quickly retrieve knowledge for your domain.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFine-tuning is especially beneficial for adapting models to your specific use case and business needs. There are several common scenarios where fine-tuning really can help models capture the nuances required for an application:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStyle, tone, or format customization:\u003c/strong\u003e Fine-tuning allows you to adapt models to match the specific style or tone required for a use case, whether it be a particular brand voice or difference in tone for speaking to various audiences.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDesired output structure: \u003c/strong\u003eFine-tuning can teach models to follow a required structure or schema in outputs. For example, you can fine-tune a summarization model to consistently include key facts in a standardized template.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHandling edge cases: \u003c/strong\u003eReal-world data often contains irregularities and edge cases. Fine-tuning allows models to learn from a wider array of examples, including rare cases. You can fine-tune the model on new data samples so that it learns to handle edge cases when deployed to production.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn short, fine-tuning allows teams to efficiently adapt powerful models to new tasks and datasets, allowing ML teams to customize general models to their specific use cases and business needs through extensive training on curated data. High-quality fine-tuning datasets are crucial to improve performance by teaching models the nuances and complexity of the target domain more extensively than possible through prompts alone.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOpen AI’s recommended dataset guidelines\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo fine-tune an Open AI model, it is required to provide at least ten examples. Research has shown clear improvements from fine-tuning on 50 to 100 training examples with GPT-3.5-turbo. Data quality, over data quantity, is also critical to the success of the fine-tuned model. \u003c/p\u003e\u003cp\u003eYou can learn more about preparing a dataset in \u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?ref=labelbox-guides.ghost.io\"\u003eOpen AI’s documentation\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"how-to-use-labelbox-for-fine-tuning\"\u003eHow to use Quantumworks Lab for fine-tuning\u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform for building intelligent applications. With a suite of powerful data curation, labeling, and model evaluation tools, the platform is built to help continuously improve and iterate on model performance. For this example, we will use the Quantumworks Lab platform to create a high-quality fine-tuning dataset. \u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can prepare a dataset of prompts and responses to fine-tune large language models (LLMs). Quantumworks Lab supports dataset creation for a variety of fine-tuning tasks including summarization, classification, question-answering, and generation.\u003c/p\u003e\u003cp\u003eWhen you set up an \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003eLLM data generation project\u003c/a\u003e in Quantumworks Lab, you will be prompted to specify how you will be using the editor. You have three choices for specifying your LLM data generation workflow:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 1: Humans generate prompts and responses\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, the prompt and response fields will be required. This will indicate to your team that they should create a prompt and response from scratch.\u003c/p\u003e\u003cp\u003eYou can make a copy of this \u003ca href=\"https://colab.research.google.com/drive/1ElV4U68ZJCJ-wIMLFTQAyzXrSSz6aU2Y?ref=labelbox-guides.ghost.io#scrollTo=HUhjjPp0mnPq\"\u003eGoogle Colab Notebook\u003c/a\u003e to generate a prompt and response dataset in Quantumworks Lab and fine-tune GPT-3.5 Turbo in OpenAI. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 2: Humans generate prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, only the prompt field will be required. This will indicate to your team that they should create a prompt from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 3: Humans generate responses to uploaded prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, a previously uploaded prompt will appear. Your team will need to create responses for that prompt.\u003c/p\u003e\u003cp\u003eYou can make a copy of this \u003ca href=\"https://colab.research.google.com/drive/1J21Nb0fqbZCcnxyR4wOKLlADwcayOlSq?ref=labelbox-guides.ghost.io#scrollTo=vGHl-lKkZ9cK\"\u003eGoogle Colab Notebook\u003c/a\u003e to generate a dataset of responses to uploaded prompts in Quantumworks Lab and fine-tune GPT-3.5 Turbo in OpenAI. \u003c/p\u003e\u003cp\u003eIn the below example, we’ll be walking through a sample use case of summarizing and removing PII from customer support chats with Quantumworks Lab and OpenAI’s GPT-3.5 Turbo. Imagining we’re a company who wishes to summarize support logs without revealing personally identifiable information in the process, we’ll be fine-tuning an LLM to summarize and remove PII from customer support logs.\u003c/p\u003e\u003ch3 id=\"step-1-evaluate-how-gpt-35-performs-against-the-desired-task\"\u003eStep 1: Evaluate how GPT-3.5 performs against the desired task\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/e00fk9u59h\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eBefore we begin the fine-tuning process, let’s first evaluate how ChatGPT (using GPT-3.5) performs against the desired task off-the-shelf. \u003c/p\u003e\u003cp\u003eWe uploaded the following sample chat log to ChatGPT:\u003c/p\u003e\u003cp\u003e“Summarize this chat log and remove any personally identifiable information in the summary:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTom: \u003c/strong\u003eI need to reset my account access.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUrsula: \u003c/strong\u003eI can help with that, Tom. What’s your account email?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTom: \u003c/strong\u003eIt’s \u003ca href=\"mailto:tom@example.com\"\u003etom@example.com\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUrsula: \u003c/strong\u003eGreat, Tom. I’ve sent you a link to update your credentials”\u003c/p\u003e\u003cp\u003eIn the above prompt, we’ve asked ChatGPT to summarize the chat log and remove any personally identifiable information. \u003c/p\u003e\u003cp\u003eUpon evaluation, the default GPT-3.5 model misses the mark for our desired use case. \u003c/p\u003e\u003cp\u003eThe summary includes both Tom and Ursula’s names and explicitly mentions Tom’s email address. In order to reliably use the model for our business use case, we need to fine-tune it so that it appropriately excludes elements of personally identifiable information. To do so, we will leverage Quantumworks Lab to generate our fine-tuning dataset and use it to fine-tune GPT-3.5 through OpenAI. \u003c/p\u003e\u003ch3 id=\"step-2-create-a-llm-data-generation-project-in-labelbox\"\u003eStep 2: Create a LLM data generation project in Quantumworks Lab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ldpv9nwh14\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe first step will be to upload our support chat logs to \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Catalog \u003c/a\u003e– this will allow us to browse, curate, and send these data rows for labeling.\u003c/p\u003e\u003cp\u003eNext, we’ll need to create a LLM data generation labeling project in \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Annotate\u003c/a\u003e.\u003c/p\u003e\u003cul\u003e\u003cli\u003eSince we have an available dataset, this will be a ‘Humans generate response to uploaded prompts’ project.\u003c/li\u003e\u003cli\u003eWhen configuring the ontology, we will set the response type as ‘text’ and make the appropriate response to “summarize and remove personally identifiable information in the summary”.\u003c/li\u003e\u003cli\u003eDuring ontology creation, you can also define a character minimum or maximum and upload necessary instructions for the labeling team.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-label-data\"\u003eStep 3: Label data\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/i8tzhezn70\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter successfully setting up an LLM data generation project, we can queue the uploaded chat logs in Catalog for labeling in Annotate. To label data, you have the option of leveraging \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost’s\u003c/a\u003e extensive workforce or use your own internal team to summarize and remove personally identifiable information in the summary.\u003c/p\u003e\u003cp\u003eFor larger or more complex fine-tuning tasks, you can scale up to hundreds or thousands of labeled data rows. Once all data has been labeled, you can review the corresponding summary to each prompt and export the data rows.\u003c/p\u003e\u003ch3 id=\"step-4-export-data-from-labelbox-and-fine-tune-it-in-openai\"\u003e\u003cbr\u003eStep 4: Export data from Quantumworks Lab and fine-tune it in OpenAI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bx1z0xy4db\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWith all necessary data labeled, we can export the dataset from Quantumworks Lab and upload it in a format that is readable by OpenAI. \u003c/p\u003e\u003cp\u003eOpenAI requires a dataset to be in the structure of their \u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create?ref=labelbox-guides.ghost.io\"\u003echat completions API\u003c/a\u003e, whereby each message has a role, content, and optional name. You can learn more about specific dataset requirements in OpenAI’s \u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. Using a script, we can convert the Quantumworks Lab export into OpenAI’s required conversational chat format.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e{\n \"messages\":\n \t{\"role\":\"system\",\n    \"content\":\"Given a chat log, summarize and remove personal \t\tidentifiable information in the summary.\"},\n    {\"role\":\"user\",\n    \"content\":\"Andy:Why has my order not shipped yet?! Bella: I \tapologize for the delay, Andy:May I have your order number? Andy: \t  It's ORDER5678. Please hurry! Bella: Thank you Andy. It's \t\t\texpedited and will ship today.\"},\n\t{\"role\":\"assistant\",\n    \"content\":\"Customer inquires about the delay in the shipment of his order. Support agent requests the order number and upon receiving \t  it, assures customer that the order has been expedited and will \t\tship that day.\"\n    }\n ]\n}\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003eAfter formatting our dataset, we can upload it and \u003ca href=\"https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model?ref=labelbox-guides.ghost.io\"\u003estart a fine-tuning job\u003c/a\u003e using the OpenAI SDK.\u003c/p\u003e\u003cp\u003eYou can use a copy of the following \u003ca href=\"https://colab.research.google.com/drive/1J21Nb0fqbZCcnxyR4wOKLlADwcayOlSq?ref=labelbox-guides.ghost.io#scrollTo=vGHl-lKkZ9cK\"\u003eGoogle Colab notebook\u003c/a\u003e to export data from Quantumworks Lab in a format compatible with fine-tuning GPT-3.5 Turbo and begin a fine-tuning job. \u003c/p\u003e\u003ch3 id=\"step-5-assess-the-fine-tuned-model%E2%80%99s-performance-in-openai\"\u003eStep 5: Assess the fine-tuned model’s performance in OpenAI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9kdb8m0xm0\" title=\"How to fine-tune OpenAI's GPT-3.5 Turbo using Quantumworks Lab (Part 5) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eAfter the fine-tuning job has succeeded, you can navigate to the OpenAI playground and select the newly fine-tuned model for evaluation. Similarly to evaluating the initial GPT-3.5 model, we can enter a sample chat log and see how the newly fine-tuned model performs.\u003c/p\u003e\u003cp\u003eCompared to the off-the-shelf GPT-3.5 model, this model that has been fine-tuned on our training data is performing as expected. We can see that all names and relevant information that would be considered as personally identifiable information has been retracted. \u003c/p\u003e\u003cp\u003eWe can also compare the fine-tuned model to the initial GPT-3.5 model and see how it performs on the same prompt. Again, we can see that while GPT-3.5 excludes some aspects of personally identifiable information, it still includes the user’s first name, so it doesn’t quite meet the expectations for our business use case.\u003c/p\u003e\u003cp\u003eThe newly fine-tuned model has allowed us to adapt GPT-3.5 to our specific use case of concealing personally identifiable information. With Quantumworks Lab, teams can iteratively identify gaps and outdated samples in the fine-tuning data, then generate fresh high-quality data, allowing model accuracy to be maintained over time. Updating fine-tuning datasets through this circular feedback process is crucial for adapting to new concepts and keeping models performing at a high level within continuously changing environments.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eTo improve LLM performance, Quantumworks Lab simplifies the process for subject matter experts to generate high-quality datasets for fine-tuning with leading model providers and tools, like OpenAI. \u003c/p\u003e\u003cp\u003eUnlock the full potential of large language models with Quantumworks Lab’s end-to-end platform and a \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox.ghost.io\"\u003enew suite of LLM tools\u003c/a\u003e to generate high-quality training data and optimize LLMs for your most valuable AI use cases.\u003c/p\u003e","comment_id":"64ff69ca935e200001ee0180","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Group-3081.jpg","featured":false,"visibility":"public","created_at":"2023-09-11T19:26:02.000+00:00","updated_at":"2024-05-28T17:02:27.000+00:00","published_at":"2023-09-11T19:58:50.000+00:00","custom_excerpt":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-openais-gpt-3-5-turbo-using-labelbox/","excerpt":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","og_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Group-3081-1.jpg","twitter_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","twitter_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","meta_title":"How to fine-tune OpenAI’s GPT-3.5 Turbo using Quantumworks Lab","meta_description":"In this guide, we’ll cover how to leverage Open AI’s GPT-3.5 and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64fb3e9f935e200001ee0106","uuid":"9f549cdf-80e9-4441-8b4e-f1ce92572f56","title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","slug":"how-to-fine-tune-vertex-ai-models-with-labelbox","html":"\u003cp\u003eIn machine learning, fine-tuning pre-trained models is a powerful technique that adapts models to new tasks and datasets. Fine-tuning takes a model that has already learned representations on a large dataset, such as a large language model, and leverages prior knowledge to efficiently “teach” the model a new task. \u003c/p\u003e\u003cp\u003eThe key benefit of fine-tuning is that it allows you to take advantage of transfer learning. Rather than training a model from scratch, which requires massive datasets and compute resources, you can start with an existing model and specialize it to your use case with much less data and resources. Fine-tuning allows ML teams to efficiently adapt powerful models to new tasks with limited data and compute. It is essential for applying state-of-the-art models to real-world applications of AI.\u003c/p\u003e\u003cp\u003eVertex AI provides \u003ca href=\"https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models?ref=labelbox-guides.ghost.io#supported_models\"\u003eseveral base models\u003c/a\u003e that can be fine-tuned:\u003c/p\u003e\u003cul\u003e\u003cli\u003etext-bison@001\u003c/li\u003e\u003cli\u003ecode-bison@001\u003c/li\u003e\u003cli\u003ecodechat-bison@001\u003c/li\u003e\u003cli\u003echat-bison@001\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.\u003c/p\u003e\u003chr\u003e\u003cp\u003eThe goal of model fine-tuning is to improve the model’s performance against a specific task. Over other techniques to optimize model output, such as prompt design, fine-tuning can help achieve:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigher quality results: \u003c/strong\u003eFine-tuning allows the model to learn from a much larger and more diverse dataset than can fit into a prompt. The model can learn more granular patterns and semantics that are relevant to your use case through extensive fine-tuning training. Prompts are limited in how much task-specific context they can provide, while fine-tuning teaches the model your specific domain.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eToken savings:\u003c/strong\u003e Fine-tuned models require less prompting to produce quality outputs. With fine-tuning, you can leverage a shorter, more general prompt since the model has learned your domain – saving prompt engineering effort and tokens. Whereas highly-specific prompts can often hit token limits.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLower latency: \u003c/strong\u003eHeavily engineered prompts can increase latency as they require more processing. As fine-tuned models are optimized for your specific task, they allow faster inference and can quickly retrieve knowledge for your domain.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFine-tuning is especially beneficial for adapting models to your specific use case and business needs. There are several common scenarios where fine-tuning really can help models capture the nuances required for an application:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStyle, tone, or format customization:\u003c/strong\u003e Fine-tuning allows you to adapt models to match the specific style or tone required for a use case, whether it be a particular brand voice or difference in tone for speaking to various audiences.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDesired output structure: \u003c/strong\u003eFine-tuning can teach models to follow a required structure or schema in outputs. For example, you can fine-tune a summarization model to consistently include key facts in a standardized template. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHandling edge cases: \u003c/strong\u003eReal-world data often contains irregularities and edge cases. Fine-tuning allows models to learn from a wider array of examples, including rare cases. You can fine-tune the model on new data samples so that it learns to handle edge cases when deployed to production.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn short, fine-tuning allows teams to efficiently adapt powerful models to new tasks and datasets, allowing ML teams to customize general models to their specific use cases and business needs through extensive training on curated data. High-quality fine-tuning datasets are crucial to improve performance by teaching models the nuances and complexity of the target domain more extensively than possible through prompts alone.\u003c/p\u003e\u003ch2 id=\"how-to-use-labelbox-for-fine-tuning\"\u003eHow to use Quantumworks Lab for fine-tuning\u003c/h2\u003e\u003cp\u003eLabelbox is a data-centric AI platform for building intelligent applications. With a suite of powerful data curation, labeling, and model evaluation tools, the platform is built to help continuously improve and iterate on model performance. We will use the Quantumworks Lab platform to create a high-quality fine-tuning dataset. \u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can prepare a dataset of prompts and responses to fine-tune large language models (LLMs). Quantumworks Lab supports dataset creation for a variety of fine-tuning tasks including summarization, classification, question-answering, and generation.\u003c/p\u003e\u003ch3 id=\"step-1-evaluate-how-a-model-performs-against-the-desired-task\"\u003eStep 1: Evaluate how a model performs against the desired task\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/n4ob00h4cj\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-2-create-an-llm-data-generation-dataset-in-labelbox\"\u003eStep 2: Create an LLM data generation dataset in Quantumworks Lab\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1248\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.22.14-AM.png 1248w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eWhen you set up an \u003ca href=\"https://docs.labelbox.com/docs/llm-data-generation?ref=labelbox-guides.ghost.io\"\u003eLLM data generation project\u003c/a\u003e in Quantumworks Lab, you will be prompted to specify how you will be using the editor. You have three choices for specifying your LLM data generation workflow:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 1: Humans generate prompts and responses\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1252\" height=\"578\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.19-AM.png 1252w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn the editor, the prompt and response fields will be required. This will indicate to your team that they should create a prompt and response from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 2: Humans generate prompts\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1260\" height=\"576\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.24.53-AM.png 1260w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn the editor, only the prompt field will be required. This will indicate to your team that they should create a prompt from scratch.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWorkflow 3: Humans generate responses to uploaded prompts\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn the editor, a previously uploaded prompt will appear. Your team will need to create responses for that prompt.\u003c/p\u003e\u003cp\u003eFor our project, we'll want to create a \"Humans generate responses to uploaded prompts\" project. Namely, we want humans to create responses in the form of a list of airlines. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/irnaoi0u64\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 3) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-3-upload-data-to-vertex-ai\"\u003eStep 3: Upload data to Vertex AI\u003c/h3\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/1dwwdeggyj\" title=\"How to fine-tune Vertex AI LLMs with Quantumworks Lab (Part 4) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eExport the Quantumworks Lab fine-tuning dataset \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve constructed a fine-tuning dataset with Quantumworks Lab, you can export it using our \u003ca href=\"https://colab.research.google.com/drive/1imCvNhd1rZNEf_dCIsPT-wLfnHkv3o_A?ref=labelbox-guides.ghost.io\"\u003eLabelbox to Vertex AI conversion script\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStart a model tuning job using Vertex AI \u0026amp; deploy the model\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAfter exporting the fine-tuned dataset, start a \u003ca href=\"https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models?ref=labelbox-guides.ghost.io\"\u003emodel tuning job using Vertex\u003c/a\u003e. When a fine-tuning job is run, the model learns additional parameters that help it encode the necessary information to perform the desired behavior or learn the desired behavior. \u003c/p\u003e\u003cp\u003eThe output of the tuning job is a new model, which is effectively a combination of the newly learned parameters and the original model. Once the fine-tuning job is complete, you can deploy the model and return to Quantumworks Lab for model evaluation.\u003c/p\u003e\u003ch3 id=\"step-4-evaluate-and-iterate-on-fine-tuning-dataset-quality\"\u003eStep 4: Evaluate and iterate on fine-tuning dataset quality\u003c/h3\u003e\u003cp\u003eA well-performing fine-tuned model indicates the effective optimization of model architecture, training data, and hyperparameters. It signifies that the training dataset used for fine-tuning is high-quality and is representative of the real-world use case. This allows for the fine-tuned model to achieve better performance on tasks compared to the base model in less time than it would have to train a model from scratch. \u003c/p\u003e\u003cp\u003eReal-world conditions and data are often dynamic. As the use case evolves, it's crucial to maintain representativeness and relevance in the fine-tuning data. Continuous evaluation of the fine-tuned model’s performance can help detect edge cases or model errors.  You can evaluate model performance and debug errors leveraging \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e. Utilize interactive auto-populated model metrics, such as a confusion matrix, precision, recall, F1 score, and more to surface model errors. Detect and visualize corner-cases where the model is underperforming and generate high-impact data to drastically improve model performance. After running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1258\" height=\"632\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/03/Screenshot-2024-03-26-at-9.23.35-AM.png 1258w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eLabelbox Model\u003c/em\u003e\u003c/i\u003e\u003c/a\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e allows teams to debug models and iteratively improve model performance\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBy iteratively identifying gaps and outdated samples in the fine-tuning data, then generating fresh high-quality data, model accuracy can be maintained over time. Updating fine-tuning datasets through this circular feedback process is crucial for adapting to new concepts and keeping models performing at a high level within continuously changing environments.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eTo improve LLM performance, Quantumworks Lab simplifies the process for subject matter experts to generate high-quality datasets for fine-tuning with leading model providers and tools, like Google Vertex AI. \u003c/p\u003e\u003cp\u003eUnlock the full potential of large language models with Quantumworks Lab’s end-to-end platform and a \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox.ghost.io\"\u003enew suite of LLM tools\u003c/a\u003e to generate high-quality training data and optimize LLMs for your most valuable AI use cases. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..\u0026ref=labelbox-guides.ghost.io\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e","comment_id":"64fb3e9f935e200001ee0106","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299.png","featured":false,"visibility":"public","created_at":"2023-09-08T15:32:47.000+00:00","updated_at":"2024-03-26T16:25:35.000+00:00","published_at":"2023-09-08T18:03:36.000+00:00","custom_excerpt":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-vertex-ai-models-with-labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-vertex-ai-models-with-labelbox/","excerpt":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","reading_time":6,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299-1.png","og_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","og_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/09/Frame-2299-2.png","twitter_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","twitter_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","meta_title":"How to fine-tune Vertex AI LLMs with Quantumworks Lab","meta_description":"In this guide, we’ll cover how to leverage Vertex AI and Quantumworks Lab to simplify the fine-tuning process, allowing you to rapidly iterate and refine your models’ performance on specific data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64e7a2c3b0c09f000179dbed","uuid":"2b64de59-c4f8-4bf5-aa4b-07d1aabcb933","title":"How to build a powerful product recommendation system for retail","slug":"how-to-build-a-powerful-product-recommendation-system-for-retail","html":"\u003cp\u003ePersonalized experiences are at the heart of customer satisfaction and are key to long-term brand loyalty and success. Amidst the abundance of choices available to the modern consumer, businesses must find innovative ways to stand out and forge meaningful relationships with their audience. \u003c/p\u003e\u003cp\u003eThe rise of AI has enabled companies to craft personalized experiences at an unprecedented scale. Organizations can now rely on algorithms taught to recognize customer preferences, behavior, and provide recommendations based on purchase history, and more. With a powerful product recommendation system, retailers can create individualized customer interactions and foster stronger connections to boost customer loyalty and increase key metrics such as conversion rate, average order value, and repeat purchase rates. \u003c/p\u003e\u003cp\u003eHowever, building a robust and effective AI-powered product recommendation system can be challenging for many teams. Some key challenges include: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eData quality and quantity: \u003c/strong\u003eBuilding a strong recommendation system that makes accurate predictions requires a vast amount of high-quality data. Orchestrating data from various sources can not only be challenging to maintain, but even more difficult to sort, analyze, and enrich with quality insights.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability: \u003c/strong\u003eAs a business grows and their product catalog expands, the recommendation system should be able to handle new and incoming data. Ensuring scalability and maintaining model performance with new data can be particularly challenging for teams relying on in-house solutions or disparate ML tools.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePrivacy and Security: \u003c/strong\u003eWhen it comes to customer data and specific product information, ensuring user privacy and safeguarding against potential security violations is critical to maintain trust with customers and build a successful recommendation system. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLabelbox is a data-centric AI platform to help build the best personalized product recommendation engine. Rather than spending valuable time building in-house or relying on disparate systems and applications, teams can leverage Quantumworks Lab’s platform to seamlessly build an end-to-end workflow that integrates with your existing tech stack and helps teams build AI systems faster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/11/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1425\" height=\"635\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/11/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/11/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/11/image-3.png 1425w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we’ll walk through how your team can leverage Quantumworks Lab’s platform to build a powerful recommendation system, ensuring your customers embark on a seamless and delightful shopping journey that keeps them coming back for more.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"see-it-in-action-how-to-build-a-powerful-product-recommendation-system-in-labelbox\"\u003eSee it in action: How to build a powerful product recommendation system in Quantumworks Lab\u003c/h2\u003e\u003cp\u003e\u003cem\u003eThe  walkthrough below covers Quantumworks Lab’s platform across \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eCatalog\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eAnnotate\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eModel\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. We recommend that you \u003c/em\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003ecreate a Quantumworks Lab account\u003c/em\u003e\u003c/a\u003e\u003cem\u003e to best follow along with this tutorial.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 1: \u003c/strong\u003eExplore and enhance your data (\u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003eGoogle Colab Notebook\u003c/a\u003e)\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePart 2:\u003c/strong\u003e Prepare data and evaluate model performance: (\u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\"\u003eGoogle Colab Notebook\u003c/a\u003e)\u003c/p\u003e\u003ch2 id=\"part-1-explore-and-prepare-your-data\"\u003ePart 1: Explore and prepare your data\u003c/h2\u003e\u003cp\u003eFollow along with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u003c/strong\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\" class=\"kg-btn kg-btn-accent\"\u003ePart 1: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"painlessly-consolidate-all-your-product-data\"\u003ePainlessly consolidate all your product data\u003c/h3\u003e\u003cp\u003eBuilding a recommendation engine requires consolidating data of different types from various sources. Such data can include product, business, and customer information that might be siloed or stored in different databases. To holistically browse and visualize your entire product catalog, leverage Quantumworks Lab Catalog to bring and view all of your data in a single place.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rdlcin4zh8\" title=\"[Personalized Experiences Demo] Data ingestion Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eIngest data into Quantumworks Lab\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor this tutorial, we’ve provided a sample retail dataset for you to view in your Quantumworks Lab app: \u003c/p\u003e\u003cp\u003e1) Input your Quantumworks Lab API key into the provided \u003ca href=\"https://colab.research.google.com/drive/1nOSff67KXhNgX_XSfnv3xnddobRoaK0d?ref=labelbox-guides.ghost.io#scrollTo=F9FCBVHysBdO\"\u003eGoogle Colab notebook\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e2) Specify an amount of data that you wish to ingest — we’ve provided up to 44,000 data rows for ingestion, but keep in mind that this will accrue \u003ca href=\"https://docs.labelbox.com/docs/billing?ref=labelbox-guides.ghost.io#labelbox-units-lbus\"\u003eLBUs\u003c/a\u003e in your account. \u003cem\u003eIf you are using Quantumworks Lab for free, we suggest that you ingest around 5,000 data rows.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e3) Select ‘Runtime’ in the navigation bar and hit ‘Run all’ to bring the selected amount of data rows into your \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Catalog \u003c/a\u003e— where you can browse, explore, and curate the data for insights and model development.\u003c/p\u003e\u003ch3 id=\"accelerate-product-discovery-across-your-entire-catalog\"\u003eAccelerate product discovery across your entire catalog\u003c/h3\u003e\u003cp\u003eAn effective product recommendation relies on training a model with a thorough understanding of your product data, encompassing product tags, categories, and more. However, retailers often have an ever-growing product list with hundreds or thousands of products. Dealing with this volume of data at scale and effectively searching, organizing, and managing data for machine learning tasks can be a challenge.\u003c/p\u003e\u003cp\u003eYou can leverage Quantumworks Lab Catalog to visualize, browse, and curate your product listings.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ehcuz2b4rk\" title=\"[Personalized Experiences Demo] Search and curate Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eSearch and curate data\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eYou’ll now be able to see the sample retail dataset in your Quantumworks Lab Catalog. Try searching across key product-specific metadata such as category, the year the item was released, season, gender, and more. With Catalog, you can contextualize your data with \u003ca href=\"https://docs.labelbox.com/docs/datarow-metadata?ref=labelbox-guides.ghost.io\"\u003ecustom metadata\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#visualize-attachments\"\u003eattachments\u003c/a\u003e to each asset for greater context. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1226\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.05.27-PM-1.png 2304w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeverage custom and out-of-the-box smart filters and embeddings to quickly explore product listings, surface similar data, and optimize data curation for ML. You can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003eSearch across datasets\u003c/a\u003e to narrow in on data containing specific attributes (e.g metadata, media attributes, datasets, project, etc.)\u003c/li\u003e\u003cli\u003eAutomatically \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003efind similar data\u003c/a\u003e in seconds with off-the-shelf embeddings \u003c/li\u003e\u003cli\u003eFilter data based on \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003enatural language\u003c/a\u003e and flexibly \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io#how-filters-work\"\u003elayer structured and unstructured filters\u003c/a\u003e for more granular data curation\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"categorize-and-curate-product-listings-faster\"\u003eCategorize and curate product listings faster\u003c/h3\u003e\u003cp\u003eAdvanced ML teams often adopt partially automated labeling workflows to mitigate costs and accelerate model development. Product recommendation models require a vast amount of accurately labeled data with a wide array of features. Manually labeling this data can not only be time consuming, but can also get exponentially expensive. Scaling data curation and enrichment effectively is key to quickly creating a powerful ML solution. \u003c/p\u003e\u003cp\u003eOne simple way to achieve this is by leveraging bulk classification and using human-in-the-loop review for quality assurance. Some AI teams using this technique have cut labeling costs by nearly 90%.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/epl8hstb7c\" title=\"[Personalized Experiences Demo] Streamlined labeling automation through bulk classification Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eStreamline labeling automation through bulk classification\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCreate a new labeling project\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"905\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/08/Screenshot-2023-08-24-at-3.05.00-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve narrowed in on a specific slice of data that you’d like to take action on, you can send them to a labeling project of interest in just a few clicks.\u003c/p\u003e\u003cp\u003e1) Create a new image project in \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e2) Configure the editor and create a new ontology. For the ontology, create a new classification called ‘Occasions’ with options such as ‘Casual’, ‘Sports’, ‘Formal’, etc. feel free to add any other classifications of interest.\u003c/p\u003e\u003cp\u003e3) Save your labeling project.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSend a subset of data to the labeling project\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"739\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/08/Screenshot-2023-08-24-at-3.04.11-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eReturn to Catalog and surface a specific subset of data that you want to bulk classify. For example, you can surface all instances of ‘Sporty apparel’ clothing with a natural language search.\u003c/p\u003e\u003cp\u003e1) Highlight any data rows of interest and select ‘Manage selection’ \u0026gt; ‘Add classifications’.\u003c/p\u003e\u003cp\u003e2) Select the labeling project that you made in the previous step and determine a step of the project’s review workflow that you would like to send the classifications to. In the above demo, we are sending these to the ‘Done’ stage because we have verified that these images fall under the ‘Sports’ category and want to automatically create ground truth labels.\u003c/p\u003e\u003cp\u003e3) Pick ‘Sports’ under the Classification section and you can submit the classification batch. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSave high-impact searches\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1029\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/08/Screenshot-2023-08-24-at-3.03.42-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can save any combination of searches as a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e in Catalog. For example, saving the natural language search of ‘Sporty apparel’ as a slice called ‘Sporty apparel’, creates a dynamic slice of data that can easily be revisited or edited as project needs evolve. Any future data that gets uploaded to Quantumworks Lab will automatically populate in any relevant slices based on its filters, creating an automatic data curation pipeline as your product catalog grows.\u003c/p\u003e\u003ch2 id=\"part-2-prepare-data-and-evaluate-model-performance\"\u003ePart 2: Prepare data and evaluate model performance\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eFollow along with the below with the tutorial and walkthrough in this \u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\"\u003eColab Notebook\u003c/a\u003e. If you are following along, \u003cstrong\u003eplease make a copy of the notebook.\u003c/strong\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\" class=\"kg-btn kg-btn-accent\"\u003ePart 2: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"prepare-a-training-dataset-for-model-diagnosis\"\u003ePrepare a training dataset for model diagnosis\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eModel diagnosis can play a pivotal role when training a model for personalized shopping experiences. The success of personalized recommendation systems hinge on the accuracy of a model’s understanding of a retailer’s product catalog or individual customer preferences. A properly curated and organized training dataset serves as the foundation for accurate model performance evaluation and fine-tuning.\u003c/p\u003e\u003cp\u003eSend the curated training dataset from Quantumworks Lab Annotate to Model in a few clicks to efficiently diagnose model performance of the training dataset.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate a training dataset for evaluation\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/zo945lphbi\" title=\"[Personalized Experiences Demo] Prepare a training dataset for model diagnosis Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eCreating a new model\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1866\" height=\"1408\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.51-PM.png 1866w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Navigate to the Model tab, select ‘Experiments’, and create a new model.\u003c/p\u003e\u003cp\u003e2) Name your model (e.g ‘Occasions’) and select a model thumbnail.\u003c/p\u003e\u003cp\u003e3) Select the same ontology that was used to bulk classify the data rows in the previous step.\u003c/p\u003e\u003cp\u003e4) Select the project that the classifications were sent to in the previous step. After selecting both the correct ontology and project, you should see the number of data rows that were bulk classified and ready to be added to the new model.\u003c/p\u003e\u003cp\u003e5) Hit ‘Create model’.\u003c/p\u003e\u003cp\u003e\u003cem\u003eCreating a new model run\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"938\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.02.34-PM-1.png 2306w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Once you’ve created your model, you can navigate back to the ‘Experiments’ tab and find the newly created model.\u003c/p\u003e\u003cp\u003e2) Create a ‘new model run.’ \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003eA model run\u003c/a\u003e is a model training experiment within a model, providing a versioned data snapshot of all data rows, annotations, and data splits for that model run.\u003c/p\u003e\u003cp\u003e3) To create a model run, you’ll need to give it a name (e.g ‘dataset version 1’) and can adjust the balance of the data split. For this demo, we will leave them in the default setting (80% train, 10% validate, 10% test).\u003c/p\u003e\u003cp\u003e4) After creating the model run, you’ll be able to see the populated training data classifications.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTrain a model on a provided training dataset\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ab1phltynu\" title=\"[Personalized Experiences] Train a model on the provided training dataset Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eModel training occurs outside of Labelbox. Quantumworks Lab Model works with any model training and inference framework, major cloud providers (AWS, Azure, GCS), and any data lake (Databricks, Snowflake).\u003c/p\u003e\u003cp\u003eWe’ll be using the \u003ca href=\"https://colab.research.google.com/drive/1CSyAE9DhwGTl7bLaSoo7QSyMuoEqJpCj?ref=labelbox-guides.ghost.io#scrollTo=eZnixGEAkscW\"\u003eColab notebook\u003c/a\u003e to train a model on the training dataset and bring back inferences from the trained model for evaluation and diagnosis.\u003c/p\u003e\u003cp\u003eFor this step, you will need:\u003c/p\u003e\u003cul\u003e\u003cli\u003eYour Ontology ID — found in the Settings tab on the model run page\u003c/li\u003e\u003cli\u003eYour Model Run ID — found in the gear icon on the top-right of the model run page\u003c/li\u003e\u003cli\u003eYour API key\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e1) Enter your API key, Ontology ID, and Model Run ID in the Colab Notebook.\u003c/p\u003e\u003cp\u003e2)  Once those are inputted, you can select ‘Runtime’ in the navigation bar and hit ‘Run all’ – this will take the classifications from your model run and train a provided image classification model. After training, the notebook will also take the trained model and use it to run inference on the data.\u003c/p\u003e\u003cp\u003e3)  If you want to adjust your data splits, you can leverage search filters in Model to surface any data rows and move them to the train, test, and validation splits.\u003c/p\u003e\u003ch3 id=\"evaluate-and-diagnose-model-effectiveness-for-retail\"\u003eEvaluate and diagnose model effectiveness for retail\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eA well-performing model can accurately predict consumer behavior or recommend products based on past preferences. Effective model diagnosis helps fine-tune recommendation algorithms, resulting in more accurate and appealing product suggestions. \u003c/p\u003e\u003cp\u003eModel diagnosis and evaluation are not one-time tasks. By leveraging effective diagnostic tools and an active learning workflow, retailers can continuously identify areas of improvement and adapt to changing customer behavior or an evolving product catalog. This iterative approach keeps personalized experiences relevant and effective for driving business outcomes over time.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qwqdx21ml2\" title=\"[Personalized Experiences Demo] Diagnose model performance and fix model errors Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eDiagnose model performance with model metrics\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"692\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.00.44-PM.png 2330w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) After running the notebook, you’ll be able to visually compare ground truth labels (in green) to the model predictions (in red).\u003c/p\u003e\u003cp\u003e2) Use the ‘Metrics view’ to drill into crucial model metrics, such as confusion matrix, precision, recall, F1 score, and more, to surface model errors.\u003c/p\u003e\u003cp\u003e3) Model metrics are auto-populated and interactive. You can click on any chart or metric to open up the gallery view of the model run and see corresponding examples.\u003c/p\u003e\u003cp\u003e4) Detect and visualize corner-cases where the model is underperforming. For example, in the demo above, we notice that the model is classifying this type of white shoe as a ‘Sports’ shoe when in fact it is a ‘Casual’ shoe.\u003c/p\u003e\u003cp\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCurate high-impact data to drastically improve model performance\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1172\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/08/Screenshot-2023-08-24-at-3.01.05-PM.png 2324w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e1) Select any corner-cases and select ‘Find similar in Catalog’ from the Manage Selection dropdown. This will bring you back into Catalog and will automatically surface all similar data rows to the selected example.\u003c/p\u003e\u003cp\u003e2) In addition to the similarity search, you can filter on ‘Annotation’ \u0026gt; ‘is none’ to surface only unlabeled data rows that you can retrain the model on to boost model performance.\u003c/p\u003e\u003cp\u003e3) Select any relevant examples and ‘Manage selection’ \u0026gt; ‘Add classifications’. In this case, we’d want to bulk classify these examples to reinforce to the model that these images are ‘casual’ shoes.\u003c/p\u003e\u003cp\u003e4) This step is similar to the bulk classification step in part 1. Select the labeling project that you made in the previous step and determine a step of the project’s review workflow that you would like to send the classifications to. We can send these to the ‘Done’ stage because we want to tell the model these white shoes fall under the ‘Casual’ category and want to automatically create ground truth labels.\u003c/p\u003e\u003cp\u003e5) Create a new model run (within the same model) and have the newly added classifications as a part of the training dataset.\u003c/p\u003e\u003cp\u003e6) Run the notebook again to train the model on this new training dataset and evaluate model performance with model metrics. You can compare results from the initial model run with the new model run to evaluate how the adjustment influenced model performance.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAs consumer businesses strive to distinguish themselves in a competitive market, the power of AI-driven product recommendation systems cannot be underestimated. Companies can tap into their vast data stores and harness the capabilities of advanced algorithms to forge deeper connections with their customers.\u003c/p\u003e\u003cp\u003eLabelbox is a data-centric AI platform that empowers teams to iteratively build powerful product recommendation engines to fuel lasting customer relationships. To get started, \u003ca href=\"https://app.labelbox.com/signup?_gl=1*p9sldc*_ga*MTIxOTY2MjYxMy4xNjkwNDg1NjI5*_ga_WS2TB2GPYE*MTY5MjkwMjk0OC40LjAuMTY5MjkwMzQ1Ni4wLjAuMA..\u0026ref=labelbox-guides.ghost.io\"\u003esign up for a free Quantumworks Lab account\u003c/a\u003e or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e. \u003c/p\u003e","comment_id":"64e7a2c3b0c09f000179dbed","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078--1-.png","featured":false,"visibility":"public","created_at":"2023-08-24T18:34:43.000+00:00","updated_at":"2023-11-20T19:39:23.000+00:00","published_at":"2023-08-24T19:15:09.000+00:00","custom_excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-build-a-powerful-product-recommendation-system-for-retail","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-build-a-powerful-product-recommendation-system-for-retail/","excerpt":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","reading_time":10,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078--1--2.png","og_title":"How to build a powerful product recommendation system for retail","og_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078--1--1.png","twitter_title":"How to build a powerful product recommendation system for retail","twitter_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","meta_title":"How to build a powerful product recommendation system for retail","meta_description":"Learn how to leverage Quantumworks Lab's data-centric AI platform to build a robust AI-powered product recommendation system to power personalized experiences for retail. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64cab907306044000155752f","uuid":"baf1efed-584a-42c0-975b-78b8368fad3d","title":"A guide to the Data I/O process in Quantumworks Lab","slug":"a-guide-to-the-data-i-o-process-in-labelbox","html":"\u003cp\u003eAs you navigate the world of intelligent application creation, one element remains pivotal - your data. At Quantumworks Lab, we recognize the value of your data and its role in driving your operations, which is why we focus on simplifying the Data In and Out (I/O) process. In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with our platform.\u003c/p\u003e\u003cp\u003eWhether your goal is to store data in a cloud-hosted table, a ML training pipeline, a database, or even a production environment, our aim is to equip you with the knowledge and tools needed for a flexible, robust, and effective data management system.\u003c/p\u003e\u003ch2 id=\"understanding-data-io\"\u003eUnderstanding Data I/O\u003c/h2\u003e\u003cp\u003eAt its core, Data I/O refers to the import and export of data in your Quantumworks Lab workflow. As simple as it sounds, the process can be quite intricate, given the variety of data formats and storage locations. To manage data effectively, Quantumworks Lab uses a structured approach that tackles each data type - \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io\"\u003edata rows\u003c/a\u003e, \u003ca href=\"https://docs.labelbox.com/docs/import-metadata?ref=labelbox-guides.ghost.io\"\u003emetadata\u003c/a\u003e (including embeddings), \u003ca href=\"https://docs.labelbox.com/reference/attachments?ref=labelbox-guides.ghost.io\"\u003eattachments\u003c/a\u003e, and \u003ca href=\"https://docs.labelbox.com/reference/feature?ref=labelbox-guides.ghost.io\"\u003eannotations\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"data-in\"\u003eData In\u003c/h2\u003e\u003ch3 id=\"data-rows\"\u003eData Rows\u003c/h3\u003e\u003cp\u003eData rows typically exist in cloud storage like Amazon Web Services (AWS), Google Cloud Storage (GCS), or Microsoft Azure (Azure). However, data rows can also exist as local files, offering you flexibility in how you access and utilize your data.\u003c/p\u003e\u003ch3 id=\"setting-up-data-in\"\u003eSetting Up Data In\u003c/h3\u003e\u003cp\u003eFor those using cloud storage, setting up \u003ca href=\"https://docs.labelbox.com/docs/iam-delegated-access?ref=labelbox-guides.ghost.io\"\u003eDelegated Access\u003c/a\u003e is the first step. This involves granting Quantumworks Lab permission to securely gain read access to your unlabeled data as hosted in your preferred cloud storage provider while providing Quantumworks Lab with the limited access necessary to display and label your data. Once access is granted, you need to identify where your metadata (including embeddings) and attachments are stored.\u003c/p\u003e\u003cp\u003eRefer to the below links to learn more about setting up Delegated Access with the below cloud storage providers:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/import-aws-s3-data?ref=labelbox-guides.ghost.io\"\u003eAmazon S3\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/using-google-cloud-storage?ref=labelbox-guides.ghost.io\"\u003eGoogle Cloud\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/microsoft-azure-blob-storage?ref=labelbox-guides.ghost.io\"\u003eMicrosoft Azure\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"attachments-and-metadata\"\u003eAttachments and Metadata\u003c/h3\u003e\u003cp\u003eAttachments and metadata traditionally exist in a table (Databricks, Excel, BigQuery, CSV, etc.) alongside or separately from the data rows. In an attempt to maintain consistency and streamline the process, Quantumworks Lab encourages users to upload metadata and attachments directly with the data rows.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Metadata\u003c/strong\u003e is any information known about an asset pre-Labelbox that could be useful in data filtering and selection Metadata also includes embeddings - these are representations of your data in a vector space. These vectors capture the essential features of your data and represent them in a form that can be processed by machine learning algorithms.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/metadata?ref=labelbox-guides.ghost.io\"\u003eDeveloper guide on metadata\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e2) Attachments\u003c/strong\u003e on the other hand could be any additional files or information that supplement your data and assist in the creation of high quality human labels. This could include anything from text documents with descriptive data, additional images, audio files, or any other data type that provides more context to your main data row.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/attachments?ref=labelbox-guides.ghost.io\"\u003eDeveloper guide on attachments\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Embeddings \u003c/strong\u003ecan improve your data exploration and allow you to make use of similarity search within Catalog. Lablebox computes off-the-shelf embeddings using neural networks trained on publicly available data. Off-the-shelf embeddings provide a useful starting point to explore your data, but to get the most out of similarity search you’ll want to experiment with different embeddings to power your selection based on your particular data. \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/custom-embeddings?ref=labelbox-guides.ghost.io\"\u003eDeveloper guide on embeddings\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHere are a few pointers:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf your data is in a BigQuery table, you can refer to this \u003ca href=\"https://github.com/Quantumworks Lab/labelbox-bigquery/tree/main?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eIf your data is in a Databricks table, check out this \u003ca href=\"https://github.com/Quantumworks Lab/labelspark?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eFor data in a CSV format, use this \u003ca href=\"https://github.com/Quantumworks Lab/labelpandas?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eFor unique data formats, our comprehensive Quantumworks Lab documentation will be your guide\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"annotations\"\u003eAnnotations\u003c/h3\u003e\u003cp\u003eAs an output from machine learning models, annotations typically exist in JSON files and can be stored either locally or in cloud storage. The beauty of annotations lies in their versatility; they come in various forms including bounding box, mask, radio classification, and others, giving you the freedom to choose what best suits your application.\u003c/p\u003e\u003cp\u003eWhen it comes to Quantumworks Lab, if you want to upload pre-labels or submitted labels (labels made elsewhere), the process involves a few crucial steps:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Setting up a Quantumworks Lab Project and Ontology:\u003c/strong\u003e An ontology serves as a blueprint for your labeling project. It defines the labels and the structure for the annotation data you are handling. Setting up an ontology in Quantumworks Lab that aligns with your annotation data is an essential step to ensure that your pre-labels or submitted labels can be properly read and processed by the system. You can learn more about how to set up ontologies in our \u003ca href=\"https://chat.openai.com/c/link?ref=labelbox-guides.ghost.io\"\u003edeveloper guide on ontologies\u003c/a\u003e.\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Understanding the Annotation Data Format:\u003c/strong\u003e Quantumworks Lab supports a wide range of data formats including JSON, CSV, and others. It is important to understand the format of your annotation data to ensure compatibility with the Quantumworks Lab platform.\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Identifying the Annotation Types:\u003c/strong\u003e The type of annotation used is dependent on your use case. It could be a bounding box annotation for object detection tasks, a mask annotation for semantic segmentation tasks, or a radio classification for multi-choice tasks. By identifying the annotation types that your use case requires, you can ensure that your data is appropriately annotated for your model.\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4) Determining the Annotation Format:\u003c/strong\u003e Annotation formats can either be customer-specific or adhere to industry standards like COCO. Understanding this will help you prepare your data in a way that fits the requirements of Quantumworks Lab and aids in efficient data processing.\u003c/p\u003e\u003cp\u003eBy paying close attention to these steps, you can maximize the utility of your annotations, thereby boosting the effectiveness of your labeling projects and the performance of your machine learning models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5) Converting Annotations to Quantumworks Lab Format: \u003c/strong\u003eLabelbox supports two formats for importing annotations: NDJSON and Python annotation type. How these annotations are uploaded depends on your media type, see the links below for further information.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-image-annotations?ref=labelbox-guides.ghost.io\"\u003eImport image annotations \u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-video-annotations?ref=labelbox-guides.ghost.io\"\u003eImport video annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-text-annotations?ref=labelbox-guides.ghost.io\"\u003eImport text annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-geospatial-annotations?ref=labelbox-guides.ghost.io\"\u003eImport geospaital annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-document-annotations?ref=labelbox-guides.ghost.io\"\u003eImport document annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-conversational-text-annotations?ref=labelbox-guides.ghost.io\"\u003eImport conversational text annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/import-audio-annotations?ref=labelbox-guides.ghost.io\"\u003eImport audio conversations\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy paying close attention to these steps, you can maximize the utility of your annotations, thereby boosting the effectiveness of your labeling projects and the performance of your machine learning models.\u003c/p\u003e\u003ch2 id=\"data-out\"\u003e\u003cbr\u003eData Out\u003c/h2\u003e\u003cp\u003eContrary to data in, the data out process focuses on how data is extracted from Labelbox. Extracting data from Quantumworks Lab involves exporting labeled data rows, along with their associated metadata, attachments, and annotations. Data exported from Quantumworks Lab can be used for a variety of purposes, such as model training or data enrichment. Given that every model has unique input requirements and organizations have unique data storage formats, the export process is often more unique than the import.\u003c/p\u003e\u003ch3 id=\"streamlining-data-out\"\u003eStreamlining Data Out\u003c/h3\u003e\u003cp\u003eThe Quantumworks Lab platform supports a variety of data formats and storage solutions to ensure that your data is exported in a format that suits your needs and is compatible with your storage system. Whether you're using BigQuery, Databricks, CSV, or other formats, Quantumworks Lab has a solution for you. Here's how you can navigate the process:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Choose an Export Format:\u003c/strong\u003e Determine the export format that suits your needs and is compatible with your storage system. Quantumworks Lab supports a wide variety of formats including JSON, CSV, and others. This gives you the flexibility to choose a format that best aligns with your downstream workflows.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Select a Connector:\u003c/strong\u003e Just as with data in, Quantumworks Lab offers connectors to aid in exporting data. These connectors are designed to seamlessly bridge the gap between Quantumworks Lab and your storage system, making the data out process efficient and hassle-free.  Quantumworks Lab offers connectors to aid in exporting data:\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor BigQuery users, this \u003ca href=\"https://github.com/Quantumworks Lab/labelbox-bigquery/tree/main?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e will come in handy\u003c/li\u003e\u003cli\u003eDatabricks users can refer to this \u003ca href=\"https://github.com/Quantumworks Lab/labelspark?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e\u003c/li\u003e\u003cli\u003eFor CSV-formatted data, this \u003ca href=\"https://github.com/Quantumworks Lab/labelpandas?ref=labelbox-guides.ghost.io\"\u003econnector\u003c/a\u003e is useful\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3) Export your Data:\u003c/strong\u003e Initiate the export process via your chosen connector. During the export, Quantumworks Lab compiles your labeled data rows, associated metadata, attachments, and annotations, and organizes them in your chosen export format.\u003c/p\u003e\u003cp\u003eFor a deeper understanding of the process, our \u003ca href=\"https://docs.labelbox.com/reference/export-v2-glossary?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e provides a wealth of information.\u003c/p\u003e\u003cp\u003eIn essence, the Data I/O process is a crucial aspect of your interaction with Quantumworks Lab, designed to make your data management effortless. As you become more familiar with the process, you'll find it an essential tool in creating intelligent applications with Labelbox.\u003c/p\u003e\u003ch3 id=\"utilizing-your-exported-data\"\u003eUtilizing Your Exported Data\u003c/h3\u003e\u003cp\u003eOnce your data is exported, it is ready to be utilized for a variety of purposes:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1) Model Training:\u003c/strong\u003e The primary use case of exported data is to feed it into your machine learning models. The labeled data serves as the training data, guiding your models to recognize patterns and make predictions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2) Data Analysis \u0026amp; Enrichment:\u003c/strong\u003e The exported data, especially metadata and annotations, can provide valuable insights when analyzed. Additionally, it can enrich your existing data sets, enhancing the accuracy and detail of your data and leading to more effective models and analytics. This could guide decision-making processes and strategies within your organization.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3) Iterative Improvements:\u003c/strong\u003e In some cases, exported data can also be fed back into your annotation process for iterative improvements, creating a feedback loop that continually enhances your data quality and model performance.\u003c/p\u003e\u003cp\u003eIn conclusion, the Data I/O process in Quantumworks Lab is an integral component to successful application creation. This guide provides an in-depth understanding of the process, allowing you to accurately import and export data, whether that be data rows, metadata, attachments, or annotations. The process ensures compatibility with various data formats and storage systems, while facilitating efficient data management for your machine learning projects. \u003c/p\u003e\u003cp\u003eFor detailed instructions or further understanding, our comprehensive \u003ca href=\"https://docs.labelbox.com/?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e is always available.\u003c/p\u003e","comment_id":"64cab907306044000155752f","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/08/Group-3078.png","featured":false,"visibility":"public","created_at":"2023-08-02T20:13:59.000+00:00","updated_at":"2023-10-27T17:13:05.000+00:00","published_at":"2023-08-03T13:44:19.000+00:00","custom_excerpt":"In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guide/a-guide-to-the-data-i-o-process-in-labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/a-guide-to-the-data-i-o-process-in-labelbox/","excerpt":"In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"A guide to the Data I/O process in Quantumworks Lab","meta_description":"In this guide, we take an in-depth look at the Data I/O process and offer a step-by-step guide to streamline your interaction with the Quantumworks Lab platform. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"648a3024f45029000168a9d5","uuid":"df001e73-f4a2-4a45-8bc7-8ebbaa093778","title":"How to uncover model errors with Quantumworks Lab Model","slug":"how-to-uncover-model-errors-with-labelbox-model","html":"\u003cp\u003eMachine learning models are only as good as the quality of their predictions. But how do you ensure that your model is making accurate predictions, and more importantly, how do you identify and address the errors your model might be making? In this guide, we will be exploring how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.\u003c/p\u003e\u003cp\u003eIn the examples below, we’ll start by uploading predictions from YOLOv8 along with confidence scores to Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/model-runs?ref=labelbox-guides.ghost.io\"\u003emodel run\u003c/a\u003e. We’ll also use an open source dataset to provide a diverse dataset for testing the model. You can get instructions on how to upload model inferences to Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/page/tutorials?ref=labelbox-guides.ghost.io#upload-predictions-to-a-model-run\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"gallery-view-identifying-disagreements\"\u003eGallery View: Identifying disagreements\u003c/h2\u003e\u003cp\u003eGallery View is a powerful tool for identifying disagreements between model predictions and ground truth labels. Here's how to use it for error analysis:\u003c/p\u003e\u003cp\u003e1) Go to the Gallery View within a model run. You may choose to focus on the Validation or Test split if you prefer.\u003c/p\u003e\u003cp\u003e2) Apply a Metrics filter to identify images with metrics that could indicate disagreements between model predictions and ground truth annotations. Users can sort the assets based on any combination of IOU, confidence, recall, false negative, and false positive etc.\u003c/p\u003e\u003cp\u003e3) Sort the data rows either by increasing metrics or increasing order of confidence. This can help surface rows where the model is least confident or is likely to be erroneous.\u003c/p\u003e\u003cp\u003e4) Inspect the surfaced data rows in detail to identify patterns of edge cases where the model is struggling. This may involve manually inspecting hundreds of data rows.\u003c/p\u003e\u003cp\u003eThe video below shows an uncertainty sampling based on a low-confidence example.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7mair0juaw\" title=\"[Uncover model errors] MEA low confidence Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eThe video below shows how to find model errors for a particular class by sorting in ascending order for IOU. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/tlgntztpau\" title=\"[Uncover model errors] IOU ascending Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eIdentifying incorrect model prediction based on low intersection over union\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"model-metrics-identifying-struggling-classes\"\u003eModel Metrics: Identifying struggling classes\u003c/h2\u003e\u003cp\u003eThe Metrics View provides a comprehensive overview of how your model is performing. You can leverage this view to quickly identify classes that your model might be struggling with by:\u003c/p\u003e\u003col\u003e\u003cli\u003eInspecting the metrics in the Metrics View. In this example, we’re finding predictions that are false negatives and have recall value for a person between .2 to .3.\u003c/li\u003e\u003cli\u003eClicking on the recall value for person between .2 to .3 will open the Gallery View, which will have filtering and sorting activated to show assets associated with the particular class.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/xumtw94n2i\" title=\"[Uncover model errors] Metrics view Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch2 id=\"fix-model-errors\"\u003eFix model errors\u003c/h2\u003e\u003cp\u003eOnce you have identified a pattern of incorrect model predictions, you can find similar assets that the model will also struggle with and send them to be labeled before retraining your model.\u003c/p\u003e\u003col\u003e\u003cli\u003eSelect data rows on which your model is struggling.\u003c/li\u003e\u003cli\u003eOpen the selected data rows in Catalog by clicking on [n] selected \u0026gt; View in Catalog. You will then be redirected to a filtered view of your Catalog showing only the previously selected data rows.\u003c/li\u003e\u003cli\u003eUse \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003esimilarity search\u003c/a\u003e to surface data similar to this pattern of model failures among all of the data in your Catalog. Optionally, you could create a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e that will automatically collect any similar data uploaded in the future.\u003c/li\u003e\u003cli\u003eNext, you could filter on Annotation \u0026gt; is none to surface only unlabeled data rows. Labeling this high-impact data and then re-training your model is a powerful way to boost model performance. Create a \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003ebatch\u003c/a\u003e and send it to a labeling project.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/htzhdxduhh\" title=\"[Uncover model errors] Curating a batch Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLabelbox offers powerful tools and workflows to not only uncover model errors but also to improve the performance of your machine learning models over time. By leveraging the Gallery View, Model Metrics, and Projector View, you can identify where model might be struggling. Additionally, with the ability to fix these errors through data-centric iterations, you can ensure that your model becomes more accurate and reliable with each iteration.\u003c/p\u003e","comment_id":"648a3024f45029000168a9d5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3065--2-.png","featured":false,"visibility":"public","created_at":"2023-06-14T21:24:52.000+00:00","updated_at":"2023-10-27T17:14:26.000+00:00","published_at":"2023-06-15T17:22:00.000+00:00","custom_excerpt":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-uncover-model-errors-with-labelbox-model/","excerpt":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":"How to uncover model errors with Gallery View and Model Metrics","og_description":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","twitter_image":null,"twitter_title":"How to uncover model errors with Gallery View and Model Metrics","twitter_description":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","meta_title":"How to uncover model errors with Gallery View and Model Metrics","meta_description":"Explore how to perform model error analysis using Quantumworks Lab, with a specific focus on utilizing the Gallery View and Model Metrics.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6478fbf1f06ecd0001d70bc5","uuid":"506ee668-bb83-4bc8-80b1-8c9b4c76e759","title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling","slug":"using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWhile Yolov8 is no longer supported on Quantumworks Lab, this blog remains relevant if you are working with other object detection models. Alternatives such as OWL-ViT, Rekognition, GroundingDINO, and GroundingDINO + SAM can still be found and used on Quantumworks Lab’s platform.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this guide, we will demonstrate the application of foundation models, such as Meta’s Segment Anything and YOLOv8, to automatically detect, classify and draw masks on objects of interest in a video. This is a follow-up to earlier guide: \u003ca href=\"https://labelbox.com/guides/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/?ref=labelbox-guides.ghost.io\"\u003eUsing Meta’s Segment Anything with YOLOv8 to Automatically Classify Masks\u003c/a\u003e. In this guide, we’ll automatically detect and segment objects in a video.\u003c/p\u003e\u003cp\u003eVideos have many frames and are tedious to label. Segmentation masks are even more time consuming to label as they vary ever so slightly frame-by-frame, requiring manual fine-tuning each time. With foundation models, you can automate and significantly speed up the labeling process to label more video data, in less time. This allows you to focus valuable time on review, simply correcting the AI models’ output.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"498\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/06/data-src-image-09b636f8-1f2c-4d2a-a4db-a03122468636.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn this guide, we will be walking through a simple semantic segmentation task: drawing masks around a person as they skateboard.\u003c/p\u003e\u003cp\u003eHere’s a high-level summary of the process that we will be walking through step-by-step below, with code:\u003c/p\u003e\u003cp\u003e1) Load YOLOv8, SAM and Quantumworks Lab Python SDK\u003c/p\u003e\u003cp\u003e2) For each frame of the video:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun an object detector to generate bounding boxes with classifications for specified classes\u003c/li\u003e\u003cli\u003eFeed the bounding boxes as inputs to Meta’s Segment Anything model which will produce segmentation masks\u003c/li\u003e\u003cli\u003ePrepare mask predictions in a format that Quantumworks Lab Python SDK expects\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e3) Upload all frames at once to Quantumworks Lab via prediction import\u003c/p\u003e\u003cp\u003e4) Open up video editor and review or modify the pre-labels as you usually do\u003c/p\u003e\u003cp\u003eYou can run all of the above out-of-the-box on your video(s) using our \u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/sam/meta_sam_labelbox_video.ipynb?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e. Simply load in your video and get automatically segmented masks, with classes in Quantumworks Lab, in minutes!\u003c/p\u003e\u003cp\u003eFor this guide, we will use the following video:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-orig.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-orig.gif 600w\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-load-yolov8\"\u003eStep 1: Load YOLOv8\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.ultralytics.com/?ref=labelbox-guides.ghost.io\"\u003eYOLOv8\u003c/a\u003e is a state-of-the-art object detector that produces bounding boxes and classes around common objects. It's the latest iteration of the YOLO (You Only Look Once) family of models, and it boasts some impressive features. YOLOv8 is known for its speed and accuracy, making it an invaluable tool for a wide range of applications. Here, we use YOLOv8 to automatically detect and localize the person skateboarding in the video.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport ultralytics\nultralytics.checks()\nfrom ultralytics import YOLO\nmodel = YOLO(f'{HOME}/yolov8n.pt')\n\n# each class id is assigned a different color\ncolors = np.random.randint(0, 256, size=(len(model.names), 3))\nprint(model.names)\n\n# Specify which classes you care about. The rest of classes will be filtered out.\nchosen_class_ids = [0] # 0 refers to person, as per model.names\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-2-load-sam\"\u003eStep 2: Load SAM\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eMeta's SAM model\u003c/a\u003e is a state-of-the-art computer vision model that is designed to accurately segment images and videos into distinct objects. Using advanced deep learning techniques, Segment Anything is able to identify and segment objects in images, making it a powerful tool for a wide range of applications. The SAM model is able to generate segmentation masks based on prompts, including bounding box prompts, which we will use in the code below.\u003c/p\u003e\u003cp\u003e\u003cbr\u003eFor an in-editor experience of SAM, please read our other blog post \u003ca href=\"https://labelbox.com/blog/coming-soon-auto-segment-powered-by-sam/?ref=labelbox-guides.ghost.io\"\u003eAuto-Segment 2.0 powered by Meta’s Segment Anything Model\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport torch\nimport matplotlib.pyplot as plt\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nsam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\nmask_predictor = SamPredictor(sam)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-3-load-labelboxs-python-sdk\"\u003eStep 3: Load Quantumworks Lab's Python SDK\u003c/h2\u003e\u003cp\u003eLabelbox’s Python SDK gives you easy methods to create ontologies, projects and datasets, and upload masks to a video.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab as lb\nimport labelbox.types as lb_types\n\n# Create a Quantumworks Lab API key for your account by following the instructions here:\n# https://docs.labelbox.com/reference/create-api-key\n# Then, fill it in here\nAPI_KEY = \"\"\nclient = lb.Client(API_KEY)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-4-run-yolov8-and-sam-per-frame\"\u003eStep 4: Run YOLOv8 and SAM per-frame\u003c/h2\u003e\u003cp\u003eHere we run the models on each frame and generate masks automatically.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003ecap = cv2.VideoCapture(VIDEO_PATH)\n\n# This will contain the resulting mask predictions for upload to Quantumworks Lab\nmask_frames = []\n\nframe_num = 1\nwhile cap.isOpened():\n  ret, frame = cap.read()\n  if not ret:\n    break\n\n  # Run frame through YOLOv8 to get detections\n  detections = model.predict(frame, conf=0.7)\n \n  # Run frame and detections through SAM to get masks\n  transformed_boxes = mask_predictor.transform.apply_boxes_torch(detections[0].boxes.xyxy, list(get_video_dimensions(cap)))\n  mask_predictor.set_image(frame)\n  masks, scores, logits = mask_predictor.predict_torch(\n    boxes = transformed_boxes,\n    multimask_output=False,\n    point_coords=None,\n    point_labels=None\n  )\n\n  # Combine mask predictions into a single mask, each with a different color\n  class_ids = detections[0].boxes.cpu().cls\n  merged_with_colors = add_color_to_mask(masks[0][0], colors[int(class_ids[0])]).astype(np.uint8)\n  for i in range(1, len(masks)):\n    curr_mask_with_colors = add_color_to_mask(masks[i][0], colors[int(class_ids[i])])\n    merged_with_colors = np.bitwise_or(merged_with_colors, curr_mask_with_colors)\n\n  # Upload multi-colored combined mask to temp location\n  # to get temp instance uri\n  instance_uri = get_instance_uri(client, global_key, merged_colored_mask)\n\n  # Create MaskFrame object to be uploaded to Quantumworks Lab\n  mask_frame = lb_types.MaskFrame(index=frame_num, instance_uri=instance_uri)\n  mask_frames.append(mask_frame)\n\n  frame_num += 1\n\ncap.release()\n\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-bboxes.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-bboxes.gif 600w\"\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-masks.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-masks.gif 600w\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-5-upload-the-predicted-masks-as-pre-labels-onto-labelbox\"\u003eStep 5: Upload the predicted masks as pre-labels onto Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThe predicted masks can be easily and seamlessly integrated into Quantumworks Lab via our SDK.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Create MaskInstance per unique class predicted / chosen\ninstances = []\n for cid in chosen_class_ids:\n   color = get_color(colors[int(cid)])\n   name = model.names[int(cid)]\n   instances.append(lb_types.MaskInstance(color_rgb=color, name=name))\n\n# Create list of VideoMaskAnnotation objects, one for each unique class\nannotations = []\nfor instance in instances:\n  video_mask_annotation = lb_types.VideoMaskAnnotation(\n       frames=mask_frames,\n       instances=[instance]\n   )\n  annotations.append(video_mask_annotation)\n\n# Create Label object\nlabels = [\nlb_types.Label(data=lb_types.VideoData(global_key=global_key),\n                  annotations=annotations))\n]\n\n# Run import job\nupload_job = lb.MALPredictionImport.create_from_objects(\n   client=client,\n   project_id=project.uid,\n   name=\"mal_import_job\" + str(uuid.uuid4()),\n   predictions=labels\n)\nupload_job.wait_until_done()\n\nprint(f\"Errors: {upload_job.errors}\", )\nprint(f\"Status of uploads: {upload_job.statuses}\")\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-LB.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"374\" srcset=\"https://labelbox-guides.ghost.io/content/images/2023/06/ezgif.com-video-to-gif-LB.gif 600w\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eCreating segmentation masks on video data can be tedious and time-consuming. Using the power of foundation models in Quantumworks Lab, you can easily generate masks with classifications in a matter of minutes. Rather than spending hours labeling video data, you now have a way to accelerate video labeling and not only reduce time to market, but also the cost of developing your models.\u003c/p\u003e","comment_id":"6478fbf1f06ecd0001d70bc5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074.png","featured":false,"visibility":"public","created_at":"2023-06-01T20:13:37.000+00:00","updated_at":"2024-11-25T21:18:27.000+00:00","published_at":"2023-06-01T22:14:48.000+00:00","custom_excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/","excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074-1.png","og_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","og_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/06/Group-3074-2.png","twitter_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","twitter_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","meta_title":"Using Meta's Segment Anything (SAM) model on video with Quantumworks Lab's model-assisted labeling ","meta_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically detect, classify, and draw masks on video. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"646f88789568720001240642","uuid":"b8289ca0-7d94-41c4-84df-a5e0541ba9bb","title":"Leveraging YOLO and Quantumworks Lab to make videos queryable by content","slug":"leveraging-yolo-and-labelbox-to-make-videos-queryable-by-content","html":"\u003cp\u003eVideo data can offer a wealth of information for AI use cases, but because of their dense and dynamic nature, manually extracting insights can be a tedious task. To accelerate this process, leading AI teams often use off-the-shelf, pre-trained models like \u003ca href=\"https://ultralytics.com/yolov8?ref=labelbox-guides.ghost.io\"\u003eYOLO\u003c/a\u003e (short for You Only Look Once) to take a quick first pass at detecting the contents of a video dataset. \u003c/p\u003e\u003cp\u003eYOLO is a real-time object detection model. Unlike traditional models, which scan an image multiple times at different scales, YOLO looks at the entire image only once, making it particularly well-suited for video object detection. It can detect various objects and provide a bounding box for each detected object. Using this model to enrich video data can make it easier for AI teams to understand their data.\u003c/p\u003e\u003cp\u003eHowever, the real magic lies in going a step further by organizing and categorizing this information, making the video content searchable. In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.\u003c/p\u003e\u003ch2 id=\"how-to-make-videos-queryable\"\u003eHow to make videos queryable\u003c/h2\u003e\u003cp\u003eTo make a video queryable, the first step is to run the YOLO model on the video to detect the objects in each frame. Once objects are detected, they are classified, and a bounding box is added to the objects in Labelbox. You can then search your video content based on these annotations with Catalog. Want to find all instances where a \"bowl\" appears in your video? Simply search for \"bowl\" in the Quantumworks Lab UI. Catalog will return all the videos that contain this annotation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"633\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/YOLOGuide_1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/YOLOGuide_1.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_1.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOnce YOLO has added annotations to your video data, you can simply search for specific content within that dataset using Quantumworks Lab Catalog.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can also create a custom workflow in Quantumworks Lab to create a bespoke review process, where you can create tasks that only have assets containing a specific annotation in the video, such as vehicles or other objects of interest. You can let YOLO take the first pass at labeling your video data, and then have human labelers review and/or add annotations in frames that contain specific objects of interest, reducing the time and costs required to label high-quality training data.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"961\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/YOLOGuide_2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/YOLOGuide_2.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_2.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eYou can create a custom labeling workflow that targets specific data in Quantumworks Lab to save labeling time and costs while ensuring high labeling quality.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRead on for step-by-step instructions on using YOLO and Quantumworks Lab to make your videos queryable.\u003c/p\u003e\u003ch2 id=\"part-1-create-project-and-ontology\"\u003ePart 1: Create project and ontology\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eproject = client.create_project(name=\"Video project with YOLO\",\n                                   media_type=lb.MediaType.Video)\n\n\n#connect ontology to your project\nproject.setup_editor(ontology)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"part-2-queue-assets-to-project-based-on-global-keys\"\u003ePart 2: Queue assets to project based on global keys\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003ebatch = project.create_batch(\n \"first-batch-video\"+str(uuid.uuid4()), # Each batch in a project must have a unique name\n global_keys= global_keys, # A paginated collection of data row objects, a list of data rows or global keys\n priority=5 # priority between 1(Highest) - 5(lowest)\n)\nprint(\"Batch: \", batch)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"part-3-load-yolo-model\"\u003ePart 3: Load YOLO model\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003emodel = YOLO(f'{HOME}/yolov8n.pt')\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"part-4-create-predictions-and-upload-them-to-labelbox\"\u003ePart 4: Create predictions and upload them to Quantumworks Lab\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Export queued data rows from the project\nqueued_data_rows = project.export_queued_data_rows()\n\n\n# Initialize an empty list to store labels\nlabels = []\n\n\n# Loop over each data row. Here, it's only looping over the first data row.\nfor data_row in queued_data_rows[:1]:\n\n\n # Extract the URL of the video from the data row.\n video_url = data_row[\"rowData\"]\n\n\n # Extract the global key from the data row.\n global_key = data_row[\"globalKey\"]\n\n\n # Make a GET request to the video URL.\n response = requests.get(video_url)\n\n\n # Open a file in write-binary mode and write the content of the response to it.\n # This is downloading the video and saving it as 'sample_video.mp4'.\n with open('sample_video.mp4', 'wb') as f:\n     f.write(response.content)\n\n\n # Create a VideoCapture object to read frames from the downloaded video.\n cap = cv2.VideoCapture(\"sample_video.mp4\")\n\n\n # Set up the VideoWriter for the output video. The 'mp4v' argument specifies the codec to be used.\n fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n\n # Initialize a counter for the frame number.\n frame_number = 0\n\n\n # Start a loop to read frames from the video.\n while True:\n     # Read the next frame from the video.\n     ret, frame = cap.read()\n\n\n     # If the frame could not be read (i.e., we're at the end of the video), break the loop.\n     if not ret:\n         break\n\n\n     # Increment the frame number.\n     frame_number += 1\n\n\n     # Use the model to predict objects in the current frame. The confidence threshold is set to 0.5.\n     results = model.predict(frame, conf=0.50)\n\n\n     # Loop over each predicted class.\n     for c in results[0].boxes.cls:\n       # Loop over each bounding box predicted for the current class.\n       for idx, box in enumerate(results[0].boxes.xyxy):\n         # Get the class number from the model's class names.\n         class_number = model.names[int(c)]\n\n\n         # Get the coordinates of the bounding box.\n         xmin, ymin, xmax, ymax  = float(box[0]), float(box[1]), float(box[2]), float(box[3])\n\n\n         # Create an annotation for the bounding box.\n         bbox_annotation = [\n           lb_types.VideoObjectAnnotation(\n             name = class_number,\n             keyframe= True,\n             frame=frame_number,\n             segment_index=0,\n            \n             # Define the bounding box as a rectangle with a start and end point.\n             value = lb_types.Rectangle(\n                   start=lb_types.Point(x=xmin, y=ymin), # x = left, y = top\n                   end=lb_types.Point(x= xmax, y=ymax)))] # x= left + width , y = top + height\n        \n         # Append a new Label object to the labels list. Each Label represents one detected object in one frame.\n         labels.append(\n             Label(\n                 data=lb_types.VideoData(global_key=global_key),\n                 annotations = bbox_annotation\n             )\n         )\n\n\n # Release the VideoCapture object.\n cap.release()\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"923\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/YOLOGuide_3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/YOLOGuide_3.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_3.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of Quantumworks Lab video editor with YOLO predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1182\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/YOLOGuide_4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/YOLOGuide_4.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/YOLOGuide_4.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAnalytics view of a dataset with YOLO predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Large-GIF--1466x882-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1466\" height=\"882\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Large-GIF--1466x882-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Large-GIF--1466x882-.gif 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/Large-GIF--1466x882-.gif 1466w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eVideo is enriched with annotations from YOLO, and the contents can be queried in Quantumworks Lab Catalog.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eCombining the power of the YOLO — or any off-the-shelf AI model that suits your needs — with Quantumworks Lab opens up exciting possibilities for video content. It not only makes the video content queryable, but also helps bring a new level of understanding to what's inside the videos. This combination can be especially beneficial for use cases like surveillance, content creation, content moderation, and advertising, where insights from video content are crucial.\u003c/p\u003e\u003cp\u003eUsing an AI model as a first pass enables users to search videos based on their content, enabling AI teams to learn how many annotations of each object exist within their dataset and what types of annotations the training dataset might lack. This further reduces friction when it comes to finding the next set of assets that will improve the model's performance by doing active learning. Try enriching your videos using YOLO or any other AI model using this \u003ca href=\"https://colab.research.google.com/drive/1vOVo4MtsoUxNJ-tIdmI-GKWjVFo9RrQe?ref=labelbox-guides.ghost.io#scrollTo=rJXXOJdpWD48\"\u003escript\u003c/a\u003e to discover exactly what content already exists in your troves of unstructured videos and find specific videos quickly and easily.\u003c/p\u003e","comment_id":"646f88789568720001240642","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Screen-Shot-2023-05-24-at-1.45.31-PM.png","featured":false,"visibility":"public","created_at":"2023-05-25T16:10:32.000+00:00","updated_at":"2023-10-27T16:58:02.000+00:00","published_at":"2023-05-25T16:33:51.000+00:00","custom_excerpt":"In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/leveraging-yolo-and-labelbox-to-make-videos-queryable-by-content/","excerpt":"In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":"Leveraging YOLO and Quantumworks Lab to make videos queryable by content","og_description":"In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.","twitter_image":null,"twitter_title":"Leveraging YOLO and Quantumworks Lab to make videos queryable by content","twitter_description":"In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.","meta_title":"Leveraging YOLO and Quantumworks Lab to make videos queryable by content","meta_description":"In this guide, we'll explore how to use YOLO and Quantumworks Lab Catalog together to make a dataset of unlabeled videos searchable.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"632ccf4016e912003d39b2a7","uuid":"fc620d97-ea51-468e-ae87-a99b87ca45b8","title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","slug":"using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks","html":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWhile YOLOv8 is no longer supported on Quantumworks Lab, this blog remains relevant if you are working with other object detection models. Alternatives such as OWL-ViT, Rekognition, GroundingDINO, and GroundingDINO + SAM are fully supported on the Quantumworks Lab platform.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIn this guide, you will learn how to chain \u003ca href=\"https://labelbox.com/solutions/computer-vision/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision\u003c/a\u003e foundation models together to automatically populate pre-labels with class in Quantumworks Lab very quickly. We will be walking through a simple semantic segmentation task: drawing masks around all objects of a particular class in an image.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1086\" height=\"332\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/Screen-Shot-2023-05-09-at-1.51.00-PM.png 1086w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLeveraging chained foundation models in Quantumworks Lab will greatly reduce the time it takes you or your team to draw segmentation masks; by augmenting the recently-released \u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eSAM\u003c/a\u003e model with classifications, you will automate the task of assigning classes. Rather than performing a tedious labeling effort, you can focus your valuable efforts reviewing, verifying, and correcting labels drawn by AI models.\u003c/p\u003e\u003cp\u003eHere’s a high-level summary of the process that we will be walk through step-by-step below:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRun an object detector on the image to generate bounding boxes with classifications for specified classes\u003c/li\u003e\u003cli\u003eFeed the bounding boxes as inputs to Meta’s Segment Anything model which will produce segmentation masks for each one\u003c/li\u003e\u003cli\u003eUpload the mask predictions onto Quantumworks Lab as pre-labels\u003c/li\u003e\u003cli\u003eOpen up image editor and review or modify the pre-labels as you usually do\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can run all of the above out-of-the-box on your image(s) using our \u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/develop/examples/integrations/sam/meta_sam_labelbox.ipynb?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e. Simply load the images and automatically get segmented masks, with classes, in just a few minutes. \u003c/p\u003e\u003cp\u003eFor this guide, we will use the following image of a lot of colorful chairs: \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"896\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6164ed0e-354a-4464-8d7e-f103c26cdc4c.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-run-yolov8-on-the-image\"\u003eStep 1: Run YOLOv8 on the image\u003c/h2\u003e\u003cp\u003eThe latest iteration of the YOLO (You Only Look Once) family of models, \u003ca href=\"https://docs.ultralytics.com/?ref=labelbox-guides.ghost.io\"\u003eYOLOv8\u003c/a\u003e is an object detector that produces bounding boxes and classes around common objects. Known for its speed and accuracy, YOLOv8 boasts some impressive features – making it an invaluable tool for a wide range of applications. Here, we use YOLOv8 to automatically detect and localize all the chairs in the image.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# load the YOLOv8 model\nmodel = YOLO(f'{HOME}/yolov8n.pt')\n\n# run the model on the image\nresults = model.predict(source='chairs.jpg', conf=0.25)\npredicted_boxes = results[0].boxes.xyxy\n\n\n# read in the image for visualization\nimage_bgr = cv2.imread(IMAGE_PATH, cv2.IMREAD_COLOR)\n\n# use cv2 to visualize the bounding boxes on the image\nfor box in predicted_boxes:\n cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\ncv2.imshow(\"YOLOv8 predictions\", image_bgr)\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"896\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-bcd688a3-c398-436b-a6fd-b0739f7785ff.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-2-feed-bounding-boxes-as-inputs-to-meta%E2%80%99s-sam-model\"\u003eStep 2: Feed bounding boxes as inputs to Meta’s SAM model\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://segment-anything.com/?ref=labelbox-guides.ghost.io\"\u003eSAM (Segment Anything Model)\u003c/a\u003e – recently released by Meta AI, is an advanced computer vision model designed to accurately segment images and videos into distinct objects. Using advanced deep learning techniques, SAM is able to identify and segment objects in images, making it a powerful tool for a wide range of applications. The SAM model is able to generate segmentation masks based on prompts, including bounding box prompts, which we will use in the code below.\u003c/p\u003e\u003cp\u003eTo see an in-editor experience of SAM, please check out our blog post \u003ca href=\"https://labelbox.com/blog/coming-soon-auto-segment-powered-by-sam/?ref=labelbox-guides.ghost.io\"\u003eAuto-Segment 2.0 powered by Meta’s Segment Anything Model\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# load the SAM model\nsam = sam_model_registry[\"vit_h\"](checkpoint=\"/sam_vit_h_4b8939.pth\n\").to(device=torch.device('cuda:0'))\n\nmask_predictor = SamPredictor(sam)\n\n# transform the YOLOv8 predicted boxes to match input format expected by SAM model\ntransformed_boxes = mask_predictor.transform.apply_boxes_torch(predicted_boxes, image_bgr.shape[:2])\n\n\n# run SAM model on all the boxes\nmask_predictor.set_image(image_bgr)\nmasks, scores, logits = mask_predictor.predict_torch(\n   boxes = transformed_boxes,\n   multimask_output=False,\n   point_coords=None,\n   point_labels=None\n)\n\n# combine all masks into one for easy visualization\nfinal_mask = None\nfor i in range(len(masks) - 1):\n  if final_mask is None:\n    final_mask = np.bitwise_or(masks[i][0], masks[i+1][0])\n  else:\n    final_mask = np.bitwise_or(final_mask, masks[i+1][0])\n\n# visualize the predicted masks\nplt.figure(figsize=(10, 10))\nplt.imshow(image_rgb)\nplt.imshow(final_mask, cmap='gray', alpha=0.7)\nplt.show()\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1daa38c9-1485-4ca8-8c03-57f6a87b9cc7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"794\" height=\"454\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-1daa38c9-1485-4ca8-8c03-57f6a87b9cc7.png 600w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1daa38c9-1485-4ca8-8c03-57f6a87b9cc7.png 794w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-3-upload-the-predicted-masks-as-pre-labels-onto-labelbox\"\u003eStep 3: Upload the predicted masks as pre-labels onto Quantumworks Lab\u003c/h2\u003e\u003cp\u003eThe predicted masks can be easily and seamlessly integrated into Quantumworks Lab via our SDK. The upload is just a few lines of code that run in less than a minute.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass_names = []\nfor c in results[0].boxes.cls:\n class_names.append(model.names[int(c)])\n\nannotations = []\nfor mask in masks:\n  # convert a 2D array to 3D array\n  mask_data = lb_types.MaskData.from_2D_arr(np.asarray(mask[0], dtype=\"uint8\"))\n  mask_annotation = lb_types.ObjectAnnotation(\n    name = class_names[idx], # assign class from Step 1\n    value=lb_types.Mask(mask=mask_data, color=color),\n  )\n  annotations.append(mask_annotation)\n\nlabels = [\nlb_types.Label(data=lb_types.ImageData(global_key=\"image_name\"),annotations=annotations)\n]\nupload_job = lb.MALPredictionImport.create_from_objects(\n   client=client,\n   project_id=project.uid,\n   name=\"mal_job\" + str(uuid.uuid4()),\n   predictions=labels\n)\nupload_job.wait_until_done()\n\nprint(f\"Errors: {upload_job.errors}\", )\nprint(f\"Status of uploads: {upload_job.statuses}\")\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"815\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6c2e603a-c308-4e0f-9521-4f5be3d3d688.gif 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"final-thoughts-on-using-meta%E2%80%99s-sam-model-with-yolov8-to-automatically-classify-masks\"\u003eFinal thoughts on using Meta’s SAM model with YOLOv8 to automatically classify masks\u003c/h2\u003e\u003cp\u003eWhile Meta’s AI's SAM is really powerful at segmentation, it leaves out the crucial task of classification. In this guide, we demonstrated how you can use YOLOv8 (or another object detector) to generate bounding boxes with classes and then automatically apply those classes to the masks generated by SAM. We also showed how this seamlessly integrates with the Quantumworks Lab Model Assisted Labeling SDK.\u003c/p\u003e\u003cp\u003eIf you are interested in applying SAM on images through our image editor, you can \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003esign up\u003c/a\u003e for a Quantumworks Lab account and give it a try today. \u003c/p\u003e","comment_id":"632ccf4016e912003d39b2a7","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3062.png","featured":false,"visibility":"public","created_at":"2022-09-22T21:10:24.000+00:00","updated_at":"2024-11-26T18:38:02.000+00:00","published_at":"2023-05-09T18:26:38.000+00:00","custom_excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-metas-segment-anything-sam-model-with-yolov8-to-automatically-classify-masks/","excerpt":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","og_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3062-2.png","twitter_title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","twitter_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","meta_title":"Using Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks","meta_description":"Learn how to use Meta’s Segment Anything (SAM) model with YOLOv8 to automatically classify masks. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"645a7494225a9a00012fcfae","uuid":"e07975bd-27c6-4c00-bf8c-bc96c772ce56","title":"How to accelerate image-text pair generation with BLIP-2","slug":"how-to-accelerate-image-text-pair-generation-with-blip-2","html":"\u003cp\u003e\u003ca href=\"https://labelbox.com/solutions/generative-ai/?ref=labelbox-guides.ghost.io\"\u003eGenerative AI\u003c/a\u003e has taken the world by storm, opening doors to a plethora of applications, from creating realistic images and videos to generating novel text and music. The success of these applications often hinges on the \u003ca href=\"https://labelbox.com/blog/data-quality-for-machine-learning/?ref=labelbox-guides.ghost.io\"\u003equality and quantity of data used to train the underlying machine learning models\u003c/a\u003e, the production of which is often time consuming and costly. As a result, leading AI teams have been innovating on ways to streamline the caption creation process and empower human annotators to work more efficiently without sacrificing quality.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2301.12597?ref=labelbox-guides.ghost.io\"\u003eBLIP-2\u003c/a\u003e (Bootstrapping Language-Image Pre-training) is an AI model that can perform various multi-modal tasks like visual question answering, image-text retrieval (image-text matching) and image captioning. It can analyze an image, understand its content, and generate a relevant and concise caption. BLIP-2 helps language models understand images without changing their original structure. It does this by using querying transformer (q-former) that acts as a bridge between the image and the language model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"527\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-10.48.13-AM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSee the original flowchart as published in the \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2301.12597.pdf?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eBLIP-2 research paper\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBLIP-2 achieves state-of-the-art performance on various vision-language tasks while being more compute efficient than existing methods. Powered by \u003ca href=\"https://labelbox.com/solutions/large-language-models/?ref=labelbox-guides.ghost.io\"\u003eLarge Language Models (LLMs)\u003c/a\u003e, it can perform \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot image-to-text generation\u003c/a\u003e based on natural language instructions, enabling capabilities like visual knowledge reasoning and visual conversation.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"898\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-1.38.04-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExamples of images and their BLIP 2 captions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, we'll explore how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions. Additionally, you can use any model to make pre-labels in Quantumworks Lab as shown \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e. Quantumworks Lab customers using \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003emodel-assisted labeling\u003c/a\u003e have seen 50-70% reductions in labeling costs driven by dramatic reductions in labeling time and complexity. Therefore, using a model like BLIP-2 will further reduce labeling time.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/BLIP-2-lucidchart--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1340\" height=\"420\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/BLIP-2-lucidchart--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/BLIP-2-lucidchart--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/BLIP-2-lucidchart--1-.png 1340w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHow to use BLIP-2 with Quantumworks Lab\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"step-1-create-a-project-and-attach-an-ontology\"\u003eStep 1: Create a project and attach an ontology.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eproject = client.create_project(name = \"BLIP project\", media_type=labelbox.MediaType.Image)\nproject.setup_editor(ontology)\nontology_from_project = labelbox.OntologyBuilder.from_project(project)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-initialize-and-load-a-pre-trained-blip-2-model\"\u003eStep 2: Initialize and load a pre-trained BLIP-2 model.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003edevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n   \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n)\nmodel.to(device)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-3-collect-inferences-to-be-used-as-pre-labels\"\u003eStep 3: Collect inferences to be used as pre-labels.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003equeued_data_rows = project.export_queued_data_rows()\nground_truth_list = list()\n\n\nfor data_row in queued_data_rows:\n url = data_row[\"rowData\"]\n image = Image.open(requests.get(url, stream=True).raw)\n inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n generated_ids = model.generate(**inputs, max_new_tokens=30)\n generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n print(generated_text) \n  text_annotation = labelbox.data.annotation_types.ClassificationAnnotation(\n     name=\"BLIP model prediction\",\n     value=labelbox.data.annotation_types.Text(answer = generated_text)\n   )\n  ground_truth_list.append(Label(\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-4-upload-pre-labels-to-your-project\"\u003eStep 4: Upload pre-labels to your project.\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eupload_task = labelbox.MALPredictionImport.create_from_objects(client, project.uid, str(uuid.uuid4()), ground_truth_list)\nupload_task.wait_until_done()\nprint(upload_task.errors)\u003c/code\u003e\u003c/pre\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1132\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/05/Screenshot-2023-05-07-at-2.19.01-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of asset with a pre-label and no human-generated caption\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/unknown.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"893\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/unknown.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/unknown.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/unknown.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of an asset with a pre-label and a human-generated caption\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"review-labels\"\u003eReview labels\u003c/h2\u003e\u003cp\u003eAfter the labels have been annotated, you can use \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e to create highly customizable, multi-step review pipelines, making your review process more efficient and automated. Workflows offer granular control over how your data rows get reviewed, saving you both time and resources. You can create tasks that enable you to filter based on who created the label, what annotations exist and when the label was created.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/unknown--1--2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"804\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/unknown--1--2.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/unknown--1--2.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/unknown--1--2.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLabelbox lets you customize labeling and review workflows to your exact requirements.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"export-labels\"\u003eExport labels\u003c/h2\u003e\u003cp\u003eAfter you are done reviewing the labels, you can easily export the annotations as show \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-JSON\"\u003e         \"annotations\": {\n             \"objects\": [],\n             \"classifications\": [\n               {\n                 \"feature_id\": \"clhdn79ae0ent076c4h579rxu\",\n                 \"name\": \"BLIP model prediction\",\n                 \"text_answer\": {\n                   \"content\": \"a yellow flower with a green background\"\n                 }\n               }\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eBy using captions generated by BLIP-2 or inferences by AI models as pre-labels, AI teams can significantly reduce labeling time and costs. To learn more, explore this \u003ca href=\"https://colab.research.google.com/drive/1vnD4gVBu8uAE3dn44h8qIXx3R_DYv5E2?ref=labelbox-guides.ghost.io#scrollTo=PcfsMaUu1GrV\"\u003efull script\u003c/a\u003e for using the BLIP-2 model to generate pre-labels. These captions can then be amended or approved in Quantumworks Lab by labelers. You can also learn more about the BLIP-2 model \u003ca href=\"https://huggingface.co/docs/transformers/main/model_doc/blip-2?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e and model-assisted labeling \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"645a7494225a9a00012fcfae","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Guide_BLIP-2.png","featured":false,"visibility":"public","created_at":"2023-05-09T16:28:04.000+00:00","updated_at":"2023-10-27T17:11:22.000+00:00","published_at":"2023-05-09T17:09:47.000+00:00","custom_excerpt":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-accelerate-image-text-pair-generation-with-blip-2/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-accelerate-image-text-pair-generation-with-blip-2/","excerpt":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"How to accelerate image-text pair generation with BLIP-2","og_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","twitter_image":null,"twitter_title":"How to accelerate image-text pair generation with BLIP-2","twitter_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so that a specialized workforce can further improve the image captions.","meta_title":"How to accelerate image-text pair generation with BLIP-2 | Quantumworks Lab","meta_description":"Learn how to use BLIP-2-generated captions to create pre-labels for images so a specialized workforce can further improve the image captions.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64590b32638f810001454877","uuid":"25f18d3c-fa4c-4206-a4c2-db2f9c6ba390","title":"Automatically label text with 96%+ accuracy using foundation models","slug":"automatically-label-text-with-96-accuracy-using-foundation-models","html":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, you’ll learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab and foundation models. We will be walking through a \u003ca href=\"https://labelbox.com/product/annotate/text/?ref=labelbox-guides.ghost.io\"\u003etext classification\u003c/a\u003e task: identifying news articles that talk about sports. \u003c/p\u003e\u003cp\u003eHere's a high-level summary of the process:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConnect your data to Quantumworks Lab with just a few lines of code.\u003c/li\u003e\u003cli\u003eLabelbox leverages foundation models to automatically enrich your data.\u003c/li\u003e\u003cli\u003eUse the powerful search capabilities of Quantumworks Lab to quickly find articles with similar traits and classify them in one click. With the help of foundation models, you can instantaneously label large amounts of data. \u003cem\u003ePro tip: Combine a variety of search techniques, such as a similarity search, natural language search, keyword search, and investigate clusters of similar data, to boost your results.\u003c/em\u003e\u003c/li\u003e\u003cli\u003eWhile foundation models are a helpful starting point, they might not always correctly classify data, especially on challenging or rare data points. In this case, utilize human-in-the-loop labeling and QA by pre-labeling data using foundation models and sending it for your internal or external labeling team to review.\u003c/li\u003e\u003cli\u003eAutomatically apply these rules to all new incoming data by creating a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e in Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, let’s take a look at how we can do the above in Labelbox. As a sneak peek into the process, by leveraging foundation models, we managed to \u003cstrong\u003eclassify 88% of our news articles in minutes with a 96.5% accuracy rate\u003c/strong\u003e. An additional 15% of our news articles were successfully pre-labeled using foundation models, with 85% accuracy, and sent for human review. This left us with only 493 data points that were missed by foundation models – a massive efficiency gain for any labeling team.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"step-1-connect-your-data-to-labelbox-with-a-few-lines-of-code\"\u003eStep 1: Connect your data to Quantumworks Lab with a few lines of code\u003c/h2\u003e\u003cp\u003eSince this is a classification task, our goal is to correctly have the model identify news articles about sports. We will be using the following Hugging Face dataset: \u003ca href=\"https://huggingface.co/datasets/ag_news?ref=labelbox-guides.ghost.io\"\u003eag_news\u003c/a\u003e which contains 120,000 articles, including 30,000 about sports, for our analysis.\u003c/p\u003e\u003cp\u003eTo begin, let's connect this data to Labelbox. Simply retrieve the dataset from Hugging Face and integrate it with Quantumworks Lab in just a few lines of code.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom datasets import load_dataset\ndataset = load_dataset(\"ag_news\",split='train')\n\n# iterate over the data\npayload_imgs = []\ncounter = 0\n\n\n# iterate over the data\npayloads = []\nglobal_keys = []\ncounter = 0\n\nfor data in dataset:\n\n  text = data['text']\n  label = data['label']\n  global_key = \"ag_news_\" + str(counter)\n  global_keys.append(global_key)\n\n  # create payload for texts\n  payloads.append({\n    \"row_data\": text, \n    \"global_key\": global_key,\n  })\n\n  counter += 1\n\n\n# create dataset in Quantumworks Lab\nlb_dataset = client.create_dataset(name=\"ag_news\") \n\n# add data in Quantumworks Lab\ntask = lb_dataset.create_data_rows(payloads)  \ntask.wait_till_done() # async\nprint(task.errors) # check errors \u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-leverage-foundation-models-to-instantly-enhance-your-data\"\u003eStep 2: Leverage foundation models to instantly enhance your data\u003c/h2\u003e\u003cp\u003eLabelbox will automatically compute and store MPNet embeddings for your data. We are using \u003ca href=\"https://huggingface.co/sentence-transformers/all-mpnet-base-v2?ref=labelbox-guides.ghost.io\"\u003ethis implementation\u003c/a\u003e available through Hugging Face. \u003c/p\u003e\u003cp\u003eOnce your data has been uploaded, watch as Quantumworks Lab enriches your data with foundation model embeddings. These embeddings are powerful in that they can be harnessed to automatically label, or pre-label, your data.  \u003c/p\u003e\u003cp\u003eIf you don’t want to use the default embeddings by Quantumworks Lab, you can also \u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003eupload custom embeddings\u003c/a\u003e from any other foundation model, with up to 100 custom embeddings for each data point. \u003c/p\u003e\u003cp\u003eWhether you’re using the default or custom embeddings, embeddings are helpful in curating and finding subsets of data that share similar characteristics. For instance, embeddings power \u003ca href=\"https://labelbox.com/blog/how-vector-similarity-search-works/?ref=labelbox-guides.ghost.io\"\u003eLabelbox’s similarity search\u003c/a\u003e, natural language search, and 2D projector view. You can search and explore all of your data with tools that help you powerfully surface specific subsets of similar texts.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"927\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-328f9bca-a9e7-4c5a-808e-96a7b92ab577.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch2 id=\"step-3-use-powerful-search-capabilities-to-quickly-find-data\"\u003eStep 3: Use powerful search capabilities to quickly find data\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities in Quantumworks Lab, you can easily find and classify data that share similar characteristics. This is a special case of \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot and few-shot learning\u003c/a\u003e: the challenge is to find all examples of sports articles, based on zero (or a few) examples. With the help of foundation models, and minimal human signal, you can quickly label a lot of data in just a few clicks. The following are tools in Quantumworks Lab that help provide labeling signal to make it easy to automatically classify your data:\u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-projector-view-for-classification\"\u003eZero-shot Labeling: Projector View for Classification\u003c/h3\u003e\u003cp\u003eLabelbox allows you to visualize data clusters in 2D. For this example, we can see distinct clusters. By inspecting a few examples, we discover that some of the data clusters correspond to sports news articles. We manually select each cluster and tag it with \"UMAP: sports. We intentionally leave out data points situated between clusters, as these represent challenging data points. This is expected since each labeling function won’t be perfect in isolation, and some data points are difficult and challenging.\u003c/p\u003e\u003cp\u003eWe then repeat the process with t-SNE instead of UMAP and tag each sports cluster with \"t-SNE: sports\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"569\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-45f58400-ce0b-46b9-ac80-7fe7f68e95c3.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUMAP: a cluster of news articles about sports\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"705\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6e0a161f-7c1e-42c0-acb8-7f7cb4e05557.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003et-SNE: a subcluster of news articles about basketball\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"zero-shot-labeling-keyword-search\"\u003eZero-shot Labeling: Keyword search\u003c/h3\u003e\u003cp\u003eLabelbox enables you to search all data points that contain some keywords. We filter all data points that contain the following keywords\u003cem\u003e: sport, sports, basketball, baseball, soccer, football, tennis, hockey\u003c/em\u003e. And tag these 5,990 texts as “Keyword search: sports”. \u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-natural-language-search-for-classification\"\u003eZero-shot Labeling: Natural language search for classification\u003c/h3\u003e\u003cp\u003eLabelbox enables you to conduct natural language searches on text. For example you can type in “news articles about sports” to surface all pieces of text about sports. Adjusting the similarity threshold will narrow the search to only relevant articles. For this use case, we filter for a similarity score higher than 0.85 and tag all of the 6,468 texts as “Natural language search: sports”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"922\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-44fe6347-be04-4805-8ba8-43535ddc8f92.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA natural language search will surface thousands of news articles about sports. By adjusting the similarity score, we can keep the most confident zero-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"few-shot-labeling-similarity-search-for-classification\"\u003eFew-shot Labeling: Similarity search for classification\u003c/h3\u003e\u003cp\u003eLabelbox also streamlines few-shot labeling. Quickly browse all your data in Catalog to surface 5 news articles about sports. For each of them, run a similarity search and tag the top results (e.g with a similarity score higher than 0.85) as “Similarity search: sports”. This provides us with 5 new labeling functions that surface sports news articles.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"967\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a14a4bfb-85ef-4fbc-8543-4b6d4bd296b5.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA similarity search example with an anchor article about college basketball. We can filter to keep the most confident few-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"combining-different-sources-of-signal-weak-labeling\"\u003eCombining different sources of signal: weak labeling\u003c/h3\u003e\u003cp\u003eWhile each of these labeling signals is powerful on its own, you can combine multiple sources in Labelbox. This allows you to apply simple rules in a weak supervision fashion to further enhance your results. Integrate different labeling signals, such as similarity searches, natural language searches, and data clusters, to boost your outcomes. You can \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003ecombine various filters\u003c/a\u003e by using the AND and OR functions.\u003c/p\u003e\u003ch2 id=\"step-4-automatically-classify-data-with-foundation-models-and-use-human-in-the-loop-qa-for-challenging-cases\"\u003eStep 4: Automatically classify data with foundation models and use human-in-the-loop QA for challenging cases\u003c/h2\u003e\u003ch3 id=\"high-confidence-data-points-direct-classification\"\u003eHigh confidence data points: direct classification\u003c/h3\u003e\u003cp\u003eFoundation models are highly confident about most data points. So much so that we can directly classify data points leveraging Quantumworks Lab’s \u003ca href=\"https://labelbox.com/blog/pre-label-and-enrich-your-data-with-bulk-classifications/?ref=labelbox-guides.ghost.io\"\u003ebulk classification feature\u003c/a\u003e. With this new feature, you can specify and send your texts to a specific step of the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox.ghost.io#how-it-works\"\u003elabeling and review workflow\u003c/a\u003e. We can directly move these high-confidence data points straight to the “Done” task.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-ae901cac-31f2-4bf6-b91d-5a7ae8f1f2bc.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe classify thousands of texts in bulk, and send them to the “Done” task of our labeling project, in just a click, since foundation models are confident on those.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these high-confident data points are those that belong to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe sports clusters, both with UMAP and t-SNE. But just how accurate are these predictions? To answer this question, we looked at the Hugging Face ground truths. 704 out of the 21,560 sports predictions are incorrect.\u003c/li\u003e\u003cli\u003eOr, the similarity search score to two or more anchors is higher than 0.85. This results in 3,548 sports classifications, all of which are accurate except 185.\u003c/li\u003e\u003cli\u003eThe sports cluster in UMAP or t-SNE and a natural language search higher than 0.85. This results in 1,219 sports classifications, all of which are accurate except 58. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis method of surfacing high-confident data points enables us to directly classify 26,427 pieces of text - with only 947 errors - achieving an \u003cstrong\u003eaccuracy of 96.5%.\u003c/strong\u003e Since 26,427 out of 30,000 sports articles have been classified directly by foundation models, the \u003cstrong\u003ecoverage is 88%.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhat about the 947 errors? Upon closer inspection of these errors, it turns out that they are all related to sports, but in the context of News, World, or Science, and hence have been labeled on Hugging Face according to those categories instead of Sports.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"952\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-42d3908e-2d36-42e7-a3b6-7c592b538597.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFoundation models failed on 947 news articles. It turns out that these articles are all related to sports, but are classified on HuggingFace as World news, or Business news, or Science \u0026amp; Tech news.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eNow, let’s move on to classify the remaining 12% of data rows, on which foundation models are less confident.\u003c/p\u003e\u003ch3 id=\"low-confidence-data-points-human-in-the-loop-labeling\"\u003eLow confidence data points: Human-in-the-Loop labeling\u003c/h3\u003e\u003cp\u003eFor some pieces of text, foundation models exhibit low confidence. We can bulk classify these data points in Quantumworks Lab, but move them to the “To Review” task. This will ensure a human is looped in and will review the classifications coming from foundation models.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1237f738-96d9-420b-8a0b-457c6c373231.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe pre-label thousands of data points in bulk, and send them to “Review” in our labeling project, in just a click, since foundation models are moderately confident on these pieces of text.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these data points are those that belong to the sports cluster, with UMAP or t-SNE, and that haven’t been classified yet.\u003c/p\u003e\u003cp\u003eUsing this approach, we managed to classify 4,723 additional data rows, with an \u003cstrong\u003eaccuracy of 85%\u003c/strong\u003e (696 errors). We can send these low-confident data rows for Human-in-the-Loop review. \u003c/p\u003e\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eDirect classification with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eHuman in the loop with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eSports articles missed by foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of data rows classified\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e26,427\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e4,723\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e493\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of errors\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e947\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e696\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e-\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAccuracy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e96.5%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e85%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e-\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eFraction of sports articles\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e88%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e15%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e1.8%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 26,427 pieces of text (88%) in minutes, with \u003cstrong\u003e96.5% accuracy\u003c/strong\u003e thanks to foundation models. An additional 4,723 data points (13.5%) have been pre-labeled with foundation models, with \u003cstrong\u003e85% accuracy\u003c/strong\u003e, and sent for human review. This leaves only 493 data points talking about sports, missed by foundation models.\u003c/p\u003e\u003ch2 id=\"step-5-set-it-and-forget-it-%E2%80%93-automatically-apply-these-rules-to-fresh-incoming-data\"\u003eStep 5: Set it and forget it – automatically apply these rules to fresh, incoming data\u003c/h2\u003e\u003cp\u003eWith Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e, we automatically classify fresh, incoming news articles about sports. \u003c/p\u003e\u003cp\u003eFor example, we can set up a slice that automatically surfaces all new pieces of text, that have been connected to Quantumworks Lab in the past week, that haven’t been classified yet. We can set the slice's criteria to include only text data rows where the natural language search for the prompt is \"news articles about sports\" and is higher than 0.85 (since we know that these data rows are very likely to be on sports). \u003c/p\u003e\u003cp\u003eWith slices, you can easily surface and inspect any new, high-impact data that gets added to your data lake.\u003c/p\u003e\u003cp\u003eFrom there, it only takes one click to classify all of these text data rows as sports articles. \u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 26,427 pieces of text (88%) in minutes, with \u003cstrong\u003e96.5% accuracy\u003c/strong\u003e thanks to foundation models. An additional 4,723 data points (15%) have been pre-labeled with foundation models, with \u003cstrong\u003e85% accuracy\u003c/strong\u003e, and sent for human review. This leaves only 493 data points talking about sports, missed by foundation models.\u003c/p\u003e\u003chr\u003e\u003cp\u003eIf you’re a current Quantumworks Lab user who wants to leverage any foundation model to supercharge your data labeling process in just a few clicks,\u003ca href=\"https://app.labelbox.com/catalog?ref=labelbox-guides.ghost.io\"\u003e try our bulk classification feature today\u003c/a\u003e or \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started with a free Quantumworks Lab account\u003c/a\u003e.\u003cbr\u003eIf you’re interested in seeing how quickly you can label images leveraging foundation models, check out our guide on \u003ca href=\"https://labelbox.com/guides/automatically-label-images-with-99-accuracy-using-foundation-models/?ref=labelbox-guides.ghost.io\"\u003ehow to automatically label images with 99% accuracy.\u003c/a\u003e\u003c/p\u003e","comment_id":"64590b32638f810001454877","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1-.png","featured":false,"visibility":"public","created_at":"2023-05-08T14:46:10.000+00:00","updated_at":"2023-10-27T17:11:48.000+00:00","published_at":"2023-05-08T20:43:24.000+00:00","custom_excerpt":"Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/automatically-label-text-with-96-accuracy-using-foundation-models","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/automatically-label-text-with-96-accuracy-using-foundation-models/","excerpt":"Learn how to automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","reading_time":9,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1--2.png","og_title":"Automatically label text with 96%+ accuracy using foundation models","og_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3060--1--1.png","twitter_title":"Automatically label text with 96%+ accuracy using foundation models","twitter_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","meta_title":"Automatically label text with 96%+ accuracy using foundation models","meta_description":"Automatically label text with 96%+ accuracy by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","email_subject":null,"frontmatter":null,"feature_image_alt":"Automatically label text with 96%+ accuracy using foundation models","feature_image_caption":null},{"id":"6452689090e75e0001b3546c","uuid":"5a55750c-5b23-4ce7-b075-117a24a4f392","title":"Automatically label images with 99% accuracy using foundation models","slug":"automatically-label-images-with-99-accuracy-using-foundation-models","html":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this guide, you’ll learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab and foundation models. We will be walking through a sample \u003ca href=\"https://labelbox.com/guides/image-annotation/?ref=labelbox-guides.ghost.io\"\u003eimage classification task\u003c/a\u003e: figuring out if images contain cats or dogs.\u003c/p\u003e\u003cp\u003eHere's a high-level summary of the process that we'll be walking through step-by-step below:\u003c/p\u003e\u003cul\u003e\u003cli\u003eConnect your data to Quantumworks Lab with just a few lines of code.\u003c/li\u003e\u003cli\u003eLabelbox leverages foundation models to magically enrich your data.\u003c/li\u003e\u003cli\u003eUse the powerful search capabilities of Quantumworks Lab to quickly find data with similar traits and classify them in one click. With the help of foundation models, you can instantaneously label large amounts of data. \u003cem\u003ePro tip: Combine a variety of search techniques, such as a similarity search, natural language search, and investigate clusters of similar data, to boost your results.\u003c/em\u003e\u003c/li\u003e\u003cli\u003eWhile foundation models are a helpful starting point, they might not always correctly classify data, especially on challenging or rare data points. In this case, utilize human-in-the-loop labeling and QA by pre-labeling data using foundation models and sending it for your internal or external labeling team to review.\u003c/li\u003e\u003cli\u003eAutomatically apply these rules to all new incoming data by creating a \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslice\u003c/a\u003e in Labelbox.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, let’s take a look at how we can do the above in Labelbox. As a sneak peek into the process, by leveraging foundation models, we managed to \u003cstrong\u003eclassify 86% of our images in minutes with a 99.9% accuracy rate\u003c/strong\u003e. An additional 13.5% of our images were successfully pre-labeled using foundation models, with 98% accuracy, and were sent for human review. This left us with less than 0.5% of images to manually label – a massive efficiency gain for any labeling team.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"step-1-connect-your-data-to-labelbox-with-a-few-lines-of-code\"\u003eStep 1: Connect your data to Quantumworks Lab with a few lines of code\u003c/h2\u003e\u003cp\u003eSince this is a classification task, our goal is to correctly have the model identify cats and dogs in images. We will be using the following Hugging Face dataset - \u003ca href=\"https://huggingface.co/datasets/cats_vs_dogs?ref=labelbox-guides.ghost.io\"\u003ecats_vs_dogs\u003c/a\u003e, containing 18,699 images for our analysis.\u003c/p\u003e\u003cp\u003eTo begin, let's connect this data to Labelbox. Simply retrieve the dataset from Hugging Face and integrate it with Quantumworks Lab in just a few lines of code.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom datasets import load_dataset\ndataset = load_dataset(\"cats_vs_dogs\",split='train')\n\n# iterate over the data\npayload_imgs = []\ncounter = 0\n\nfor data in dataset:\n  image = data['image']\n  label = data['labels'] # 0 is cat, 1 is dog\n  global_key = \"cat_vs_dog_\" + str(counter)\n\n  # save image locally\n  path = \"/content/images/\"+global_key+\".jpg\"\n  image.save(path) \n\n  # create payload for images\n  payload_imgs.append({\"row_data\": path, \"global_key\": global_key})\n  counter += 1\n\n# create dataset in Quantumworks Lab\nlb_dataset = client.create_dataset(name=\"Cat_vs_dog\") \n\n\n# add data in Quantumworks Lab\ntask = lb_dataset.create_data_rows(payload_imgs[i:i+1000])  task.wait_till_done() # async\nprint(task.errors) # check errors \u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-2-leverage-foundation-models-to-instantly-enhance-your-data\"\u003eStep 2: Leverage foundation models to instantly enhance your data\u003c/h2\u003e\u003cp\u003eLabelbox will automatically compute and store CLIP embeddings for your data. Our CLIP model leverages \u003ca href=\"https://openai.com/research/clip?ref=labelbox-guides.ghost.io\"\u003eOpenAI\u003c/a\u003e and we are using \u003ca href=\"https://huggingface.co/sentence-transformers/clip-ViT-B-32?ref=labelbox-guides.ghost.io\"\u003ethis implementation\u003c/a\u003e available through Hugging Face. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"922\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-1141e64d-4de4-4a81-b6b2-9f4faf059871.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce your data has been uploaded, you can enrich your data with foundation model embeddings. These embeddings are powerful in that they can be harnessed to automatically label, or pre-label, your data.  \u003c/p\u003e\u003cp\u003eIf you don’t want to use the default embeddings by Quantumworks Lab, you can also \u003ca href=\"https://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/?ref=labelbox-guides.ghost.io\"\u003eupload custom embeddings\u003c/a\u003e from any other foundation model, with up to 100 custom embeddings for each data point. \u003c/p\u003e\u003cp\u003eWhether you’re using the default or custom embeddings, embeddings are helpful in curating and finding subsets of data that share similar characteristics. For instance, embeddings power Quantumworks Lab’s \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003esimilarity search\u003c/a\u003e, natural language search, and 2D projector view. You can search and explore all of your data with tools that help you powerfully surface specific subsets of data.\u003c/p\u003e\u003ch2 id=\"step-3-use-powerful-search-capabilities-to-quickly-find-data\"\u003eStep 3: Use powerful search capabilities to quickly find data\u003c/h2\u003e\u003cp\u003eWith \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003epowerful search capabilities\u003c/a\u003e in Quantumworks Lab, you can easily find and classify data that share similar characteristics. This is a special case of \u003ca href=\"https://labelbox.com/blog/few-shot-learning-and-zero-shot-learning-with-openai-embeddings/?ref=labelbox-guides.ghost.io\"\u003ezero-shot and few-shot learning\u003c/a\u003e: the challenge is to find all examples of cats and dogs, based on zero (or a few) examples from each class. \u003c/p\u003e\u003cp\u003eWith the help of foundation models, and minimal human signal, you can quickly label a lot of data in just a few clicks. The following are tools in Quantumworks Lab that help provide labeling signal to make it easy to automatically classify your data:\u003c/p\u003e\u003ch3 id=\"zero-shot-labeling-projector-view-for-classification\"\u003eZero-shot Labeling: Projector View for classification\u003c/h3\u003e\u003cp\u003eLabelbox allows you to visualize data clusters in 2D. For this example, we can see two distinct clusters: one for cats and another for dogs. By inspecting a few examples, we can ensure the data clustering is accurate. We then manually select each cluster and tag it with \"UMAP: cats: high confidence\" and \"UMAP: dogs: high confidence\". We intentionally left out data points situated between clusters, as these represent challenging data points. This is expected since each labeling function won’t be perfect in isolation and some data points are difficult and challenging.\u003c/p\u003e\u003cp\u003eWe then repeat the process with t-SNE, instead of UMAP, and tag each cluster with \"t-SNE: cats: high confidence\" and \"t-SNE: dogs: high confidence\".\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"924\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-6a50c54a-5528-475c-ba6f-95478cb3d84c.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUMAP: a cluster of cats with high confidence\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"923\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d8eca6af-f97b-4cd3-b61f-a9b457327695.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUMAP: a sub-cluster of images with two cats\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-efcbe4a2-78b9-46e8-a9ce-008614b29057.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003et-SNE: the same sub-cluster of images with two cats\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"zero-shot-labeling-natural-language-search-for-classification\"\u003eZero-shot Labeling: Natural language search for classification\u003c/h3\u003e\u003cp\u003eLabelbox enables you to conduct \u003ca href=\"https://docs.labelbox.com/docs/natural-language-search?ref=labelbox-guides.ghost.io\"\u003enatural language searches\u003c/a\u003e, for example you can type in “photos of cats” to surface all cat images. Adjusting the similarity threshold will narrow the search parameters to show only the images that contain cats. For this use case, we can filter for a similarity score higher than 0.61 and tag all of the 7,125 images as “Natural language search: Cats (high confidence)”. If we adjust the similarity score to be between 0.6 and 0.61, we can tag the 1,299 images as “Natural language search: Cats (low confidence)”. \u003c/p\u003e\u003cp\u003eWe take the same approach for images containing dogs. Using the same technique above, we tag 7,738 images of dogs as “Natural language search: Dogs (high confidence)” and surface and tag 2,750 images as “Natural language search: Dogs (low confidence)”.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"926\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-fba57f44-9231-45b0-91ad-693c9ba4a092.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA natural language search will surface thousands of images of cats. By adjusting the similarity score, we can keep the most confident zero-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"few-shot-labeling-similarity-search-for-classification\"\u003eFew-shot Labeling: Similarity search for classification\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eLabelbox also streamlines few-shot labeling. Quickly browse all your data in Catalog to surface 5 images of cats and 5 images of dogs. Perform a similarity search in one click using these 10 images as anchors. For each anchor image, run a similarity search and tag the top results (e.g  with a similarity score of higher than 0.895) as “Labeling function: similarity search (cats: high confidence)”. This provides us with 10 new labeling functions that surface images similar to the anchor images.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"925\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-78d77476-30e5-4f45-8445-c68a49192fe4.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eA similarity search example with anchor images of dogs. We can filter to keep the most confident few-shot predictions.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"weak-labeling-combining-different-sources-of-signal\"\u003eWeak Labeling: Combining different sources of signal\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eWhile each of these labeling signals is powerful on its own, you can combine multiple sources in Labelbox. This allows you to apply simple rules in a weak supervision fashion to further enhance your results. Integrate different labeling signals, such as similarity searches, natural language searches, and data clusters, to boost your outcomes. You can \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003ecombine various filters\u003c/a\u003e by using the AND and OR functions.\u003c/p\u003e\u003ch2 id=\"step-4-automatically-classify-data-with-foundation-models-and-use-human-in-the-loop-qa-for-challenging-cases\"\u003eStep 4: Automatically classify data with foundation models and use human-in-the-loop QA for challenging cases\u003c/h2\u003e\u003ch3 id=\"high-confidence-data-points-direct-classification\"\u003eHigh confidence data points: direct classification\u003c/h3\u003e\u003cp\u003eFoundation models are highly confident about most data points. So much so that we can directly classify data points leveraging Quantumworks Lab’s \u003ca href=\"https://labelbox.com/blog/pre-label-and-enrich-your-data-with-bulk-classifications/?ref=labelbox-guides.ghost.io\"\u003ebulk classification feature\u003c/a\u003e. With this new feature, you can specify and send your data rows to a specific step of the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox.ghost.io#how-it-works\"\u003elabeling and review workflow\u003c/a\u003e. We can directly move these high-confidence data points straight to the “Done” task.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"927\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-71fdc0c1-5398-4205-ac97-8e7e839b6a17.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe can classify thousands of data points in bulk, and send them to the “Done” task of our labeling project, in just a click, since foundation models are confident on those.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these high-confident data points are those that belong to the cat or dog cluster, both with UMAP and t-SNE, and where the natural language score is higher than 0.61. This results in 7,627 dog classifications. But just how accurate are these classification predictions? \u003c/p\u003e\u003cp\u003eTo answer this question, we looked at the Hugging Face ground truths. On the surface, 10 out of the 7,627 dog predictions are incorrect (0.13%). However, upon closer inspection, it turns out that the Hugging Face dataset contains a few labeling mistakes and only 6 out of the 7,627 predictions (0.078%) of the foundation model’s predictions are actually incorrect. Similarly, there were 6,587 cat classifications. Only 6 out of the 6,587 cat predictions (0.09%) of the foundation model’s predictions are incorrect. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"761\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-d5d8573f-0102-4600-a61f-433bb79b78a7.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eOut of 7,627 images, foundation models failed on these 10 by predicting dogs instead of cats.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eHigh-confident data points are also those that are found with a similarity search proximity to 2 or more anchors with a 0.895 or higher score. There were 907 dog classifications that fit this criteria, all of which were accurate except 1, and 1,022 cat classifications, all of which were accurate except 6. \u003c/p\u003e\u003cp\u003eBy leveraging the above methods, we were able to classify 16,143 data rows - with only 19 errors - achieving an \u003cstrong\u003eaccuracy of 99.9%. \u003c/strong\u003eSince 16,143 out of 18,699 data rows have been classified directly by foundation models, the \u003cstrong\u003ecoverage is 86%.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eNow, let’s move on to classify the remaining 14% of data rows, on which the foundation model appears to be less confident.\u003c/p\u003e\u003ch3 id=\"medium-confidence-data-points-human-in-the-loop-labeling\"\u003eMedium confidence data points: Human-in-the-Loop labeling\u003c/h3\u003e\u003cp\u003eFor some data points, foundation models exhibit moderate confidence. We can bulk classify these data points in Quantumworks Lab, but move them to the “To Review” task. This will ensure a human is looped in and will review the classifications coming from foundation models.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"922\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-8e8a1006-ec9e-452f-9712-8fa2a081aa97.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWe can pre-label thousands of data points in bulk and send them to “Review” in our labeling project, since foundation models are moderately confident on these images.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn practice, these data points are those that belong to the cat or dog cluster, with UMAP or t-SNE, and that we hadn’t classified before:\u003c/p\u003e\u003cul\u003e\u003cli\u003e1,622 dogs classifications, which turn out to be all accurate except 10.\u003c/li\u003e\u003cli\u003e907 cat classifications, which turn out to be all accurate except 38. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eUsing this approach, we manage to classify 2,529 data rows, with an \u003cstrong\u003eaccuracy of 98%\u003c/strong\u003e (48 errors). Good that we send them to humans for review! So far, we’ve classified all data rows except 27, so the coverage is \u003cstrong\u003e99.85%\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eNow, let’s move on to classify the remaining 27 images.\u003c/p\u003e\u003ch3 id=\"low-confidence-data-points-manual-labeling\"\u003eLow confidence data points: Manual labeling\u003c/h3\u003e\u003cp\u003eAfter applying these rules, 18,672 data rows out of 18,699 (99.85%) have been labeled, leaving only 27 data rows unclassified. Foundation models lack the confidence to label these remaining data points.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1135\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-08106dea-7c38-497e-afdf-da718ed520d9.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSome data rows, 27 in our case, were not classified through foundation models. This includes images that are blurry, where animals are turning their backs or are barely visible behind a cage.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThese 27 data points will require manual labeling by humans, which represents only 0.14% of data rows - a massive efficiency gain in labeling effort and speed!\u003c/p\u003e\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"\u003e\u003ccolgroup\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003ccol\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eDirect classification with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eHuman in the loop with foundation models\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eManual classification\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of data rows classified\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e16,143\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e2,529\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e27\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e# of errors\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e19\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e48\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e0\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAccuracy\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e99.9%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e98%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e100%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eCoverage\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e86%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e13.5%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e0.15%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eCumulative coverage\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e86%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e99.8%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003e100%\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003eWith powerful search capabilities and the bulk classification feature, we managed to classify 16,143 images (86%) in minutes, with \u003cstrong\u003e99.9% accuracy\u003c/strong\u003e thanks to foundation models. An additional 2,529 data points (13.5%) have been pre-labeled with foundation models, with \u003cstrong\u003e98% accuracy\u003c/strong\u003e, and sent for human review. This leaves with only 27 very challenging images to label manually!\u003c/p\u003e\u003ch2 id=\"step-5-set-it-and-forget-it-%E2%80%93-automatically-apply-these-rules-to-fresh-incoming-data\"\u003eStep 5: Set it and forget it – automatically apply these rules to fresh, incoming data\u003c/h2\u003e\u003cp\u003eWith Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e, we can automatically classify fresh, incoming data as cats or dogs. \u003c/p\u003e\u003cp\u003eFor example, we can set up a slice that automatically surfaces all new images that have been connected to Quantumworks Lab in the past week, that haven’t been classified yet. We can set the slice’s criteria to include only images where the natural language search for the prompt “photo of a cat” is higher than 0.61 (since we know that these images are very likely to contain cats). \u003c/p\u003e\u003cp\u003eWith slices, you can easily surface and inspect any new and high-impact data that gets added to your data lake. \u003c/p\u003e\u003cp\u003eFrom there, it only takes one click to classify all of these images as cats.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"1216\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/05/data-src-image-a4cde4e1-6b94-4380-a322-3d14c9917d69.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eWith slices and the ability to automatically surface high-impact data, we surfaced 7,000+ new images of cats that were connected to Quantumworks Lab in the past week. We can easily add a cat classification to all these images in one click.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can learn more about how to bulk classify data in our \u003ca href=\"https://docs.labelbox.com/docs/bulk-classification?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e or in our \u003ca href=\"https://labelbox.com/blog/pre-label-and-enrich-your-data-with-bulk-classifications/?ref=labelbox-guides.ghost.io\"\u003erecent blog post\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eWith powerful search capabilities, the bulk classification feature, and foundation models, we managed to classify 16,143 images (86%) in minutes, with \u003cstrong\u003e99.9% accuracy\u003c/strong\u003e. An additional 2,529 data points (13.5%) have been pre-labeled with \u003cstrong\u003e98% accuracy\u003c/strong\u003e and sent for human review. This only left us with 27 very challenging images that we needed to label manually. \u003c/p\u003e\u003chr\u003e\u003cp\u003eIf you’re a current Quantumworks Lab user who wants to leverage any foundation model to supercharge your data labeling process in just a few clicks,\u003ca href=\"https://app.labelbox.com/catalog?ref=labelbox-guides.ghost.io\"\u003e try our bulk classification feature today\u003c/a\u003e or \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started with a free Quantumworks Lab account\u003c/a\u003e.\u003c/p\u003e","comment_id":"6452689090e75e0001b3546c","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Group-3058--2-.png","featured":false,"visibility":"public","created_at":"2023-05-03T13:58:40.000+00:00","updated_at":"2023-10-27T17:01:07.000+00:00","published_at":"2023-05-03T17:28:24.000+00:00","custom_excerpt":"Automatically label images with 99% accuracy leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/automatically-label-images-with-99-accuracy-using-foundation-models","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/automatically-label-images-with-99-accuracy-using-foundation-models/","excerpt":"Automatically label images with 99% accuracy leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","reading_time":10,"access":true,"comments":false,"og_image":null,"og_title":"Automatically label images with 99% accuracy using foundation models","og_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","twitter_image":null,"twitter_title":"Automatically label images with 99% accuracy using foundation models","twitter_description":"Learn how to exponentially increase your labeling speed and efficiency by leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models. ","meta_title":"Automatically label images with 99% accuracy using foundation models","meta_description":"Automatically label images with 99% accuracy leveraging Quantumworks Lab's search capabilities, bulk classification, and foundation models.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"645007f474911d003db0cb11","uuid":"3d216637-f0d9-4995-bfd5-d289ba48b7d0","title":"Introducing Export V2: How to export data with more granular control","slug":"how-to-export-your-data-with-more-granular-control","html":"\u003cp\u003eIf you already leverage Quantumworks Lab to enrich and label your unstructured data, you know how important it is to export your data insights in the right format and connect it with your downstream data workflow. Whether you want to store your data in a database, a cloud-hosted table, an ML training pipeline, or a production environment, you need a flexible and powerful export system that can handle your specific needs.\u003c/p\u003e\u003cp\u003eThat’s why \u003cstrong\u003ewe’re excited to introduce a new way to export your data\u003c/strong\u003e. This new system gives you more granular control over your data exports across the Quantumworks Lab platform and SDK. With this new way to export, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003eExport the exact Data Rows you are interested in. You can use various filters in the Catalog and Data row tabs, or hand-select the data rows to export. For example, you can grab only the data rows that have received new labels, metadata, or issues updates within the last 24 hours.\u003c/li\u003e\u003cli\u003eConfigure the export to include exactly the right information you need. You can build a custom export payload that meets your specific data workflow needs with much faster performance. Learn more about these improvements \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io#required--optional-fields\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eOrganize your assets into a data-row-centric framework that’s easy to structure and analyze.\u003c/li\u003e\u003cli\u003eLeverage\u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io\"\u003e improved annotation formats\u003c/a\u003e to rapidly export annotations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhile we will continue supporting the Export v1 system until December 31, 2023, we encourage you to gradually start migrating all of your export workflows to this updated export workflow (Export v2). The Export v1 and Export v2 workflows may be used in tandem until Export v1 is sunset on December 31st.\u003c/strong\u003e \u003c/p\u003e\u003cp\u003ePlease refer to our documentation to learn more about export specifications and compare the old Export v1 and new Export v2 systems: \u003ca href=\"https://docs.labelbox.com/reference/export-image-annotations?ref=labelbox-guides.ghost.io\"\u003eimage\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-video-annotations?ref=labelbox-guides.ghost.io\"\u003evideo\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-text-annotations?ref=labelbox-guides.ghost.io\"\u003etext\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-document-annotations?ref=labelbox-guides.ghost.io\"\u003edocuments\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-geospatial-annotations?ref=labelbox-guides.ghost.io\"\u003egeospatial/tiled imagery\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-audio-annotations?ref=labelbox-guides.ghost.io\"\u003eaudio\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-conversational-text-annotations?ref=labelbox-guides.ghost.io\"\u003econversational text\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-html-annotations?ref=labelbox-guides.ghost.io\"\u003eHTML\u003c/a\u003e | \u003ca href=\"https://docs.labelbox.com/reference/export-dicom-annotations?ref=labelbox-guides.ghost.io\"\u003eDICOM\u003c/a\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2 id=\"what%E2%80%99s-new-in-export-v2\"\u003eWhat’s new in Export v2\u003c/h2\u003e\u003ch3 id=\"data-row-centric-asynchronous-exports\"\u003eData row-centric asynchronous exports\u003c/h3\u003e\u003cp\u003eThe previous export system (Export v1) relied upon a less flexible label-centric export that limited access to all the information you might need about a Data Row. Within Export v2’s Data Row-centric context, you can access much more information — including fields like:\u003c/p\u003e\u003cul\u003e\u003cli\u003emetadata,\u003c/li\u003e\u003cli\u003eattachment,\u003c/li\u003e\u003cli\u003eworkflow history,\u003c/li\u003e\u003cli\u003emodel predictions,\u003c/li\u003e\u003cli\u003emedia attributes\u003c/li\u003e\u003cli\u003ebatch id,\u003c/li\u003e\u003cli\u003eand issues, alongside with the labels.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy reframing exports based on Data Rows, we’ve made it much more intuitive for you to integrate with your data tables that are organized around your team’s unique assets and data rows. \u003c/p\u003e\u003cul\u003e\u003cli\u003eAdditionally , now when you trigger an export job, it will occur asynchronously in the background, unblocking your workflow so you can avoid waiting. \u003c/li\u003e\u003cli\u003eIf you are using the Quantumworks Lab UI, you can access the task status in the notification center. \u003c/li\u003e\u003cli\u003eIf you are using SDK, you can query the task for status and results. \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"export-from-a-dataset-or-a-slice-with-the-option-to-grab-labels-from-multiple-projects-and-model-runs\"\u003eExport from a dataset or a slice, with the option to grab labels from multiple projects and model runs.\u003c/h3\u003e\u003cp\u003eA data row can have labels from multiple projects, or have predictions from multiple model runs. Using this new way to export through Catalog, or through the SDK, you can easily grab all the information about a Data Row. \u003c/p\u003e\u003cp\u003eUse data row filters to select a subset of data rows for export:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn the UI, you can build your \u003ca href=\"https://docs.labelbox.com/docs/data-rows-activity?ref=labelbox-guides.ghost.io#filter-data-rows\"\u003efilters within a project in the Data Rows tab\u003c/a\u003e, build \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003efilters in a dataset / slice in Catalog\u003c/a\u003e, or build \u003ca href=\"https://docs.labelbox.com/docs/filtering-and-sorting?ref=labelbox-guides.ghost.io\"\u003efilters in Model using model run filters\u003c/a\u003e. You can then choose to export only the filtered data rows. \u003c/li\u003e\u003cli\u003eIn the UI and SDK, you can use the \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io#filters\"\u003e'last_activity_at' filter\u003c/a\u003e to export only the data rows that have the creation and modification of labels, metadata, status, comments and reviews in a user-specified time range. This applies to a project in Annotate and a dataset or slice export. \u003c/li\u003e\u003cli\u003eIn the UI and SDK, we added a support for a \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io#filters\"\u003e'label_created_at' filter\u003c/a\u003e for you to export only the data rows that have the creation of labels in a user-specified time range. This applies to a project in Annotate and a dataset or slice export.\u003c/li\u003e\u003cli\u003eIn the UI, you can hand-pick data rows for export. Similarly in SDK, we added support for \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io#filters\"\u003e'data_row_ids'\u003c/a\u003e filter to export only the data rows that you are interested in.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"configure-exports-to-selectively-include-or-exclude-certain-information-on-a-data-row\"\u003eConfigure exports to selectively include or exclude certain information on a data row\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWe recognize different teams have unique needs around what they need from exports. For example, labeling team would love to understand the performance and consensus scores of a labeling project, whereas a developer would like to know the metadata and media attributes on a data row.\u003c/li\u003e\u003cli\u003eExport v2 now not only covers all possible fields on a data row, but also makes it configurable so that you can grab only the necessary payload of export information with faster performance. See \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io#required--optional-fields\"\u003ethis table\u003c/a\u003e to check all available option fields that you can include/exclude in your exports. \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"remove-the-caching-for-exports\"\u003eRemove the caching for exports\u003c/h3\u003e\u003cp\u003eExport v1 used to cache exports for 30 minutes. In Export v2, you will always get a fresh export and you can run one export asynchronous task on a project at a time. \u003c/p\u003e\u003ch3 id=\"simplified-and-improved-export-payloads\"\u003eSimplified and improved export payloads\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWe've converted all fields in the export payload into snake case.\u003c/li\u003e\u003cli\u003eRather than isolating them into another JSON file, we've improved the video and DICOM exports to contain all frame annotations in the export ndjson file. and Export v2 provides three representations of objects in frames: “frames”, “segments”, and “key_frame_feature_map” to facilitate your different needs of downstream workflows. See examples in \u003ca href=\"https://docs.labelbox.com/reference/export-video-annotations?ref=labelbox-guides.ghost.io#annotation-export-formats\"\u003eVideo\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/reference/export-dicom-annotations?ref=labelbox-guides.ghost.io#annotation-export-formats\"\u003eDICOM\u003c/a\u003e exports.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"exporting-via-ui\"\u003eExporting via UI\u003c/h2\u003e\u003cp\u003eFrom Quantumworks Lab's UI, you can access the export function through the drop-down menu after selecting a subset of data rows. You can export the entire project, model, dataset, or slice from a set of filters or a selection of data rows within them. \u003c/p\u003e\u003cp\u003eBelow are some examples of Export v2 in action. For more detailed information, please refer to our \u003ca href=\"https://docs.labelbox.com/docs/export-labels?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"annotate-a-labeling-project\"\u003eAnnotate (A labeling project)\u003cbr\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-5691075b-e1cf-448b-9fa1-fa21e739f71f.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"884\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSelect data rows from filteres in the Data Rows tab.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-9e91c4d6-8d20-49fa-a1fb-739a4247487a.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"882\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHand-select specific data rows.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"model-a-model-run\"\u003eModel (A model run)\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-be94fe0b-4fe4-40ba-a93d-a8268afcfe70.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"787\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eSelect from metrics filters.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003ch3 id=\"catalog-a-dataset-and-slices\"\u003eCatalog (A dataset and slices)\u003cbr\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-51102232-fd36-41f0-a343-0f063d17c719.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"850\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExport a slice with labels from multiple projects.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"exporting-data-via-sdk\"\u003eExporting data via SDK\u003c/h2\u003e\u003cp\u003eFor developers that would like to programmatically feed exports directly into downstream data workflows or build automatic workflows to retrieve fresh data exports on a regular basis, we recommend Export v2 SDK. It provides flexibility to control what data you want to export. \u003c/p\u003e\u003ch3 id=\"project-export-v2\"\u003eProject Export v2\u003c/h3\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Set the export params to include/exclude certain fields. Make sure each of these fields are correctly grabbed \nexport_params= {\n  \"attachments\": True,\n  \"metadata_fields\": True,\n  \"data_row_details\": True,\n  \"project_details\": True,\n  \"performance_details\": True\n}\n\n# You can set the range for last_activity_at and label_created_at. You can also set a list of data \n# row ids to export. \n# For context, last_activity_at captures the creation and modification of labels, metadata, status, comments and reviews.\n\n# Note: This is an AND logic between the filters, so usually using one filter is sufficient.\nfilters= {\n  \"last_activity_at\": [\"2000-01-01 00:00:00\", \"2050-01-01 00:00:00\"],\n  \"label_created_at\": [\"2000-01-01 00:00:00\", \"2050-01-01 00:00:00\"],\n  \"data_row_ids\": [\"data_row_id_1\", \"data_row_id_2\"] \n}\n\nexport_task = project.export_v2(params=export_params, filters=filters)\nexport_task.wait_till_done()\n\nif export_task.errors:\n  print(export_task.errors)\n\nexport_json = export_task.result\nprint(\"results: \", export_json)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou can check out SDK examples of exporting from datasets, slices, and model runs in this \u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"receiving-updates-via-webhooks\"\u003eReceiving updates via Webhooks\u003cbr\u003e\u003c/h2\u003e\u003cp\u003eFor teams that would like to get near real-time updates for each change on a data row, we recommend webhooks as a better option. Export v2 format can now be used for webhooks to receive the following events from project:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLABEL_CREATED\u003c/li\u003e\u003cli\u003eLABEL_UPDATED\u003c/li\u003e\u003cli\u003eLABEL_DELETED\u003c/li\u003e\u003cli\u003eREVIEW_CREATED\u003c/li\u003e\u003cli\u003eREVIEW_UPDATED\u003c/li\u003e\u003cli\u003eREVIEW_DELETED\u003c/li\u003e\u003cli\u003eWORKFLOW\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can configure a webhook that returns Export v2 in Project whenever an event is triggered. See more details in this \u003ca href=\"https://docs.labelbox.com/reference/webhook?ref=labelbox-guides.ghost.io\"\u003eWebhook Guide\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eFor example, you can use ngrok to expose a local port.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003engrok http 3001\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis will generate an address that looks like ` \u003ca href=\"https://887d-2601-645-8000-3a90-9cb4-7d1b-d9b4-6714.ngrok.io/?ref=labelbox-guides.ghost.io\"\u003ehttps://887d-2601-645-8000-3a90-9cb4-7d1b-d9b4-6714.ngrok.io\u003c/a\u003e` and it will forward all requests to your localhost:3001. \u003c/p\u003e\u003cp\u003eIn your terminal, create a python file that contains the following code to receive webhook payload. Make sure to change your secret.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom flask import Flask, request\nimport hmac, hashlib\nimport json\nimport threading\nfrom werkzeug.serving import run_simple\n\n\n# This can be any secret that matches your webhook config (we will set later)\nsecret = b\"CHANGE-ME\"\n\n\n# Example for server-side code to receive webhook events\napp = Flask(__name__)\n\n\n@app.route(\"/webhook-endpoint\", methods=[\"POST\"])\ndef print_webhook_info():\n   payload = request.data\n   computed_signature = hmac.new(secret, msg=payload,\n                                 digestmod=hashlib.sha1).hexdigest()\n   if request.headers[\"X-Hub-Signature\"] != \"sha1=\" + computed_signature:\n       print(\n           \"Error: computed_signature does not match signature provided in the headers\"\n       )\n       return \"Error\", 500, 200\n\n\n   print(\"=========== New Webhook Delivery ============\")\n   print(\"Delivery ID: %s\" % request.headers[\"X-Labelbox-Id\"])\n   print(\"Event: %s\" % request.headers[\"X-Labelbox-Event\"])\n   print(\"Payload: %s\" %\n         json.dumps(json.loads(payload.decode(\"utf8\")), indent=4))\n   return \"Success\"\n\n\n\n\nthread = threading.Thread(target=lambda: run_simple(\"0.0.0.0\", 3001, app))\nthread.start()\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen run this script to start receiving requests from the ngrok address:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003engrok http 3001\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow, you can configure a webhook in Quantumworks Lab's Project setting. \u003c/p\u003e\u003cul\u003e\u003cli\u003eClick \u003cem\u003eSet up webhook\u003c/em\u003e\u003c/li\u003e\u003cli\u003eChoose V2 as the version of the webhook\u003c/li\u003e\u003cli\u003ePaste in the ngrok address plus /webhook-endpoint. You will need to write the secret to match the secret you specified in your app.py script \u003c/li\u003e\u003cli\u003eFinally, select the topics you want to subscribe to\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-122ef535-e118-494c-b065-4981a49f313d.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"876\" height=\"1110\"\u003e\u003c/figure\u003e\u003cp\u003eNow that you've created a webhook, everytime there is a new event triggered (such as updating a label), you will receieve the payload at / webhook-endpoint. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/05/data-src-image-bfd67059-8a6a-4e42-a232-ab9621d473a1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"927\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003eThe improved and datarow-centric format of Export v2 empowers you to export with more granularity by including or excluding variables based on your project’s unique needs. Offering a more seamless user experience, the new export format more consistently mirrors our import format and aligns with annotation schema available in the platform.\u003c/p\u003e\u003cp\u003eAs you migrate from Export v1 to Export v2 workflows, please refer to our \u003ca href=\"https://docs.labelbox.com/reference/export-v2-glossary?ref=labelbox-guides.ghost.io\"\u003edocumentation \u003c/a\u003efor more detailed instructions on how to export your data through the UI or through the Python SDK.\u003c/p\u003e","comment_id":"645007f474911d003db0cb11","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/05/Frame-2299--3-.png","featured":false,"visibility":"public","created_at":"2023-05-01T18:41:56.000+00:00","updated_at":"2023-10-27T17:01:36.000+00:00","published_at":"2023-05-01T23:08:32.000+00:00","custom_excerpt":"With Export V2, you can export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-export-your-data-with-more-granular-control","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-export-your-data-with-more-granular-control/","excerpt":"With Export V2, you can export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. ","reading_time":7,"access":true,"comments":false,"og_image":null,"og_title":"Introducing Export V2: How to export data with more granular control","og_description":"Learn how to export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. ","twitter_image":null,"twitter_title":"Introducing Export V2: How to export data with more granular control","twitter_description":"Learn how to export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. ","meta_title":"Introducing Export V2: How to export data with more granular control","meta_description":"Learn how to export your data with more granular control. Filter for specific data rows to export, configure the export to include or exclude information, and more. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6446b44974911d003db0cad0","uuid":"1b76d2d0-679c-4198-a77b-ab4acdb50498","title":"How to create and label text layers from PDF documents for AI","slug":"how-to-create-and-label-text-layers-from-pdf-documents-for-ai","html":"\u003cp\u003ePDF documents are one of the toughest data types to handle when it comes to building AI models. While most other data types usually contain information in one format, PDFs can comprise multiple types of data within them, such as photos, graphs and charts, text formatted in paragraphs, outlines, lists, links, and more. To add to the complexity, PDF documents can be hundreds of pages long, so a single data point can amount to hundreds of hours of work to label for training a model.\u003c/p\u003e\u003cp\u003eThat's why many AI teams working with this data type use PDF layers to separate and organize different information formats within a document, making them easier to view, edit, and enrich with labels. As most PDF documents contain primarily text, creating a text layer is often essential to the process of building AI on PDF data.\u003c/p\u003e\u003cp\u003eIn this post, we’ll take a look at the different types of PDF layers and discuss various methods for creating PDF text layers, such as using PDF creation software, OCR software, or Python scripts with libraries like reportlab or PyPDF2. Additionally, we will highlight the usefulness of PDF text layers in applications such as Quantumworks Lab, where they can be imported for annotation and used to build robust AI models with contextual information from PDF documents.\u003c/p\u003e\u003ch3 id=\"what-are-pdf-layers\"\u003eWhat are PDF layers?\u003c/h3\u003e\u003cp\u003eA PDF layer comprises a specific type of data contained within a PDF document. Separating a document into layers enables AI builders to process the different types of information within it. \u003c/p\u003e\u003cp\u003eTypes of layers include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eText layer: Refers to the selectable and searchable text contained within a PDF document. The text layer is an essential component of the PDF format, as it enables users to interact with the document's content by copying, searching, and editing text.\u003c/li\u003e\u003cli\u003eImage and graphics layers: Contain images, illustrations, and graphical elements.\u003c/li\u003e\u003cli\u003eBackground layer: Contains the background color or images, usually placed behind the main content.\u003c/li\u003e\u003cli\u003eAnnotations and comments layer: Contains any added annotations, comments, or interactive elements like form fields.\u003c/li\u003e\u003cli\u003eWatermarks or overlays: Contains watermarks, stamps, or other overlay elements that appear over the main content.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"how-to-create-a-pdf-text-layer\"\u003eHow to create a PDF text layer\u003c/h3\u003e\u003cp\u003ePDF text layers can be created in several ways, depending on the source of the document and the tools available. Here are a few methods for creating a PDF text layer:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eUsing PDF creation software.\u003c/strong\u003e There are dedicated PDF creation tools, such as Adobe Acrobat, that allow you to create a PDF document from various file formats, including images, Word files, and other document types. These tools often have built-in OCR (Optical Character Recognition) capabilities that can detect and create text layers from scanned or image-based documents.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOCR software.\u003c/strong\u003e If you have a scanned or image-based document without a text layer, you can use OCR software to recognize the text and create a new PDF with a text layer. There are many OCR tools available, such as Adobe Acrobat, AWS Textract, and Tesseract.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePython script.\u003c/strong\u003e You can separate a document into layers using libraries like reportlab or PyPDF2.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"why-is-a-pdf-text-layer-useful\"\u003eWhy is a PDF text layer useful?\u003c/h3\u003e\u003cp\u003ePDF text layers enable AI teams to annotate text data on its own, and then later add context to that information by combining it with the other data types within the document. Quantumworks Lab enables you to import your PDF and text layer so that you can annotate on the text layer and then export the labeled text with photos, graphics, and other information from the document to build robust AI models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to convert your PDF into Quantumworks Lab PDF text layer format\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eYou can convert your PDF into the Quantumworks Lab PDF text layer format using the CLI shown \u003ca href=\"https://github.com/Quantumworks Lab/PDF-OCR-Transform-CLI?ref=labelbox-guides.ghost.io\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eFirst, you will need to have AWS CLI installed globally and configured for your AWS user with permissions to S3 and Textract. The CLI will upload your PDF to S3 and save the Quantumworks Lab formatted PDF text layer JSON file in the specified folder.\u003c/p\u003e\u003cp\u003eThere is a configuration file named config.json at the root level of the directory that must be updated before first running the CLI.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-CLI\"\u003e{\n  // The name of the bucket in your cloud provider that pdfs will be uploaded to\n  \"bucketName\": \"\u0026lt;name_of_s3_bucket\u0026gt;\"\n}\nconvert - Run OCR on all pdfs contained in the input folder and convert the result into Quantumworks Lab's text layer JSON.\n\nconvert --inputFolder \u0026lt;input_folder_containing_pdfs\u0026gt; --format \u0026lt;aws-textract\u0026gt; --outputFolder \u0026lt;output_folder\u0026gt; --concurrency 10\n\n--inputFolder The input folder containing the pdfs\n\n--format The OCR format to use (aws-textract, google-cloud-vision)\n\n--outputFolder The output folder to place the generated text layer json files\n\n--concurrency How many pdfs to process at the same time. CAUTION: Setting this value too high can result in rate limits being reached.\n\nExample (Mac)\n\n./textlayer-macos convert --inputFolder input --format aws-textract --outputFolder output --concurrency 10\nvalidate - Validates the provided text layer json\n\nvalidate --textLayerFilepath \u0026lt;text_layer_filepath\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOnce you have the PDF text layer in the Quantumworks Lab format, you can upload it and the PDF file to Quantumworks Lab as shown \u003ca href=\"https://docs.labelbox.com/reference/documents?ref=labelbox-guides.ghost.io#import-format\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eYou can also convert GCP OCR JSON or Adobe OCR JSON formats into Quantumworks Lab's format as shown \u003ca href=\"https://github.com/Quantumworks Lab/PDF-OCR-Transform-CLI?ref=labelbox-guides.ghost.io#ad-hoc-transform-scripts\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"view-and-label-pdf-text-layers\"\u003e\u003cbr\u003eView and label PDF text layers\u003c/h3\u003e\u003cp\u003eSee how you can visualize and annotate PDF text layers for your AI projects using Quantumworks Lab by selecting \"Show text layer\" (illustrated below).\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card kg-card-hascaption\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/80w2yj6yqi\" title=\"text layer Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eShow text layers within Quantumworks Lab\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"627\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/10/data-src-image-1a2e2f9f-835a-47ae-8cac-11b981900953.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample view of a text layer within the Quantumworks Lab UI\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe hope what we covered in this post helps inspire you to better work with the different ways to create PDF text layers, from using software to OCR or Python scripts. These text layers can come in handy when building AI models with PDF context. Feel free to give it a try for yourself and we'd love to hear your feedback and what else you'd like to see when it comes to creating and labeling text layers for your ML use cases.\u003c/p\u003e\u003cp\u003eLearn more about:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHow to \u003ca href=\"https://docs.labelbox.com/reference/documents?ref=labelbox-guides.ghost.io#import-format\"\u003eupload\u003c/a\u003e text layers to Quantumworks Lab\u003c/li\u003e\u003cli\u003eHow to natively \u003ca href=\"https://labelbox.com/guides/how-to-natively-annotate-a-pdf-document/?ref=labelbox-guides.ghost.io\"\u003eannotate\u003c/a\u003e a PDF document in Quantumworks Lab\u003c/li\u003e\u003cli\u003eWhat types of \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#import-document-data\"\u003eDocument\u003c/a\u003e annotation tasks can be used\u003c/li\u003e\u003c/ul\u003e","comment_id":"6446b44974911d003db0cad0","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/04/Screen-Shot-2023-04-24-at-9.16.14-AM.png","featured":false,"visibility":"public","created_at":"2023-04-24T16:54:33.000+00:00","updated_at":"2023-10-26T18:10:32.000+00:00","published_at":"2023-04-20T16:54:00.000+00:00","custom_excerpt":"Learn about the different types of PDF layers and how to import annotations to build robust AI models with contextual information from PDF documents.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa608375d13000123d7fa","name":"Industry: Finance \u0026 insurance","slug":"industry-finance-insurance","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-finance-insurance/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-create-and-label-text-layers-from-pdf-documents-for-ai/","excerpt":"Learn about the different types of PDF layers and how to import annotations to build robust AI models with contextual information from PDF documents.","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"64134c4235aad5003db6f2b2","uuid":"72160526-3ea8-42ee-ab34-75b7b0fa3878","title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","slug":"how-to-fine-tune-large-language-models-with-labelbox","html":"\u003ch2 id=\"what-are-large-language-models-llms\"\u003eWhat are large language models (LLMs)?\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/usecases/large-language-models/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLarge language models\u003c/a\u003e leverage deep learning techniques to recognize, classify, analyze, generate and even predict text. Critical in \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003enatural language processing (NLP)\u003c/a\u003e applications like AI voice assistants, chatbots, translation, and sentiment analysis — large language models rely upon large volumes of data to consistently comprehend, capture and convey the nuances of human language.\u003c/p\u003e\u003ch2 id=\"why-should-you-consider-using-one\"\u003eWhy should you consider using one?\u003c/h2\u003e\u003cp\u003eRecent advancements and accessibility of large language models can serve as a powerful starting point for your machine learning team. Although you’ll still need to retrain these base models on data that is contextually-relevant to your use case, leveraging a foundational model saves significant times and costs. Large language models significantly improve with Reinforcement Learning from Human Preferences (RLHP). This guide provides a framework for using Quantumworks Lab to fine-tune OpenAI 's popular GPT-3 large language model for your use case. \u003c/p\u003e\u003ch2 id=\"getting-started\"\u003e\u003cstrong\u003eGetting Started\u003c/strong\u003e \u003c/h2\u003e\u003cp\u003eThis guide and Colab notebook will walk you through how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect your own relevant classification ontology to use with foundational large language models like GPT-3.\u003c/li\u003e\u003cli\u003eCreate a Project in Quantumworks Lab Annotate to generate labeled training data.\u003c/li\u003e\u003cli\u003eLeverage iterative model runs to rapidly tune OpenAI large language models.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab Model to diagnose performance, find high impact data, get the data labeled, and create another model run for the next iteration of fine tuning.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/deya57pbun\" title=\"How to fine-tune large language models (LLMs) with Quantumworks Lab Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"1-import-colab-notebook-packages\"\u003e1. Import Colab notebook packages\u003c/h3\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1p3nwxBHUpnUMP4B3mWAACwL-g4uOpjzw?ref=labelbox-guides.ghost.io\" class=\"kg-btn kg-btn-accent\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003eTo help you get started, for this workflow, we’ve created a \u003ca href=\"https://colab.research.google.com/drive/1p3nwxBHUpnUMP4B3mWAACwL-g4uOpjzw?ref=labelbox-guides.ghost.io\"\u003eColab notebook\u003c/a\u003e which has installed Open AI and Quantumworks Lab packages within the same python notebook. Import all of the packages and use the corresponding Open AI and Quantumworks Lab API keys to connect to your instances. \u003c/p\u003e\u003cp\u003e\u003ccode\u003e!pip install Quantumworks Lab[data] --upgrade -q !pip install openai -q \u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"2-create-a-project-based-on-your-desired-data-ontology\"\u003e\u003cstrong\u003e2\u003c/strong\u003e.\u003cstrong\u003e Create a Project based on your desired data ontology\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNext, \u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003ecreate a Project\u003c/a\u003e in the platform that matches the defined ontology for the data you want to classify using Open AI’s GPT-3 model. Add this ontology to a Project. Our Colab notebook example focuses on classifying e-commerce assets into the following four categories that comprise 80% of all e-commerce assets sold:\u003c/p\u003e\u003cul\u003e\u003cli\u003e“Electronics”\u003c/li\u003e\u003cli\u003e“Household”\u003c/li\u003e\u003cli\u003e“Books”\u003c/li\u003e\u003cli\u003e“Clothing \u0026amp; Accessories” up  common e-commerce use case.  \u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e``import os \nimport openai\nfrom labelbox.schema.ontology import OntologyBuilder, Tool, Classification, Option\nfrom Quantumworks Lab import Client, LabelingFrontend, LabelImport, MALPredictionImport``\n\n``from labelbox.data.annotation_types import (\n    Label, ImageData, ObjectAnnotation, MaskData,\n    Rectangle, Point, Line, Mask, Polygon,\n    Radio, Checklist, Text,\n    ClassificationAnnotation, ClassificationAnswer\n    )``\n\n``from labelbox.data.serialization import NDJsonConverter\nimport pandas as pd\nimport shutil\nimport labelbox.data\nimport json\nimport uuid ``\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"3-generate-training-data\"\u003e3. Generate training data\u003c/h3\u003e\u003cp\u003eOnce you’ve determined the ontology and added it to your Quantumworks Lab Project, you can begin curating the training data that supports your requirements. This will ultimately play a critical role towards adapting GPT-3 to support your use case. Once you add your ontology to a project, you have multiple options for creating training data, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUsing Quantumworks Lab Annotate to label the data yourself\u003c/li\u003e\u003cli\u003eImporting existing annotations you have in your data lake\u003c/li\u003e\u003cli\u003eLeveraging our experts from \u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003edata labeling services\u003c/a\u003e for support rapidly curating training-quality data \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"4-push-the-batch-of-training-data-labels-to-model-runs\"\u003e4. Push the batch of training data labels to model runs\u003c/h3\u003e\u003cp\u003eOnce you've created the training data, select the appropriate, corresponding data rows and export them to \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox Model\u003c/a\u003e for model training. Note that you'll need to implement model training settings that connect your model training environment and cloud service provider (CSP) to Labelbox. Our Colab Notebook demo uses Google Cloud Platform (GCP) but this same workflow and Quantumworks Lab’s cloud-agnostic platform works well with any model training environment. \u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1148\" height=\"586\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.47.42-PM.png 1148w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eYou will iteratively run the last section of this notebook as you build momentum fine-tuning your model and improving performance.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"5-iteratively-improve-and-fine-tune-the-model\"\u003e5. Iteratively improve and fine-tune the model\u003c/h3\u003e\u003cp\u003eNext the Colab notebook features a list of prompts to iteratively train the Open AI GPT-3 model based on the annotations that fit your unique use case. As you review the model predictions in Quantumworks Lab Model, the platform will help you easily identify mis-predictions and target areas where the model consistently performs poorly. \u003c/p\u003e\u003cp\u003eIn Quantumworks Lab Catalog, you can leverage embeddings and similarity search features to find training data samples that exhibit similar characteristics to data where your model is consistently performing poorly – then queue that data for labeling so it can be incorporated during your next retraining iteration. You'll do this by iteratively \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003esubmitting a batch\u003c/a\u003e – featuring these newly-curated Data Rows – to the same Project you created earlier (in step two) for fine-tuning your LLM. Ultimately, this iterative loop of exposing the model to new prompts will allow you to continuously fine-tune the GPT-3 model to perform based on your own data priorities.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1656\" height=\"834\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-17-at-8.40.57-PM.png 1656w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eFind and prioritize data from Catalog most likely to have the highest impact on your next training iteration.\u0026nbsp;\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe notebook feeds the output results from Open AI back into the Quantumworks Lab Model tab. This iterative back and forth workflow empowers you to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLeverage Quantumworks Lab Model to measure model performance \u003c/li\u003e\u003cli\u003eEvaluate model output predictions and identify areas where the model performs poorly\u003c/li\u003e\u003cli\u003eLeverage Catalog to prioritize data from your Catalog that will have the most maximal impact towards addressing your edge cases\u003c/li\u003e\u003cli\u003eFine-tune the Open AI large language model by iteratively feeding it relevant data addressing your ontology, finding erroneous model predictions and fixing areas where the model performs poorly.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eGet started today by following the prompts in this \u003ca href=\"https://colab.research.google.com/drive/1p3nwxBHUpnUMP4B3mWAACwL-g4uOpjzw?ref=labelbox-guides.ghost.io#scrollTo=KSsXvqbaeLXs\"\u003eColab notebook\u003c/a\u003e. To learn more about Quantumworks Lab Model, check out the following guides below:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-train-evaluate-and-improve-your-ml-models/?ref=labelbox-guides.ghost.io\"\u003eHow to get started in Quantumworks Lab Model: Train, evaluate, and improve your ML models\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-run-model-assisted-labeling-with-active-learning-on-ner-data-with-a-hugging-face-model/?ref=labelbox-guides.ghost.io\"\u003eHow to run model-assisted labeling and active learning on NER data with a Hugging Face model\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"64134c4235aad5003db6f2b2","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Frame-2299--2-.png","featured":false,"visibility":"public","created_at":"2023-03-16T17:05:06.000+00:00","updated_at":"2024-10-02T22:30:29.000+00:00","published_at":"2023-03-22T23:00:29.000+00:00","custom_excerpt":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-fine-tune-large-language-models-with-Labelbox","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa506375d13000123d7e8","name":"Using LLMs","slug":"using-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-llms/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-fine-tune-large-language-models-with-labelbox/","excerpt":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","reading_time":4,"access":true,"comments":false,"og_image":null,"og_title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","og_description":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","twitter_image":null,"twitter_title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","twitter_description":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","meta_title":"How to fine-tune large language models (LLMs) with Quantumworks Lab","meta_description":"Learn how to iterate and rapidly fine-tune OpenAI large language models with Quantumworks Lab Model. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6418777a35aad5003db6f4d6","uuid":"7958bb69-5b16-4a54-b471-76f9db6354cc","title":"How to prepare unstructured data for AI and analytics in Databricks","slug":"how-to-use-the-labelbox-connector-to-prepare-unstructured-data-for-ai-and-analytics-in-databricks","html":"\u003cp\u003eLarge data lakes typically house a combination of structured and unstructured data. As the amount of unstructured data grows, you need a way to effectively prepare and analyze your unstructured datasets. \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://labelbox.com/blog/announcing-labelbox-on-databricks-partner-connect/?ref=labelbox-guides.ghost.io\"\u003eDatabricks and Quantumworks Lab partnership\u003c/a\u003e gives you an end-to-end environment for unstructured data workflows – a query engine built around Delta Lake, fast annotation tools, and a powerful machine learning compute environment. With the Quantumworks Lab Connector for Databricks and the LabelSpark API, you can easily integrate the two platforms and add structure to your data. \u003c/p\u003e\u003cp\u003eStart with unstructured data in your data lake, pass it to Quantumworks Lab for annotation, and load your annotations into Databricks for analysis.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.16.23-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1604\" height=\"980\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-10.16.23-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-02-at-10.16.23-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2024/02/Screenshot-2024-02-02-at-10.16.23-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.16.23-AM.png 1604w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox empowers you to quickly explore, organize, and annotate a variety of unstructured data from your data lake. You can apply insights to modalities such as \u003ca href=\"https://labelbox.com/product/image?ref=labelbox-guides.ghost.io\"\u003eimages\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/video?ref=labelbox-guides.ghost.io\"\u003evideo\u003c/a\u003e, \u003ca href=\"https://labelbox.com/product/text?ref=labelbox-guides.ghost.io\"\u003etext\u003c/a\u003e, and \u003ca href=\"https://docs.labelbox.com/docs/tiled-imagery-editor?ref=labelbox-guides.ghost.io\"\u003egeospatial tiled imagery\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"what-will-you-learn-in-this-guide\"\u003eWhat will you learn in this guide?\u003c/h2\u003e\u003cp\u003eIn this guide, you’ll learn how you can access the Quantumworks Lab Connector for Databricks to prepare unstructured data for AI and analytics. This includes:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRequirements to get started\u003c/li\u003e\u003cli\u003eHow to create data rows with the Quantumworks Lab Connector for Databricks\u003c/li\u003e\u003cli\u003eHow to create data rows with metadata, annotations, and attachments \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCheck out \u003ca href=\"https://www.databricks.com/session_na21/productionizing-unstructured-data-for-ai-and-analytics?ref=labelbox-guides.ghost.io\"\u003ethis end-to-end video demo\u003c/a\u003e of this workflow.\u003c/p\u003e\u003ch3 id=\"requirements-to-get-started\"\u003eRequirements to get started:\u003c/h3\u003e\u003cp\u003eYou can find the necessary requirements for all data rows \u003ca href=\"https://github.com/Quantumworks Lab/labelspark/blob/master/notebooks/intro.ipynb?ref=labelbox-guides.ghost.io\"\u003ein this notebook\u003c/a\u003e. \u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://github.com/Quantumworks Lab/labelspark/blob/master/notebooks/intro.ipynb?ref=labelbox-guides.ghost.io\" class=\"kg-btn kg-btn-accent\"\u003eView notebook\u003c/a\u003e\u003c/div\u003e\u003ch3 id=\"requirements\"\u003eRequirements:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eA \u003ccode\u003erow_data\u003c/code\u003e column — This column must be URLs that point to the asset to-be-uploaded\u003c/li\u003e\u003cli\u003eEither a \u003ccode\u003edataset_id\u003c/code\u003e column or an input argument for \u003ccode\u003edataset_id\u003c/code\u003e\u003c/li\u003e\u003cli\u003eIf uploading to multiple datasets, provide a \u003ccode\u003edataset_id\u003c/code\u003e column\u003c/li\u003e\u003cli\u003eIf uploading to one dataset, provide a \u003ccode\u003edataset_id\u003c/code\u003e input argument (\u003cem\u003eThis can still be a column if it's already in your dataframe) \u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"recommended\"\u003eRecommended:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eA \u003ccode\u003eglobal_key\u003c/code\u003e column\u003c/li\u003e\u003cli\u003eThis column contains unique identifiers for your data rows\u003c/li\u003e\u003cli\u003eIf none is provided, will default to your \u003ccode\u003erow_data\u003c/code\u003e column\u003c/li\u003e\u003cli\u003eAn \u003ccode\u003eexternal_id\u003c/code\u003e column\u003c/li\u003e\u003cli\u003eThis column contains non-unique identifiers for your data rows\u003c/li\u003e\u003cli\u003eIf none is provided, will default to your \u003ccode\u003eglobal_key\u003c/code\u003e column\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"optional\"\u003eOptional:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eA \u003ccode\u003eproject_id\u003c/code\u003e column or an input argument for \u003ccode\u003eproject_id\u003c/code\u003e\u003c/li\u003e\u003cli\u003eIf batching to multiple projects, provide a \u003ccode\u003eproject_id\u003c/code\u003e column\u003c/li\u003e\u003cli\u003eIf batching to one project, provide a \u003ccode\u003eproject_id\u003c/code\u003e input argument (\u003cem\u003eThis can still be a column if it's already in your dataframe)\u003c/em\u003e\u003c/li\u003e\u003cli\u003eA row_data column - this column must be URLs that point to the asset to-be-uploaded\u003c/li\u003e\u003cli\u003eA dataset_id column or an input argument for dataset_id\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIf uploading to multiple datasets, provide a dataset_id column\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIf uploading to one dataset, provide a dataset_id input argument\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"creating-data-rows-with-the-labelbox-connector-for-databricks\"\u003eCreating data rows with the Quantumworks Lab Connector for Databricks\u003c/h2\u003e\u003cp\u003eOnce you have set up the above requirements, you can create data rows with the Quantumworks Lab Connector for Databricks. \u003c/p\u003e\u003cp\u003eFollow the steps outlined \u003ca href=\"https://github.com/Quantumworks Lab/labelspark/blob/master/notebooks/intro.ipynb?ref=labelbox-guides.ghost.io\"\u003ein this notebook\u003c/a\u003e to create data rows in Labelbox. \u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://github.com/Quantumworks Lab/labelspark/blob/master/notebooks/intro.ipynb?ref=labelbox-guides.ghost.io\" class=\"kg-btn kg-btn-accent\"\u003eView notebook\u003c/a\u003e\u003c/div\u003e\u003ch2 id=\"an-end-to-end-demo-creating-data-rows-with-metadata-annotations-and-attachments-with-the-labelbox-connector-for-databricks\"\u003eAn end-to-end demo: Creating data rows with metadata, annotations, and attachments with the Quantumworks Lab Connector for Databricks\u003c/h2\u003e\u003cp\u003eIn addition to being able to create data rows with this connector, you can create data rows with attributes such as metadata, annotations, and attachments. \u003c/p\u003e\u003cp\u003eFollow the steps \u003ca href=\"https://github.com/Quantumworks Lab/labelspark/blob/master/notebooks/full-demo.ipynb?ref=labelbox-guides.ghost.io\"\u003ein this notebook\u003c/a\u003e to learn more about the requirements for creating metadata, annotations, and attachments and the steps needed to create data rows with all three attributes in Labelbox. \u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://github.com/Quantumworks Lab/labelspark/blob/master/notebooks/full-demo.ipynb?ref=labelbox-guides.ghost.io\" class=\"kg-btn kg-btn-accent\"\u003eView notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003eYou can also refer to the below links for specific instructions on how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://github.com/Quantumworks Lab/labelspark/blob/master/notebooks/metadata.ipynb?ref=labelbox-guides.ghost.io\"\u003eCreate a data row with metadata\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/Quantumworks Lab/labelspark/blob/master/notebooks/annotations.ipynb?ref=labelbox-guides.ghost.io\"\u003eCreate a data row with annotations\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/Quantumworks Lab/labelspark/blob/master/notebooks/attachments.ipynb?ref=labelbox-guides.ghost.io\"\u003eCreate a data row with attachments\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003eIf you have questions or encounter issues with the Quantumworks Lab Connector for Databricks, please reach out to \u003ca href=\"https://labelbox.atlassian.net/servicedesk/customer/portal/2?ref=labelbox-guides.ghost.io\"\u003eLabelbox Support\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eIn the meantime, you can learn more about \u003ca href=\"https://labelbox.com/customers/burberry-customer-story/?ref=labelbox-guides.ghost.io\"\u003ehow Burberry is harnessing the power of Quantumworks Lab and Databricks\u003c/a\u003e to curate their strategic marketing assets. \u003c/p\u003e","comment_id":"6418777a35aad5003db6f4d6","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-3013--3-.png","featured":false,"visibility":"public","created_at":"2023-03-20T15:10:50.000+00:00","updated_at":"2024-02-02T18:16:59.000+00:00","published_at":"2023-03-20T17:17:50.000+00:00","custom_excerpt":"Learn how to use the Quantumworks Lab Connector to quickly explore, organize, and annotate a variety of unstructured data from your Data Lake. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/prepare-unstructured-data-for-AI-and-analytics-in-Databricks","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-use-the-labelbox-connector-to-prepare-unstructured-data-for-ai-and-analytics-in-databricks/","excerpt":"Learn how to use the Quantumworks Lab Connector to quickly explore, organize, and annotate a variety of unstructured data from your Data Lake. ","reading_time":3,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-3013--3--2.png","og_title":"How to prepare unstructured data for AI and analytics in Databricks","og_description":"Learn how to use the Quantumworks Lab Connector to quickly explore, organize, and annotate a variety of unstructured data from your Data Lake. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-3013--3--1.png","twitter_title":"How to prepare unstructured data for AI and analytics in Databricks","twitter_description":"Learn how to use the Quantumworks Lab Connector to quickly explore, organize, and annotate a variety of unstructured data from your Data Lake. ","meta_title":"How to prepare unstructured data for AI and analytics in Databricks","meta_description":"Learn how to use the Quantumworks Lab Connector to quickly explore, organize, and annotate a variety of unstructured data from your Data Lake. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6406883577b361003d3619d5","uuid":"4ab62fd8-b9da-474e-a833-f24758c230db","title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","slug":"using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data","html":"\u003cp\u003eOne of the biggest challenges that ML teams face is how difficult it is to select the right data to improve their ML models. From working with hundreds of teams, we’ve seen that ML teams possess a vast amount of unlabeled data, but lack a structured process for effectively finding and prioritizing \u003cem\u003especific data\u003c/em\u003e that can dramatically improve model performance. \u003c/p\u003e\u003cp\u003eThis manifests itself in the form of trying to find specific examples of an edge case where your model is struggling, or in the case of wanting to surface all occurrences of a rare data point that needs to be labeled in priority. In these cases, what is the best way for your team to efficiently surface this high-impact data?\u003c/p\u003e\u003ch2 id=\"what-will-you-learn-in-this-guide\"\u003eWhat will you learn in this guide? \u003c/h2\u003e\u003cp\u003eIn this guide, we'll show you how you can use a foundation model, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data. This technique will help your team quickly enrich your data with the latest advances in off-the-shelf models and embeddings.\u003c/p\u003e\u003cp\u003eBy the end of this guide, you’ll know how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGenerate custom embeddings with Hugging Face using a single line of code and upload your data to Quantumworks Lab in order to better explore and visualize your data.\u003c/li\u003e\u003cli\u003eBetter understand the distribution of your data and quickly find similar high-impact data.\u003c/li\u003e\u003cli\u003eUse Quantumworks Lab as a native similarity search engine, where you can leverage both off-the-shelf embeddings computed by Quantumworks Lab (for image, text, and documents) and upload your own custom embeddings to quickly find all instances of similar data.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"what-are-embeddings\"\u003eWhat are embeddings? \u003c/h2\u003e\u003cp\u003eIn machine learning, an embedding, or feature vector, is an array of numbers assigned to an asset by a neural net. Assets that have similar content will also have similar embeddings. \u003c/p\u003e\u003cp\u003eFor example, in a dataset comprising images of apples and oranges, an appropriate embedding used for image similarity will show that all the vectors corresponding to apples have similar values. The vectors for all images of oranges will also be grouped together. \u003c/p\u003e\u003cp\u003eIn other words, the neural network acts as a feature extractor: it extracts an embedding vector that contains rich information about the data.\u003c/p\u003e\u003ch3 id=\"off-the-shelf-embeddings-vs-custom-embeddings\"\u003eOff-the-shelf embeddings vs custom embeddings\u003c/h3\u003e\u003cp\u003eWhen you connect your data to Quantumworks Lab, we automatically compute \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io#supported-embeddings\"\u003eoff-the-shelf\u003c/a\u003e\u003cstrong\u003e \u003c/strong\u003eembeddings on your data – this includes CLIP embeddings for images and PDFs and All-mpnet-base-v2 embeddings for text. These off-the-shelf embeddings are a useful starting point for you to explore your data and conduct similarity searches. \u003c/p\u003e\u003cp\u003eHowever, in some cases where your data has unique attributes, you may want to use your own \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io#how-to-upload-custom-embeddings\"\u003ecustom embeddings\u003c/a\u003e to power your data selection. Quantumworks Lab allows you to upload up to 100 custom embeddings in addition to the off-the-shelf embeddings that are automatically computed. \u003c/p\u003e\u003cp\u003eYou can easily compare the results of these custom and provided off-the-shelf embeddings in Quantumworks Lab to discover the best embeddings to use for data selection.\u003c/p\u003e\u003ch2 id=\"how-to-upload-custom-embeddings\"\u003eHow to upload custom embeddings\u003c/h2\u003e\u003cp\u003eFirst, connect your data with Labelbox. You can integrate your cloud storage bucket with Quantumworks Lab via IAM delegated access:\u003c/p\u003e\u003cp\u003e\u003cem\u003eHow to set up a delegated access integration with Quantumworks Lab\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-amazons3-and-labelbox/?ref=labelbox-guides.ghost.io\"\u003eAmazon S3\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-gcp-storage-labelbox/?ref=labelbox-guides.ghost.io\"\u003eGCP Storage\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-azure-and-labelbox/?ref=labelbox-guides.ghost.io\"\u003eMicrosoft Azure Blob Storage\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce you’ve successfully uploaded your data, Quantumworks Lab will automatically compute off-the-shelf embeddings on your data.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1013\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-06-at-7.57.54-PM.png 2000w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can then compute and upload custom embeddings from Hugging Face on your data:\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/huggingface/huggingface.ipynb?ref=labelbox-guides.ghost.io\" class=\"kg-btn kg-btn-accent\"\u003eGoogle Colab notebook\u003c/a\u003e\u003c/div\u003e\u003cp\u003e\u003cem\u003eFollow along in this \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/master/examples/integrations/huggingface/huggingface.ipynb?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eColab notebook \u003c/em\u003e\u003c/a\u003e\u003cem\u003ewith examples shown using ResNet-50 embeddings from Hugging Face.\u003c/em\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003eImport Quantumworks Lab into your notebook\u003c/li\u003e\u003c/ol\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# for Quantumworks Lab\n!pip3 install -q Quantumworks Lab[data]\nimport Quantumworks Lab as lb\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e2. Import the \u003ca href=\"https://github.com/Quantumworks Lab/advlib/tree/main/pylib/advlib?ref=labelbox-guides.ghost.io\"\u003eADVLib\u003c/a\u003e. This is a library built by Quantumworks Lab for you to upload custom embeddings.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# for custom embeddings in Quantumworks Lab\n!pip3 install -q 'git+https://github.com/Quantumworks Lab/advlib.git'\n#ndjson\n!pip3 install -q ndjson\nimport ndjson\nimport time\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e3. Select the data rows (images or text) in Quantumworks Lab on which you want to add custom embeddings.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# get images from a Quantumworks Lab dataset\ndataset = client.get_dataset(\"clemr01l42uil07y36qkq7ygn\")\ndrs = list(dataset.export_data_rows(timeout_seconds=9999))\ndata_row_ids = [dr.uid for dr in drs]\ndata_row_urls = [dr.row_data for dr in drs]\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e4. Use Hugging Face to generate your custom embeddings by loading a specific neural network (e.g. Resnet50).\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# import HuggingFace\n!pip3 install -q transformers\n!pip3 install -q timm\n\n# load a neural network from HuggingFace \nimport transformers\ntransformers.logging.set_verbosity(50)\nimport torch\nimport torch.nn.functional as F\nimport PIL, requests\nfrom tqdm import tqdm\n\n# get ResNet-50\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = transformers.ResNetModel.from_pretrained(\"microsoft/resnet-50\")\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e5. Generate custom embeddings by iterating over your image or text data. \u003c/p\u003e\u003cp\u003e\u003cem\u003eNote: \u003c/em\u003eThis should take approximately ~2 minutes for 512 images. For the similarity search function to work in Quantumworks Lab, you must upload at least 1,000 embeddings. \u003c/p\u003e\u003cul\u003e\u003cli\u003eRetrieve your images/text and run model inference \u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# process images\nimg_hf = image_processor(imgs, return_tensors=\"pt\")\n\n# generate resnet embeddings, thanks to inference\nwith torch.no_grad():\n\tlast_layer = model(**img_hf, output_hidden_states=True).last_hidden_state\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\u003cli\u003eRemember to do global pooling on the last layer of your embedding to reduce dimensionality\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eresnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\nresnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e6. Create the payload to upload custom embeddings to Quantumworks Lab in the form of an NDJSON file.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# create the payload\npayload = []\nfor (dr_id,resnet_embedding) in zip(dr_ids, resnet_embeddings):\n\tpayload.append({\"id\": dr_id, \"vector\": resnet_embedding})\n\n# write to NDJson file\nwith open('payload.ndjson', 'w') as f:\n\tndjson.dump(payload, f)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e7. Pick an existing custom embedding or create a custom embedding.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# max pool to reduce dimensionality\nresnet_embeddings = F.adaptive_avg_pool2d(last_layer, (1, 1))\nresnet_embeddings = torch.flatten(resnet_embeddings, start_dim=1, end_dim=3)\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e8. Upload your payload of custom embeddings into Labelbox.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e!advtool embeddings import \u0026lt;EMB ID\u0026gt; ./payload.ndjson\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e9. Use Quantumworks Lab Catalog UI to start conducting similarity searches.\u003c/p\u003e\u003ch2 id=\"how-to-quickly-find-instances-of-similar-data\"\u003eHow to quickly find instances of similar data\u003c/h2\u003e\u003cp\u003eOnce you have uploaded your custom embeddings to Quantumworks Lab, you can focus on curating data in Catalog that will dramatically improve your model’s performance.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eIdentify an edge case or rare example image/text you want to use to find similar data.\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis can include examples of data on which your model might be struggling. For example, let’s say the model is incorrectly classifying images with sparse patches of grass as having been affected by a wildfire. \u003c/p\u003e\u003cp\u003eIn the example below, the model appears to struggle on recognizing images with ‘no wildfire'\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1530\" height=\"736\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.12-AM.png 1530w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e2. Surface all instances of similar data.\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-16-47--1--1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1775\" height=\"959\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/2023-03-06_17-16-47--1--1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/2023-03-06_17-16-47--1--1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/2023-03-06_17-16-47--1--1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-16-47--1--1.gif 1775w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can run \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003esimilarity searches\u003c/a\u003e to find all instances of similar data. A similarity search will automatically surface all similar data rows – you can select multiple data rows as anchors to continue to refine your similarity search. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Combine a similarity search with other search filters.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo filter the dataset even further, you can combine a similarity search with other \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003esearch filters\u003c/a\u003e. This includes filtering on metadata, media attribute, annotation, and more.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1516\" height=\"824\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2024/02/Screenshot-2024-02-02-at-10.28.28-AM.png 1516w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e4. Compare similarity search results.\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-23-53--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1772\" height=\"964\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/2023-03-06_17-23-53--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/2023-03-06_17-23-53--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/2023-03-06_17-23-53--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-23-53--1-.gif 1772w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can compare the results of the similarity search on different embeddings (across off-the-shelf and custom embeddings). This gives you an understanding of which embeddings are most effective towards providing your desired results. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e5. Add all instances of similar data to a labeling project or save it as a slice.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve found additional examples of similar data rows on which your model is struggling, you can queue them to your labeling project in priority or save the filters as a slice. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-29-38--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"954\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/2023-03-06_17-29-38--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/2023-03-06_17-29-38--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/2023-03-06_17-29-38--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/03/2023-03-06_17-29-38--1-.gif 1774w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003eBy saving your similarity search as a slice, any new incoming data that matches the search criteria will automatically show up in the slice. This enables automatic data curation.\u003c/p\u003e\u003cp\u003eLearn more about other key ML workflows that you can perform using similarity search \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003ein this guide\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eLeveraging embeddings as a powerful similarity search technique can help you find specific data points within an ocean of data. With a similarity search, you can easily query and curate specific data that will dramatically improve your model performance. If you’re interested in learning more, please check out the additional resources below.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAdditional resources:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-kickstart-and-scale-your-data-labeling-efforts/?ref=labelbox-guides.ghost.io\"\u003eHow to kickstart and scale your data labeling efforts\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003eHow to filter and sort your data\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003eHow to find similar data in one click\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"6406883577b361003d3619d5","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-3012--4-.png","featured":false,"visibility":"public","created_at":"2023-03-07T00:41:25.000+00:00","updated_at":"2024-02-02T18:28:44.000+00:00","published_at":"2023-03-08T19:31:53.000+00:00","custom_excerpt":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-imapctful-data","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa4fb375d13000123d7e6","name":"Use AI","slug":"use-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/use-ai/"},{"id":"653aa513375d13000123d7ea","name":"Using computer vision","slug":"using-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/using-computer-vision/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/using-labelbox-and-foundation-models-to-generate-custom-embeddings-and-curate-impactful-data/","excerpt":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","og_description":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","twitter_image":null,"twitter_title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","twitter_description":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","meta_title":"Using Quantumworks Lab and foundation models to generate custom embeddings and curate impactful data","meta_description":"In this guide, we'll show you how you can use foundation models, such as Hugging Face’s embedding extractors, combined with Quantumworks Lab’s search capabilities to select impactful data.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6375c52617f6c9003d7b0fd9","uuid":"5f8b37d4-a683-4515-a584-0116c30aa131","title":"An introduction to model metrics","slug":"intro-to-model-metrics","html":"\u003cp\u003eModel metrics help you evaluate the performance of a model and allows you to qualitatively compare different models. You can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. \u003c/p\u003e\u003ch3 id=\"why-does-model-accuracy-not-give-a-complete-picture-of-the-models-performance\"\u003eWhy does model accuracy not give a complete picture of the model's performance?\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1600\" height=\"392\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.23.08-PM.png 1600w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAccuracy tells us the model's overall performance, but this metric doesn't provide all the information needed to accurately assess a model's performance. For a more holistic picture, we'll need to consider other metrics, based on the specific context that the model is used in.\u003c/p\u003e\u003cp\u003eGenerally, accuracy tends to be high in situations where a class has a very low probability of occurring, so a model can achieve high accuracy by simply predicting the most common class. For instance, the probability of finding cancer in computed topography scans or of finding swimming pools from satellite images of homes is low, so the model's accuracy can be high even if the model's ability to detect true positives is very poor.\u003c/p\u003e\u003ch3 id=\"precision\"\u003ePrecision\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1522\" height=\"702\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.36.39-PM.png 1522w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003ePrecision metrics focus on consistency and agreement among labelers, which are primarily derived from Quantumworks Lab's built-in consensus capability. Quantumworks Lab uses and tests the effectiveness of over 15 similar metrics for a wide range of supported annotations.\u003c/p\u003e\u003cp\u003ePrecision is a valuable metric when the negative cost of a false positive is high. For example, in spam detection models, a false positive would cause a vital email to be hidden and marked as spam when in fact, it is non-spam. A false positive in this case would negatively impact the user experience for seeing essential and urgent emails on time.\u003c/p\u003e\u003cp\u003ePopular precision metrics include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eKrippendorff's Alpha is a popular metric used to assess the agreement among raters because it works well for two or more raters, can handle missing data, and supports nominal, ordinal, and ranking data types.\u003c/li\u003e\u003cli\u003eStandard deviation measures the dispersion of a set of ratings from their mean (average) value. In the context of AI data quality, it quantifies how much variation or spread exists in the ratings given by different AI trainers for the same item or task.\u003c/li\u003e\u003cli\u003ePercent agreement is a straightforward measure of inter-rater reliability that calculates the proportion of times different raters agree in their judgments. This is particularly useful in classification tasks (enums).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"\"\u003e\u003c/h3\u003e\u003ch3 id=\"recall\"\u003eRecall\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1616\" height=\"774\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-08-at-4.38.13-PM.png 1616w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eRecall is a helpful metric to use when the cost of false negative is high, and you want to minimize it. For example, in fraud detection models, a false negative would cause a fraudulent transaction to be successfully processed when it should have been flagged as fraudulent. This would obviously have a negative impact on the finances of the user. Recall is also helpful for most medical condition predictions, where you would minimize false negatives to increase recall.\u003c/p\u003e\u003ch3 id=\"f-1\"\u003eF-1\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-03-at-12.47.58-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"938\" height=\"316\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-03-at-12.47.58-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-03-at-12.47.58-PM.png 938w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eIn some cases with imbalanced data problems, both precision and recall are important – we can consider the F1 score as an evaluation metric. An F1 score helps in the detection of skewed datasets and rare classes. Generally, it is best to have high precision and recall so that your F1 score is high. \u003c/p\u003e\u003cp\u003eTo demonstrate how accuracy only provides a partial assessment of a model's performance, we can compare the model metrics of two models below: \u003c/p\u003e\u003cfigure class=\"kg-card kg-gallery-card kg-width-wide\"\u003e\u003cdiv class=\"kg-gallery-container\"\u003e\u003cdiv class=\"kg-gallery-row\"\u003e\u003cdiv class=\"kg-gallery-image\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png\" width=\"1142\" height=\"808\" loading=\"lazy\" alt=\"\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.45-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/div\u003e\u003cdiv class=\"kg-gallery-image\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png\" width=\"1142\" height=\"792\" loading=\"lazy\" alt=\"\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/12/Screen-Shot-2022-12-01-at-6.20.59-PM.png 1142w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/figure\u003e\u003cp\u003eThe accuracy in model A is 73.65%, and model B is 83.69%. Based on accuracy alone, model B seems to perform better. However, if you compare their recall scores, then model A has a better recall of 87.38% vs model B's 82.97% recall. Taking this into account, model A performs better since the cost of a false negative is high. \u003c/p\u003e\u003ch3 id=\"what-do-model-metrics-look-like-in-labelbox\"\u003eWhat do model metrics look like in Quantumworks Lab? \u003c/h3\u003e\u003cp\u003eRather than having you manually compute and upload metrics, \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eLabelbox Model\u003c/a\u003e auto-computes metrics such as precision, recall, F-1, confusion matrix, etc. on individual predictions for you. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/2022-12-19_22-08-24--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1773\" height=\"884\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/2022-12-19_22-08-24--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/2022-12-19_22-08-24--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/2022-12-19_22-08-24--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/2022-12-19_22-08-24--1-.gif 1773w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLabelbox Model will auto-generate metrics on individual predictions\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eYou can simply upload your model predictions and ground truths to receive auto-generated metrics on model precision, recall, F1-score, TP/TN/FP/FN, and confusion matrix.\u003c/li\u003e\u003cli\u003eIf the auto-generated metrics aren’t sufficient for your use case, you can upload your own custom metrics as well.\u003c/li\u003e\u003cli\u003eVisualize, filter, sort, and drill into your metrics, confidence scores, predictions, and annotations. This allows you to easily surface mispredictions, mislabeled data, and allows you to quickly identify improvements to your training data.\u003c/li\u003e\u003cli\u003eYou can interact and click into the NxN confusion matrix or click into the IOU / Precision / Recall histograms to surface and view specific data rows in “gallery view.” For instance, you can understand where your model is not performing well, where your labels are off, or where your model is the least confident.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1031\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/Screenshot-2022-12-21-at-18.09.09-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAuto-computed confusion matrix\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eUpload confidence scores alongside every prediction and tune the confidence and IOU thresholds in the Quantumworks Lab Model UI to see how model metrics change as the thresholds change.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eFind the distribution of annotations and predictions in every model run via histograms\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/histogram-1.jpeg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1980\" height=\"1337\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/histogram-1.jpeg 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/histogram-1.jpeg 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/histogram-1.jpeg 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/histogram-1.jpeg 1980w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eUse the prediction and annotation distribution histogram to surface important information about your model runs\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn addition, you can easily understand the distribution of your annotations and predictions via histograms. This makes curating datasets for labeling and analyzing model performance easier than ever. You can now use distributions to find the most predicted or least-predicted class and surface classes represented in training data, but rarely predicted by the model. \u003c/p\u003e\u003ch3 id=\"labelbox-leaderboards-a-new-era-of-evaluation-for-generative-ai\"\u003e\u003cstrong\u003eLabelbox leaderboards: A new era of evaluation for generative AI\u0026nbsp;\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn the rapidly evolving landscape of artificial intelligence, traditional benchmarks are no longer sufficient to capture the full capabilities of AI models. As AI grows increasingly complex, challenges like data contamination, overfitting to public benchmarks, scalability issues, and the absence of standardized evaluation criteria necessitate a more advanced approach to model metrics. \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://labelbox.com/leaderboards/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox leaderboards\u003c/u\u003e\u003c/a\u003e are the first to tackle these challenges by conducting structured evaluations on subjective AI model outputs using human experts and a scientific process that provides detailed feature-level metrics and multiple ratings. Leaderboards are available for \u003ca href=\"https://labelbox.com/leaderboards/image-generation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eImage Generation\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/leaderboards/speech-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eSpeech Generation\u003c/u\u003e\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/leaderboards/video-generation?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eVideo Generation\u003c/u\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eBy combining expert human evaluations with our \u003ca href=\"https://www.alignerr.com/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAlignerr\u003c/u\u003e\u003c/a\u003e workforce, reliable methodology, and continuous updates, Quantumworks Lab is redefining AI evaluation. Our approach complements traditional leaderboards by offering a  comprehensive and human-based assessment of AI models. \u003c/p\u003e\u003cp\u003eRead more about Quantumworks Lab leaderboards on our blog \u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003ehere\u003c/u\u003e\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cstrong\u003e \u003c/strong\u003eLabelbox offers a robust platform coupled with expert human evaluation services to efficiently generate and visualize these metrics, empowering you to make informed decisions and improve your models.\u0026nbsp;\u003c/p\u003e\u003cp\u003eYou can learn more about Quantumworks Lab auto-metrics in our \u003ca href=\"https://docs.labelbox.com/docs/evaluate-model-performance?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e or by reviewing \u003ca href=\"https://docs.labelbox.com/reference/upload-image-predictions?ref=labelbox-guides.ghost.io\"\u003ehow to upload image predictions in Quantumworks Lab\u003c/a\u003e.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eIf you're interested in implementing this evaluation approach or leveraging Quantumworks Lab's tools for your model evaluation, \u003ca href=\"https://app.labelbox.com/signup?utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026utm_keyword=Quantumworks Lab%2520pricing\u0026gclid=CjwKCAjwjqWzBhAqEiwAQmtgT_HaRJu-zYfq545Dxl9HUqyPBNDpQAHecf-NxYsnKueRGjicsKGXfRoCzlsQAvD_BwE\u0026landingPageAnonymousId=%22a83b92ec-b8b4-41cd-9622-4e3725a530bf%22\u0026referrer_url=https://www.google.com/\u0026_r=https://www.google.com/\"\u003e\u003cu\u003esign up for a free\u003c/u\u003e\u003c/a\u003e Quantumworks Lab account to try it out, or \u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e to learn more.\u003c/p\u003e","comment_id":"6375c52617f6c9003d7b0fd9","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-2721--1-.png","featured":false,"visibility":"public","created_at":"2022-11-17T05:22:46.000+00:00","updated_at":"2024-11-27T03:03:43.000+00:00","published_at":"2023-03-03T17:53:39.000+00:00","custom_excerpt":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/intro-to-model-metrics","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/intro-to-model-metrics/","excerpt":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":"An introduction to model metrics","og_description":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Group-2721--1--2.png","twitter_title":"An introduction to model metrics","twitter_description":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","meta_title":"An introduction to model metrics","meta_description":"Learn how you can use model metrics to surface low-performing classes, find and fix labeling errors, and improve the overall performance of the model before it hits production on real-world data. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63ff856122e0a7003dacb795","uuid":"6f3a4aaf-3582-413a-857c-b7686a5bdb48","title":"Using Quantumworks Lab and Weights \u0026 Biases to fine tune your computer vision projects","slug":"using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects","html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\u003cp\u003eRepeatable, scalable, and diagnosable production artificial intelligence (AI), requires a sophisticated machine learning operations (MLOps) ecosystem. MLOps is the backbone of machine learning (ML) engineering, focused on streamlining development of AI/ML models and deploying, and monitoring those models in production. \u003c/p\u003e\u003cp\u003eIn this post, we’ll explore an MLOps architecture that uses both Quantumworks Lab and \u003ca href=\"https://wandb.ai/site?ref=labelbox-guides.ghost.io\"\u003eWeights \u0026amp; Biases\u003c/a\u003e to develop a computer vision model for a manufacturing focused use case. The goal of the model is to reduce defects on a production manufacturing line using automated visual inspection and the model requires human judgment to curate (supervise) the training data for model development.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/rx0-34Eba9ac5-xI9vIRIt8eZW7uCrVspPzHvmlLuXsF18qJCo-lWBZinRHHwm48UXGvnS3pdYgXOkEBqaeIkH5hk6epd9RfIHPtOEeVgZ1S2p8q_yMVOkOhXIypGKnl8zrwT4JpkzKXRW0IT70pBb6sDnYnl3pG1jDBCMPfbGFPbcnC9M6ai5gWBcwDhw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"465\" height=\"346\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample image of reducing defects on a production manufacturing line via the Quantumworks Lab interface.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eKey Components of AI Development\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eDeveloping a production caliber model is an extremely iterative process. Rarely, if ever, is a model trained once and ready for production. Typically, there are a series of experiments that need to be run across several combinations of datasets and models, followed by expert analysis to determine which one yields the best results. Each experiment must be meticulously tracked and analyzed as ML teams cycle through the development process. \u003c/p\u003e\u003cp\u003eIt’s not uncommon for data science teams to jump into model training without a clear understanding of their data. This can greatly increase the number of model training iterations needed to achieve the desired result, drive up development cost and increase the risks of achieving shipping ML-powered products on time.  \u003c/p\u003e\u003cp\u003eAn efficient, data centric model development approach is valuable, and consists of two key ML Ops components, a data-centric AI platform (Quantumworks Lab) and a model diagnostics platform (Weights \u0026amp; Biases).\u003c/p\u003e\u003ch3 id=\"improving-your-data-and-models\"\u003eImproving your data and models\u003cbr\u003e\u003c/h3\u003e\u003cp\u003eWhen your model is being developed/trained or when it is in production, there will be indicators that your model is either drifting or performing poorly. This will be noticed in the different statistics your model outputs - confidence, accuracy, loss, a fluctuation in the number of detections or the classes that are being detected, all of which will be tracked and triggered in a model diagnostics platform such as Weights \u0026amp; Biases. \u003c/p\u003e\u003cp\u003eBut what about improving the actual dataset? Or understanding which specific data my model is performing poorly on and determining which new data will most improve my model? How do I quickly get that data labeled and back into model training? When transitioning from model diagnostics to data diagnostics, we recommend leveraging tools to quickly understand the data that your model is performing poorly on, find more data similar to that, and curate subsequent datasets in order to iterate through the model development process faster. \u003c/p\u003e\u003cp\u003eThe bottom line is that model diagnostic tools and AI platforms supplement each other and should be used together. Let's dive deeper into what this looks like in practice.\u003c/p\u003e\u003ch2 id=\"how-it-looks-in-practice\"\u003eHow it looks in practice\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eOnce Quantumworks Lab and Weights \u0026amp; Biases have been installed and are set up in your pipeline, what does it look like to embark on the model development journey for the first time (or subsequent times)? Let’s look at an example workflow, most of which can be performed in a seamless process by leveraging both the \u003ca href=\"https://labelbox-python.readthedocs.io/en/latest/index.html?ref=labelbox-guides.ghost.io\"\u003eLabelbox\u003c/a\u003e and \u003ca href=\"https://docs.wandb.ai/quickstart?ref=labelbox-guides.ghost.io\"\u003eW\u0026amp;B\u003c/a\u003e SDKs:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1300\" height=\"706\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-10.55.06-AM.png 1300w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003col\u003e\u003cli\u003eBegin by first \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003econnecting your data source(s)\u003c/a\u003e in Quantumworks Lab Catalog for easy data visualization and curation.\u003c/li\u003e\u003c/ol\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/View-your-dataset-in-Catalog--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"995\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/View-your-dataset-in-Catalog--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/View-your-dataset-in-Catalog--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/View-your-dataset-in-Catalog--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/03/View-your-dataset-in-Catalog--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eQuickly search, explore and manage your data in one place. View raw data, metadata, and ground truth labels as shown in the mdetal cast defect example above.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e2. Label and review your data in Quantumworks Lab Annotate\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/MTDzpAqLb7Fnqe5_3GbURP2RlB56Dp4lGfprQf_difu_gRWtUxUWw3Ftkn-nhEjC25ZC66rxrZuoIc0WN7U66lBBVXL4z-p1iU1WjlFvAdiSHjOHygivHBtyVI5z2LN1IUpwBVF-FONNhYSfTUlPEvoY8RVKUp91u31dNmeDqANDdvIQShZCOBJpKgEcNQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"339\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eEasily set up a consistent ontology for your defect detection use case.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/ezgif.com-optimize--6-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1491\" height=\"976\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/ezgif.com-optimize--6-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/ezgif.com-optimize--6-.gif 1000w, https://labelbox-guides.ghost.io/content/images/2023/03/ezgif.com-optimize--6-.gif 1491w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eLeverage auto-annotation to speed up your manual labeling tasks.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e3. Easily split data into \u003ca href=\"https://docs.labelbox.com/docs/curate-data-splits?ref=labelbox-guides.ghost.io\"\u003etrain, test, and validate\u003c/a\u003e sets in Quantumworks Lab Model.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/03/Model_Run_Comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"880\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/03/Model_Run_Comparison--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/03/Model_Run_Comparison--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/03/Model_Run_Comparison--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/03/Model_Run_Comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eYou can use SDK to customize how you want to split your data rows for a given model run.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003e4. Send your data to Weights \u0026amp; Biases from Quantumworks Lab for handling model training and hyperparameter search and tuning.\u003c/p\u003e\u003cp\u003e5. Weights \u0026amp; Biases will handle model training and do a hyperparameters search to run a series of model experiments to be compared with each other. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/StJ0uM0O4-VRXym-dfdNG4JF2ArLtF3qHyoed2yp-F6fYE7dB7ztXzVn5u6AtaPlwp-eSu9fO6VugHVYneYoyOmfZSxk8cCjoEq9YdIkjJ43g7vbZg5B7U49VpraoHWt-uY2WHbDyb4EgTxtxTylEnw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"327\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eTo run a Hyperparameter search using Weights \u0026amp; Biases Sweeps you can \u003c/span\u003e\u003ca href=\"https://colab.research.google.com/drive/13eKhoSbn13kHRRQexbXEfQPz8OwjjcEh?ref=labelbox-guides.ghost.io#scrollTo=EhOKTaHcg4Xl\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003erun this colab notebook here\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e6. Visualize your results in W\u0026amp;B dashboards to quickly diagnose your model performance and create reports in order to share with colleagues and streamline communication. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/ugAyjHH-6i8xJCCdujBBbmC8L7NKrivhJ2j7_UW6Fd5rMf0S7EFPRT8r_v9x9yVrqxRxjqFqYiaikYIrzhdOY_IOjpLt_Qf9hYzBCsMVVxS-SUbKbsSQSSJYKEvuGf1qI9trJwCy_Y4HLDuh6Hgc7JE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"313\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eHere we can see the results of a search over 40 experiments to find the best training settings. \u003c/span\u003e\u003ca href=\"https://wandb.ai/morgan/industrial-images/sweeps/t88dzqwe?ref=labelbox-guides.ghost.io\"\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eClick here to see this live Weights \u0026amp; Biases dashboard\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e7. Quickly visualize your results in Quantumworks Lab Model to quickly diagnose the data that is related to your model, rapidly query all of your data sources to find data similar to where your model performs poorly (edge cases), and seamlessly queue that data for labeling.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/01/Model-metrics-view--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1753\" height=\"959\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cbr\u003e8. Rapidly repeat this iterative model development process until you have a production-worthy model, deploy that model, and continuously monitor\u003c/p\u003e\u003ch2 id=\"the-labelbox-ai-platform\"\u003eThe Quantumworks Lab AI platform\u003c/h2\u003e\u003cp\u003eOne of the biggest mistakes made when creating an AI ecosystem is not integrating a data-centric approach into your MLOps pipeline and serves as a foundational tool for data curation and fast iterative improvement through the model development lifecycle. \u003c/p\u003e\u003cp\u003eUsing Quantumworks Lab, ML Teams can easily connect to their sources (i.e. file systems, data lake platforms etc.) of unstructured data and quickly begin exploring and prioritizing their data curation efforts using Quantumworks Lab \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003eOnce data has been assessed and prioritized for curation \u0026amp; labeling, a combination of weak supervision and human supervised labeling campaigns are supported with the use of Quantumworks Lab \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003eFinally, the training data is curated into data splits for test, train and validation and can be easily integrated with a model diagnostics solution (Weights and Biases) to efficiently manage the first of many model training experiments and iterations. This integration is accomplished with the Quantumworks Lab \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003eMode\u003c/a\u003el product.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2023/03/Screen-Shot-2023-03-01-at-8.26.03-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1568\" height=\"898\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eThe Quantumworks Lab platform allows teams to more quickly go from data curation, annotation and fast iterative improvement through the model development lifecycle.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"weights-biases%E2%80%99-model-diagnostics-platform\"\u003eWeights \u0026amp; Biases’ Model Diagnostics Platform\u003c/h2\u003e\u003cp\u003eA model diagnostics platform should offer quick iteration, easy collaboration, and a centralized system of record for ML teams. Because ML development is often closer to a science than traditional software engineering, experimentation is at its core, and tracking the progress of these experiments is critical.\u003c/p\u003e\u003cp\u003eUsing Weights \u0026amp; Biases’ experiment tracking, ML teams can easily log their work in a standardized way, knowing that they can return to the results of their experiments days, months, or years later. \u003c/p\u003e\u003cp\u003eEase of collaboration is critical for ML teams so that they can move quickly, and Weights \u0026amp; Biases’ Reports enables colleagues to share quick notes, training journals, and polished analysis to teammates and managers to unlock decision making and keep the team moving forward.\u003c/p\u003e\u003cp\u003eFinally, knowing that your multiple model checkpoints are securely stored gives you the full picture of which model is best to select for deployment and to send to the W\u0026amp;B Model Registry, where your MLOps or DevOps team can pull it down for deployment.\u003cbr\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/4PUY0WUteKeig2EtGDEgFP_VVBSQSWkEV3ILX0b_rPUIFMDC2LsEdajU6cykdxVjjinMfRd9dEzEijMLgj0MkaS78BN4g_-yiPwbh3deCfVAWW7IEML2h_dI-YUUwCfUYG4LwErUmaIouAyL-ZqLR0TDH_l_cjnu4_ZhaXNiEzQ8Ds5We-caYw7dQdwc8A\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"144\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003eAI model development is an iterative process, which can be tedious and time consuming without a reliable MLOps pipeline. An efficient model development environment can be broken into two parts: a data-centric workflow for data ingestion, exploration, understanding, and preparation, and a model diagnostics platform for model training, tracking, evaluation, and versioning. Quantumworks Lab and Weights \u0026amp; Biases are leading platforms that were designed to complement this iterative process in mind, and when combined, can scale and help AI development teams build better models, faster than ever before.\u003c/p\u003e\u003cp\u003eWe'll be releasing more technical guides in the coming weeks on how to best utilize Quantumworks Lab and Weights \u0026amp; Biases so stay tuned! \u003cbr\u003e\u003c/p\u003e","comment_id":"63ff856122e0a7003dacb795","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/03/Screen-Shot-2023-03-01-at-8.57.36-AM.png","featured":false,"visibility":"public","created_at":"2023-03-01T17:03:29.000+00:00","updated_at":"2023-12-18T22:47:42.000+00:00","published_at":"2023-03-01T18:56:05.000+00:00","custom_excerpt":"Learn how you can use Quantumworks Lab and Weights \u0026 Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},"url":"https://labelbox-guides.ghost.io/using-labelbox-and-weights-biases-to-fine-tune-your-computer-vision-projects/","excerpt":"Learn how you can use Quantumworks Lab and Weights \u0026 Biases together to build better computer vision models. Follow a step-by-step workflow of data curation, annotation, model diagnostics and hyperparameter tuning. ","reading_time":6,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Using Quantumworks Lab and Weights \u0026 Biases | Quantumworks Lab","meta_description":"Use Quantumworks Lab and Weights \u0026 Biases to build better computer vision models with step-by-step workflows of data curation, annotation, \u0026 diagnostics","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6348256660b562003d250b50","uuid":"f78ce8ff-29aa-4034-9953-1b6b50823eea","title":"How to get started in Quantumworks Lab Model: Train, evaluate, and improve your ML models","slug":"how-to-train-evaluate-and-improve-your-ml-models","html":"\u003cp\u003eThe quality of your data will dictate your model’s performance. ML teams have historically had to rely on manual methods of curating data and debugging model errors. For teams who are looking to go through fast, data-centric iterations, this is not an ideal way to quickly scale and reach production AI. \u003c/p\u003e\u003cp\u003eIn order to ship performant models, you need to be able to quickly train models with collaborative data-centric tools. Quantumworks Lab Model can help you ship better models faster by leveraging collaborative tools to curate, debug, diagnose, and optimize your machine learning data and models.\u003c/p\u003e\u003cp\u003eThis guide will walk you through how to get started with \u003ca href=\"https://app.labelbox.com/models?ref=labelbox-guides.ghost.io\"\u003eModel in the Quantumworks Lab platform\u003c/a\u003e. We’ll walk through a COCO object detection example and show you how to get onboarded in Model with your first project, model, and model run. \u003c/p\u003e\u003cp\u003eBy the end of the tutorial, you will have learned how to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUpload and version a dataset in a Quantumworks Lab Model run\u003c/li\u003e\u003cli\u003eExport and train an object detection model from a pre-trained model\u003c/li\u003e\u003cli\u003eVisualize model predictions against ground truth annotations\u003c/li\u003e\u003cli\u003eView auto-generated metrics (F1, precision, recall, IOU, confusion matrix, etc.) and the distribution of annotations and predictions\u003c/li\u003e\u003cli\u003eEvaluate model performance and improve your model and data with error analysis and active learning\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eKey definitions in Quantumworks Lab Model:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eA \u003ca href=\"https://docs.labelbox.com/docs/create-a-model?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003emodel\u003c/em\u003e\u003c/a\u003e\u0026nbsp;is a large language model (LLM) integrated into Quantumworks Lab Model or your custom configuration specified by an ontology of data.\u003c/li\u003e\u003cli\u003eAn \u003ca href=\"https://docs.labelbox.com/docs/experiments?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003e\u003cem\u003eexperiment\u003c/em\u003e \u003c/u\u003e\u003c/a\u003eis a directory where you can create, manage, and compare a set of \u003cem\u003emodel runs\u003c/em\u003e related to the same machine learning task (e.g object detection on COCO). \u003c/li\u003e\u003cli\u003eA \u003ca href=\"https://docs.labelbox.com/docs/create-a-model-run?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003emodel run\u003c/em\u003e\u003c/a\u003e is a model training experiment within a model directory. Each model run provides a versioned data snapshot of the data rows, annotations, and training/validation/test splits for that model run\u003c/li\u003e\u003cli\u003eYou can specify \u003ca href=\"https://docs.labelbox.com/docs/add-model-run-config-to-track-hyperparameters?ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003emodel run configuratio\u003c/em\u003ens\u003c/a\u003e to create, version, and track your hyperparameters and any training-related configurations for a model run\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"part-1-seed-a-coco-dataset-and-create-a-project-and-model-run\"\u003ePart 1: Seed a COCO dataset and create a project and model run\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/174ebhOCA8XeQ-WavIDZw3U1EeUHh8MaV?ref=labelbox-guides.ghost.io#scrollTo=UxrfeTs6fint\" class=\"kg-btn kg-btn-accent\"\u003ePart 1: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/74ypf3u3ba\" title=\"How to get started in Quantumworks Lab Model (Part 1) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"640\" height=\"360\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eFor this onboarding tutorial, we’ll be working with a COCO dataset example. You can follow along by using the \u003c/em\u003e\u003ca href=\"https://colab.research.google.com/drive/174ebhOCA8XeQ-WavIDZw3U1EeUHh8MaV?ref=labelbox-guides.ghost.io#scrollTo=UxrfeTs6fint\"\u003e\u003cem\u003eGoogle Colab Notebook\u003c/em\u003e\u003c/a\u003e\u003cem\u003e and the accompanying video tutorials, but feel free to also bring your own dataset into Quantumworks Lab Model to create a Quantumworks Lab project, model and model run for your use case.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBefore you begin this tutorial, you’ll need to sign into your Quantumworks Lab account or \u003c/strong\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003ecreate a free account\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"step-1-upload-the-coco-dataset-to-labelbox\"\u003eStep 1: Upload the COCO dataset to Quantumworks Lab\u003c/h3\u003e\u003cul\u003e\u003cli\u003eUse the provided helper functions in the Colab Notebook to upload the provided COCO dataset to Quantumworks Lab\u003c/li\u003e\u003cli\u003eOnce you’ve successfully run the helper function, you should be able to see the COCO dataset appear in \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1068\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/Screen-Shot-2023-01-18-at-4.41.01-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"step-2-upload-the-coco-dataset%E2%80%99s-bounding-box-labels-to-labelbox\"\u003eStep 2: Upload the COCO dataset’s bounding box labels to Quantumworks Lab\u003c/h3\u003e\u003cul\u003e\u003cli\u003eWhile labels are typically created by an internal or external team of labelers, we are going to use an already labeled public dataset for this tutorial\u003c/li\u003e\u003cli\u003eUse the provided helper function to directly upload the dataset’s labels to Quantumworks Lab\u003c/li\u003e\u003cli\u003eSimilarly to the step above, you should be able to see your COCO data rows and the appropriate labels in \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-3-populate-a-model-run-version-model-run-data-and-model-hyperparameters\"\u003eStep 3: Populate a model run: Version model run data and model hyperparameters\u003c/h3\u003e\u003cul\u003e\u003cli\u003eNow, we’re ready to populate the above data (dataset + labels) in a model run\u003c/li\u003e\u003cli\u003eFollow the steps in the Colab Notebook to configure the ontology (containing classes of objects that we want to detect in the image dataset)\u003c/li\u003e\u003cli\u003eYou can also specify your model run’s hyperparameters\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-view-your-populated-model-run-in-the-model-tab\"\u003eStep 4: View your populated model run in the Model tab\u003c/h3\u003e\u003cul\u003e\u003cli\u003eOnce you’ve completed these steps, you should be able to see your versioned data in the Model tab\u003c/li\u003e\u003cli\u003eYou can view your data rows, labels, and inspect your model run configuration (hyperparameters) in the Quantumworks Lab app \u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1052\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/Screen-Shot-2023-01-18-at-4.41.26-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eNext, we’re going to use the model run data that we’ve uploaded in Quantumworks Lab to train a model.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"part-2-fine-tune-a-faster-r-cnn-model-upload-predictions-to-model-and-run-error-analysis\"\u003ePart 2: Fine-tune a Faster R-CNN model, upload predictions to Model, and run error analysis\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1mSl3RU8tz2NMQUbE5RZC9rjsmXCTjgjT?ref=labelbox-guides.ghost.io#scrollTo=kvrRY4pJk1ul\" class=\"kg-btn kg-btn-accent\"\u003ePart 2: Google Colab Notebook\u003c/a\u003e\u003c/div\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/yxk8wmgpmg\" title=\"How to get started in Quantumworks Lab Model (Part 2) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"step-1-train-an-object-detection-model-from-a-pre-trained-faster-r-cnn-model\"\u003eStep 1: Train an object detection model from a pre-trained Faster R-CNN model\u003c/h3\u003e\u003cul\u003e\u003cli\u003eExport the labels from the model run you created – the labels will be versioned by the model run\u003c/li\u003e\u003cli\u003eUse the provided helper function to transform the labels into a format that the Pytorch model can accept\u003c/li\u003e\u003cli\u003eWe’ve provided a Faster R-CNN model for fine-tuning – this replaces the last box prediction head with a new layer so that it can predict the model ontology and classes that we are concerned with\u003c/li\u003e\u003cli\u003eTrain the Faster R-CNN model for 1 epoch (4 minutes) – after this, you’ll have a model that performs decently well for this use case\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-2-generate-predictions-with-the-trained-model\"\u003eStep 2: Generate predictions with the trained model \u003c/h3\u003e\u003cul\u003e\u003cli\u003eGenerating predictions will help us visualize model performance in Quantumworks Lab Model and can help identify model errors\u003c/li\u003e\u003cli\u003eAssemble predictions and results of the model into a format that can be ingested back into Quantumworks Lab\u003c/li\u003e\u003cli\u003eTurn predictions into NumPy arrays and create annotation payloads for each object \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWant to upload a \u003c/strong\u003e\u003ca href=\"https://labelbox.com/blog/bring-your-own-models-to-labelbox-with-new-custom-model-integration/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003e\u003cu\u003ecustom model\u003c/u\u003e\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e instead?\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAt Quantumworks Lab, we want to ensure you are able to upload your own models quickly and easily, with no manual onboarding. With just a few clicks, you can seamlessly integrate your own custom models into our platform to enhance prediction, accelerate model evaluation, and improve data enrichment.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdlv2idAhfmVrY-LfZhsvYcU2L7e9a5gJ-9XLJW_f98QwSSm6SlOvKk36PC-8parhUQmEc02JMpOhffaz9IjfDYGNOzfzGZNsHb9qCRDqi3r4_KUoVr2KvLV6Tgo8PHbHL1DtUM?key=d5f31BHEZOcEnpQg1zrr_5zJ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"339\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003e\u0026nbsp;Import a Custom Model from the Model homepage\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"step-3-upload-predictions-back-into-your-labelbox-model-run\"\u003eStep 3: Upload predictions back into your Quantumworks Lab model run \u003c/h3\u003e\u003cul\u003e\u003cli\u003eUpload the model predictions to the Quantumworks Lab app in order to visualize how the model is performing\u003c/li\u003e\u003cli\u003eOther than confidence scores, you don’t have to worry about computing metrics. Quantumworks Lab Model will auto-compute model metrics such as F1 scores, precision, IOU,  confusion matrix, false positives/false negatives/true positives/true negatives, etc\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-4-visualize-model-predictions-and-auto-generated-metrics-in-labelbox-model\"\u003eStep 4: Visualize model predictions and auto-generated metrics in Quantumworks Lab Model \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/GIF--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/GIF--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/GIF--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/GIF--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/GIF--1-.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAfter you’ve completed the steps above, you should now be able to go to the Models tab to visualize model predictions and model metrics\u003c/li\u003e\u003cli\u003eClick on “Metrics View” to inspect model metrics and the model’s performance on each class\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can run \u003ca href=\"https://docs.labelbox.com/docs/find-model-errors?ref=labelbox-guides.ghost.io\"\u003eerror analysis\u003c/a\u003e on specific classes or data rows of interest:\u003c/p\u003e\u003cul\u003e\u003cli\u003eStart by inspecting how the model is doing on each class\u003c/li\u003e\u003cli\u003eIf you notice the model is struggling on a specific class, you can click into the histogram to view data rows on which the model is struggling. You can refine the search query to further drill into and inspect model performance\u003c/li\u003e\u003cli\u003eLeverage “detailed view” to better inspect disagreements and find patterns of model failures on images\u003c/li\u003e\u003cli\u003eAfter running error analysis, you can make more informed decisions on how to iterate and improve your model’s performance with corrective action or targeted data selection\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"learn-more\"\u003eLearn More \u003c/h2\u003e\u003cp\u003eBy following this step-by-step tutorial, you’ve now successfully created a model, a model run, and have uploaded model predictions into Quantumworks Lab for further analysis. \u003c/p\u003e\u003cp\u003eYou can also refer to the below guides for a more in-depth walkthrough on how to improve data selection and model performance:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eHow to search, surface, and prioritize data within a project\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/active-learning?ref=labelbox-guides.ghost.io\"\u003eHow to prioritize high-value data\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-label-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix label errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-model-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix model errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003eHow to find similar data in one click\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’re happy to help answer any questions. Reach out to us anytime on our \u003ca href=\"https://labelbox.com/sales?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e page. \u003c/p\u003e","comment_id":"6348256660b562003d250b50","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2966--1-.png","featured":false,"visibility":"public","created_at":"2022-10-13T14:49:10.000+00:00","updated_at":"2024-11-27T02:19:06.000+00:00","published_at":"2023-01-25T23:49:15.000+00:00","custom_excerpt":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-train-evaluate-and-improve-your-ML-models","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-train-evaluate-and-improve-your-ml-models/","excerpt":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","reading_time":5,"access":true,"comments":false,"og_image":null,"og_title":"How to get started in Quantumworks Lab Model: train, evaluate, and improve your ML models","og_description":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","twitter_image":null,"twitter_title":"How to get started in Quantumworks Lab Model: train, evaluate, and improve your ML models","twitter_description":"Learn how to ship better models faster by leveraging Quantumworks Lab Model. In this guide, we'll walk you through a COCO object detection example to get you onboarded in Model with your first project, model, and model run. ","meta_title":"How to get started in Quantumworks Lab Mode: Train evaluate, and improve your ML models","meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63bdef75be228d003d1e9ce3","uuid":"abe288c8-c70e-407b-b727-da6a90605cb3","title":"How to kickstart and scale your data labeling efforts","slug":"how-to-kickstart-and-scale-your-data-labeling-efforts","html":"\u003cp\u003eYour model performance will only ever be as strong as the quality of your training data. A common bottleneck for many AI teams is how to obtain vast amounts of high-quality training data for their use case at scale in the most time efficient and cost-effective way possible. \u003c/p\u003e\u003cp\u003eWhen it comes to deciding how to label your data, you might consider one of the following options:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCompletely outsource this task to a labeling service — these external teams often receive training on the specific labeling tasks required and quickly proceed to label large datasets\u003c/li\u003e\u003cli\u003eLeverage AI-powered solutions from a labeling platform to speed up the labeling process\u003c/li\u003e\u003cli\u003eManage homegrown or open source tools and rely on your own internal team of labelers to label your dataset\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOn the surface, the above options may seem sufficient, but they have their disadvantages. The labeling process itself is opaque, so by relying on completely outsourcing your task, you risk having little insight into metrics such as labeling quality, throughput, and efficiency. If you’re working with sensitive data, outsourcing labeling can be a greater challenge regarding security concerns. Many service providers also don’t provide access to a labeling platform, hindering AI teams from experimenting within the labeling process and taking advantage of techniques like automation and active learning. In addition, utilizing in-house or open source tools can quickly become hard to manage, resulting in an exorbitant amount of time and resources in maintenance and scale. This can lead to delays from quality management and labeling iteration, poor ontology creation and management, miscommunication between stakeholders, SMEs, labelers, and more.\u003c/p\u003e\u003cp\u003eTo appropriately scale and maintain the quality required for your production use case, you’ll need to leverage a \u003ca href=\"https://labelbox.com/learn/library/complete-guide-data-engines-for-ai/?ref=labelbox-guides.ghost.io\"\u003edata engine\u003c/a\u003e. An effective data engine combines data management, quality and performance monitoring, and advanced techniques and labeling services to help improve the speed and efficiency of your labeling operations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Frame_2963.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1368\" height=\"1010\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Frame_2963.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Frame_2963.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/Frame_2963.png 1368w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eLabelbox provides \u003c/em\u003e\u003c/i\u003e\u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003elabeling services\u003c/em\u003e\u003c/i\u003e\u003c/a\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003e and AI expertise, on-demand. You can outsource labeling work and partner with ML experts to fine-tune the above workflows to ensure clarity on tasks and achieve your quality targets.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eRegardless of your use case, if you’re working with an external labeling team or partnering with a service provider, you’ll want to make sure that you’re set up for success. Carefully outlining your labeling project and task, defining your project’s success criteria, measuring and maintaining quality, scaling your labeling operations, and evaluating your project’s results are all key steps to ensuring that you are producing high-quality training data.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"step-1-define-a-task-for-your-labeling-project\"\u003eStep 1: Define a task for your labeling project\u003c/h2\u003e\u003col\u003e\u003cli\u003eAlign on the key components of your labeling task so that it can be effectively communicated to your labeling team\u003c/li\u003e\u003cli\u003eDetermine the \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#supported-data-types\"\u003edata type\u003c/a\u003e or industry vertical — this allows your Labeling Team Manager to appropriately match you with a team of labelers well-suited for your task\u003c/li\u003e\u003cli\u003eOutline any specific labeling or compliance requirements for this task — this will often require a specialized workforce that is trained in your specific industry or task\u003c/li\u003e\u003cli\u003eDefine your data volume and agree on a project timeline — this will help allocate resources for your project and set expectations upon project start\u003c/li\u003e\u003cli\u003eCreate \u003ca href=\"https://labelbox.com/guides/how-to-create-and-manage-ontologies/?ref=labelbox-guides.ghost.io\"\u003ean ontology\u003c/a\u003e with the goals of proper labeling, efficiency, and reusability in mind\u003c/li\u003e\u003cli\u003eProvide labeling instructions for the labeling team to use — instructions should provide context to the task, explain what the task entails, describe the labeling steps, and be treated as a “living document”\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003eHow to define a task for your data labeling project\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"step-2-define-your-labeling-project%E2%80%99s-success-criteria\"\u003e\u003cbr\u003eStep 2: Define your labeling project’s success criteria \u003c/h2\u003e\u003col\u003e\u003cli\u003eUnderstand your project’s timeline and scope — this includes any deadlines, projected data volume, and the  average time per label\u003c/li\u003e\u003cli\u003eSelect the grading requirements for your project — this will help determine what is a “good” or “bad” quality label.\u003c/li\u003e\u003cli\u003eDecide if you want to implement a \u003ca href=\"https://labelbox.com/guides/how-to-define-your-data-labeling-projects-success-criteria/?ref=labelbox-guides.ghost.io#slas-quality-speed-throughput\"\u003equality SLA\u003c/a\u003e with your labeling team — this is a bidirectional commitment with your labeling team that is built on throughput and quality calculations \u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide,\u003cstrong\u003e \u003c/strong\u003e\u003ca href=\"https://labelbox.com/guides/how-to-define-your-data-labeling-projects-success-criteria/?ref=labelbox-guides.ghost.io\"\u003eHow to define your data labeling project’s success criteria.\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"step-3-create-a-quality-strategy-for-your-labeling-project\"\u003eStep 3: Create a quality strategy for your labeling project\u003c/h2\u003e\u003cp\u003eAfter defining and setting expectations on how quality is defined, you'll want to spend some time developing a quality strategy.\u003c/p\u003e\u003col\u003e\u003cli\u003eMake sure you have quality monitoring tools in place — Quantumworks Lab’s \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003ebenchmark\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003econsensus\u003c/a\u003e tools help measure labeling accuracy and labeling consistency so you can gauge your project’s labeling efficiency\u003c/li\u003e\u003cli\u003eIncorporate manual review and feedback throughout your projects’ duration, as labeling data is a collaborative process. Quantumworks Lab’s \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003eworkflow feature\u003c/a\u003e allows you to set up customized review steps based on your quality strategy\u003c/li\u003e\u003cli\u003eIf you have a quality SLA, regularly monitor and review your data to determine whether the SLA has been met\u003c/li\u003e\u003cli\u003eEnsure that there is an open two-way communication channel between your labeling team and key stakeholders — this can resemble Quantumworks Lab platform features such as \u003ca href=\"https://docs.labelbox.com/docs/issues-comments?ref=labelbox-guides.ghost.io\"\u003eissues \u0026amp; comments\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/boost-workforce-notifications?ref=labelbox-guides.ghost.io\"\u003eupdates\u003c/a\u003e, Slack, Google Docs, etc.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-create-a-quality-strategy-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003eHow to create a quality strategy for your data labeling project\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"step-4-scale-your-labeling-operations-while-maintaining-quality\"\u003e\u003cbr\u003e\u003cstrong\u003eStep 4:\u003c/strong\u003e \u003cstrong\u003eScale your labeling operations while maintaining quality\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eOnce a desired quality strategy has been implemented, a key question becomes how to maintain consistency and quality as team size or data volume grows.\u003c/p\u003e\u003col\u003e\u003cli\u003eManage your labeling workflow by making use of iteration with small batches and an initial calibration phase\u003c/li\u003e\u003cli\u003eThe \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io#calibration-phase\"\u003ecalibration phase\u003c/a\u003e is often a smaller subset of your task — it is used to train the labeling team on labeling instructions, the ontology, and to help them become familiar with the data in the project\u003c/li\u003e\u003cli\u003eProvide feedback and work with your labeling team to iterate on the data until the desired quality threshold is reached\u003c/li\u003e\u003cli\u003eMonitor overall quality and speed of your labeling operations as you enter the \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io#production-phase\"\u003eproduction phase\u003c/a\u003e of your project\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003eHow to scale up your labeling operations while maintaining quality\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"step-5-evaluate-and-optimize-your-labeling-project%E2%80%99s-results\"\u003eStep 5: Evaluate and optimize your labeling project’s results\u003c/h2\u003e\u003cp\u003eAfter a task is completed and you have entered the production phase, it’s important to evaluate and consider factors that can guide you toward greater optimization of future batches.\u003c/p\u003e\u003cp\u003eAfter a task is completed and you have entered the production phase, it’s important to evaluate and consider factors that can guide you toward greater optimization of future batches. \u003c/p\u003e\u003col\u003e\u003cli\u003eCrowdsource feedback from your labelers — understanding their challenges with the given task can help clarify future labeling instructions, discover edge cases, and suggest ways to improve efficiency\u003c/li\u003e\u003cli\u003eReview project results and labeler performance against your existing ontology and labeling instructions — see if project results reveal an opportunity to improve ontology structure or guidance\u003c/li\u003e\u003cli\u003eSave labeling time and cost by leveraging active learning techniques to prioritize high-impact data — Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e can help you quickly identify label and model errors, find all instances of similar data to edge cases or mislabeled data rows, and more\u003c/li\u003e\u003cli\u003eDetermine how well your project’s results aligned with your quality strategy outlined in step 3 — see if you notice areas for improvement or if further customization to improve review efficiency is needed with \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eworkflows\u003c/a\u003e\u003c/li\u003e\u003cli\u003eEvaluate whether the labeling team size and skillset was appropriate for your use case and the desired production capability \u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo learn more, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-evaluate-and-optimize-your-data-labeling-projects-results/?ref=labelbox-guides.ghost.io\"\u003eHow to evaluate and optimize your data labeling project’s results\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003ePowered by Quantumworks Lab’s data engine, experience the next level of data labeling service with direct access to curated data labeling teams for your projects in any expert domain or popular languages. Set new standards in quality and throughput at half the cost.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003eContact us\u003c/a\u003e today to access the best data labeling services with specialized labeling teams that match your use case. You can also sign up and \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eget started with Quantumworks Lab for free\u003c/a\u003e. \u003c/p\u003e","comment_id":"63bdef75be228d003d1e9ce3","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2973--1-.png","featured":false,"visibility":"public","created_at":"2023-01-10T23:06:29.000+00:00","updated_at":"2024-09-12T23:47:29.000+00:00","published_at":"2023-01-12T21:31:12.000+00:00","custom_excerpt":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-kickstart-and-scale-your-data-labeling-efforts","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-kickstart-and-scale-your-data-labeling-efforts/","excerpt":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2973--1--3.png","og_title":"How to kickstart and scale your data labeling efforts","og_description":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2973--1--1.png","twitter_title":"How to kickstart and scale your data labeling efforts","twitter_description":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","meta_title":"How to kickstart and scale your data labeling efforts","meta_description":"Learn how to effectively kickstart and scale your data labeling efforts to reduce cost, while maintaining the desired quality required for your use case. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b5df77f94604003d092822","uuid":"2b424b77-c04e-4b37-8b3e-e9343aa9c81e","title":"How to evaluate and optimize your data labeling project's results","slug":"how-to-evaluate-and-optimize-your-data-labeling-projects-results","html":"\u003cp\u003eLeading machine learning teams establish high quality labeling workflows by starting with \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003esmall iterative batches\u003c/a\u003e to develop an effective feedback loop with labeling teams early on in the process. This approach allows ML teams to quickly identify issues and make necessary changes to their labeling instructions and ontologies, enabling labeling teams to become more proficient with the task before large volumes of data are labeled. Following this waterfall approach, teams are able to generate higher quality labeled data quickly and at scale. \u003c/p\u003e\u003cp\u003eAfter a task has been completed and an ML team has entered into the production phase, it's important to evaluate and consider several factors that can guide you toward greater optimization of future batches. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"what-should-ml-teams-consider-once-a-labeling-project-is-complete\"\u003eWhat should ML teams consider once a labeling project is complete? \u003c/h2\u003e\u003ch3 id=\"feedback-from-labelers\"\u003eFeedback from labelers\u003c/h3\u003e\u003cp\u003eAfter project completion, you should consider sourcing feedback from your labelers. Machine learning teams often benefit from asking labelers to share their thoughts on the given labeling task to better understand what was challenging, to discover edge cases, and receive suggestions on how to improve efficiency. \u003c/p\u003e\u003cp\u003eA great way to crowdsource this feedback is through Quantumworks Lab's \u003ca href=\"https://docs.labelbox.com/docs/issues-comments?ref=labelbox-guides.ghost.io\"\u003eissues \u0026amp; comments\u003c/a\u003e feature that can be used on individual data rows. This allows the labeling team and your ML team to collaborate and provide feedback on specific examples or assets. \u003ca href=\"https://docs.labelbox.com/docs/boost-workforce-notifications?ref=labelbox-guides.ghost.io\"\u003eUpdates\u003c/a\u003e within a project can also be used to provide more holistic feedback on the project – enabling three-way communication between you, the Quantumworks Lab Boost admin, and the labeling workforce. Lastly, you can also rely on shared Google Docs and team syncs to collect feedback from the labeling team. \u003c/p\u003e\u003ch3 id=\"labeling-instructions-and-ontology-changes\"\u003eLabeling instructions and ontology changes\u003c/h3\u003e\u003cp\u003eAfter a project has been completed, you can review project results and labeler performance to pinpoint areas for improving your labeling instructions or ontology. \u003c/p\u003e\u003cp\u003eQuestions that you can consider during review include: \u003c/p\u003e\u003cul\u003e\u003cli\u003eDo the results of the project show any areas of labeling uncertainty that can be improved by editing the task's instructions?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003cem\u003e \u003c/em\u003e\u003c/strong\u003eCreating category definitions that are mutually exclusive can improve labeling team alignment and reduce the chance of confusion between object types. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003cem\u003e \u003c/em\u003e\u003c/strong\u003eAdding numbered steps can be a great way to organize complex labeling tasks. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003cem\u003e \u003c/em\u003e\u003c/strong\u003eIncluding decision trees and rule of thumb guidance can be helpful for subjective tasks that require critical thinking. \u003c/p\u003e\u003cul\u003e\u003cli\u003eDo project results reveal opportunities for improving ontology structure? \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003c/strong\u003e Think about if the ontology is organized in the most efficient manner. For instance, can the labeling time can be improved using nested classifications vs having a list of single objects?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip: \u003c/em\u003e\u003c/strong\u003eReview whether the ontology includes too many objects or too few objects to achieve project goals.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003c/strong\u003e If warranted, share the \"why\" behind your ontology requirements or provide additional project context to help labelers better contextualize tasks. \u003c/p\u003e\u003ch3 id=\"improve-your-data-selection\"\u003eImprove your data selection \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/catalog-06--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1344\" height=\"1024\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/catalog-06--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/catalog-06--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/catalog-06--1-.png 1344w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAfter the successful completion of a labeling project, it can be tempting to proceed by creating projects or batches of even larger datasets to label. However, to make sure that you're training your model on the most impactful examples, you can leverage Quantumworks Lab's \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e to curate and prioritize future batches. \u003c/p\u003e\u003cp\u003eWith Catalog and Model, you can quickly identify label and model errors, find all instances of similar data to edge cases or mislabeled data rows and send them to Annotate in order to retrain your model on specific slices of data. Smartly selecting high value data to label in priority is key to maximizing labeling efficiency and reduce project costs. \u003c/p\u003e\u003cp\u003eRefer to the below guides and resources to learn more about how to improve data selection with Quantumworks Lab: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eHow to search, surface, and prioritize data within a project\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/active-learning?ref=labelbox-guides.ghost.io\"\u003eHow to prioritize high-value data\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-label-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix label errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-model-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix model errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003eHow to find similar data in one click\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"review-your-quality-strategy\"\u003eReview your quality strategy\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/workflow--3-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1360\" height=\"1024\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/workflow--3-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/workflow--3-.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/workflow--3-.png 1360w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOne key area to review after the completion of a project is how well your \u003ca href=\"http://labelbox.com/guides/how-to-create-a-quality-strategy-for-your-data-labeling-project?ref=labelbox-guides.ghost.io\"\u003equality assurance strategy\u003c/a\u003e performed. ML teams will benefit from asking the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003econsensus\u003c/a\u003e was used on the project, were there enough votes to provide sufficient insights?\u003c/li\u003e\u003cli\u003eWas the number of \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003ebenchmarks\u003c/a\u003e used or was the percentage of consensus coverage applied sufficient to assess performance across the dataset?\u003c/li\u003e\u003cli\u003eWere the benchmark or consensus agreement results as expected, or lower than anticipated?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTip:\u003c/em\u003e\u003c/strong\u003e If consensus scores are consistently high or do not provide additional valuable insights, consider lowering the number of votes, coverage percentage, or removing altogether to reduce labeling time.\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf using SLAs, how well were expectations and requirements met?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYour team should also assess how well the labeling review process was. If you notice areas for improvement or the need for customization to improve review efficiency, you can customize your review step with \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eworkflows\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eTo learn more about workflows, read our guide, \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process?ref=labelbox-guides.ghost.io\"\u003eHow to customize your annotation review process\u003c/a\u003e\u003c/p\u003e\u003ch3 id=\"labeling-team-size-and-skillset\"\u003eLabeling team size and skillset\u003c/h3\u003e\u003cp\u003eEvaluating the size and makeup of the labeling team is important at every phase of your labeling operations. At the end of a project, it's recommended that you review whether the labeling team size was too small to meet the desired production capability or too large for the required volume. Additionally, it is key for teams to consider whether future batches of data will require labelers with specialized training (such as having industry-specific or language experience). \u003c/p\u003e\u003cp\u003eKeeping communication open with the labeling team is also crucial during these considerations. This is especially important when working with external labeling teams, who often benefit from having advance notice of any new batches or changing requirements so that they can effectively allocate or maintain their resources. \u003c/p\u003e\u003cp\u003eClear and timely communication of future project needs, such as the anticipated readiness of the next project or batch, the expected size of the dataset, changes to the instructions or labeling requirements, and ideal completion dates, should be communicated to help ensure a smooth ramp up on the next project. \u003c/p\u003e\u003cp\u003eYou can easily access data labeling services with specialized expertise that are fit for your specific use case through \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e. Collaborate with the workforce in real-time to monitor high-quality data,, all while managing and keeping human labeling costs to a minimum using AI-assisted tools and automation techniques. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/sales/?ref=labelbox-guides.ghost.io\"\u003eContact our Boost team\u003c/a\u003e to get paired with a specialized workforce team or learn more about how one of our customers, NASA's Jet Propulsion Laboratory, used \u003ca href=\"https://labelbox.com/customers/nasa-jpl/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost to deliver quality labels at 5x their previous speed\u003c/a\u003e. \u003c/p\u003e","comment_id":"63b5df77f94604003d092822","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2977--3-.png","featured":false,"visibility":"public","created_at":"2023-01-04T20:20:07.000+00:00","updated_at":"2023-10-26T18:15:02.000+00:00","published_at":"2023-01-12T21:31:07.000+00:00","custom_excerpt":"Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-evaluate-and-optimize-your-data-labeling-project's-results","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-evaluate-and-optimize-your-data-labeling-projects-results/","excerpt":"Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. ","reading_time":4,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2977--3--2.png","og_title":"How to evaluate and optimize your data labeling project's results","og_description":"Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2977--3--1.png","twitter_title":"How to evaluate and optimize your data labeling project's results","twitter_description":"Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. ","meta_title":"How to evaluate and optimize your data labeling project's results","meta_description":"Learn how to evaluate the results of your labeling project in order to further optimize and improve future iterations and batches of data. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b594a0f94604003d092665","uuid":"ec556dd1-ec16-401f-ba8d-685831eb0220","title":"How to create a quality strategy for your data labeling project","slug":"how-to-create-a-quality-strategy-for-your-data-labeling-project","html":"\u003cp\u003eThe outcome of a labeling project is often a reflection of how it began. A project that was put together in a rushed or careless manner often leads to non-optimal or even unusable training data. \u003ca href=\"https://labelbox.com/guides/data-labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eHigh-quality training data is the ultimate goal of all labeling projects\u003c/a\u003e, so spending time to develop a quality strategy at the beginning of your labeling project is crucial. \u003c/p\u003e\u003cp\u003eWhile carving out time at the beginning of your labeling project might take additional time, it will ultimately save you time and effort on reviews and corrections downstream. \u003c/p\u003e\u003cp\u003eQuality strategy refers to how you ensure and maintain that your labeling project is producing high quality training data. Your quality strategy might include the use of automated quality monitoring features, manual review and feedback, insight gleaned from the monitoring of quality SLA, and above all regular two-way communication with your labeling. \u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch1 id=\"labelboxs-quality-features\"\u003eLabelbox's quality features \u003c/h1\u003e\u003cp\u003eInsight into labeling quality is crucial in not only understanding labeling progress, but also in maximizing labeling efficiency to better manage labeling time and cost. The Quantumworks Lab \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003eAnnotation platform\u003c/a\u003e has several features that can assist with quality monitoring. \u003c/p\u003e\u003ch3 id=\"benchmark\"\u003eBenchmark\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003eBenchmark\u003c/a\u003e is a Quantumworks Lab QA tool that enables you to designate a labeled asset as a \"gold standard\" and automatically compares all labels on that asset to the benchmark label.\u003c/p\u003e\u003cp\u003eYou can create this label yourself or choose a \"perfect\" label done by your labeling team and mark it as a benchmark. The asset is then distributed to all labelers on the project and labelers will receive a benchmark score depending on how close their label is to the benchmark label. \u003c/p\u003e\u003cp\u003eBenchmark scores range from 0% (no match between the benchmark label and the labeler's annotations) to 100% (complete match between the benchmark and the labeler's annotations). \u003c/p\u003e\u003cp\u003eYou can learn more about how to set up benchmark labels and how benchmark scores are calculated in our \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-04-at-10.42.13-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1138\" height=\"422\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-04-at-10.42.13-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-04-at-10.42.13-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-04-at-10.42.13-AM.png 1138w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow does benchmark support quality? \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eBenchmark measures labeling \u003cem\u003eaccuracy\u003c/em\u003e. You can use benchmark on tasks that have one objectively correct answer and measure labeler performance against a \"gold standard\". \u003c/p\u003e\u003cp\u003eIt is helpful to determine a target benchmark score (per labeler or per label). The target percentage may depend on task complexity and quality expectations. A 90% benchmark score might be difficult to achieve on one task, but fall below quality expectations on another task. Therefore, you should think about and communicate your target benchmark score expectations to your labelers and reviewers.  \u003c/p\u003e\u003cp\u003eBenchmark scores should be monitored throughout the labeling project. Monitoring at the labeler level can help identify labelers who may struggle more with the task than others, resulting in lower benchmark scores, and you can retrain them or remove them from the project accordingly. Tracking the average benchmark scores across the project can help your team identify general challenges with the task. For example, a drop in scores once a new batch of data has been added may point to a new pattern in the data that needs to be addressed with updated labeling instructions. \u003c/p\u003e\u003ch3 id=\"consensus\"\u003eConsensus\u003c/h3\u003e\u003cp\u003eThe \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003eConsensus\u003c/a\u003e feature generates an agreement rate by comparing annotations provided by all labelers working on the exact same image.\u003c/p\u003e\u003cp\u003eWhen adding a new batch of data to your labeling project, you have the ability to enable or disable consensus for that batch, configure and set batch-specific priority, coverage, and the number of labels. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.19.12-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"660\" height=\"322\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-12-at-4.19.12-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.19.12-PM.png 660w\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow does consensus support quality? \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eConsensus measures labeling \u003cem\u003econsistency\u003c/em\u003e. Use consensus on tasks that are subjective where you want to go with a majority vote. You can also use it to identify data labeled with low consistency that needs to be checked by an expert.\u003c/p\u003e\u003cp\u003eAs with benchmarks, there is not a universal \"good\" or \"bad\" consensus score percentage. A “good” consensus score percentage will depend on the complexity of the task along with your quality expectations. When used for truly subjective tasks, consensus scores will not indicate the quality of a label, but rather the level of consistency between your labelers' subjective opinions. \u003c/p\u003e\u003cp\u003eConsensus scores should be monitored throughout the life of a labeling project. Monitoring consensus scores at the labeler-level can help identify a labeler that might consistently disagree with other labelers, signaling that the labeler might not have a strong understanding of the task. Average consensus scores across the project can help measure overall consistency. Low scores on an objective task may point towards gaps in labeler training or can allude to ambiguous wording in labeling instructions, leaving room for different interpretations by your labelers. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"manually-review-data-with-custom-workflows\"\u003eManually review data with custom workflows \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/workflow-diagram--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"672\" height=\"512\"\u003e\u003c/figure\u003e\u003cp\u003eWhile manual review and providing feedback can be somewhat time intensive, it is an important component in establishing quality in the early stages of your labeling project and should be a part of your quality strategy. \u003c/p\u003e\u003cp\u003eTo ensure quality, labeling should begin on small batches with manual reviews. Once quality is established, manual reviews can taper off to only apply to a percentage of the data and you can start to rely more on manual reviews on the labeling team's side and review tools like benchmark or consensus. \u003c/p\u003e\u003cp\u003eIn Quantumworks Lab Annotate, you can set up customized review steps based on your decided quality strategy in your project's \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflow tab\u003c/a\u003e. As you work with large, complex projects, having to review all labeled data rows becomes increasingly time-consuming and expensive. You can leverage workflows to create a highly-customizable, step-by-step review pipeline to drive efficiency and automation into your review process. \u003c/p\u003e\u003cp\u003eOn the Quantumworks Lab Annotation platform, you can set up customized review steps as decided in your quality strategy via the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e feature. Upon creating a new project, the default setting includes one review step (\"Initial review task\") on 100% of labels.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Workflows---review-tasks---reject--2-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1771\" height=\"912\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Workflows---review-tasks---reject--2-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Workflows---review-tasks---reject--2-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/Workflows---review-tasks---reject--2-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/Workflows---review-tasks---reject--2-.gif 1771w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can create a \"new review task\" and can customize your review process with up to 10 review tasks. For example, you can add a second review step on a sample of data performed by your internal review team after the labeling team has already performed their review. This allows for greater QA before data rows are transitioned to \"Done\" and are ready to be used for production. You further customize your review process by routing labels that include a specific annotation type to a separate review step (e.g for all data rows with this annotation type to be performed by an expert reviewer). \u003c/p\u003e\u003cp\u003eFor a more in-depth walkthrough and demonstration of workflows, please refer to our guide on \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003ehow to customize your annotation review process\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow does manual review support quality? \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eManual review can go hand-in-hand with sharing constructive feedback with individual labelers or the entire labeling team. \u003c/p\u003e\u003cp\u003eRather than simply approving or rejecting a label, you can take the time to share written feedback on why a label was rejected. This can be extremely helpful at the beginning of a new task as your labeling team gets started on your project. The labeling team can then use this feedback to better understand your expectations, fix mistakes, and avoid similar mistakes moving forward. \u003c/p\u003e\u003cp\u003eLabeling data is an inherently collaborative process that requires continuous feedback between labelers and reviewers to ensure high-quality outcomes. You can make use of \u003ca href=\"https://docs.labelbox.com/docs/issues-comments?ref=labelbox-guides.ghost.io\"\u003eissues \u0026amp; comments\u003c/a\u003e on the Quantumworks Lab platform to quickly pinpoint and share written feedback directly on the label. This feature also supports two-way communication, allowing labelers to respond or ask clarifying questions. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/comment-gif--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/comment-gif--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/comment-gif--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/comment-gif--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/comment-gif--1-.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"sla-monitoring\"\u003eSLA monitoring\u003c/h2\u003e\u003cp\u003eImplementing a \u003ca href=\"https://labelbox.com/guides/how-to-define-your-data-labeling-projects-success-criteria/?ref=labelbox-guides.ghost.io#slas-quality-speed-throughput\"\u003equality SLA\u003c/a\u003e for your project is optional. However, once the decision for an SLA is made, it requires consistent monitoring to be effective. \u003c/p\u003e\u003cp\u003eA labeling project with an SLA in place, such as a minimum labeling quality threshold to be reached, requires regular monitoring and review to determine whether the SLA has been met on a batch of data. \u003c/p\u003e\u003cp\u003eA labeling project with an SLA in place, i.e. a minimum labeling quality threshold to be reached, requires regular reviews from your side to determine whether the SLA has been met on a given batch of data. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow does SLA monitoring support quality?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhile you could rely solely on the SLA agreement with your labeling partner to meet the required quality threshold, a rigorous review process provides an opportunity to further improve quality. During your review of each batch of data, you should consider the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIs one labeler making significantly more mistakes that others? If so, consider retraining or replacing that labeler.\u003c/li\u003e\u003cli\u003eIs there one error category that shows up more frequently than other errors? If so, consider fleshing out the labeling instructions for this error or put together a small training project and retrain labelers on this error.\u003c/li\u003e\u003cli\u003eAre there changes in the data from batch to batch that lead to poor quality due to lack of experience with this new/different data? If so, consider updating your instructions with new examples found in this new batch of data. Walk the labeling team through these new changes and let them know to ask questions if they are unsure of how to label this new data. \u003c/li\u003e\u003cli\u003eAre you finding errors that do not fit into one of the error categories established as part of the quality SLA? If so, consider adding a new error category for the next batch and highlight this change to your labeling team so that labelers and reviewers are aware of what to look out for. \u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"create-communication-channels\"\u003eCreate communication channels \u003c/h2\u003e\u003cp\u003eDirect communication is of utmost importance before and during the labeling and feedback process. Communication channels should be carefully chosen, set up, and communicated clearly to all involved parties ahead of starting a project. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/annotate-collaberation--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1344\" height=\"1024\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/annotate-collaberation--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/annotate-collaberation--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/annotate-collaberation--1-.png 1344w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eSome common communication channels are listed below:\u003c/p\u003e\u003cul\u003e\u003cli\u003eChat programs (e.g. Slack)\u003c/li\u003e\u003cli\u003eEmail\u003c/li\u003e\u003cli\u003eShared documents (e.g. Google docs or slides) \u003c/li\u003e\u003cli\u003eRegular scheduled meetings (in person or remotely, ideally with screen sharing capabilities)\u003c/li\u003e\u003cli\u003eCommunication features on the labeling platform (e.g. \u003ca href=\"https://docs.labelbox.com/docs/boost-workforce-notifications?ref=labelbox-guides.ghost.io\"\u003eUpdates\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/issues-comments?ref=labelbox-guides.ghost.io\"\u003eIssues\u003c/a\u003e on the Quantumworks Lab platform)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce communication methods have been chosen, it is worthwhile to establish further guidelines around communication such as: availability, response times, definition of recipients/stakeholders for different topics, structure of messaging (e.g different chat groups or channels, multiple email threads versus one big thread), accessibility of different file formats shared (e.g Google docs versus MS doc). Initially, these details might not seem super important, however clear definitions of communication can aid with communication efficiency and affect the final outcome and quality of your project. \u003c/p\u003e\u003cp\u003eYou can rely on a combination of communication channels, such as weekly Q\u0026amp;A or feedback meetings coupled with a group chat to quickly unblock progress between weekly meetings. Communication channels are useful in providing day-to-day status updates from the labeling team or aid in the announcement of new projects or data volumes by the project owner. \u003c/p\u003e\u003cp\u003eYour chosen method of communication should foster direct communication between the labeling team and a subject-matter expert on the project owner's side. Two-way communication is encouraged at all times during the labeling process as it will help streamline and accelerate the review/rework and feedback process. \u003c/p\u003e\u003cp\u003eTo make communication efficient and direct, consider applying the following principles:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAs they aren't as familiar with the data themselves, meet all questions by the labeling team with patience and understanding. \u003c/li\u003e\u003cli\u003eMonitor and respond to your chosen communication channels regularly, this will help unblock the labeling team in a timely manner. \u003c/li\u003e\u003cli\u003eDefine how questions and feedback should be shared. For example, are screenshots or direct links to labels on the platform the most helpful?\u003c/li\u003e\u003cli\u003eBe sure to communicate the availability of all parties involved (time zone work hours, holidays, etc.).\u003c/li\u003e\u003cli\u003eBe mindful of any cultural differences. Labelers may be initially hesitant to ask you questions.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCommunication is always a two-way street – encourage labelers to reach out to subject-matter experts with any questions or concerns and be sure to respond to questions and share feedback throughout the duration of the labeling task. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow does effective communication support quality?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eImagine receiving a set of tasks, with ambiguous instructions, and having no way of following up to ask clarifying questions. It wouldn't be a huge surprise if you misinterpreted the instructions based on your assumptions and ended up getting every task wrong. In this case, it would be incredibly useful to receive specific feedback that could be used to correct mistakes and understand how to correctly carry out the task. \u003c/p\u003e\u003cp\u003eThis is where direct two-way communication can greatly help the labeling team gain assurance and learn more about your expectations. Understanding task instructions and correcting any labeling errors early on in the project's lifecycle can greatly help with labeling quality. Identifying any errors and misconceptions at the project's onset allows labelers to avoid making those same mistakes on future data rows. \u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eWhile outlining your quality strategy is essential to ensuring that your labeling task is producing high quality training data, you'll also want to make sure that you're able to scale up your labeling operations without sacrificing quality. \u003c/p\u003e\u003cp\u003eA key question for many ML teams is how to maintain consistency and quality as team size or data volume grows. To learn more about how to scale your labeling operations while maintaining quality, please refer to the next guide in this series: \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003ehow to scale up your labeling operations while maintaining quality\u003c/a\u003e. \u003c/p\u003e","comment_id":"63b594a0f94604003d092665","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2976--1-.png","featured":false,"visibility":"public","created_at":"2023-01-04T15:00:48.000+00:00","updated_at":"2024-05-06T21:11:57.000+00:00","published_at":"2023-01-12T21:31:04.000+00:00","custom_excerpt":"Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-create-a-quality-strategy-for-your-data-labeling-project","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-create-a-quality-strategy-for-your-data-labeling-project/","excerpt":"Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. ","reading_time":9,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2976--1--1.png","og_title":"How to create a quality strategy for your data labeling project","og_description":"Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2976--1--2.png","twitter_title":"How to create a quality strategy for your data labeling project","twitter_description":"Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. ","meta_title":"How to create a quality strategy for your data labeling project","meta_description":"Learn how to create a quality strategy to ensure your project is producing valuable, high-quality training data for your use case. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b48d65f94604003d0925bf","uuid":"5391783f-4c20-4647-9702-f1b9713933d6","title":"How to define your data labeling project's success criteria","slug":"how-to-define-your-data-labeling-projects-success-criteria","html":"\u003cp\u003eEvery ML project begins with the desired outcome of creating \"high-quality\" training data. But what does that really mean in practice and how exactly do you know when that has been achieved?\u003c/p\u003e\u003cp\u003eAnnotation requirements often evolve throughout the lifetime of an ML project, so it's necessary to define your success criteria at the outset of a project and be nimble enough to recognize when requirements may need to be adjusted as things evolve.\u003c/p\u003e\u003cp\u003eLeveraging a workflow that allows for \u003ca href=\"https://labelbox.com/guides/data-labeling/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003elabeling\u003c/a\u003e to be done in \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003esmall batches and that begins with a calibration phase\u003c/a\u003e can help clearly define \"high quality\" and ensure that quality is maintained.\u003c/p\u003e\u003cp\u003eIt's crucial that you take time at the beginning of a project to set expectations on labeling speed, acceptable errors, and determine how quality is defined in order to set you up for an easy to follow quality SLA. This will ensure that your labeling team fully understands the task at hand and the level of performance expected from them. While it may seem like a lot to think through at the onset of a labeling project, it will create mutual understanding that leads to a consistent output with high quality.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"timeline-and-scope-your-deadline-data-volume-and-the-average-time-per-label\"\u003eTimeline and scope: Your deadline, data volume, and the average time per label\u003c/h2\u003e\u003cp\u003eOne of the first steps in helping you get a handle on how you define success is understanding the scope and timeline of your project. Some questions to consider as you plan for high quality labels are:\u003c/p\u003e\u003cul\u003e\u003cli\u003eDoes this project have a definite volume with a deadline?\u003c/li\u003e\u003cli\u003eIs there no specific deadline, but a fixed volume?\u003c/li\u003e\u003cli\u003eDo you have a notion of how long you expect it to take to label each asset?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eRegardless of if there is a predetermined volume with a deadline or not, throughput should be part of your success criteria. Knowing your deadline and volume upfront can set expectations for your labeling team that they will need to complete 'x' amount of assets per day or week. If you have a fixed volume, but no deadline, this is when a \u003ca href=\"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/?ref=labelbox-guides.ghost.io\"\u003ecalibration phase\u003c/a\u003e comes in handy as an initial batch can be used to determine expected throughput. If you have a realistic expected time per asset, that can be used as well.\u003c/p\u003e\u003cp\u003eA project is considered successfully completed when it is finished in a timely manner or by a given deadline. Understanding labeling throughput and setting expectations allows you and your labeling team to have a mutual understanding of success.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"what-is-considered-good-quality-and-how-is-it-measured\"\u003eWhat is considered \"good quality\" and how is it measured? \u003c/h2\u003e\u003cp\u003eIt may be enticing to say that your project has been successful when labels have simply been deemed \"good\" and of \"high quality\". However, this needs to be defined further so that your labeling team knows what \"good\" looks like and how to achieve it.\u003c/p\u003e\u003cp\u003eIf your labeling project requires a simple binary label, it is easy to understand what is good (correct) and what is bad (incorrect) labeling. As the complexity of your labeling task grows, so does the need for assigning value to errors and stating specific requirements for what is \"good\".\u003c/p\u003e\u003cp\u003eAs the owner of your project and labels, you'll need to select the grading requirements for your project. In essence, you have to decide how each asset will be evaluated to have passed or failed in its initial labeling. For this, we suggest creating a definition of what causes an asset to be fully rejected.\u003c/p\u003e\u003cp\u003eFor example, let's say you have a computer vision project that requires multiple classes of bounding boxes per image. If 1 of 7 bounding boxes is mislabeled on an image, is this considered a grave enough error that results in the whole image being rejected? Or is it a lesser error where you would simply create an issue for correction, but not necessarily reject the entire image? Perhaps missing a bounding box entirely is a more serious error than a misclassification and that's a criteria for what leads to rejection.\u003c/p\u003e\u003cp\u003eIt is helpful to outline grave or fatal errors, as well as the lesser errors at the outset of a project or following your review during calibration. Defining these nuances will help your labeling team better understand your feedback and know where to focus their internal QA. We recommend making a list of these errors and keeping it handy while you review. Not only will this allow you to focus your review as well, but you can copy/paste these named errors in whatever feedback format you use to easily be able to track trends.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"slas-quality-speed-throughput\"\u003eSLAs (quality, speed, throughput)\u003c/h2\u003e\u003cp\u003eQuality SLAs (service-level agreements) with your labeling team are a bidirectional commitment that is mutually agreed upon. Quality SLAs are built on throughput and quality calculations.\u003c/p\u003e\u003cp\u003eShould you want to implement a quality SLA on a project, we recommend starting with a calibration phase to allow you to get a real world grasp on throughput (time per individual asset or amount of assets per week) and the types of errors that are surfaced when labeling is done by a team who is not intimately familiar with your project.\u003c/p\u003e\u003cp\u003eDetermining and setting a percentage for quality is at your discretion based on the type and amount of errors that you find during the first calibration phase. We suggest discussing this with the labeling team to ensure you're setting a threshold that is achievable given the complexity of your task.\u003c/p\u003e\u003cp\u003eWhen it comes to tracking whether or not a SLA is being met, throughput and quality are the two components you’ll be looking at:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eThroughput\u003c/strong\u003e can easily be gauged by ensuring the stated number of assets are in fact being completed on the cadence (daily, weekly, etc) agreed to.\u003c/li\u003e\u003cli\u003eAs for ensuring that the \u003cstrong\u003equality\u003c/strong\u003e necessary is being achieved, this will require your continued review of a sample throughout the lifespan of your project. This is when having a reusable list of error types comes in handy, while you review the assets that received serious errors will be considered rejected. Calculating quality can be as simple as dividing the number of assets rejected by the total number you reviewed. Having a clear way of understanding how many assets of a reviewed sample were rejected and why will help your labeling team understand if “good quality” is being achieved.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBelow you will find a table that outlines the milestones of implementing a quality SLA and assigns responsibility for each to a particular party. As stated, a well executed quality SLA is bidirectional and involves the continued involvement of the project owner, ensuring good quality labels is a shared responsibility.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.41.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1110\" height=\"1190\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-12-at-4.41.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-12-at-4.41.35-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.41.35-PM.png 1110w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eAfter defining and setting expectations on how quality is defined, you'll want to spend some time to develop a quality strategy. Your quality strategy will resemble how you will monitor and ensure that your project ends up with high quality data. This can include the use of automated quality monitoring features, manual review and feedback, regular two-way communication with your labeling team, and more. \u003c/p\u003e\u003cp\u003eLearn more about \u003ca href=\"https://labelbox.com/guides/how-to-create-a-quality-strategy-for-your-data-labeling-project/?ref=labelbox-guides.ghost.io\"\u003ehow to create a quality strategy for your data labeling project\u003c/a\u003e in our next guide.\u003c/p\u003e","comment_id":"63b48d65f94604003d0925bf","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2975--3-.png","featured":false,"visibility":"public","created_at":"2023-01-03T20:17:41.000+00:00","updated_at":"2024-05-06T21:10:56.000+00:00","published_at":"2023-01-12T21:30:59.000+00:00","custom_excerpt":"Learn how to effectively define your labeling project's success criteria so all tasks lead to consistent output with high quality. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-define-your-data-labeling-projects-success-criteria","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-define-your-data-labeling-projects-success-criteria/","excerpt":"Learn how to effectively define your labeling project's success criteria so all tasks lead to consistent output with high quality. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2975--3--2.png","og_title":"How to define your data labeling project's success criteria","og_description":"Learn how to effectively define your labeling project's success criteria so all tasks lead to consistent output with high quality. ","twitter_image":null,"twitter_title":"How to define your data labeling project's success criteria","twitter_description":"Learn how to effectively define your labeling project's success criteria so all tasks lead to consistent output with high quality. ","meta_title":"How to define your data labeling project's success criteria","meta_description":"Learn how to effectively define your labeling project's success criteria so all tasks lead to consistent output with high quality. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63b34450f94604003d092366","uuid":"32b97a0e-237d-4690-8906-19aa50b4a3fd","title":"How to define a task for your data labeling project","slug":"how-to-define-a-task-for-your-data-labeling-project","html":"\u003cp\u003eLarge volumes of high-quality training data are crucial to the success of any machine learning model. A labeling project is where you orchestrate and manage all of your labeling operations within Labelbox.\u003c/p\u003e\u003cp\u003eThe first step in the labeling process is to align on the key components of the labeling task within a project. This sets the tone of the project and allows Quantumworks Lab to make labeling more efficient down the line.\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"defining-the-data\"\u003eDefining the data\u003c/h2\u003e\u003ch3 id=\"data-type\"\u003eData type \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/annotate-one-platform--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"672\" height=\"512\"\u003e\u003c/figure\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/create-a-project?ref=labelbox-guides.ghost.io\"\u003eCreating a project\u003c/a\u003e and configuring your labeling task on the Quantumworks Lab platform begins alignment on which type of data needs to be labeled.\u003c/p\u003e\u003cp\u003eLabelbox supports the following \u003ca href=\"https://docs.labelbox.com/docs/datasets-datarows?ref=labelbox-guides.ghost.io#supported-data-types\"\u003edata types\u003c/a\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eImages\u003c/li\u003e\u003cli\u003eVideos\u003c/li\u003e\u003cli\u003eText\u003c/li\u003e\u003cli\u003eConversational text\u003c/li\u003e\u003cli\u003ePDF documents\u003c/li\u003e\u003cli\u003eGeospatial / Tiled imagery\u003c/li\u003e\u003cli\u003eAudio\u003c/li\u003e\u003cli\u003eHTML\u003c/li\u003e\u003cli\u003eDICOM (medical imagery)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBased on your chosen data modality, the Labeling Team Manager can leverage prior knowledge and the existing experience of different teams of labelers to work on your projects.\u003c/p\u003e\u003cp\u003eFor instance, some workforce teams have an excellent track record of successfully labeling projects with a specific type of imagery, while others have extended experience in working with the video editor or in specialized text projects. By curating labeling teams who are already well-versed in your use case and match your project needs, they can get started quicker with little friction.\u003c/p\u003e\u003ch3 id=\"industry-verticals\"\u003e\u003cstrong\u003eIndustry verticals\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/catalog-hero--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1011\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/catalog-hero--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/catalog-hero--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/catalog-hero--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/catalog-hero--1-.png 2089w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eSimilarly, our labeling partners are experienced in various industries such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eManufacturing\u003c/li\u003e\u003cli\u003eReal estate\u003c/li\u003e\u003cli\u003eFood service\u003c/li\u003e\u003cli\u003eAgrotech\u003c/li\u003e\u003cli\u003eHealthcare\u003c/li\u003e\u003cli\u003eInsurance\u003c/li\u003e\u003cli\u003eRetail\u003c/li\u003e\u003cli\u003eand many more\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLeveraging labelers who have experience in the industry of interest for your task should be discussed, as it allows the Labeling Team Manager to allocate the best workforce who meets your needs.\u003c/p\u003e\u003cp\u003eSome teams have large experience in aerial roof tagging for insurance companies, while others have been working on long-term microscopy pictures in the medical field, and many more variations of all kinds of tasks. Understanding the scope and frame of the task allows Quantumworks Lab to set your team up for success with the right workforce.\u003c/p\u003e\u003cp\u003eA workforce team who is well-versed in your industry or use case will need less time to get calibrated on your task. This means they'll be able to label more data in less time, which results in cheaper labeling costs as you only pay for the time when labeling screen-time occurs.\u003c/p\u003e\u003cp\u003eThere might be some use cases where general experience in a specific industry or data type might not be sufficient enough to meet your requirements. Quantumworks Lab also offers the option for you to onboard expert labelers for your project needs. You can learn more below under the \"Specific labeling requirements\" section.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"leveraging-labelboxs-suite-of-tools\"\u003eLeveraging Quantumworks Lab's suite of tools \u003c/h2\u003e\u003ch3 id=\"annotate\"\u003eAnnotate\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/annotate-header-image--3-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1170\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/annotate-header-image--3-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/annotate-header-image--3-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/annotate-header-image--3-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/annotate-header-image--3-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox's \u003ca href=\"https://labelbox.com/product/annotate/?ref=labelbox-guides.ghost.io\"\u003eAnnotate\u003c/a\u003e is designed to give you complete visibility and control over every aspect of your labeling operations across data modalities.\u003c/p\u003e\u003cp\u003eWhile setting up your labeling project, you'll need to acknowledge the supported file formats and annotation types in order to prevent issues down the line.\u003c/p\u003e\u003cp\u003eSimilarly, it is important to understand how to use Annotate to set up your project and labeling task, collaborate with your internal or external teams, and how to ensure that you're minimizing labeling time and spend. \u003c/p\u003e\u003cp\u003eFor instance, only one labeler can work per data row. If you have long videos to annotate, we might recommend splitting them into multiple files so more annotators can work on the data. Ultimately this will depend on your own speed and time requirements, however the Labeling Team Manager is available to work with you to determine what will work best for your team's use case.\u003c/p\u003e\u003cp\u003eYou can learn more about Annotate in our \u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"catalog\"\u003eCatalog\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/catalog-hero--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1250\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/catalog-hero--2-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/catalog-hero--2-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/catalog-hero--2-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2023/01/catalog-hero--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox's \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e is a data curation tool for you to organize, search, visualize, and explore your unstructured data. \u003c/p\u003e\u003cp\u003eUtilizing Catalog for data selection is a huge advantage in having a quality batch of data to label, according to specific parameters required for your task. You can leverage Catalog's features, such as \u003ca href=\"https://docs.labelbox.com/docs/search?ref=labelbox-guides.ghost.io\"\u003efilters\u003c/a\u003e, a one-click \u003ca href=\"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/?ref=labelbox-guides.ghost.io\"\u003esimilarity search\u003c/a\u003e, \u003ca href=\"https://docs.labelbox.com/docs/adding-metadata?ref=labelbox-guides.ghost.io\"\u003emetadata\u003c/a\u003e, and more, to ensure that the data you're queueing to your project is well-structured for your business requirements. \u003c/p\u003e\u003cp\u003eYou can learn more about Catalog in our \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"specific-labeling-requirements\"\u003eSpecific labeling requirements \u003c/h2\u003e\u003ch3 id=\"expert-workforce\"\u003eExpert workforce\u003c/h3\u003e\u003cp\u003eFor more specific and specialized tasks, Quantumworks Lab has the ability to onboard labelers who are qualified in particular domains:\u003c/p\u003e\u003cul\u003e\u003cli\u003eTechnical labelers with determined skills, such as people with software programming certifications\u003c/li\u003e\u003cli\u003eMedical labelers like nurses, clinicians, dermatologists, neurologists, and surgeons\u003c/li\u003e\u003cli\u003eLabelers fluent in one of 20+ languages covered by our partners\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYour Labeling Team Manager can source a specialist, based on the expertise needed, who can get started on your task. Since experts can take longer to source, it is essential to determine and request this requirement along with additional information needed prior to the start of labeling. \u003c/p\u003e\u003ch3 id=\"compliance\"\u003eCompliance\u003c/h3\u003e\u003cp\u003eAs Quantumworks Lab partners are spread out across different countries, it's important that geographical location is acknowledged and discussed so your business requirements are met. Depending on your discussed compliance and project needs, the Labeling Team Manager will ensure that the right workforce is onboarded accordingly. \u003c/p\u003e\u003cp\u003eLabelbox partners are compliant with the following certifications:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSOC2 Type I\u003c/li\u003e\u003cli\u003eSOC2 Type II\u003c/li\u003e\u003cli\u003eGDPR\u003c/li\u003e\u003cli\u003eHIPAA\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"labeling-forecast\"\u003eLabeling forecast  \u003c/h2\u003e\u003ch3 id=\"volume\"\u003eVolume\u003c/h3\u003e\u003cp\u003eDefining your data volume is a key element to consider when outlining your labeling task. This allows early expectations to be set in context of throughput and subsequently helps the Labeling Team Manager and the workforce to organize the labeling in the most efficient manner.\u003c/p\u003e\u003cp\u003eFor instance, the following aspects should be considered and discussed when defining your task:\u003c/p\u003e\u003cul\u003e\u003cli\u003eLarge volumes of data\u003c/li\u003e\u003cli\u003eLong-term projects\u003c/li\u003e\u003cli\u003eShort projects\u003c/li\u003e\u003cli\u003eTurnarounds\u003c/li\u003e\u003cli\u003eUploads frequency\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBased on the above, the Labeling Team Manager can allocate the task to the team most appropriate to meet the volume demands in terms of resources and availability.\u003c/p\u003e\u003ch3 id=\"timeline\"\u003eTimeline\u003c/h3\u003e\u003cp\u003eUnderstanding the timeline of your task is also crucial in effectively ramping up and scaling labeling activity. A rough outline of when you want the project to start and the target completion data helps define a structured labeling process and makes resource management easier. \u003c/p\u003e\u003cp\u003eA task that is set up for success will consider the following: \u003c/p\u003e\u003cul\u003e\u003cli\u003eHow many data rows do you plan to upload to your project? At which frequency?\u003c/li\u003e\u003cli\u003eWhat are the expectations in terms of speed?\u003c/li\u003e\u003cli\u003eDo you have a deadline?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eQuick turnarounds on high volumes of data and tight deadlines can be delicate to navigate, so planning ahead and understanding timelines in advance can help maximize the resources available and the workforce's time can be used efficiently. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"creating-an-ontology\"\u003eCreating an ontology\u003c/h2\u003e\u003cp\u003eAnother important aspect of a well-defined labeling task is the \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eontology\u003c/a\u003e. It needs to be built in a way that will work for the task at hand, and that follows the most logical workflow for a labeler. Ontologies and features should be created and managed with the goals of proper labeling, efficiency, and reusability in mind.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/2acf976-Screen_Shot_2021-12-09_at_12.28.01_PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1840\" height=\"1404\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/2acf976-Screen_Shot_2021-12-09_at_12.28.01_PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/2acf976-Screen_Shot_2021-12-09_at_12.28.01_PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2023/01/2acf976-Screen_Shot_2021-12-09_at_12.28.01_PM.png 1600w, https://labelbox-guides.ghost.io/content/images/2023/01/2acf976-Screen_Shot_2021-12-09_at_12.28.01_PM.png 1840w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"components\"\u003eComponents\u003c/h3\u003e\u003cp\u003eWithin an ontology, the three kinds of features are: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eObjects\u003c/strong\u003e (bounding boxes, polygons, segmentation masks, points, polylines, etc)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eClassifications\u003c/strong\u003e (radio, checklist, etc), that can be global or nested\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRelationships \u003c/strong\u003e(these are approached differently in Quantumworks Lab depending on the data type): With text data, you can define relationships between entity annotations as part of the objects. With image data, you can set relationship items in the ontology \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eA good ontology should define and answer the following: \u003c/p\u003e\u003cul\u003e\u003cli\u003eWhat should the labeling team be labeling? \u003c/li\u003e\u003cli\u003eHow should objects and/or classifications be labeled? \u003c/li\u003e\u003cli\u003eWhat additional information is helpful for your model? \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eReusing ontologies can be useful if you're planning on having multiple projects for the same or a very similar use case. Elements can be added to an existing ontology without affecting labels, so an ontology is not set in stone and you're encouraged to test and fine-tune your ontology. \u003c/p\u003e\u003cp\u003eYou can learn more about how to create and manage your ontologies in \u003ca href=\"https://labelbox.com/guides/how-to-create-and-manage-ontologies/?ref=labelbox-guides.ghost.io\"\u003ethis guide\u003c/a\u003e. \u003c/p\u003e\u003ch3 id=\"speed\"\u003eSpeed\u003c/h3\u003e\u003cp\u003eYou should choose tools that will allow labelers to label as fast as possible while maintaining the output needed for your model.\u003c/p\u003e\u003cp\u003eSample questions to consider when selecting tools would be: \u003c/p\u003e\u003cul\u003e\u003cli\u003eWould separate bounding boxes per class be better than one bounding box with a nested classification? \u003c/li\u003e\u003cli\u003eIs a segmentation mask necessary or would a polygon do? \u003c/li\u003e\u003cli\u003eDo you need every frame to be annotated? \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"quality\"\u003eQuality\u003c/h3\u003e\u003cp\u003eHigh-quality training data is critical to the success of your model. Ensure quality by designing a clear and efficient ontology that makes it easier for you to organize the output.\u003c/p\u003e\u003cp\u003eSample questions to consider include: \u003c/p\u003e\u003cul\u003e\u003cli\u003eDoes the ontology need to be extremely complex, or can you consider separate projects to label different things on the same data?\u003c/li\u003e\u003cli\u003eIs the free text field necessary? Avoid options that can lead to inconsistencies, typos and misspellings.\u003c/li\u003e\u003cli\u003eIs it best to skip or should there be an annotation denoting that the answer is unknown, that there was nothing to label? Do you want to understand why some assets do not meet the criteria, or is it fine to have a bucket of unlabeled assets?\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"creating-labeling-instructions\"\u003eCreating labeling instructions \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.10.49-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1450\" height=\"1208\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2023/01/Screen-Shot-2023-01-12-at-4.10.49-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2023/01/Screen-Shot-2023-01-12-at-4.10.49-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2023/01/Screen-Shot-2023-01-12-at-4.10.49-PM.png 1450w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOnce all the modalities of the task have been properly defined, you have to provide \u003ca href=\"https://docs.labelbox.com/docs/labeling-instructions?ref=labelbox-guides.ghost.io\"\u003elabeling instructions\u003c/a\u003e to the workforce. Even with an extremely simple ontology, it is necessary to offer additional information related to the labeling task. \u003c/p\u003e\u003cp\u003eLabeling instructions compliment the ontology and can be in the form of a document or a video demo. You can include anything that you deem useful and relevant to explaining the rules of your labeling task in a way that is easy for labelers to follow. \u003c/p\u003e\u003cp\u003eGood instructions will go into detail with specific examples and clearly lay out major labeling rules. Labeling instructions should provide context to the task, explain what the task entails, describe the labeling steps, and serve as a \"living document\".\u003c/p\u003e\u003cp\u003eInstructions can be altered depending on task progress and any changes in your requirements. Since changes can be made, it is advised for you to keep track of what made a label meet your success criteria so you can tailor the instructions to help the team understand important criteria of what makes a \"good label\". \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"defining-labeling-rules\"\u003eDefining labeling rules\u003c/h2\u003e\u003ch3 id=\"definition-of-all-ontology-items\"\u003eDefinition of all ontology items\u003c/h3\u003e\u003cp\u003eIt is important that you make sure to list and define the items that you want labeled. As mentioned in the \"Creating an ontology\" section above, you should be clear on the features and rules behind each expected annotation: \u003c/p\u003e\u003cul\u003e\u003cli\u003eObjects\u003c/li\u003e\u003cli\u003eClassifications\u003c/li\u003e\u003cli\u003eRelationships\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor example, if the project contains several entities/objects to be labeled with multiple classifications to choose from, explain each entity/object and each classification in sufficient detail: \u003c/p\u003e\u003cul\u003e\u003cli\u003eHow tight around the object does the bounding box need to be? \u003c/li\u003e\u003cli\u003eHow do labelers pinpoint a precise point on a blurry image with shapes that are not sharp? \u003c/li\u003e\u003cli\u003eIs there a maximum number of points a polygon can have before it becomes excessive for your model? \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhen defining your labeling rules, you should aim to: \u003c/p\u003e\u003cul\u003e\u003cli\u003eProvide clear definitions of all concepts for text projects to prevent ambiguities in the labeling\u003c/li\u003e\u003cli\u003eTry to format your instructions in a way that is easy to read, and make sure the golden rules pop out in a clear and distinguishable manner to minimize any chance that these could be missed\u003c/li\u003e\u003cli\u003eDefine your approach on a frame basis for video projects\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"step-by-step-labeling-workflow\"\u003eStep-by-step labeling workflow\u003c/h3\u003e\u003cp\u003eMake sure to describe each step in the labeling workflow so that your labelers are not lost in the ontology:\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor projects with multiple objects per asset, is there a specific order in which you need the annotations to be added?\u003c/li\u003e\u003cli\u003eDescribe your expectations for the review process\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"examples\"\u003eExamples\u003c/h3\u003e\u003cp\u003eThe best way to convey the results you want is by providing clear examples of the data to the labelers in the form of screenshots in your instructions. There are several approaches to this:\u003c/p\u003e\u003cul\u003e\u003cli\u003eProvide screenshots of unlabeled and labeled data for the labelers to have an overview of the assets they will be working on and what the outcome should be. Try to include several images that represent the variations of the full set.\u003c/li\u003e\u003cli\u003eClarify the variability of the data by sharing “edge cases” in your guidelines. An asset that would stand out from the rest of the set should be explained in detail, so the labelers know how to approach these types of cases.\u003c/li\u003e\u003cli\u003eInclude incorrectly labeled (negative) examples as well. This helps the team to identify mistakes to avoid. Common mistakes can be mentioned to prevent making them in this task.\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003eAll of these key components contribute to defining a task that is set for success on the Quantumworks Lab platform. These aspects also help the Labeling Team Manager ensure that your project outcomes are successful. \u003c/p\u003e\u003cp\u003eAfter the initial task setup, the next step in your labeling journey is to define your success criteria. Along with your volumes and deadlines, learn more about how to get a notion of the average time per label, describe how a \"good label\" is measured, and learn about SLAs in our next guide: How to define your data labeling project's success criteria. \u003c/p\u003e","comment_id":"63b34450f94604003d092366","feature_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2974--1-.png","featured":false,"visibility":"public","created_at":"2023-01-02T20:53:36.000+00:00","updated_at":"2024-10-02T22:27:59.000+00:00","published_at":"2023-01-12T21:30:52.000+00:00","custom_excerpt":"Learn how to align on key components of your project: define a task, create an ontology, and determine timelines for your labeling project.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"http://labelbox.com/guides/how-to-define-a-task-for-your-data-labeling-project","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-define-a-task-for-your-data-labeling-project/","excerpt":"Learn how to align on key components of your project: define a task, create an ontology, and determine timelines for your labeling project.","reading_time":9,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2974--1--2.png","og_title":"How to define a task for your labeling project","og_description":"A crucial first step in the labeling process is to align on key components of the labeling task. Learn how to define a task, create an ontology, and determine timelines for your labeling project.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2023/01/Group-2974--1--1.png","twitter_title":"How to define a task for your labeling project","twitter_description":"A crucial first step in the labeling process is to align on key components of the labeling task. Learn how to define a task, create an ontology, and determine timelines for your labeling project.","meta_title":"How to define a task for your labeling project","meta_description":"A key first step in the labeling process is to align on components of the labeling task. Learn how to define a task, create an ontology, and determine timelines for your labeling project. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"638e795d26221b003dd3cb95","uuid":"2d10380a-0764-4d34-b1b5-4b2e576b7be5","title":"How to find similar data in one click","slug":"how-to-find-similar-data-in-one-click","html":"\u003cp\u003eThe most successful training datasets are carefully visualized, curated, and debugged to increase model performance at each iteration.\u003c/p\u003e\u003cp\u003eML teams mine data by looking for all examples of rare assets or edge cases that will dramatically improve model performance. Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Building a similarity search engine that scales to hundreds of millions of data points and generates instant results is difficult for even the most advanced ML teams. \u003c/p\u003e\u003cp\u003eWith similarity search, you can easily query and explore your unstructured data and develop a holistic understanding of your training data. Plus, it helps break down silos across datasets, so teams can focus on curating and labeling the data that will dramatically improve model performance.\u003c/p\u003e\u003cp\u003eLabelbox provides a native similarity search engine, where you can leverage both off-the-shelf embeddings (for image, text, and documents)\u003cstrong\u003e \u003c/strong\u003eor upload your own custom embeddings to quickly find all instances of similar data.\u003c/p\u003e\u003ch2 id=\"how-to-conduct-a-similarity-search-query\"\u003eHow to conduct a similarity search query\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/mz9zvhw4mo\" title=\"Instantly find ​similar data in one click [Catalog] Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e1) Hover and click on the bottom right icon of any data row OR select all data rows of interest and click \"Similar to selection\" \u003c/p\u003e\u003cp\u003e2) This will automatically surface similar data rows – you can select multiple data rows as anchors to refine your similarity search \u003c/p\u003e\u003cp\u003e3) Combine similarity search with other filters and save these searches as \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e. This will allow you to revisit all current and incoming data rows that match the specific search criteria.\u003c/p\u003e\u003chr\u003e\u003cp\u003eWith Quantumworks Lab' similarity search, you can unlock the following workflows: \u003c/p\u003e\u003ch2 id=\"explore-visualize-and-understand-your-data-in-one-click\"\u003eExplore, visualize, and understand your data in one click\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/2022-11-28_08-44-21--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1756\" height=\"967\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eBefore you train your ML model, you can explore all of your data in Catalog\u003c/li\u003e\u003cli\u003eIn just a few clicks, you can surface all examples of data rows of interest, and either save them as a slice of data of interest, or send them to a labeling project as a batch\u003c/li\u003e\u003cli\u003eIn the example above, we can filter all images of a single flower from almost five million data rows in Catalog with just one click\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"quickly-mine-edge-cases-or-rare-examples\"\u003eQuickly mine edge cases or rare examples \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/2022-11-28_09-03-44--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1764\" height=\"975\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAfter training your model, you might find an edge case where your model is struggling\u003c/li\u003e\u003cli\u003eYou can use similarity search in Catalog to easily confirm whether this is a pattern of model failures, or simply a one-off mistake\u003c/li\u003e\u003cli\u003eIn the above example, the model appears to struggle with images with many flowers, so we can quickly mine edge cases to find all images containing many flowers\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"find-all-labeling-mistakes-in-your-project-and-send-them-to-re-labeling\"\u003eFind all labeling mistakes in your project and send them to re-labeling\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/bleh--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eWithin a labeling project, you might identify data rows with problems – such as labeling quality issues or mislabeled data\u003c/li\u003e\u003cli\u003eYou can leverage similarity search to find all similar labeled data (which might contain labeling errors and need additional review) and submit them to a specific review step \u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"select-even-more-high-impact-data-to-label\"\u003e\u003cbr\u003eSelect even more high-impact data to label \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/2022-11-28_09-18-49--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1764\" height=\"972\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eOnce you’ve identified data rows on which your model is struggling, you can find all similar unlabeled data in your datasets, label that data, and retrain the model to improve performance\u003c/li\u003e\u003cli\u003eIn this example, the model has low confidence with green bananas, so we used a similarity search and filter to show only unlabeled images of green bananas —\u0026nbsp;which can then be labeled and used to train the model\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"automatically-curate-data\"\u003e\u003cbr\u003eAutomatically curate data \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/My-Movie-47--4-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eBy saving your similarity search as a slice, any new incoming data point uploaded to Quantumworks Lab — and that matches the similarity search — will show up in the slice\u003c/li\u003e\u003cli\u003eWith data curation pipelines that update even when you're offline, you can continuously upload data from production, and data points that look similar to data of interest will show up in the corresponding slice\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"find-duplicate-data\"\u003eFind duplicate data \u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/2022-11-28_09-24-41--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1765\" height=\"964\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eEasily find all instances of duplicate data that you don't want to appear in your labeling project by leveraging similarity search\u003c/li\u003e\u003cli\u003eOnce you've found all similar duplicate data, you can save this search as a slice and this will automatically filter and all similar images, including past and incoming data that gets added to Catalog\u003c/li\u003e\u003cli\u003eYou can then take action on duplicate data, such as deleting them from your Quantumworks Lab instance\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"automatically-label-data-from-catalog\"\u003eAutomatically label data from Catalog\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox.ghost.io/blog/content/images/2022/11/My-Movie-47--2-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eLeverage similarity search and metadata to automatically identify and label data in bulk without sending data to a labeling project\u003c/li\u003e\u003cli\u003eSave search criteria for a cluster of similar data as a data slice, so that all new and old data that matches that criteria will automatically get added to that slice\u003c/li\u003e\u003cli\u003eYou can then select data rows of interest within the slice, or select the entire slice and tag these data rows with metadata\u003c/li\u003e\u003cli\u003eIn the above example, we surfaced a cluster of data rows containing green stamps using similarity search, selected the data rows of interest, and added a metadata tag called 'green stamps'\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003cp\u003eYou can learn more about \u003ca href=\"https://docs.labelbox.com/docs/similarity?ref=labelbox-guides.ghost.io\"\u003esimilarity search\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/slices?ref=labelbox-guides.ghost.io\"\u003eslices\u003c/a\u003e in our documentation. \u003c/p\u003e","comment_id":"638e795d26221b003dd3cb95","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2965.png","featured":false,"visibility":"public","created_at":"2022-12-05T23:06:05.000+00:00","updated_at":"2023-10-27T17:06:03.000+00:00","published_at":"2022-12-19T14:46:22.000+00:00","custom_excerpt":"Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Learn more about how to find similar data in one click with Labelbox. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-find-similar-data-in-one-click/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-find-similar-data-in-one-click/","excerpt":"Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Learn more about how to find similar data in one click with Labelbox. ","reading_time":4,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2965-2.png","og_title":"How to find similar data in one click","og_description":"Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Learn more about how to find similar data in one click with Labelbox. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2965-1.png","twitter_title":"How to find similar data in one click","twitter_description":"Powerful similarity search capabilities can give your team an edge by helping find specific data points in an ocean of data. Learn more about how to find similar data in one click with Labelbox. ","meta_title":"How to find similar data in one click | Quantumworks Lab","meta_description":"Find examples of edge cases that will improve model performance. Learn more about how to find similar data in one click with Labelbox. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6383ebc240fdcb003d956de5","uuid":"b5555a52-fdfc-4dcc-9521-39b7981ee85c","title":"How to train a chatbot","slug":"how-to-train-a-chatbot","html":"\u003cp\u003eThe rise in natural language processing (NLP) language models have given machine learning (ML) teams the opportunity to build custom, tailored experiences. Common use cases include improving customer support metrics, creating delightful customer experiences, and preserving brand identity and loyalty. \u003c/p\u003e\u003cp\u003eAs a result, companies are able to drive tangible business results such as reducing support costs by routing customer support chats to relevant channels or increasing the conversion rate of upsell opportunities through AI augmented sales conversations. As we’ve seen with the success of OpenAI's ChatGPT, we’ll likely continue to see AI powered language experiences penetrate all major industries.\u003c/p\u003e\u003ch3 id=\"building-a-domain-specific-chatbot-on-question-and-answer-data\"\u003eBuilding a domain-specific chatbot on question and answer data\u003c/h3\u003e\u003cp\u003eYou can harness the potential of the most powerful language models, such as ChatGPT, Gemini, Llama, etc., and tailor them to your unique business application. Domain-specific chatbots will need to be trained on quality annotated data that relates to your specific use case. \u003c/p\u003e\u003cp\u003eIn this guide, we’ll walk you through how you can use Quantumworks Lab to create and train a chatbot. For the particular use case below, we wanted to train our chatbot to identify and answer specific customer questions with the appropriate answer. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"step-1-gather-and-label-data-needed-to-build-a-chatbot\"\u003eStep 1: Gather and label data needed to build a chatbot\u003c/h2\u003e\u003cp\u003eAfter gathering your data, the first step will be to identify the main components that are needed to build your chatbot. In this case, the two main components are: questions and answers.\u003c/p\u003e\u003cp\u003eOnce you’ve identified the data that you want to label and have determined the components, you’ll need to create an ontology and label your data.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/My-Movie-47--8-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/My-Movie-47--8-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/My-Movie-47--8-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/My-Movie-47--8-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/My-Movie-47--8-.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eDepending on the amount of data you're labeling, this step can be particularly challenging and time consuming. However, it can be drastically sped up with the use of a labeling service, such as \u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox labeling services\u003c/u\u003e\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eWe used Quantumworks Lab labeling services to annotate our examples of conversations. The labeling workforce annotated whether the message is a question or an answer as well as classified intent tags for each pair of questions and answers. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-28-at-4.57.41-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1078\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-28-at-4.57.41-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-28-at-4.57.41-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Screen-Shot-2022-11-28-at-4.57.41-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2022/11/Screen-Shot-2022-11-28-at-4.57.41-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eNow, we have a group of intents and the aim of our chatbot will be to receive a message and figure out what the intent behind it is. \u003c/p\u003e\u003cp\u003eIf a chatbot is trained on unsupervised ML, it may misclassify intent and can end up saying things that don’t make sense. Since we are working with annotated datasets, we are hardcoding the output, so we can ensure that our NLP chatbot is always replying with a sensible response. For all unexpected scenarios, you can have an intent that says something along the lines of “I don’t understand, please try again”.\u003c/p\u003e\u003ch2 id=\"step-2-download-and-import-modules\"\u003eStep 2: Download and import modules\u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport nltk\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer = LancasterStemmer()\nnltk.download ('punkt')\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nimport tflearn\nimport tensorflow as tf\nimport random\nimport json\nimport urllib3\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSince this is a python tutorial for building a chatbot, we’ll be using a python notebook as well as the following:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003ca href=\"https://www.nltk.org/?ref=labelbox-guides.ghost.io\"\u003eNLTK\u003c/a\u003e (Natural Language Toolkit) - considered the Swiss-knife of NLP, this will help us trim down our words and help with some pre-processing steps\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://numpy.org/?ref=labelbox-guides.ghost.io\"\u003eNumPy\u003c/a\u003e - for array management \u003c/li\u003e\u003cli\u003e\u003ca href=\"http://tflearn.org/?ref=labelbox-guides.ghost.io\"\u003eTFLearn\u003c/a\u003e and \u003ca href=\"https://www.tensorflow.org/?ref=labelbox-guides.ghost.io\"\u003eTensorFlow\u003c/a\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003cstrong\u003eModules for data:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWe’ll need our data as well as the annotations exported from Quantumworks Lab in a JSON file. \u003c/p\u003e\u003cp\u003e\u003cem\u003eUrllib3\u003c/em\u003e - this is relevant when working with Quantumworks Lab, we accept a URL of your file so your data can live in your own cloud storage. In the below code snippet, the URL refers to our data: \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e[ ] http = urllib3.PoolManager()\n\tr = http.request ('GET', url)\n    data = json.loads (r.data.decode('utf-8'))\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOnce the data has been imported, you can start playing around with it. We recommend printing your data to confirm that you’ve imported it correctly.\u003c/p\u003e\u003cp\u003eThe next step in building our chatbot will be to loop in the data by creating lists for intents, questions, and their answers.\u003c/p\u003e\u003ch2 id=\"step-3-pre-processing-the-data\"\u003eStep 3: Pre-processing the data\u003c/h2\u003e\u003cp\u003eWe need to pre-process the data in order to reduce the size of vocabulary and to allow the model to read the data faster and more efficiently. This allows the model to get to the meaningful words faster and in turn will lead to more accurate predictions.\u003c/p\u003e\u003cp\u003eIn addition to tokenization and stemming (discussed below), we’ll need to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRemove punctuation\u003c/li\u003e\u003cli\u003eTransform all of our text to lowercase\u003c/li\u003e\u003cli\u003eRemove all duplicates\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"step-4-tokenization\"\u003eStep 4: Tokenization\u003c/h2\u003e\u003cp\u003eTokenization is the process of dividing text into a set of meaningful pieces, such as words or letters, and these pieces are called tokens. A token is essentially the smallest meaningful unit of your data. This is an important step in building a chatbot as it ensures that the chatbot is able to recognize meaningful tokens.\u003c/p\u003e\u003cp\u003eThe first thing we’ll need to do in order to get our data ready to be ingested into the model is to tokenize this data.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e[ ] for intent in data[\"intents\"]:\n\t\tfor question in intent[\"questions\"]:\n          tokens = nltk.word_tokenize(question)\n          words.extend(tokens)\n          docs_questions.append(tokens)\n          docs_intents.append(intent['tag'])\n          \n        if intent[\"tag\"] not in labels:\n        \tlabels.append(intent[\"tag\"])\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-5-stemming\"\u003e\u003cbr\u003eStep 5: Stemming\u003c/h2\u003e\u003cp\u003eStemming is a process where words are reduced to a root by removing inflection through dropping unnecessary characters.\u003c/p\u003e\u003cp\u003eFor example, reducing words with suffixes such as ‘smarter’, ‘smartest’, etc. to their stem, which is simply ‘smart’.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003ewords = [stemmer.stem(token.lower()) for token in words if token != ['\u0026gt;', '\u0026lt;', '\\\\', ':', '-', ',', '#','[' , ']', '/', '//', '_', '(', ')']]\n                                                                     \nwords = sorted(list(set(words)))\nlabels = sorted(labels)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-6-set-up-training-and-test-the-output\"\u003e\u003cbr\u003eStep 6: Set up training and test the output \u003c/h2\u003e\u003cp\u003eSo far, we’ve successfully pre-processed the data and have defined lists of intents, questions, and answers. \u003c/p\u003e\u003cp\u003eHowever, these are ‘strings’ and in order for a neural network model to be able to ingest this data, we have to convert them into \u003ca href=\"https://numpy.org/doc/stable/user/absolute_beginners.html?ref=labelbox-guides.ghost.io#what-is-an-array\"\u003enumPy arrays\u003c/a\u003e. In order to do this, we will create \u003ca href=\"https://scikit-learn.org/stable/modules/feature_extraction.html?ref=labelbox-guides.ghost.io#the-bag-of-words-representation\"\u003ebag-of-words\u003c/a\u003e (BoW) and convert those into numPy arrays.\u003c/p\u003e\u003ch2 id=\"step-7-create-a-bag-of-words-bow\"\u003eStep 7: Create a bag-of-words (BoW)\u003c/h2\u003e\u003cp\u003eA bag-of-words are one-hot encoded (categorical representations of binary vectors) and are extracted features from text for use in modeling. They serve as an excellent vector representation input into our neural network. \u003c/p\u003e\u003cp\u003eFor our chatbot and use case, the bag-of-words will be used to help the model determine whether the words asked by the user are present in our dataset or not.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-28-at-5.38.15-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"548\" height=\"298\"\u003e\u003c/figure\u003e\u003cp\u003eTo create a bag-of-words, simply append a 1 to an already existent list of 0s, where there are as many 0s as there are intents.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e[ ] for intent in data[\"intents\"]:\n\t\tfor question in intent[\"questions\"]:\n          tokens = nltk.word_tokenize(question)\n          words.extend(tokens)\n          docs_questions.append(tokens)\n          docs_intents.append(intent['tag'])\n          \n        if intent[\"tag\"] not in labels:\n        \tlabels.append(intent[\"tag\"])\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-8-convert-bows-into-numpy-arrays\"\u003eStep 8: Convert BoWs into numPy arrays \u003c/h2\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003einput = np.array(input)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAfter creating a bag-of-words, the array should look like the below:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003ebag_of_words(\"Hey how are you\", words)\n\narray([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eWe recommend storing the pre-processed lists and/or numPy arrays into a pickle file so that you don’t have to run the pre-processing pipeline every time.\u003c/p\u003e\u003ch2 id=\"step-9-build-the-model-for-the-chatbot\"\u003eStep 9: Build the model for the chatbot \u003c/h2\u003e\u003cp\u003eAfter the bag-of-words have been converted into numPy arrays, they are ready to be ingested by the model and the next step will be to start building the model that will be used as the basis for the chatbot. \u003c/p\u003e\u003cp\u003eSince this is a classification task, where we will assign a class (intent) to any given input, a neural network model of two hidden layers is sufficient. \u003c/p\u003e\u003cp\u003eFor this step, we’ll be using \u003ca href=\"http://tflearn.org/?ref=labelbox-guides.ghost.io\"\u003eTFLearn\u003c/a\u003e and will start by resetting the default graph data to get rid of the previous graph settings.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e#resetting default settings\ntf.compat.v1.reset_default_graph()\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eWe can then proceed with defining the input shape for our model. For our use case, we can set the length of training as ‘0’, because each training input will be the same length. The below code snippet tells the model to expect a certain length on input arrays. \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003enet = tflearn.input_data(shape=[None, len(training[0])])\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe next step will be to define the hidden layers of our neural network. The below code snippet allows us to add two fully connected hidden layers, each with 8 neurons. \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003enet_h1 = tflearn.fully_connected(net, 8)\nnet_h2 = tflearn.fully_connected(net, 8)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSimilar to the input hidden layers, we will need to define our output layer. We’ll use the softmax activation function, which allows us to extract probabilities for each output. Lastly, we’ll apply regression to our neural network. \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003enet = tflearn.fully_connected(net, len(output[0], activation=\"softmax\")\nnet = tflearn.regression(net)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAfter these steps have been completed, we are finally ready to build our deep neural network model by calling ‘tflearn.DNN’ on our neural network. \u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003emodel = tflearn.DNN(net)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-10-model-fitting-for-the-chatbot\"\u003e\u003cbr\u003eStep 10: Model fitting for the chatbot\u003c/h2\u003e\u003cp\u003eModel fitting is the calculation of how well a model generalizes data on which it hasn’t been trained on. A well-fitted model is able to more accurately predict outcomes. This is an important step as your customers may ask your NLP chatbot questions in different ways that it has not been trained on.\u003c/p\u003e\u003cp\u003eOnce our model is built, we’re ready to pass it our training data by calling ‘the.fit()’ function. The ‘n_epochs’ represents how many times the model is going to see our data. In this case, our epoch is 1000, so our model will look at our data 1000 times. \u003c/p\u003e\u003cp\u003eWhen our model is done going through all of the epochs, it will output an accuracy score.\u003c/p\u003e\u003ch2 id=\"step-11-model-predictions-for-the-chatbot\"\u003eStep 11: Model predictions for the chatbot\u003c/h2\u003e\u003cp\u003eSince our model was trained on a bag-of-words, it is expecting a bag-of-words as the input from the user. \u003c/p\u003e\u003cp\u003eIn order for us to train our model to make predictions on new data, questions that a customer might ask will have to be converted to B.O.Ws, we’ll need to create a function that will allow us to convert incoming questions into bag-of-words.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef bag_of_words(sentence, words):\n\tbag = np.zeros(len(words))\n    sentence_words = nltk.word_tokenize(sentence)\n    sentence_words = [stemmer.stem(wod.lower)) for word in sentence_words]\n    \n\tfor sw in sentence_words:\n    \tfor i,word in enumerate(words):\n        \tif word==sw:\n            \tbag[i] +=1\n              \n   \treturn np.array(bag)\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-12-create-a-chat-function-for-the-chatbot\"\u003eStep 12: Create a chat function for the chatbot \u003c/h2\u003e\u003cp\u003eThe next step will be to create a chat function that allows the user to interact with our chatbot. We’ll likely want to include an initial message alongside instructions to exit the chat when they are done with the chatbot. \u003c/p\u003e\u003cp\u003eThe user can simply ask questions by using the input() method as follows:\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003einput = input(\"You: \")\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-13-classifying-incoming-questions-for-the-chatbot\"\u003e\u003cbr\u003eStep 13: Classifying incoming questions for the chatbot\u003c/h2\u003e\u003cp\u003eWhen a customer asks our chatbot a question, the input from the user will be converted into a bag-of-words using the above function (in step 10) and will run through the model using ‘model.predict()’, where the model will then try to classify the input into the various classes (intent) by predicting their probabilities. \u003c/p\u003e\u003cp\u003eThe arg max function will then locate the highest probability intent and choose a response from that class.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eresults_index = np.argmax(results) \u003c/code\u003e\u003c/pre\u003e\u003cp\u003eA safe measure is to always define a confidence threshold for cases where the input from the user is out of vocabulary (OOV) for the chatbot. In this case, if the chatbot comes across vocabulary that is not in its vocabulary, it will respond with “I don’t quite understand. Try again or ask a different question”.\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e#adding a confidence threshold \n\tif results[results_index] \u0026gt; 0.7:\n    \tprint(random.choice(responses))\n    else:\n    \tprint(\"I don't quite understand. Try again or ask a different question.\")\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"step-14-customize-your-chatbot\"\u003e\u003cbr\u003eStep 14: Customize your chatbot\u003c/h2\u003e\u003cp\u003eYou can customize your chatbot to make it specific to your use case or business needs. \u003c/p\u003e\u003cp\u003eIn order to do so, you’ll need to:\u003c/p\u003e\u003col\u003e\u003cli\u003eCreate your own domain specific dataset\u003c/li\u003e\u003cli\u003eCreate an ontology that would be your tags\u003c/li\u003e\u003cli\u003eHave your data labeled by AI experts with \u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox labeling services\u003c/u\u003e\u003c/a\u003e,\u0026nbsp; \u003ca href=\"https://labelbox.com/services/alignerr-connect/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox Alignerr Connect\u003c/u\u003e\u003c/a\u003e, or an internal labeling team using the \u003ca href=\"https://labelbox.com/product/platform/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox Platform\u003c/u\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eYou can now reference the tags to specific questions and answers in your data and train the model to use those tags to narrow down the best response to a user’s question.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"get-started-today\"\u003e\u003cstrong\u003eGet started today\u0026nbsp;\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eDomain specific chatbots can be tailored from powerful language models for your specific use case or unique business application.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTo start training your own AI chat bot, check out the solutions Quantumworks Lab offers here:\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eExplore our on-demand labeling services: \u003c/u\u003e\u003c/a\u003eUtilize and build your own team of SMEs to create specialized, high-quality conversational data.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eTry out the conversational text editor for free:\u003c/u\u003e\u003c/a\u003e Ready to experiment with building your own chatbots? Easily sign up and get started.\u0026nbsp;\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-annotate-conversational-text-for-chatbot-use-cases/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLearn more:\u003c/u\u003e\u003c/a\u003e See in action how to annotate conversational text for chatbot use cases. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’re happy to help answer any questions. Reach out to us anytime on our \u003ca href=\"https://labelbox.com/sales?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003econtact us\u003c/u\u003e\u003c/a\u003e page.\u003c/p\u003e","comment_id":"6383ebc240fdcb003d956de5","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2962--1-.png","featured":false,"visibility":"public","created_at":"2022-11-27T22:59:14.000+00:00","updated_at":"2024-11-27T02:59:54.000+00:00","published_at":"2022-12-14T23:11:39.000+00:00","custom_excerpt":"The rise in natural language processing (NLP) language models have given machine learning (ML) teams the opportunity to build custom, tailored experiences for their customers. Learn how to train a domain-specific chatbot.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-train-a-chatbot/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-train-a-chatbot/","excerpt":"The rise in natural language processing (NLP) language models have given machine learning (ML) teams the opportunity to build custom, tailored experiences for their customers. Learn how to train a domain-specific chatbot.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":"How to train a chatbot","og_description":"Learn how to train a domain-specific chatbot to create custom, tailored experiences for your customers. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2962--1--1.png","twitter_title":"How to train a chatbot","twitter_description":"Learn how to train a domain-specific chatbot to create custom, tailored experiences for your customers. ","meta_title":"How to train a chatbot | Quantumworks Lab","meta_description":"Learn how to train a domain-specific chatbot to create custom, tailored experiences for your customers. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6373f09e17f6c9003d7b0dbc","uuid":"157143d6-16c6-451c-83ea-73c8d192eb26","title":"Get started with active learning","slug":"the-guide-to-getting-started-with-active-learning","html":"\u003ch2 id=\"what-is-active-learning\"\u003eWhat is active learning?\u003c/h2\u003e\u003cp\u003eMachine learning (ML) teams working with large volumes of unstructured data in order to generate \u003ca href=\"https://labelbox.com/blog/what-does-it-mean-to-make-high-quality-training-data/?ref=labelbox-guides.ghost.io\"\u003ehigh-quality training data\u003c/a\u003e typically encounter a critical question: If not all my data is created equal, among all of my unlabeled data, what should I label in priority? Active learning in machine learning is the process of answering this very question, by identifying what data will most dramatically improve model performance and feeding these insights into the prioritization of data for labeling.\u003c/p\u003e\u003ch2 id=\"why-is-active-learning-important-in-machine-learning\"\u003eWhy is active learning important in machine learning?\u003c/h2\u003e\u003ch3 id=\"1-avoid-overspending\"\u003e#1: Avoid overspending\u003c/h3\u003e\u003cp\u003eYour goal should be to label as little data as possible by focusing your data labeling and data debugging efforts on the data that will most dramatically improve model performance. By doing so, you'll be able to save significant time and resources upfront. Active learning in machine learning helps with this prioritization.\u003c/p\u003e\u003ch3 id=\"2-better-prioritization\"\u003e#2: Better prioritization\u003c/h3\u003e\u003cp\u003eWhen ML teams possess a wealth of unlabeled data and cannot label everything at once, the best option is to select a fraction of your assets to label, and choose from the highest impact assets first.\u003c/p\u003e\u003ch3 id=\"3-boost-model-performance\"\u003e#3: Boost model performance\u003c/h3\u003e\u003cp\u003eIt turns out that being very intentional about what to label next is one of the best ways to improve the performance of your machine learning models. This is evidenced by other leading AI companies such as Tesla, who have stated that, “\u003cem\u003ein academia, we often see that people keep data constant, but at Tesla it’s very much the opposite. We see time and again that data is one of the best, if not the most deterministic levers to solving these [challenging cases].” - Source: \u003ca href=\"https://youtu.be/ODSJsviD_SU?t=6921\u0026ref=labelbox-guides.ghost.io\"\u003e\u003cem\u003eTesla AI Day 2022\u003c/em\u003e\u003c/a\u003e \u003c/em\u003e\u003c/p\u003e\u003cp\u003eIn addition, leading AGI research organizations such as OpenAI have also utilized active learning to better train, deploy, and maintain popular models such as GPT-3 and DALL-E 2. To train DALL-E 2, they utilized two different active learning techniques to iterate on the image classifiers: one to find and fix false positives, and another to find and fix false negatives. The organization has published a \u003ca href=\"https://openai.com/blog/dall-e-2-pre-training-mitigations/?ref=labelbox-guides.ghost.io\"\u003eblog post\u003c/a\u003e about the active learning techniques used to modify and process the data used to train DALL-E 2 to ensure that no violent or lewd images are included in the training dataset. Their active learning process used classifiers to select a handful of unlabeled images that are likely to improve classifier performance. Finally, humans produced labels for these images, adding them to the labeled dataset. The process was repeated to iteratively improve the classifier’s performance.\u003c/p\u003e\u003cp\u003eThe ML engineering team at Doordash also \u003ca href=\"https://doordash.engineering/2020/08/28/overcome-the-cold-start-problem-in-menu-item-tagging/?ref=labelbox-guides.ghost.io\"\u003eshared\u003c/a\u003e how they utilized active learning techniques to select low-confidence samples and then built a human-in-the-loop workflow for better menu item labeling. By leveraging active learning based on low confidence predictions, they were able to utilize similarity search and fix model performance on low performing slices of data.\u003c/p\u003e\u003cp\u003eBuilding off these examples, the goal of your AI team and your labeling team should be to identify the unique assets that will boost model performance the most, and to focus all of your labeling time, efforts and budget on these high impact assets. The benefits of this data-centric approach yields higher model performance – allowing teams to rapidly ship models to production, continuously improve models once in production, and effortlessly accumulate competitive advantages faster than ever before.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/wjFWwmp5eggt1gSYX0yLfZgq2suwhXkJCrRyh7IhxFcwAub5VjACL2-pOJXPBM9cxPBJLYhNyDIyw6JxYPqjxw4qjKrWmeBu4PW5JsqBHR3fi4Sl3VzwMTWfcwOTkdss-cdbqMwm6aeEiAacgLq5h8rZkhxRyK6iV3prOJlBSbvAKI1XApYTp6Cjfscjvw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"321\" height=\"247\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAn illustrative Active Learning workflow and the key steps associated with the process.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"how-active-learning-unlocks-value-for-all-ml-teams\"\u003eHow active learning unlocks value for all ML teams\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eWithin the model development lifecycle, there are generally two phases of iterative and data-centric performance improvements. Although both encounter a set of different challenges, active learning is highly relevant for both stages:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eModels in development: \u003c/strong\u003ethese models are not yet deployed into production or accepting live / new ingested data. However, ML teams continuously evaluate and improve these models, trying to reach the performance metrics required to get them into production as quickly and as confidently as possible. Active learning will help you select the right data to include in your training sets so you can reach desired model performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eModels deployed in production: T\u003c/strong\u003ehese models have either met or exceeded the initial minimum viable performance levels required for them to be deployed into production. At this point, they are generally accepting live / newly-ingested data in the “real world.”\u003c/p\u003e\u003cp\u003eEven after a model has been deployed to production, ML teams still need to keep improving their models for two key reasons:\u003c/p\u003e\u003cp\u003e1. Improving their product/service relies upon continuously improving their model\u003c/p\u003e\u003cp\u003e2. The data from production changes over time; hence model performance tends to decrease over time if no steps are taken to tune the model based on these changes\u003cstrong\u003e.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eActive learning will help you select what to label, among all this production data, in order to keep improving model performance.\u003c/p\u003e\n\u003c!--kg-card-begin: html--\u003e\n\u003ctable style=\"border:none;border-collapse:collapse;\"\u003e\u003ccolgroup\u003e\u003ccol width=\"98\"\u003e\u003ccol width=\"267\"\u003e\u003ccol width=\"298\"\u003e\u003c/colgroup\u003e\u003ctbody\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#262626;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eModels in development\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#262626;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eModels deployed in production\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:14.14892578125pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #0b5394 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;background-color:#0b5394;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #0b5394 1pt;border-right:solid #0b5394 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;background-color:#0b5394;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #0b5394 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;background-color:#0b5394;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cbr\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:49.19677734375pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#262626;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eFor ML teams working with:\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eLabeled training data (train/validation/test splits)\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eUnlabeled data, from production,\u003c/span\u003e\u003c/p\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003ewith model predictions \u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr style=\"height:0pt\"\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#262626;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eKey questions:\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAmong the pool of unlabeled data sitting unused in my company, \u003c/span\u003e\u003cspan style=\"font-size:11pt;font-family:'IBM Plex Sans',sans-serif;color:#1c4587;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003ewhat should I add to my training data to boost model performance?\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003ctd style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"\u003e\u003cp dir=\"ltr\" style=\"line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;\"\u003e\u003cspan style=\"font-size:10.5pt;font-family:'IBM Plex Sans',sans-serif;color:#262626;background-color:transparent;font-weight:400;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003eAmong this constant stream of unlabeled data with model predictions, coming from production, \u003c/span\u003e\u003cspan style=\"font-size:10.5pt;font-family:'IBM Plex Sans',sans-serif;color:#1c4587;background-color:transparent;font-weight:700;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"\u003ewhat is the high impact data that I should incorporate into my training data to update and keep improving model performance? \u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003ch2 id=\"key-challenges-to-overcome-when-leveraging-active-learning\"\u003eKey challenges to overcome when leveraging active learning\u003c/h2\u003e\u003cp\u003eEven with an organized data lake, selecting the most useful data can be a complex challenge. \u003ca href=\"https://labelbox.com/blog/how-to-improve-model-performance-with-less-data/?ref=labelbox-guides.ghost.io\"\u003eTo improve model performance\u003c/a\u003e, you don’t need volumes of just any data – you need the most relevant data. Filtering out the value from the noise becomes even more challenging and time consuming when deployed models confront the continuously evolving landscape of real data.\u003cbr\u003e\u003cbr\u003eHowever, implementing this data-centric strategy poses a challenge as it requires ML teams to have purpose-built tools with the ability to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eVisualize assets, model predictions, and ground truths alongside each other. Identify model predictions.\u003c/li\u003e\u003cli\u003eUnderstand where models are performing well and struggling. Evaluate the performance of a model on the training data splits, on incoming production data, and on any data slice.\u003c/li\u003e\u003cli\u003eUnderstand the distribution of your data.\u003c/li\u003e\u003cli\u003eAnalyze model confidence scores.\u003c/li\u003e\u003cli\u003eEnsure that every ML experiment is reproducible.\u003c/li\u003e\u003cli\u003eCompare models against each other, as you go through data-centric iterations. The goal is to track whether models are actually making progress with each successive iteration.\u003c/li\u003e\u003cli\u003eTrack model performance over time as performance requirements and real ground truth conditions change.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe complexity compounds because the workflows typically need to be continuously repeated in order to meet the demands of surging capacity to address real world changes or data conditions that suddenly and significantly reduce model performance. Fortunately, dedicated tooling now exists to support these critical active learning workflows as we will introduce later in this post. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-16-at-1.24.07-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1466\" height=\"534\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-16-at-1.24.07-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-16-at-1.24.07-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-16-at-1.24.07-PM.png 1466w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAn example Active Learning workflow that depicts how ML teams can identify and execute targeted improvements to your data and subsequently, model performance.\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"current-active-learning-strategies\"\u003eCurrent active learning strategies\u003c/h2\u003e\u003cp\u003e\u003cbr\u003eThe three dominant active learning strategies we’ve encountered in the field can be grouped into broad categories:\u003c/p\u003e\u003cul\u003e\u003cli\u003eActive learning that leverages your data distribution\u003c/li\u003e\u003cli\u003eActive learning that leverages your model predictions\u003c/li\u003e\u003cli\u003eActive learning that leverages model error analysis\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"active-learning-strategies-that-leverage-your-data-distribution\"\u003eActive learning strategies that leverage your data distribution\u003c/h3\u003e\u003cp\u003eActive learning techniques of this type can be applied even before you have labeled data or a trained a ML model, as the primary focus of these techniques is to analyze your data distribution.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003ePrioritize diverse data\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eSelecting diverse data is one of the critical steps to follow before you begin training your model. The process focuses on picking a small, diverse subset of data, e.g. by using random sampling, or e.g. by clustering your unlabeled data and sampling from each cluster.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh6.googleusercontent.com/3zHUw2hm3BhK33iyPMUft8COSIh34O5ZmLVM9C6mpKp0XCfik-HJLhkzZOeJsoCy1zaMqrzd8zwtbsjoMexKZg2exBhJZt3atSG-Ta4chCv61KBOwOJbCOedLkfW7_TT1ANZKesmoSpKjQvEAmBGoNO_WTHh0fPkPcjdzV8SCvnBj8UeMcAoY5BA9T35JU5L\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"464\" height=\"304\"\u003e\u003c/figure\u003e\u003cp\u003e2.\u003cstrong\u003e Prioritize out-of-distribution data\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eBased on the nature of continuous change in the real world, once models are delivered and deployed, they are often exposed to data samples they were not trained on. As a function of time and volume, the model will make predictions on data with increasingly more varied characteristics that often outpace the data diversity of the initial training data set. \u003c/p\u003e\u003cp\u003eIn essence, once deployed, the model becomes stale and outdated quickly –— posing inherent challenges for improving, let alone maintaining machine learning models. \u003c/p\u003e\u003cp\u003eActive learning workflows alert ML teams to indications of change or differences in what’s happening lately compared to what had been happening before.\u003c/p\u003e\u003cp\u003eFor example, using \u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\"\u003eimage segmentation\u003c/a\u003e to identify geographic features after a natural disaster may be a much more nuanced challenge to the model than it was pre-disaster. As a natural language processing (NLP) example, the model can report that it is seeing many more instances of deceptive fake news articles than it has previously encountered during past model runs on real / live data.  \u003c/p\u003e\u003cp\u003eA model reporting a sharp change in sentiment analysis can alert end users to new service disruptions experienced by customers. Active learning helps not only to identify these changes, but also expedite retraining on areas that are outside the training distribution.\u003c/p\u003e\u003cp\u003eML Teams will have to confront this data drift – when the underlying data distribution changes over time. If the model was engaged within an Active Learning workflow, the model would:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAlert the ML team that it is encountering data with new qualities than previously observed.\u003c/li\u003e\u003cli\u003eTriage and identify data that is outside the training distribution and cue labeling for data samples that exhibit similar characteristics.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Out-of-distribution-data.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Out-of-distribution-data.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Out-of-distribution-data.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/12/Out-of-distribution-data.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/12/Out-of-distribution-data.gif 1920w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"active-learning-strategies-that-leverage-your-model-predictions\"\u003eActive learning strategies that leverage your model predictions\u003c/h3\u003e\u003cp\u003eOnce you have a trained model, you can use it as a guide to select high impact data that will boost model performance. The following active learning techniques can be applied to your unlabeled data, as long as you generate model predictions on them. This is a typical setting during model development (with your unlabeled data that sits unused in a data bucket) and once your model is in production (with real data that your model is being confronted with in production).\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003ePrioritize low confidence predictions (uncertainty sampling)\u003c/strong\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eActive learning workflows of this type quickly identify and triage areas where model confidence is low. For models that have already been deployed and spent time confronting real world data, active learning identifies areas where model confidence has declined based on a variety of factors, such as shifts in the data itself. \u003c/p\u003e\u003cp\u003eActive learning workflows select the data necessary to boost model performance among these areas of uncertainty as well as track the impact of each retraining on model performance, across all data splits, data slices and model classes.\u003cbr\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-15-at-12.22.35-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"972\" height=\"568\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-15-at-12.22.35-PM.png 600w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-15-at-12.22.35-PM.png 972w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e2. Prioritize rare predictions\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003eWe've observed that active learning workflows become even more critical and valuable for teams that work with rare data — defined as data where there are minimal observable or found samples to use for training even the initial model. These use cases demand that ML teams bring to  bear every bit of data they have, at any given time, in order to attain any sort of reliable model performance. \u003cbr\u003e\u003cbr\u003eActive learning workflows select the data necessary to improve performance in the prioritized model prediction classes or metadata tags, as well as track the impact of each retraining on model performance, across all data splits, data slices and model classes.\u003c/p\u003e\u003cp\u003eImprovement cycles for rare predictions and rare metadata tags can happen at the moment that new real and rare data becomes available, rather than waiting a longer period of time until a larger quantity batch has become available.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/12/Rare-predictions.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1773\" height=\"973\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/12/Rare-predictions.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/12/Rare-predictions.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/12/Rare-predictions.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/12/Rare-predictions.gif 1773w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"active-learning-strategies-that-leverage-model-error-analysis\"\u003eActive learning strategies that leverage model error analysis\u003c/h3\u003e\u003cp\u003eSome advanced active learning techniques may require both model predictions and ground truth data. While training data typically is labeled, it is rarely the case for production data. The solution is to label a subset of your production data and then to apply the following active learning techniques on this subset of labeled data.\u003c/p\u003e\u003ch3 id=\"prioritize-data-on-which-your-model-is-struggling\"\u003ePrioritize data on which your model is struggling\u003c/h3\u003e\u003cp\u003eActive learning techniques help find and fix model errors by:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIdentifying and surfacing areas of disagreement that occur between the model predictions.\u003c/li\u003e\u003cli\u003eBucketing model errors into patterns of model failures.\u003c/li\u003e\u003cli\u003eMining all of your unlabeled data, looking for similar challenging assets, and sending them to labeling.\u003c/li\u003e\u003cli\u003eTracking the impact of each retraining on model performance, across all of the data splits, model classes and data slices. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLearn more about how to find and fix model errors in our “\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-model-errors/?ref=labelbox-guides.ghost.io\"\u003eFind and Fix Model Errors\u003c/a\u003e” Guide.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1344\" height=\"1024\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/image-1.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/image-1.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/11/image-1.png 1344w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"prioritize-labeling-mistakes-that-undermine-your-model-training\"\u003ePrioritize labeling mistakes that undermine your model training\u003c/h3\u003e\u003cp\u003eActive learning techniques help find and fix label errors by:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIdentifying and surfacing areas of disagreement that occur between the model and labels, but where the model also shows high confidence.\u003c/li\u003e\u003cli\u003eVisualizing these candidate label errors.\u003c/li\u003e\u003cli\u003eMining all of your labeled data, looking for similar annotation mistakes, and sending them to re-labeling.\u003c/li\u003e\u003cli\u003eTracking the impact of each retraining on model performance, across all of the data splits, model classes and data slices.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLearn more about how to find and fix label errors in our “\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-label-errors/?ref=labelbox-guides.ghost.io\"\u003eFind and Fix Label Errors\u003c/a\u003e” Guide.\u003c/p\u003e\u003ch3 id=\"combining-active-learning-techniques-to-compound-your-gains\"\u003eCombining active learning techniques to compound your gains\u003c/h3\u003e\u003cp\u003e\u003cbr\u003eHaving shared some of the multiple techniques to implement active learning, we recommend evaluating these techniques based on your use case and seeing these building blocks as tools in your team’s ML workbench to solve different model performance challenges. We have seen that ML teams that mix together the core building blocks for active learning techniques are able to compound the value of these techniques to deliver further model improvements. By setting up the infrastructure and workflows that allow you to do this easily – and even automatically – apply all of these active learning techniques, ML teams can unlock compounding and exponential value, especially compared to doing just one active learning technique (or not doing any active learning at all).\u003c/p\u003e\u003ch2 id=\"leverage-labelbox-for-active-learning\"\u003eLeverage Quantumworks Lab for active learning\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1972\" height=\"1490\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/image.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/image.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/image.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/image.png 1972w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eLabelbox introduces a convenient way to work with your data and \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003emodels\u003c/a\u003e to help you get started faster with your active learning workflows. Whether the most relevant strategy calls for retraining on edge cases and outliers; updating human labeling queues; adopting \u003ca href=\"https://docs.labelbox.com/docs/model-assisted-labeling?ref=labelbox-guides.ghost.io\"\u003emodel-assisted labeling\u003c/a\u003e; or refining data splits, Quantumworks Lab empowers your team with a diverse workbench of active learning tools necessary to accelerate, streamline, and scale implementation of these model improvement techniques above.\u003c/p\u003e\u003cul\u003e\u003cli\u003eAmplify impact by going through many active learning iterations, as frequently as possible.\u003c/li\u003e\u003cli\u003eInteract with data visually; compare model predictions to ground truth; identify edge cases and identify trends in model behavior.\u003c/li\u003e\u003cli\u003eCurate high-quality, balanced training data. Split labeled data into training, validation, and test datasets, including adjusting the configuration of these datasets across versions.\u003c/li\u003e\u003cli\u003eCluster visually similar data to understand trends in real data distribution and how those trends impact model performance.\u003c/li\u003e\u003cli\u003eComprehend the nuances of a model's performance and add crucial context to key decision-making metrics like Intersection over Union (IoU); Precision and Recall, and more.\u003c/li\u003e\u003cli\u003eVersion your model experiments and model metrics. Easily track each data version to reproduce model results, by preserving the original datasets used to train the model.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eML teams can leverage Quantumworks Lab's data engine to deliver targeted improvements to your training data, so that your team can save crucial time and achieve better and faster model performance.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-16-at-11.17.21-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1432\" height=\"944\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-16-at-11.17.21-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-16-at-11.17.21-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-16-at-11.17.21-AM.png 1432w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"active-learning-customer-spotlight-cape-analytics\"\u003eActive Learning customer spotlight: Cape Analytics\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://capeanalytics.com/?ref=labelbox-guides.ghost.io\"\u003eCape Analytics\u003c/a\u003e enables insurers and other property stakeholders to access valuable property attributes during underwriting, by using \u003ca href=\"https://labelbox.com/guides/computer-vision/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision\u003c/a\u003e algorithms to extract information from geospatial imagery. The power of this solution lies in combining the accuracy and detail of property information traditionally relegated to in-person inspections, with the lightning speed of a living database covering the entire property base of the US, and delivering this information in a matter of seconds.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/customers/cape-analytics/?ref=labelbox-guides.ghost.io\"\u003eCape Analytics\u003c/a\u003e currently uses active learning loops to study the uncertainty of their models. They first select data where the model is uncertain and feed it into Quantumworks Lab, and then back into their model and execute this loop as many times as needed in order to hit their model performance goals. The aggregate time saved from utilizing Quantumworks Lab’s active learning workflows represented an estimated 30%+ increase in total time savings, as well as months of custom development work in engineering hours.\u003c/p\u003e\u003cp\u003eAccording to their principal data scientist, Giacomo Vianello, \"we've found that some active learning techniques are more successful than others depending on the project at hand. Also, it’s not just always about performance, but also robustness. Active learning finds corner cases that may not move the needle for model performance, but improve robustness. This is important to our customers because we're making our models more reliable.\"\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/image-3.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/image-3.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/image-3.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/image-3.png 2048w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"active-learning-customer-spotlight-deque\"\u003eActive learning customer spotlight: Deque \u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://www.deque.com/blog/deque-furthers-digital-accessibility-through-machine-learning/?ref=labelbox-guides.ghost.io\"\u003eDeque\u003c/a\u003e is a leader in digital accessibility which is the practice of making digital documents, web and mobile apps accessible to everyone, including people with disabilities. Using the power of machine learning, the Deque team is now focused on the next generation of accessibility testing. Building out the components of their ML program, Deque leverages a sophisticated data engine using model diagnostics and active learning that’s capable of prioritizing the most performant classes of data, discovering model errors quickly, and fueling their iterations with high-quality data.\u003c/p\u003e\u003cp\u003eThe ML team at Deque were able to make considerable improvements to model performance by seeing the ability to evaluate and visualize model performance. According to Noe Barrell, Deque's machine learning engineer, “we detected some noise issues in our dataset and thanks to model diagnostics, we were able to filter out about one-third of data points we considered less trustworthy. By doing so, model performance went up 5%. We re-labeled some data and we saw the performance went up again after we added the re-labeled points. It was challenging data for humans to label and for the model to understand. Being able to target the data we already had in Quantumworks Lab and make changes and fixes to it was really helpful to us as a team to save time and target where we knew it would make a difference in our model’s performance.”\u003c/p\u003e\u003cp\u003eThe Deque team made huge leaps in several areas via \u003ca href=\"https://labelbox.com/product/model/?ref=labelbox-guides.ghost.io\"\u003emodel diagnostics\u003c/a\u003e and were able to target their data collection in a way that addresses model failures more quickly. For instance, they boosted performance on classes of data such as improving detection of checkboxes from 47% accuracy to 75% accuracy, presentational tables from 66% accuracy to 79% accuracy, and radio buttons from 37.9% accuracy to 74% accuracy. \u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003eTry Quantumworks Lab and its active learning features yourself for free\u003c/a\u003e, or \u003ca href=\"https://app.labelbox.com/signup?ref=labelbox-guides.ghost.io\"\u003erequest a demo\u003c/a\u003e to learn how it works.\u003cbr\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","comment_id":"6373f09e17f6c9003d7b0dbc","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2963--1-.png","featured":false,"visibility":"public","created_at":"2022-11-15T20:03:42.000+00:00","updated_at":"2023-10-27T17:17:50.000+00:00","published_at":"2022-11-16T17:17:06.000+00:00","custom_excerpt":"Discover how to get started with active learning by leveraging the 3 techniques that consistently help ML teams more quickly identify what data will most dramatically improve model performance.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/the-guide-to-getting-started-with-active-learning/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/the-guide-to-getting-started-with-active-learning/","excerpt":"Discover how to get started with active learning by leveraging the 3 techniques that consistently help ML teams more quickly identify what data will most dramatically improve model performance.","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":"Getting started with active learning","og_description":"Discover how to leverage the 3 active learning techniques that consistently help ML teams more quickly identify what data will most dramatically improve model performance.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/12/Group-2963--1--2.png","twitter_title":"Getting started with active learning","twitter_description":"Discover how to leverage the 3 active learning techniques that consistently help ML teams more quickly identify what data will most dramatically improve model performance.","meta_title":"Getting started with active learning | Quantumworks Lab","meta_description":"Get started with active learning by leveraging 3 techniques that will help identify what data will most dramatically improve model performance.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"636a8910e7ffab003d39af7f","uuid":"d6136668-60ff-4c55-8716-030ec388b338","title":"How to scale up your labeling operations while maintaining quality","slug":"how-to-scale-up-your-labeling-operations-while-maintaining-quality","html":"\u003ch2 id=\"\"\u003e\u003c/h2\u003e\u003cp\u003eMany machine learning (ML) teams are eager to label all their data at once. While this approach seems like the quickest way to feed training data into your models, it can often backfire by actually increasing time and cost. This “waterfall” approach to AI data labeling makes it difficult to ensure accuracy, as data annotation requirements often evolve as a ML project progresses.\u003c/p\u003e\u003cp\u003eInstead, leading ML teams manage their labeling workflow by making use of iteration with small batches and an initial calibration phase. During the calibration phase, more supervision and feedback is given to labelers to ensure they understand the task at hand. This iterative loop builds a strategy to tackle any edge cases that arise, and allows teams to adjust the ontology as both labelers and other stakeholders develop a better understanding of the project.\u003c/p\u003e\u003cp\u003eWhile incorporating these phases into your ML data labeling process, to train and validate, might seem like it lengthens your timeline, they’ll be minutes compared to the delays caused by a large labeled dataset full of errors. Despite the initial timeline looking shorter for a non-iterative labeling process, teams find that iterating quickly and early delivers better results faster.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"an-iterative-approach-to-producing-training-data\"\u003eAn iterative approach to producing training data\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-08-at-10.07.12-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1454\" height=\"1024\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-08-at-10.07.12-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-08-at-10.07.12-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-08-at-10.07.12-AM.png 1454w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch3 id=\"maintaining-quality-while-scaling\"\u003eMaintaining quality while scaling\u003c/h3\u003e\u003cp\u003eThe key question we aim to answer is: how to maintain consistency and quality with a larger team and volume. \u003c/p\u003e\u003cp\u003eTo do so we recommend implementing the following two phases into your workflow:\u003c/p\u003e\u003ch3 id=\"calibration-phase\"\u003e\u003cem\u003eCalibration Phase\u003c/em\u003e\u003c/h3\u003e\u003cp\u003eThe calibration phase can be thought of as a smaller subset of your task. It is used to train the labeling team on the labeling instructions, the ontology, and to help them become familiar with the data in the project. This phase helps raise questions from the labeling team and identify edge cases within the data and how to handle them. \u003c/p\u003e\u003cp\u003eBuilding a sustainable communication loop here is key. The labeling team should be very communicative with questions and feedback.  As the project owner, you want to be as responsive as possible in this phase and provide lots of feedback on the annotations.\u003c/p\u003e\u003cp\u003eConstructive feedback at this stage will help the labeling team understand your requirements better and will be used to train the labelers and reviewers going forward.\u003c/p\u003e\u003cp\u003eWithin Quantumworks Lab there are a few ways you can incorporate feedback:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/issues-comments?ref=labelbox-guides.ghost.io\"\u003eIssues and comments\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/boost-workforce-notifications?ref=labelbox-guides.ghost.io\"\u003eUpdates\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e\u003cbr\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAdditional ways teams provide feedback is through:\u003c/p\u003e\u003cul\u003e\u003cli\u003eShared feedback documents (Google doc, quip etc.)\u003c/li\u003e\u003cli\u003eFeedback through external communication channels via email, Slack etc.\u003c/li\u003e\u003cli\u003eRegular syncs via Zoom, Teams etc.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe calibration phase is meant for the labeling team to iterate on the data until they reach the desired quality expectation. Speed is often not as important in this phase as the labelers overcome the learning curve of the project, and allow for new team members to get hands on and train prior to labeling in the production phase. Focus on quality first, and the speed will come.\u003c/p\u003e\u003ch3 id=\"production-phase\"\u003e\u003cbr\u003e\u003cem\u003eProduction Phase\u003c/em\u003e\u003c/h3\u003e\u003cp\u003eDuring the production phase of a project, monitoring overall quality and speed is of utmost importance as it is the key for the successful outcome of your labeled data. Establishing SLA’s in this phase will ensure your team and the labeling team are synchronized on all requirements.\u003c/p\u003e\u003cp\u003eIt is best to increase the workforce gradually. By incrementally increasing the size of your labeling team, it allows you to not be overwhelmed by metrics and ensures that each member is meeting your standards.\u003c/p\u003e\u003cp\u003eHaving additional QA performed throughout the lifetime of a project allows errors to be noticed. When errors surface, labelers can be put back through the training cycle so that only those labelers that meet your quality standards are labeling in production.\u003c/p\u003e\u003cp\u003eA valuable tip when creating production projects and evaluating labeling speed is to set up multiple \u003cem\u003emilestones\u003c/em\u003e instead of one big project. This will help you test your model earlier and make sure that your project is trending in the right direction. For a labeling team, completing a milestone should not only entail labeling all necessary assets, but also QAing them to reach high quality and to improve with every milestone. \u003c/p\u003e\u003cp\u003eMilestones are a great way to set up your data labeling projects because it allows for maximum flexibility. For example, if your model does not perform well or the output is not quite right, then you can still change your guidelines, your task set up or retrain the team accordingly before starting the following milestone. This way you have not lost a lot of time or created duplicative work. \u003c/p\u003e\u003cp\u003eAll of these steps work together to ensure that your exported labels are exactly what you need and were achieved in a timely and cost effective manner. \u003c/p\u003e","comment_id":"636a8910e7ffab003d39af7f","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/11/Group-2906.png","featured":false,"visibility":"public","created_at":"2022-11-08T16:51:28.000+00:00","updated_at":"2023-10-26T18:00:16.000+00:00","published_at":"2022-11-08T19:55:51.000+00:00","custom_excerpt":"Many ML teams are eager to label all their data at once. However, this can actually increase time and cost. Learn how you can effectively build an iterative approach to your labeling operations to ensure quality while scaling. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-scale-up-your-labeling-operations-while-maintaining-quality/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-scale-up-your-labeling-operations-while-maintaining-quality/","excerpt":"Many ML teams are eager to label all their data at once. However, this can actually increase time and cost. Learn how you can effectively build an iterative approach to your labeling operations to ensure quality while scaling. ","reading_time":3,"access":true,"comments":false,"og_image":null,"og_title":"How to scale up your labeling operations while maintaining quality","og_description":"Many ML teams are eager to label all their data at once. However, this can actually increase time, cost, and quality. Learn how you can effectively build an iterative approach to your labeling operations to ensure quality while scaling. ","twitter_image":null,"twitter_title":"How to scale up your labeling operations while maintaining quality","twitter_description":"Many ML teams are eager to label all their data at once. However, this can actually increase time, cost, and quality. Learn how you can effectively build an iterative approach to your labeling operations to ensure quality while scaling. ","meta_title":"Scaling up labeling operations while maintaining quality | Quantumworks Lab","meta_description":"Learn how you can effectively build an iterative approach to your labeling operations to ensure quality while scaling. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63697e40941944003d5a8772","uuid":"4a1b1a39-8d7d-45e9-a9fc-c046ade2feaf","title":"How to maintain quality and cost with advanced analytics","slug":"how-to-maintain-labeling-quality-and-cost-with-advanced-analytics","html":"\u003cp\u003eA strong data engine involves the creation of large volumes of high-quality training data. Delays from quality management or the lack of metrics and insight into labeling quality, throughput, and efficiency during the labeling process can significantly hinder model development. \u003c/p\u003e\u003cp\u003eIn order to maximize efficiency and cost while ensuring labeling quality, ML teams require strong data management, quality and performance monitoring, and advanced techniques to improve the speed and efficiency of their labeling operations. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cs57pq8xb7\" title=\"How to maintain labeling quality and cost with advanced analytics Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"get-full-transparency-with-labeler-and-annotation-analytics\"\u003eGet full transparency with labeler and annotation analytics \u003c/h2\u003e\u003cp\u003eAI team managers and administrators are able to track live analytics throughout their labeling projects to monitor quality, throughput, and efficiency. With Quantumworks Lab, teams can drill further into metrics on individual labeler progress and performance on labeling time, review and rework time, total time spent, and more.\u003c/p\u003e\u003ch2 id=\"project-performance-dashboard\"\u003eProject performance dashboard\u003c/h2\u003e\u003cp\u003eAs one of the primary tools for managing your labeling operations in a Quantumworks Lab project, you have a holistic view of your project's throughput, efficiency, and quality throughout the labeling process. \u003c/p\u003e\u003ch3 id=\"throughput\"\u003eThroughput \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-08-at-8.46.57-AM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1640\" height=\"924\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-08-at-8.46.57-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-08-at-8.46.57-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Screen-Shot-2022-11-08-at-8.46.57-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-08-at-8.46.57-AM.png 1640w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe Throughput view provides insight into the amount of labeling work being produced. It can help provide answers to questions such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHow many assets were labeled in the last 30 days? \u003c/li\u003e\u003cli\u003eHow much time is being spent reviewing labeled assets? \u003c/li\u003e\u003cli\u003eWhat is the average amount of labeling work being produced? \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"efficiency\"\u003eEfficiency \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-08-at-8.47.05-AM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1640\" height=\"764\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-08-at-8.47.05-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-08-at-8.47.05-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Screen-Shot-2022-11-08-at-8.47.05-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-08-at-8.47.05-AM.png 1640w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe Efficiency view provides insight into the time spent per unit of work. It can help provide answers to questions such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eWhat is the average amount of time spent labeling an asset? \u003c/li\u003e\u003cli\u003eHow can I reduce time spent per labeled asset to be more efficient?\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"quality\"\u003eQuality \u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-08-at-8.47.17-AM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1644\" height=\"692\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-08-at-8.47.17-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-08-at-8.47.17-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Screen-Shot-2022-11-08-at-8.47.17-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-08-at-8.47.17-AM.png 1644w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe Quality view provides teams with insight into the accuracy and consistency of the labeling work being produced. It can help provide answers to questions such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003eWhat is the average quality of a labeled asset? \u003c/li\u003e\u003cli\u003eHow can I track and ensure label quality is more consistent across the team?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEach of the above metrics are reported at the overall project level and at the user level, giving you a single source of truth for your project's annotation and labeler analytics. \u003c/p\u003e\u003cp\u003eLearn more about the project performance dashboard in our \u003ca href=\"https://docs.labelbox.com/docs/performance-dashboard?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"advanced-analytics-for-enterprise-teams\"\u003eAdvanced analytics for Enterprise teams \u003c/h2\u003e\u003cp\u003eFor larger Enterprise teams, Quantumworks Lab provides advanced analytics to better track labeling time and spend. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/22b9a76-Dashboard_Screenshot--1-.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1208\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/22b9a76-Dashboard_Screenshot--1-.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/22b9a76-Dashboard_Screenshot--1-.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/22b9a76-Dashboard_Screenshot--1-.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2022/11/22b9a76-Dashboard_Screenshot--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eA global view of your entire labeling operations across all Quantumworks Lab projects.\u003c/li\u003e\u003cli\u003eFilter for specific metrics (e.g labeler email, date submitted, project, or dataset) to better manage cost \u0026amp; quality.\u003c/li\u003e\u003cli\u003eInteract with and customize your data for maximum insights and schedule detailed reports.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLearn more about advanced analytics for Enterprise teams in our \u003ca href=\"https://docs.labelbox.com/docs/enterprise-dashboards?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e","comment_id":"63697e40941944003d5a8772","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/11/Group-2848--3-.png","featured":false,"visibility":"public","created_at":"2022-11-07T21:53:04.000+00:00","updated_at":"2022-12-06T21:27:33.000+00:00","published_at":"2022-11-08T19:53:45.000+00:00","custom_excerpt":"Delays from quality management or the lack of insight into labeling quality can hinder model development. Learn how to maintain quality and cost with project performance dashboard and advanced analytics for Enterprise teams. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-maintain-labeling-quality-and-cost-with-advanced-analytics/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-maintain-labeling-quality-and-cost-with-advanced-analytics/","excerpt":"Delays from quality management or the lack of insight into labeling quality can hinder model development. Learn how to maintain quality and cost with project performance dashboard and advanced analytics for Enterprise teams. ","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":"How to maintain quality and cost with advanced analytics","og_description":"Delays from quality management or the lack of insight into labeling quality can hinder model development. Learn how to maintain quality and cost with Quantumworks Lab's Project Performance Dashboard and advanced analytics for Enterprise teams. ","twitter_image":null,"twitter_title":"How to maintain quality and cost with advanced analytics","twitter_description":"Delays from quality management or the lack of insight into labeling quality can hinder model development. Learn how to maintain quality and cost with Quantumworks Lab's Project Performance Dashboard and advanced analytics for Enterprise teams. ","meta_title":"How to maintain quality and cost with advanced analytics","meta_description":"Learn how to maintain quality and cost with Quantumworks Lab's Project Performance Dashboard and advanced analytics for Enterprise teams. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63605052acbcc9003db097ea","uuid":"1519e539-41d7-4282-9b6e-7e9126dfdd31","title":"How to customize your annotation review process","slug":"how-to-customize-your-annotation-review-process","html":"\u003cp\u003eHigher-quality data results in higher-quality models. However, obtaining high-quality data can be challenging. Every ML engineer or labeling operations manager often faces a recurring challenge – how to complete large, complex training data projects in a systematic way. \u003c/p\u003e\u003cp\u003eThere are so many variables and decision trees, that it can be challenging to envision the most efficient way to structure, label, and QA your projects. While teams have had to traditionally rely on weekly team meetings or other manual methods, we built \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e – the ultimate orchestration tool for your training data cycles.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"what-is-workflows\"\u003eWhat is Workflows?\u003c/h2\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/hpjlazpqln\" title=\"Workflows Demo Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eCustom workflows can help optimize how labeled data gets reviewed across multiple tasks and reviewers. Workflows is a new feature that allows teams the flexibility to tailor their review workflows for faster iteration cycles. \u003c/p\u003e\u003cp\u003eTeams can set up dynamic workflows based on attributes, such as annotation type or labeler, to reduce costs and increase labeling and review throughput, quality, and efficiency.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"why-are-workflows-important\"\u003eWhy are workflows important?\u003c/h2\u003e\u003cp\u003eWorkflows works in conjunction with \u003ca href=\"https://labelbox.com/guides/how-to-prepare-and-submit-a-batch-for-labeling/?ref=labelbox-guides.ghost.io\"\u003eBatches \u003c/a\u003eand the \u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eData Rows tab\u003c/a\u003e to enable a more highly customizable, step-by-step review pipeline.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Workflows---review-tasks---reject--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1771\" height=\"912\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Workflows---review-tasks---reject--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Workflows---review-tasks---reject--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Workflows---review-tasks---reject--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/Workflows---review-tasks---reject--1-.gif 1771w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eAt a glance, Workflows allows you to easily understand the state of your data within your labeling operations pipeline. Rather than relying on manual spreadsheets or ad-hoc methods, you have a systematic way of understanding how many data rows are ready for model training, how many are currently in-review, how many are being re-labeled, and how many haven’t been labeled yet.\u003c/p\u003e\u003cp\u003eBy using Workflows, teams have more granular control over how data rows get reviewed – teams can use Workflows to customize their review pipeline to drive efficiency and automation into their review process.\u003c/p\u003e\u003cp\u003eDesigned to help save your team labeling time and cost, Workflows enables you to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eEasily understand the status of your data within your labeling operations pipeline\u003c/li\u003e\u003cli\u003eHave multiple groups, like your core labeling team and subject matter experts, review labeled data in a specific manner by configuring custom review steps\u003c/li\u003e\u003cli\u003eSelect labels for rework individually or by bulk with minimal manual work\u003c/li\u003e\u003cli\u003eCreate ad-hoc review steps to ensure quality\u003c/li\u003e\u003cli\u003eView a Data Row's history with the audit log for greater context\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/wmry22y3ou\" title=\"How to set up Workflows for custom review and faster iterations Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch3 id=\"customize-and-set-up-your-project%E2%80%99s-review-task\"\u003eCustomize and set up your project’s review task\u003c/h3\u003e\u003cp\u003eBy default, there are three tasks that appear when you create a new project:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInitial labeling task\u003c/strong\u003e: reserved for all data rows that have been queued for labeling\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInitial review task: \u003c/strong\u003ereserved for all data rows that are currently in the first review step\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRework task:\u003c/strong\u003e reserved for data rows that have been Rejected\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDone\u003c/strong\u003e \u003cstrong\u003etask:\u003c/strong\u003e reserved for data rows that have a) moved through their qualified tasks in the workflow or b) did not qualify for any of the tasks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can \u003cstrong\u003ehave up to 10 review tasks\u003c/strong\u003e within a workflow.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/v3ltrtgomo\" title=\"Customize and set up your project's review task Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eHow it works with a project’s Overview page\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe new overview page provides teams with a holistic view of where your data rows are in the labeling pipeline. \u003c/p\u003e\u003cp\u003eAt a glance, you’re able to understand which data rows can be used downstream. For a deeper dive, you can click on a stage to view all data rows in the filtered state in the Data Rows tab.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/voabxv25ud\" title=\"How workflows works with the Overview page Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch3 id=\"move-batches-between-stages-of-your-workflow\"\u003eMove Batches between stages of your workflow\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/how-to-prepare-and-submit-a-batch-for-labeling/?ref=labelbox-guides.ghost.io\"\u003eBatches\u003c/a\u003e are a collection of data rows that are queued from \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e and added to your labeling project. They are critical in enabling faster data-centric iterations and in helping unlock active learning workflows to improve label or model errors.  \u003c/p\u003e\u003cp\u003eIn the \u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eData Rows tab\u003c/a\u003e, you can filter by batch or annotation to send subsets of your data to 'Rework' in bulk.  You can also send bulk data rows to different stages of your workflow.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/2nhdr7tr1o\" title=\"Move Batches between stages of your workflow Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch3 id=\"better-collaborate-and-create-ad-hoc-review-tasks-based-on-project-needs\"\u003eBetter collaborate and create ad-hoc review tasks based on project needs\u003c/h3\u003e\u003cp\u003eBased on project progress or ad-hoc needs, you can create a new review task. This allows you to modify your workflow based on feedback from a Labeling Operations Manager or based on your team’s labeling performance. \u003c/p\u003e\u003cp\u003eAnother reason why teams might want to create an ad-hoc review task is based on subject-matter expert review. Teams can add a review step and set parameters to maximize what gets reviewed at this stage in order to maximize the reviewer’s time and cost. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cc1n47hhh4\" title=\"Better collaborate and create ad-hoc review tasks based on project needs Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eTeams can also send a data row to a specific review task from the \u003ca href=\"https://docs.labelbox.com/docs/viewing-and-editing-labels-in-the-data-row-browser?ref=labelbox-guides.ghost.io\"\u003eData Row browser\u003c/a\u003e for ad-hoc review. For example, if you find some poorly labeled data rows that you want to send to re-work, you can do so directly from the Data Row browser. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-21-at-2.27.07-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"925\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-21-at-2.27.07-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-21-at-2.27.07-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Screen-Shot-2022-11-21-at-2.27.07-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2022/11/Screen-Shot-2022-11-21-at-2.27.07-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003chr\u003e\u003ch3 id=\"easily-review-your-data-automatically-send-incorrect-labels-to-be-fixed\"\u003eEasily review your data \u0026amp; automatically send incorrect labels to be fixed\u003c/h3\u003e\u003cp\u003eWhen reviewing your data rows, you can easily approve or reject data rows based on if it's a Benchmark-based project or a Consensus-level batch. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eApproving a data row:\u003c/strong\u003e Data rows that have been approved will be automatically moved to the next review task in your queue. Data rows that are in the \"Done\" step give you a real-time, high-level view of how much of your data is ready to incorporate into your model. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eRejecting a data row:\u003c/strong\u003e\u003cem\u003e \u003c/em\u003eData rows that have been rejected will be automatically sent to “Rework” for correction. You can mark the issue and leave a comment as to why the label was rejected – allowing your labelers to understand and correct any labeling errors.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eHow to approve / reject data rows for a Benchmark-based project\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rvjnb1gupa\" title=\"How to approve or reject data rows for a Benchmark-level batch Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eHow to approve / reject data rows for a Consensus-based project\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/sudz91xfmk\" title=\"How to approve or reject data rows for a Consensus project Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch3 id=\"better-understand-your-project%E2%80%99s-data-lineage-to-iterate-provide-feedback\"\u003eBetter understand your project’s data lineage to iterate \u0026amp; provide feedback\u003c/h3\u003e\u003cp\u003eWith large complex projects, it can be hard to manage and keep track of a Data Row’s lifecycle. \u003c/p\u003e\u003cp\u003eA multi-step review workflow means that a Data Row can travel between different stages of the workflow. You can quickly do spot checks with the audit log to better understand how a Data Row has ended up in a particular stage of your workflow. This helps ensure that no Data Row has skipped an important step of the review process and can give you a clear picture of how a Data Row has ended up from “to Label” to “Done”. \u003c/p\u003e\u003cp\u003eThis ability to track the journey of a Data Row through a review cycle can help teams make more informed decisions on how to make edits to their review workflow for future iterations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/501cdu71kt\" title=\"Understand your project's data lineage with the audit log Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"workflows-faq\"\u003eWorkflows FAQ\u003c/h2\u003e\u003cp\u003e\u003cstrong\u003eWhat replaces thumbs up / thumbs down voting?\u003c/strong\u003e \u003c/p\u003e\u003cp\u003eFor all new projects, thumbs up / thumbs down voting will be replaced by the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io#reject--rework-data-rows\"\u003eapprove / reject workflow\u003c/a\u003e. In review tasks, the following actions can be taken:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eBenchmark\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eApprove:\u003c/strong\u003e Reviewer will approve a data row\u003c/p\u003e\u003cul\u003e\u003cli\u003eThis will move the data row to quality for the next task in the Workflow\u003c/li\u003e\u003cli\u003eIn case there is only a single review task (such as the \"Initial review task\" that Quantumworks Lab automatically creates), the data row will end up in \"Done\". \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eReject: \u003c/strong\u003eReviewer will reject a data row\u003c/p\u003e\u003cul\u003e\u003cli\u003eThis will move the data row to the Rework task\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eConsensus\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eApprove:\u003c/strong\u003e Reviewer will choose a winner label to approve the data row \u003c/p\u003e\u003cul\u003e\u003cli\u003eThis will move the data row to quality for the next task in the Workflow\u003c/li\u003e\u003cli\u003eIn case there is only a single review task (such as the \"Initial review task\" that Quantumworks Lab automatically creates), the data row will end up in \"Done\". \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eReject:\u003c/strong\u003e Reviewer will reject a data row, essentially rejecting all labels on that data row\u003c/p\u003e\u003cul\u003e\u003cli\u003eThis will move the data row to the \"Rework\" task\u003c/li\u003e\u003cli\u003eIn \"Rework\", any labeler or reviewer can modify all labels associated with the data row \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can also use the 'Move to step' functionality for ad-hoc review. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow do you keep track of the journey of a Data Row?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWorkflows allows you to see the journey of your Data Rows as they move from 'To label' to 'Done' through your customized review steps. \u003c/p\u003e\u003cp\u003eWith the \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io#view-audit-logs\"\u003eaudit log\u003c/a\u003e, admins can view the entire set of events that happened on a Data Row after it was labeled, allowing you to understand the complete journey of each Data Row. This can be especially helpful for when teams need to investigate the review history of a particular data row or a set of Data Rows. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow do you know when a data row is ready to be used for model training?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eData Rows that appear in the 'Done' stage of your workflow can be used for model training. The 'Done' stage signifies that the Data Row has passed through all necessary review steps, giving you a real-time, high-level view of how much of your data is ready to be used for model training. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow do you ensure that you’re only reviewing the most important assets?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThis will likely depend on your particular workflow, but teams can use filters in the Data Rows tab to surface specific data rows of interest for review. \u003c/p\u003e\u003cp\u003eMulti-step review workflows can give teams the flexibility to review Data Rows at a specific step of the review process. Rather than having to review or sort through all of your data rows, the Data Row tab gives teams a holistic look into all the review steps within a project’s workflow\u003c/p\u003e\u003cp\u003eYou can read more about this in the \u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eData Rows tab guide\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCan I use the Rework task instead of Delete-and-requeue?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e'Delete and requeue' will still be supported for a little while longer. Eventually, the 'Rework' step of your Workflow will replace Delete-and-reqeueue. Quantumworks Lab is currently working on parity between the asynchronous functionality in Rework tasks and 'Delete and requeue'.\u003c/p\u003e","comment_id":"63605052acbcc9003db097ea","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/11/Group-2889--2-.png","featured":false,"visibility":"public","created_at":"2022-10-31T22:46:42.000+00:00","updated_at":"2023-10-26T18:11:40.000+00:00","published_at":"2022-11-07T18:31:42.000+00:00","custom_excerpt":"Custom workflows can help optimize how labeled data gets reviewed across multiple tasks and reviewers. Workflows is a new feature that allows teams the flexibility to tailor their review workflows for faster iteration cycles. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-customize-your-annotation-review-process/","excerpt":"Custom workflows can help optimize how labeled data gets reviewed across multiple tasks and reviewers. Workflows is a new feature that allows teams the flexibility to tailor their review workflows for faster iteration cycles. ","reading_time":6,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/11/Group-2889--1--2.png","og_title":"How to create a customizable review pipeline to understand the status of your data ","og_description":"Custom workflows can help optimize how labeled data gets reviewed across multiple tasks and reviewers. Workflows is a new feature that allows teams the flexibility to tailor their review workflows for faster iteration cycles. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/11/Group-2889--1--1.png","twitter_title":"How to create a customizable review pipeline to understand the status of your data ","twitter_description":"Custom workflows can help optimize how labeled data gets reviewed across multiple tasks and reviewers. Workflows is a new feature that allows teams the flexibility to tailor their review workflows for faster iteration cycles. ","meta_title":"How to customize your annotation review process | Quantumworks Lab","meta_description":"Custom workflows can help optimize how labeled data gets reviewed across multiple tasks and reviewers. Learn more about workflows with Labelbox.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6360450aacbcc9003db09731","uuid":"6e6c54f0-593c-4ca2-aeac-511b4e40e7f0","title":"How to search, surface, and prioritize data within a project","slug":"how-to-search-surface-and-prioritize-data-within-a-project","html":"\u003cp\u003eA vital aspect of a strong data engine involves the creation of large volumes of high-quality training data.\u003c/p\u003e\u003cp\u003eA common bottleneck for many AI teams is the inability to have a holistic view across their  data to filter and prioritize the right data to label. Resembling a black-box approach to labeling operations, this lack of insight and visibility can cause teams to spend more time and cost on labeling. \u003c/p\u003e\u003cp\u003eThe better you’re able to search, understand, and manage your data, the faster you’ll be able to prioritize Data Rows for labeling and accelerate model development.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"what-is-the-data-rows-tab\"\u003eWhat is the Data Rows tab?\u003c/h2\u003e\u003cp\u003eThe \u003ca href=\"https://docs.labelbox.com/docs/data-rows-activity?ref=labelbox-guides.ghost.io\"\u003eData Rows tab\u003c/a\u003e is the central hub for all Data Rows within a given project. It allows you to view, manage, and filter for Data Rows within your project.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-10_11-48-29--1--2.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1783\" height=\"967\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/2022-11-10_11-48-29--1--2.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/2022-11-10_11-48-29--1--2.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/2022-11-10_11-48-29--1--2.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-10_11-48-29--1--2.gif 1783w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eWith the Data Rows tab, teams have a holistic view of:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe \u003cstrong\u003estatus and number of Data Rows \u003c/strong\u003eacross: \u003cem\u003eTo Label, Labeled, In Review, In Rework, and Done\u003c/em\u003e\u003c/li\u003e\u003cli\u003eA data row’s\u003cstrong\u003e Data Row ID / External ID / Global Key\u003c/strong\u003e\u003c/li\u003e\u003cli\u003eWhat \u003cstrong\u003ereview task\u003c/strong\u003e it belongs to\u003c/li\u003e\u003cli\u003eThe \u003cstrong\u003equality setting and agreement score \u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\t\u003cem\u003eBenchmark - shows the agreement score\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\t\u003cem\u003eConsensus - shows the number of labels completed and the agreement score\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLabel \u0026amp; Review time\u003c/strong\u003e\u003c/li\u003e\u003cli\u003eWhat \u003cstrong\u003edataset\u003c/strong\u003e a data row belongs to\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOpen issues\u003c/strong\u003e on the data row\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eYou can sort Data Rows by:\u003c/strong\u003e\t\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-04-at-4.23.37-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1166\" height=\"668\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-04-at-4.23.37-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-04-at-4.23.37-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-04-at-4.23.37-PM.png 1166w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eData Rows created at\u003c/li\u003e\u003cli\u003eLabel created at\u003c/li\u003e\u003cli\u003eLabel updated at\u003c/li\u003e\u003cli\u003eBenchmark agreement score\u003c/li\u003e\u003cli\u003eLabel time (coming soon)\u003c/li\u003e\u003cli\u003eReview time (coming soon)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTo surface a subset of Data Rows, teams can filter on:\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-04-at-4.22.20-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1242\" height=\"456\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-04-at-4.22.20-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-04-at-4.22.20-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-04-at-4.22.20-PM.png 1242w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eAnnotation\u003c/li\u003e\u003cli\u003eFunction\u003c/li\u003e\u003cli\u003eData Row\u003c/li\u003e\u003cli\u003eDataset\u003c/li\u003e\u003cli\u003eMedia Attribute\u003c/li\u003e\u003cli\u003eSpecific label actions\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cem\u003e\t\tCreated at\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tLabeled by\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tReviewed by\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tRe-worked by (coming soon)\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tSkipped\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eMetadata\u003c/li\u003e\u003cli\u003eBatch\u003c/li\u003e\u003cli\u003eReview task\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"why-is-the-data-rows-tab-important\"\u003eWhy is the Data Rows tab important?\u003c/h2\u003e\u003cp\u003eThe Data Rows tab works with the new \u003ca href=\"https://docs.labelbox.com/docs/project-overview-tab?ref=labelbox-guides.ghost.io\"\u003eOverview page\u003c/a\u003e to give teams a holistic picture of all their Data Rows and where each data row is in their labeling workflow. \u003c/p\u003e\u003cp\u003ePrior to the Data Rows tab, teams were using the \u003ca href=\"https://docs.labelbox.com/docs/label-activity?ref=labelbox-guides.ghost.io\"\u003eLabel tab\u003c/a\u003e to keep track of data row activity. While it provided a view of all Data Rows within a project, teams were limited in how they were able to quickly surface data rows of interest. \u003c/p\u003e\u003cp\u003eOver the Labels tab, the Data Rows tab supports more advanced filtering capabilities, so teams can find Data Rows and quickly understand the status of a data row. Teams now have a more cohesive way of filtering and surfacing specific data rows across \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io\"\u003eCatalog\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/models-overview?ref=labelbox-guides.ghost.io\"\u003eModel\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eDesigned to work with batches and multi-step review workflows, the Data Rows tab gives you a much better and holistic view of your labeling operations.\u003c/p\u003e\u003cp\u003eYou can learn more about the Data Rows tab in our \u003ca href=\"https://docs.labelbox.com/docs/data-rows-activity?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch3 id=\"a-holistic-view-of-your-projects-progress\"\u003eA holistic view of your project's progress\u003c/h3\u003e\u003cp\u003eThe Data Rows tab will update in sync with your project’s overview page, allowing you to easily see how your Data Rows are progressing through your project’s workflow.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/qt66gzwh8m\" title=\"A holistic view of your project's progress Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch3 id=\"drill-into-specific-data-rows-at-each-step-of-your-workflow\"\u003eDrill into specific Data Rows at each step of your workflow\u003c/h3\u003e\u003cp\u003eView all Data Rows within a specific stage of your \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003eworkflow\u003c/a\u003e in the left panel of the Data Rows tab. Clicking into each status will bring up all the data rows within that stage of your workflow.\u003c/p\u003e\u003cp\u003eYou can also view your Data Rows in “gallery view” – allowing you to view Data Rows with a thumbnail view. This view will display and render any bounding box annotations in the preview.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/hi4j3d15ys\" title=\"Drill into specific data rows at each step of your workflow Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch3 id=\"filter-and-find-specific-slices-of-data\"\u003eFilter and find specific slices of data\u003c/h3\u003e\u003cp\u003eTeams can use dynamic filters to query and surface specific Data Rows of interest. Mirroring the search capabilities in Catalog, you can query for Data Rows within a project faster than ever.\u003c/p\u003e\u003cp\u003eWith flexible querying, you can use a combination of AND/OR conditions on attributes for more granular searches. Filter on:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAnnotation\u003c/li\u003e\u003cli\u003eFunction\u003c/li\u003e\u003cli\u003eData Row\u003c/li\u003e\u003cli\u003eDataset\u003c/li\u003e\u003cli\u003eMedia Attribute\u003c/li\u003e\u003cli\u003eSpecific label actions\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\t\t\u003cem\u003eCreated at\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tLabeled by\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tReviewed by\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tRe-worked by (coming soon)\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tSkipped\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eMetadata\u003c/li\u003e\u003cli\u003eBatch\u003c/li\u003e\u003cli\u003eReview task\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/2inx3tbt6x\" title=\"Filter and find specific slices of data Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch3 id=\"manage-batches-and-view-batch-history\"\u003eManage batches and view batch history\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/how-to-prepare-and-submit-a-batch-for-labeling?ref=labelbox-guides.ghost.io\"\u003eBatches\u003c/a\u003e are a collection of Data Rows that are queued from \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eCatalog \u003c/a\u003eand added to your labeling project. They are critical in enabling faster data-centric iterations and in helping unlock active learning workflows to improve label or model errors.\u003c/p\u003e\u003cp\u003eYou can easily manage and view batches directly from the Data Rows tab:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to add, view \u0026amp; manage batches for a Benchmark project\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/azpsiago0o\" title=\"How to add, view \u0026amp; manage batches for a Benchmark project Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to add, view \u0026amp; manage batches for a Consensus project\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/k9nokisedi\" title=\"How to add, view \u0026amp; manage batches for a Consensus project Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eHow to delete a batch\u003cbr\u003e\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/pv771dra0u\" title=\"How to delete a batch Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou can learn more about batches \u003ca href=\"https://labelbox.com/guides/how-to-prepare-and-submit-a-batch-for-labeling?ref=labelbox-guides.ghost.io\"\u003ein this guide\u003c/a\u003e or in our \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch3 id=\"conduct-actions-in-bulk-to-improve-efficiency\"\u003eConduct actions in bulk to improve efficiency\u003c/h3\u003e\u003cp\u003eComplex projects might feature a high number of Data Rows. It’s important that teams are able to effectively manage data rows of interest to improve labeling efficiency.\u003c/p\u003e\u003cp\u003eYou can conduct actions in bulk by selecting bulk Data Rows together and completing one of the desired actions below:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eMove to a specific step of your workflow: \u003c/strong\u003eAllows you to bulk move certain Data Rows to a review step or rework in your labeling workflow. For example, you can easily move bulk data rows to “Rework” or “Done” steps.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDelete \u0026amp; re-queue:\u003c/strong\u003e Allows you to send data rows back to the labeling queue and delete current labels. You also have the option to preserve the existing label as a template.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHide or unhide from labelers: \u003c/strong\u003eAllows you to hide certain data rows from labelers (i.e sensitive or inappropriate content). This will restrict all labelers from being able to view selected data rows in the editor.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/czuutm8ahu\" title=\"Conduct actions in bulk to improve efficiency Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"what-value-does-the-data-rows-tab-unlock\"\u003eWhat value does the Data Rows tab unlock?\u003c/h2\u003e\u003ch3 id=\"designed-to-work-with-multi-step-review-workflows\"\u003eDesigned to work with multi-step review workflows\u003c/h3\u003e\u003cp\u003eFor many Enterprise teams working on larger and more complex projects, a key question becomes how to structure, review, and complete training data projects in a systematic way.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003eMulti-step review workflows\u003c/a\u003e can give teams the flexibility to review Data Rows at a specific step of the review process. Rather than having to review or sort through all of your Data Rows, the Data Row tab gives teams a holistic look into all the review steps within a project’s workflow.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFlexible querying with dynamic filters:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe search capabilities in the Data Rows tab mirror Catalog – you can query to surface a specific subset of Data Rows within a project to better QA and understand all the data rows within your project.\u003c/p\u003e\u003cp\u003eFilters like Annotation type or sorting by Function allows teams to identify and QA a subset of Data Rows that meet a specific criteria. In addition to Workflows, the Data Rows tab unlocks the unique ability for ad-hoc review and flexible QA in addition to being able to view \u0026amp; manage your entire labeling operations. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/so8f0ig4dh\" title=\"Designed to work with multi-step review workflows Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e","comment_id":"6360450aacbcc9003db09731","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/11/Group-2877--3--2.png","featured":false,"visibility":"public","created_at":"2022-10-31T21:58:34.000+00:00","updated_at":"2023-10-26T18:12:34.000+00:00","published_at":"2022-11-05T00:40:24.000+00:00","custom_excerpt":"The Data Rows tab is the central hub for all data rows within a given project. You can view, manage, and filter for data rows within your project to better prioritize data for labeling and to accelerate model development.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-search-surface-and-prioritize-data-within-a-project/","excerpt":"The Data Rows tab is the central hub for all data rows within a given project. You can view, manage, and filter for data rows within your project to better prioritize data for labeling and to accelerate model development.","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/11/Group-2877--3--4.png","og_title":"How to better manage \u0026 search for data with the Data Rows tab","og_description":"The Data Rows tab is the central hub for all data rows within a given project. You can view, manage, and filter for data rows within your project to better prioritize data for labeling and to accelerate model development.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/11/Group-2877--3--3.png","twitter_title":"How to better manage \u0026 search for data with the Data Rows tab","twitter_description":"The Data Rows tab is the central hub for all data rows within a given project. You can view, manage, and filter for data rows within your project to better prioritize data for labeling and to accelerate model development.","meta_title":"How to better manage \u0026 search for data with the Data Rows tab | Quantumworks Lab","meta_description":"Data Rows tab is the central hub for all data rows within a given project. Learn how to better prioritize data to accelerate model development.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63604bc4acbcc9003db09788","uuid":"2ba33c16-544e-4e2f-8b89-4ace7bef6542","title":"How to prepare and submit a batch for labeling","slug":"how-to-prepare-and-submit-a-batch-for-labeling","html":"\u003cp\u003eHigh-quality training data is crucial to the success of any ML project. In order to improve model performance, it is crucial for teams to not only queue data for labeling, but to prioritize specific data in order to enable faster iteration cycles and decrease labeling cost. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eLabelbox’s Catalog\u003c/a\u003e is a single place for teams to upload their datasets, quickly visualize their data, and make informed decisions on what data to prioritize for labeling.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"what-are-batches\"\u003eWhat are Batches?\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-09-at-3.28.28-PM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"971\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-09-at-3.28.28-PM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-09-at-3.28.28-PM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Screen-Shot-2022-11-09-at-3.28.28-PM.png 1600w, https://labelbox-guides.ghost.io/content/images/size/w2400/2022/11/Screen-Shot-2022-11-09-at-3.28.28-PM.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eWhile Batches will replace dataset-based queueing, datasets in Quantumworks Lab are not going away.\u003c/p\u003e\u003cp\u003eIn order to upload data, teams will still need to upload their relevant data as datasets. From there, teams can filter and add relevant Data Rows to their labeling project in groups through batches. A batch is a collection of Data Rows from Catalog that can be queued and added to your labeling project.\u003c/p\u003e\u003cp\u003eOnce your dataset is in Catalog, you can filter and sort Data Rows to determine which specific ones you’d like to add to your project as a batch. If you want to send a whole dataset to your queue, you can simply select all Data Rows within the dataset and send it as a batch.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"why-is-batch-based-queueing-so-powerful\"\u003eWhy is Batch-based queueing so powerful?\u003c/h2\u003e\u003cp\u003eMachine learning teams often have tons of unlabeled data — it can be incredibly time consuming and expensive to label all of your data. An important question becomes how to smartly decide what data to label and prioritize in order to accelerate model development. \u003c/p\u003e\u003cp\u003eRather than queueing an entire dataset for labeling, queuing Data Rows with batches gives teams greater control and flexibility in the prioritization of a project’s labeling queue. \u003c/p\u003e\u003cp\u003eWhen creating and adding a batch to a new \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003eBenchmark\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003eConsensus\u003c/a\u003e project, you’ll need to name and set the batch’s priority from 1-5 (with 1 being the highest). If you’re creating a batch for a Consensus project, you have the ability to enable or disable Consensus for that batch. You can also set your coverage percentage and set the number of labels. \u003c/p\u003e\u003cp\u003eWith no limit to how many batches you can add to your labeling project, teams have ultimate flexibility to prioritize certain edge cases and create ad-hoc labeling tasks without needing to modify entire datasets every time.\u003c/p\u003e\u003cp\u003eLearn more about batch-based queueing in our \u003ca href=\"https://docs.labelbox.com/docs/batch-based-queueing?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch3 id=\"how-to-create-and-queue-a-batch-for-labeling\"\u003eHow to create and queue a batch for labeling\u003c/h3\u003e\u003cp\u003eWhen you create a new project in Quantumworks Lab, you'll be prompted to queue your data for labeling through a batch. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to create \u0026amp; queue a batch for labeling (Benchmark)\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cv4xsbh2ww\" title=\"How to create \u0026amp; queue a batch for labeling (Benchmark) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to create \u0026amp; queue a batch for labeling (Consensus)\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/l1mdgnlghm\" title=\"How to create \u0026amp; queue a batch for labeling (Consensus) Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eLearn more about how to create a batch in our \u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003ch3 id=\"manage-and-view-batches-within-a-project-in-the-data-rows-tab\"\u003eManage and view batches within a project in the Data Rows tab \u003c/h3\u003e\u003cp\u003eYou can add, view, and manage batches directly in the \u003ca href=\"https://docs.labelbox.com/reference/data-row?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eData Rows tab\u003c/a\u003e, making it easy for teams to have a holistic view of their entire labeling operations. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to add, view \u0026amp; manage batches for a Benchmark project\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/azpsiago0o\" title=\"How to add, view \u0026amp; manage batches for a Benchmark project Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to add, view \u0026amp; manage batches for a Consensus project\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/k9nokisedi\" title=\"How to add, view \u0026amp; manage batches for a Consensus project Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to delete a batch\u003c/strong\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/pv771dra0u\" title=\"How to delete a batch Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow to do this through our SDK:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io#create-a-batch-1\"\u003eCreating a batch\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io#list-batches-in-a-project\"\u003eView list of all batches associated with a project\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/batches?ref=labelbox-guides.ghost.io#archive-batch-remove-queued-data-from-a-project\"\u003eDeleting a batch\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003chr\u003e\u003ch2 id=\"what-unique-workflows-can-batches-unlock\"\u003eWhat unique workflows can Batches unlock?\u003c/h2\u003e\u003ch3 id=\"sample-your-data-to-make-data-selection-faster\"\u003eSample your data to make data selection faster\u003c/h3\u003e\u003cp\u003eWith larger datasets, you may want to sample data in order to make the data selection process faster and more efficient. In Quantumworks Lab, you can either randomly sample a number of Data Rows or choose to do an ordered sample. \u003c/p\u003e\u003cp\u003eYou can learn more about sampling methods in our \u003ca href=\"https://docs.labelbox.com/docs/sampling-methods?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eRandom sampling\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eRandom sampling can help reduce selection bias when teams are deciding which Data Rows they want to send for labeling.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9zp4i8pggm\" title=\"How to do a random sample Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eClick “Sample” at the top-right of Catalog\u003c/li\u003e\u003cli\u003eChoose “Random” from the top dropdown\u003c/li\u003e\u003cli\u003eSpecify the number of Data Rows that you wish to sample\u003c/li\u003e\u003cli\u003eSelect the project, name your batch, and set the batch’s priority\u003c/li\u003e\u003cli\u003eClick “Submit” \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eOrdered sampling\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOrdered sampling can be helpful if you want to quickly sample and send \u003cem\u003ex \u003c/em\u003enumber of Data Rows to a project. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/cnztsvp2oe\" title=\"How to do an ordered sample Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eClick “Sample” at the top-right of Catalog\u003c/li\u003e\u003cli\u003eChoose “Ordered” from the top dropdown\u003c/li\u003e\u003cli\u003eSpecify the number of Data Rows that you wish to sample\u003c/li\u003e\u003cli\u003eSelect the project, name your batch, and set the batch’s priority\u003c/li\u003e\u003cli\u003eClick “Submit”\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"better-find-and-fix-labeling-or-model-errors\"\u003eBetter find and fix labeling or model errors\u003c/h3\u003e\u003cp\u003eYour model is only as good as the data that it gets trained on – high-quality labels are crucial for training your model. \u003c/p\u003e\u003cp\u003eYou can use Quantumworks Lab Model to surface Data Rows where your model predictions and ground truth labels disagree and view them in Catalog. Once in Catalog, you can send the subset of poorly labeled data rows to your labeling project. You can mark the batch as “high-priority” since fixing these errors will dramatically help improve model performance.\u003c/p\u003e\u003cp\u003eLearn more about finding and fixing label and model errors in the below guides:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-label-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix label errors\u003c/a\u003e \u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-model-errors/?ref=labelbox-guides.ghost.io\"\u003eHow to find and fix model errors\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"prioritize-specific-data-to-improve-labeling-accuracy-model-performance\"\u003ePrioritize specific data to improve labeling accuracy \u0026amp; model performance\u003c/h3\u003e\u003cp\u003eIt is known that not all data will impact model performance equally. In a sea of unlabeled data, teams will want to decide what data to label in priority. \u003c/p\u003e\u003cp\u003eIf you notice that your labelers are struggling on a specific image or object annotation, you can find similar data and send it to your labeling project as a batch to improve labeling accuracy. \u003c/p\u003e\u003cp\u003eIf you notice that your model is struggling on a specific class, you can send more similar data to be labeled so that your model performs better on the newly added data points. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"faq-on-batches\"\u003eFAQ on Batches\u003c/h2\u003e\u003cp\u003e\u003cstrong\u003eTo add a batch to a project, am I still required to upload a dataset to Quantumworks Lab?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eYes – you’ll still need to upload data as a dataset in Labelbox. After you upload a dataset, the dataset will live in Catalog. \u003c/p\u003e\u003cp\u003eFrom there, you can filter and choose to send specific Data Rows to a project as a batch. You can also send the entire dataset as a batch to a project if needed.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow many Data Rows can you have in a batch?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eYou can now have up to 100k Data Rows in a given batch. There is also no limit to how many batches you can add to a project, giving teams the ability to add all necessary data rows to a project for labeling.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCan I submit the same Data Row to a project multiple times?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eA Data Row cannot be part of more than one batch in a project at a time. This is to help teams prevent having duplicate data rows within a project. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCan a batch be shared between multiple projects?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eA batch cannot be shared between multiple projects. However, if you wish to use the same set of Data Rows for another project, you can create a new batch using the same Data Rows. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow do I set Consensus at the batch level?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWe enable you to choose a quality mode when creating your project – between \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003eBenchmarks\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003eConsensus\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eIf you’ve selected Consensus for your project, you can configure Consensus for batches that you add to your project.\u003c/p\u003e\u003cp\u003eWhen creating a batch for a Consensus project, you’ll see 3 configuration settings:\u003c/p\u003e\u003cul\u003e\u003cli\u003eA toggle to enable / disable Consensus for that batch\u003c/li\u003e\u003cli\u003eA slider to set the coverage percentage\u003c/li\u003e\u003cli\u003eA place to enter the number of labels \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cem\u003eThe slider and entering the number of labels replaces the old Labeling Parameter Overrides (LPO) feature as it enables you to customize the assets in the queue at the batch level. \u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCan I enable Benchmarks AND Consensus on a project?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eHaving both Benchmarks and Consensus batches on a project is not yet supported.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eI don’t want to use Batches for my project right now. What should I do?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eBatches can enable data-centric iterations driven by prioritizing high-impact data. If you don’t want to add specific Data Rows to a project as a batch, you can add your entire dataset to a batch for labeling (up to 100k data rows).\u003c/p\u003e","comment_id":"63604bc4acbcc9003db09788","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/11/Group-2878--2-.png","featured":false,"visibility":"public","created_at":"2022-10-31T22:27:16.000+00:00","updated_at":"2024-10-02T22:34:22.000+00:00","published_at":"2022-11-04T22:58:56.000+00:00","custom_excerpt":"High-quality training data is crucial to the success of any ML project. Rather than queueing an entire dataset for labeling, queuing Data Rows with batches gives teams greater control and flexibility in the prioritization of a project’s labeling queue. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-prepare-and-submit-a-batch-for-labeling/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-prepare-and-submit-a-batch-for-labeling/","excerpt":"High-quality training data is crucial to the success of any ML project. Rather than queueing an entire dataset for labeling, queuing Data Rows with batches gives teams greater control and flexibility in the prioritization of a project’s labeling queue. ","reading_time":5,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2878-1.png","og_title":"How to queue your data rows with Batches","og_description":"High-quality training data is crucial to the success of any ML project. Rather than queueing an entire dataset for labeling, queuing data rows with batches gives teams greater control and flexibility in the prioritization of a project’s labeling queue. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2878-2.png","twitter_title":"How to queue your data rows with Batches","twitter_description":"High-quality training data is crucial to the success of any ML project. Rather than queueing an entire dataset for labeling, queuing data rows with batches gives teams greater control and flexibility in the prioritization of a project’s labeling queue. ","meta_title":"How to prepare and submit a batch for labeling | Quantumworks Lab","meta_description":"Queuing data rows with batches gives teams greater control and flexibility in the prioritization of a project’s labeling queue. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63349b941beec0003d38bdab","uuid":"25dd87f7-2581-4c17-a657-b5bb27dfab21","title":"A new way to queue \u0026 review","slug":"a-new-way-to-queue-review","html":"\u003cp\u003e\u003cem\u003eA migration guide for Workflows, Batches, \u0026amp; the Data Rows tab. \u003c/em\u003e\u003c/p\u003e\u003cp\u003eOften, AI teams struggle to prioritize the right data to label and end up spending more money on data labeling than they should. A vital aspect of a data engine involves the creation of large volumes of high-quality training data.\u003c/p\u003e\u003cp\u003eThis year, we've released two features – \u003ca href=\"https://labelbox.com/guides/how-to-prepare-and-submit-a-batch-for-labeling/?ref=labelbox-guides.ghost.io\"\u003eBatches\u003c/a\u003e and the \u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eData Rows tab\u003c/a\u003e – to help teams better prioritize, queue, and manage their data. Now, we’re excited to introduce a third feature called \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAll three features, Batches, the Data Rows tab, and Workflows, work to help teams label data more quickly and efficiently to create high-quality training data, faster.\u003c/p\u003e\u003ch2 id=\"what-is-the-new-way-to-queue-review-my-data\"\u003e\u003cstrong\u003eWhat is the new way to queue \u0026amp; review my data?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFor \u003cstrong\u003eall new projects\u003c/strong\u003e after the launch of Workflows, teams will need to use:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-prepare-and-submit-a-batch-for-labeling/?ref=labelbox-guides.ghost.io\"\u003eBatches\u003c/a\u003e to queue data rows for labeling, configure data row priority, and set the number of labels (for Consensus projects)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003e[New] \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e\u003c/strong\u003e for custom review\u003c/li\u003e\u003cli\u003eThe \u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eData Rows tab\u003c/a\u003e to manage \u0026amp; view the status of data rows\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWatch the video demo below to learn more about how teams can use Batches, Workflows, and the Data Rows tab for greater control over their labeling operations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/wf38jpayp3\" title=\"A new way to queue \u0026amp; review: Workflows, Batches, \u0026amp; the Data Rows tab Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003chr\u003e\u003ch2 id=\"batches\"\u003e\u003ca href=\"https://labelbox.com/guides/how-to-prepare-and-submit-a-batch-for-labeling/?ref=labelbox-guides.ghost.io\"\u003eBatches\u003c/a\u003e\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/batch-based-queueing?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eBatches\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e will replace \u003ca href=\"https://docs.labelbox.com/docs/dataset-based-queueing?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003edataset-based queueing\u003c/strong\u003e\u003c/a\u003e for all new projects.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhile Batches will replace dataset-based queueing, datasets in Quantumworks Lab are not going away. In order to upload data, teams will still need to upload their relevant data as a dataset to Catalog. In Catalog, teams can use advanced search filters to surface relevant high-impact data for labeling. Once surfaced, teams will add the relevant data rows to their labeling project as a batch.\u003c/p\u003e\u003cp\u003eMachine learning teams often have tons of unlabeled data — it can be incredibly time consuming and expensive to label all of your data. An important question becomes how to smartly decide what data to label and prioritize in order to accelerate model development.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Batch---Consensus.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1779\" height=\"972\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Batch---Consensus.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Batch---Consensus.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Batch---Consensus.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/Batch---Consensus.gif 1779w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eRather than having to queue an entire dataset, batch-based queueing gives teams greater flexibility and control over what gets sent to a labeling project. With Batches, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePrioritize slices of data by adding batches to a project in priority\u003c/li\u003e\u003cli\u003eManage batches \u0026amp; view batch history\u003c/li\u003e\u003cli\u003eEnable active learning workflows to identify the most high-impact data rows for labeling\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe support batch-based queueing for both quality settings of Benchmarks and Consensus. Consensus settings will be configured at the batch-level – you’ll be prompted to set your coverage percentages and set the number of labels for a batch.\u003c/p\u003e\u003cp\u003eLearn more about how to create a batch and prioritize batches \u003ca href=\"https://labelbox.com/guides/how-to-prepare-and-submit-a-batch-for-labeling/?ref=labelbox-guides.ghost.io\"\u003ein this guide\u003c/a\u003e or read more in our \u003ca href=\"https://docs.labelbox.com/docs/batch-based-queueing?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"new-workflows\"\u003e[New] \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eWorkflows \u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003ewill replace the \u003ca href=\"https://docs.labelbox.com/docs/review-step?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eReview Step\u003c/strong\u003e\u003c/a\u003e for all new projects. \u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe quicker you can review and iterate on training data cycles, the faster you can obtain high quality data and improve model performance.\u003c/p\u003e\u003cp\u003eAt a glance, Workflows allows you to easily understand the state of your data within your labeling operations pipeline. Rather than relying on manual spreadsheets or ad-hoc methods, you have a systematic way of understanding how many data rows are ready for model training, how many are currently in-review, how many are being re-labeled, and how many haven’t been labeled yet. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Workflows---review-tasks---reject-1.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1771\" height=\"912\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Workflows---review-tasks---reject-1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Workflows---review-tasks---reject-1.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Workflows---review-tasks---reject-1.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/Workflows---review-tasks---reject-1.gif 1771w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eWorkflows works in conjunction with batches and the Data Rows tab to enable a more highly customizable, step-by-step review pipeline. It gives teams more granular control over how data rows get reviewed – teams can use Workflows to customize their review pipeline to drive efficiency and automation into their review process.\u003c/p\u003e\u003cp\u003eHere are a few ways Workflows can unlock greater functionalities to save your team labeling time and cost. Teams can now:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHave multiple groups, like your core labeling team and subject matter experts, review labeled data in a specific manner by configuring custom review steps\u003c/li\u003e\u003cli\u003eSelect labels for rework individually or by bulk with minimal manual work\u003c/li\u003e\u003cli\u003eCreate ad-hoc review steps to ensure quality\u003c/li\u003e\u003cli\u003eView a data row's history with the audit log for greater context\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBy default, there are four tasks that appear when you create a new project:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-01_16-13-55--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1784\" height=\"977\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/2022-11-01_16-13-55--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/2022-11-01_16-13-55--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/2022-11-01_16-13-55--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-01_16-13-55--1-.gif 1784w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInitial labeling task\u003c/strong\u003e: reserved for all data rows that have been queued for labeling\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInitial review task: \u003c/strong\u003ereserved for all data rows that are currently in the first review step\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRework task:\u003c/strong\u003e reserved for data rows that have been Rejected\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDone\u003c/strong\u003e \u003cstrong\u003etask:\u003c/strong\u003e reserved for data rows that have a) moved through their qualified tasks in the workflow or b) did not qualify for any of the tasks\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can \u003cstrong\u003ehave up to 10 review tasks\u003c/strong\u003e within a workflow.\u003c/p\u003e\u003cp\u003eYou can learn more about the power of workflows and how to set up customized review sequences \u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003ein this guide\u003c/a\u003e or in our \u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"the-data-rows-tab\"\u003e\u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eThe Data Rows tab\u003c/a\u003e\u003c/h2\u003e\u003cp\u003e\u003cstrong\u003eThe \u003ca href=\"https://docs.labelbox.com/docs/data-rows-activity?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eData Rows tab\u003c/strong\u003e\u003c/a\u003e will replace the \u003ca href=\"https://docs.labelbox.com/docs/label-activity?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eLabel tab\u003c/strong\u003e\u003c/a\u003e for all new projects.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOver the Labels tab, the Data Rows tab supports more advanced filtering capabilities, so teams can easily find data rows and quickly understand data row status. Designed to work with batches and multi-step review workflows, the Data Rows tab gives you a holistic view of your labeling operations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eA holistic view of your project's progress\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe new Overview page works with the Data Rows tab and Workflows to provide a holistic count of data rows in each stage of the workflow as well as a snapshot of annotation metrics.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-02_16-55-59--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1776\" height=\"971\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/2022-11-02_16-55-59--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/2022-11-02_16-55-59--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/2022-11-02_16-55-59--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-02_16-55-59--1-.gif 1776w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003eYou can click into each stage of your workflow to open up the Data Rows tab where you can further inspect data row details for each stage.\u003c/li\u003e\u003cli\u003eYou can view annotation metrics and click into an annotation of interest to view specific slices of data in the Data Rows tab.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eDrill into specific data rows at each step of your workflow\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eUse the panel on the left of the Data Rows tab to quickly understand what data rows belong to each stage of the workflow.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-02_15-25-12--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1773\" height=\"977\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/2022-11-02_15-25-12--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/2022-11-02_15-25-12--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/2022-11-02_15-25-12--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-02_15-25-12--1-.gif 1773w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eFilter and find specific slices of data\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eUse improved filters for more complex queries and to surface data rows of interest like annotation, metadata, dataset, media attribute, data row ID, and more. With flexible querying, you can search for more granular data with AND/OR conditions.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-01_14-32-20--2-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1777\" height=\"913\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/2022-11-01_14-32-20--2-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/2022-11-01_14-32-20--2-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/2022-11-01_14-32-20--2-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-01_14-32-20--2-.gif 1777w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eManage batches and view batch history\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhen batches are submitted to a project, they'll appear in the Data Rows tab. By naming each batch, teams can keep track of when a batch was submitted, view the data rows within a batch, and remove a batch and any unlabeled data rows if needed.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-02_16-27-40--1-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1778\" height=\"972\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/2022-11-02_16-27-40--1-.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/2022-11-02_16-27-40--1-.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/2022-11-02_16-27-40--1-.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/2022-11-02_16-27-40--1-.gif 1778w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eConduct actions in bulk to improve efficiency\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eComplex projects might feature a high number of data rows. It’s important that teams are able to effectively manage data rows of interest to improve labeling efficiency.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Workflows---Bulk-send-to-rework.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1772\" height=\"904\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Workflows---Bulk-send-to-rework.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Workflows---Bulk-send-to-rework.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Workflows---Bulk-send-to-rework.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/Workflows---Bulk-send-to-rework.gif 1772w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eYou can conduct actions in bulk by selecting bulk data rows together and completing one of the desired actions below:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eMove to a specific step of your workflow: \u003c/strong\u003eAllows you to bulk move certain data rows to a review step or rework in your labeling workflow. For example, you can easily move bulk data rows to “Rework” or “Done” steps.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDelete \u0026amp; re-queue:\u003c/strong\u003e Allows you to send data rows back to the labeling queue and delete current labels. You also have the option to preserve the existing label as a template.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHide or unhide from labelers: \u003c/strong\u003eAllows you to hide certain data rows from labelers (i.e sensitive or inappropriate content). This will restrict all labelers from being able to view selected data rows in the editor.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLearn more about the value of the Data Rows tab and in-depth tutorials \u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003ein this guide\u003c/a\u003e. You can also read more about this change in our \u003ca href=\"https://docs.labelbox.com/docs/data-rows-activity?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"what-exactly-is-changing-and-when\"\u003e\u003cstrong\u003eWhat exactly is changing and when?\u003c/strong\u003e\u003c/h2\u003e\u003ch3 id=\"new-projects\"\u003eNew projects\u003c/h3\u003e\u003cp\u003eLabelbox will automatically be configuring \u003cstrong\u003enew projects with Batches, Workflows, and the Data Rows tab.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThis will happen on a rolling basis across our customer base. The table below indicates when you can expect to see this new paradigm (Batches, Workflows, and Data Rows tab) for new projects:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-22-at-11.26.52-AM.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1640\" height=\"818\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/11/Screen-Shot-2022-11-22-at-11.26.52-AM.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/11/Screen-Shot-2022-11-22-at-11.26.52-AM.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/11/Screen-Shot-2022-11-22-at-11.26.52-AM.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/11/Screen-Shot-2022-11-22-at-11.26.52-AM.png 1640w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eWith these changes, when you create a new project:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cu\u003eThrough the UI:\u003c/u\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eSelect a quality setting (\u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io\"\u003ebenchmark\u003c/a\u003e or \u003ca href=\"https://docs.labelbox.com/docs/consensus?ref=labelbox-guides.ghost.io\"\u003econsensus\u003c/a\u003e)\u003c/li\u003e\u003cli\u003eRather than queueing an entire dataset, you’ll be directed to use \u003ca href=\"https://docs.labelbox.com/docs/batch-based-queueing?ref=labelbox-guides.ghost.io\"\u003ebatch-based queueing \u003c/a\u003eto send a subset of data rows for labeling\u003c/li\u003e\u003cli\u003eIf you select consensus as your quality mode during project creation, you’ll be able to set the \u003ca href=\"https://docs.labelbox.com/docs/batch-based-queueing?ref=labelbox-guides.ghost.io#queue-data-through-batches-in-consensus-projects\"\u003econsensus labeling parameters at the batch level\u003c/a\u003e (% coverage and # of labels) \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"sdk-related-changes\"\u003eSDK-related changes\u003c/h3\u003e\u003cp\u003e\u003cu\u003eThrough the SDK:\u003c/u\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eAll new projects will require media type upon creation\u003c/li\u003e\u003cli\u003eSelect a queuing mode and quality mode at the time of project creation. You cannot change the queue or quality mode after project creation (learn how to set this up \u003ca href=\"https://docs.labelbox.com/reference/project?ref=labelbox-guides.ghost.io#setting-the-queueing-mode-while-setting-up-a-project\"\u003ehere\u003c/a\u003e)\u003c/li\u003e\u003cli\u003eRather than queueing an entire dataset, you’ll be directed to use \u003ca href=\"https://docs.labelbox.com/docs/batch-based-queueing?ref=labelbox-guides.ghost.io\"\u003ebatch-based queueing\u003c/a\u003e to send a subset of data rows for labeling.\u003c/li\u003e\u003cli\u003eIf you select consensus as your quality mode during project creation, you’ll be able to set the \u003ca href=\"https://docs.labelbox.com/docs/batch-based-queueing?ref=labelbox-guides.ghost.io#queue-data-through-batches-in-consensus-projects\"\u003econsensus labeling parameters at the batch level\u003c/a\u003e (% coverage and # of labels)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWith the launch of Workflows and the move to batches, the Data Rows tab, and Workflows, we’ll be releasing \u003cstrong\u003enew Python SDK versions\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eLearn more about SDK changes \u003ca href=\"https://labelbox.com/guides/sdk-changes-a-new-way-to-queue-review/?ref=labelbox-guides.ghost.io\"\u003ein this guide\u003c/a\u003e. \u003c/p\u003e\u003ch3 id=\"old-projects\"\u003eOld projects\u003c/h3\u003e\u003cp\u003eThe migration of legacy projects to the new paradigm will take place on a rolling basis \u003cstrong\u003estarting March 12th\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eYou will receive an email specifying your migration date. We will be sending out expected migration dates the week of Monday, March 6th – \u003cstrong\u003eplease check your email to see when your migration will take place.\u003c/strong\u003e\u003c/p\u003e\u003ch3 id=\"what-will-happen-to-my-old-or-existing-projects\"\u003eWhat will happen to my old or existing projects?\u003c/h3\u003e\u003cp\u003eOld or existing projects that were created before the launch of workflows still contain dataset-based queueing, the Labels tab, and the Review step.\u003c/p\u003e\u003cp\u003eLabelbox will \u003cstrong\u003eautomatically be migrating your old/existing projects to the new paradigm\u003c/strong\u003e (batch-based queuing, Data Rows tab, and workflows). This will happen on Quantumworks Lab’s backend and \u003cstrong\u003eno action is required on your end\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eWe will preserve all data rows, along with the associated labels and reviews (thumbs up/down) in the migrated projects. You will be able to query the review data in these projects and take actions required, if any, in the Workflow paradigm at your discretion, including moving data rows to the appropriate workflow task. \u003c/p\u003e\u003cp\u003eLearn more about the upcoming migration in our \u003ca href=\"https://docs.labelbox.com/docs/migration-faq?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e\u003chr\u003e\u003ch2 id=\"why-are-we-making-this-change\"\u003eWhy are we making this change?\u003c/h2\u003e\u003cp\u003eHaving worked with hundreds of AI teams, we recognized the need for more granular control over labeling workflows.\u003c/p\u003e\u003cp\u003eIn order to streamline and improve the creation, maintenance, and quality control of data rows, we’re moving to using Batches, the Data Rows tab, and Workflows as a new way for teams to queue \u0026amp; review their data.\u003c/p\u003e\u003ch2 id=\"where-can-i-learn-more\"\u003eWhere can I learn more?\u003c/h2\u003e\u003cp\u003eWe understand that change is never easy. We'll be continuing to update this guide with additional information throughout the migration.\u003c/p\u003e\u003cp\u003eIf you have specific questions related to new project creation or the migration of old projects, you can \u003ca href=\"https://labelbox.atlassian.net/servicedesk/customer/portal/2?ref=labelbox-guides.ghost.io\"\u003ecreate a support ticket\u003c/a\u003e or reach out to your dedicated Customer Success Manager.\u003c/p\u003e\u003cp\u003eIn the meantime, here are a few resources to learn more:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eGuides\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-prepare-and-submit-a-batch-for-labeling/?ref=labelbox-guides.ghost.io\"\u003eHow to prepare and submit a batch for labeling\u003c/a\u003e (Batches)\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-customize-your-annotation-review-process/?ref=labelbox-guides.ghost.io\"\u003eHow to customize your annotation review process\u003c/a\u003e (Workflows) \u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/how-to-search-surface-and-prioritize-data-within-a-project/?ref=labelbox-guides.ghost.io\"\u003eHow to search, surface, and prioritize data within a project\u003c/a\u003e (Data Rows tab) \u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/sdk-changes-a-new-way-to-queue-review/?ref=labelbox-guides.ghost.io\"\u003eSDK changes: A new way to queue \u0026amp; review\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eDocumentation\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/batch-based-queueing?ref=labelbox-guides.ghost.io\"\u003eBatches\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/workflows?ref=labelbox-guides.ghost.io\"\u003eWorkflows\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/data-rows-activity?ref=labelbox-guides.ghost.io\"\u003eData Rows tab\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/migrating-to-workflows?ref=labelbox-guides.ghost.io\"\u003eMigrating to Workflows\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/real-time-labeling?ref=labelbox-guides.ghost.io\"\u003eFeatures that are being deprecated\u003c/a\u003e due to this migration\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"where-can-i-provide-feedback\"\u003eWhere can I provide feedback?\u003c/h2\u003e\u003cp\u003eAs we strive to make the Quantumworks Lab experience even better, we value any feedback.\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you'd like to leave feedback on your team's current review \u0026amp; rework process, you can fill out this\u003ca href=\"https://labelbox.atlassian.net/servicedesk/customer/portal/2/create/129?customize=true\u0026ref=labelbox-guides.ghost.io\"\u003e survey\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eIf you have any feedback or questions on the current migration to batches, the Data Rows tab, and Workflows, feel free to fill out this\u003ca href=\"https://docs.google.com/forms/d/e/1FAIpQLScqvU_JD0DWLt5qgOuOKkP61D8pnPDYfMkDsb_6NyqJ5AT37w/viewform?ref=labelbox-guides.ghost.io\"\u003e short survey\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e","comment_id":"63349b941beec0003d38bdab","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-8--2--1.png","featured":false,"visibility":"public","created_at":"2022-09-28T19:08:04.000+00:00","updated_at":"2023-10-26T18:14:37.000+00:00","published_at":"2022-11-02T00:07:00.000+00:00","custom_excerpt":"A migration guide for the switch to Batch-based queueing, Workflows, and the Data Rows tab.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/a-new-way-to-queue-review/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/a-new-way-to-queue-review/","excerpt":"A migration guide for the switch to Batch-based queueing, Workflows, and the Data Rows tab.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":"A new way to queue \u0026 review","og_description":"A migration guide for the switch to Batch-based queueing, Workflows, and the Data Rows tab.","twitter_image":null,"twitter_title":"A new way to queue \u0026 review","twitter_description":"A migration guide for the switch to Batch-based queueing, Workflows, and the Data Rows tab.","meta_title":"A new way to queue \u0026 review | Quantumworks Lab","meta_description":"A migration guide for the switch to Batch-based queueing, Workflows, and the Data Rows tab.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635be086a31ffb004d18ffa3","uuid":"d2bc698e-2db1-4b6a-abeb-751efc7e8b12","title":"SDK Changes: A new way to queue \u0026 review","slug":"sdk-changes-a-new-way-to-queue-review","html":"\u003ch2 id=\"what-is-changing\"\u003eWhat is changing?\u003c/h2\u003e\u003cp\u003e\u003cem\u003eLabelbox is deprecating \u003ca href=\"https://docs.labelbox.com/reference/project?ref=labelbox-guides.ghost.io#setting-the-queueing-mode--quality-settings-while-setting-up-a-project\"\u003e\u003cem\u003eQueueMode\u003c/em\u003e\u003c/a\u003e, meaning all projects will be required to use Batch-based queueing for all projects by the end of Q1 2023. We will release a new SDK version that will automatically set up all new projects with this new functionality.\u003c/em\u003e\u003c/p\u003e\u003ch3 id=\"labelbox-will-release-the-following-changes\"\u003eLabelbox will release the following changes:\u003c/h3\u003e\u003cp\u003e\u003cem\u003eFree / EDU / Starter customers\u003c/em\u003e can expect the changes on \u003cstrong\u003eNovember 21st. \u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003ePro and Enterprise customers\u003c/em\u003e can expect the changes \u003cstrong\u003eon a rolling basis starting December 14th.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eAutomatic updates\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eIf you do not specify a queueing mode when creating a project, the queueing mode will default to Batches unless you explicitly opt out (\u003ca href=\"https://docs.labelbox.com/reference/project?ref=labelbox-guides.ghost.io#setting-the-queueing-mode--quality-settings-while-setting-up-a-project\"\u003eset queue mode to Dataset\u003c/a\u003e to opt out).\u003c/li\u003e\u003cli\u003eIf you do not specify a quality mode at project creation, the quality mode will default to Benchmarks.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eNote: \u003c/strong\u003eProjects created before queue mode deprecation will still use dataset-based queueing. Dataset-based projects will be migrated to batches by the end of Q1 2023.\u003c/p\u003e\u003cp\u003e\u003cem\u003eSDK upgrade required\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eAll new projects will require media type upon creation.\u003c/li\u003e\u003cli\u003eOnce you add a batch to a project, you cannot update the Consensus configurations on that batch.\u003c/li\u003e\u003cli\u003eIf you enable Consensus for your project, you will be able to set the Data Row priority in the label queue and number of times a Data Row gets labeled at the batch level. New projects created after this date will not support Labeling Parameter Overrides as it will be obsolete.\u003c/li\u003e\u003cli\u003eYou will be able to set data row priority, % coverage, and # labels at the batch level (for Consensus projects) \u003c/li\u003e\u003cli\u003eIf you want to add a batch of up to 100k data rows, you'll need to upgrade to the latest version of the SDK (see \u003ca href=\"https://github.com/Quantumworks Lab/labelbox-python/blob/master/CHANGELOG.md?ref=labelbox-guides.ghost.io#version-3301-2022-11-16\"\u003echangelog\u003c/a\u003e) \u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"by-mid-december\"\u003eBy mid-December\u003c/h3\u003e\u003cul\u003e\u003cli\u003eYou may add your entire dataset to a batch as long as the batch does not exceed 100k data rows. There is no limit to how many batches you can add to a project.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"q1-2023\"\u003eQ1 2023\u003c/h3\u003e\u003cul\u003e\u003cli\u003eQueueMode will be removed, meaning you will no longer be presented the option to select Dataset-based queueing via the SDK. Batch mode will automatically be configured for queueing Data Rows to a project.\u003c/li\u003e\u003cli\u003eLabelbox will throw an exception if you attempt to create a project and do not specify the media type.\u003c/li\u003e\u003cli\u003eLabeling Parameter Overrides (LPO) will no longer be supported for any project as it will be replaced by setting priority and number of labels at the batch level.\u003c/li\u003e\u003cli\u003eThe old Review step will no longer be supported for any project. This functionality will be replaced by “send to task” within Workflows.\u003c/li\u003e\u003cli\u003eBulk actions in Workflows will be available.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"what-action-do-i-need-to-take\"\u003eWhat action do I need to take?\u003c/h2\u003e\u003cp\u003eOn the appropriate date, upgrade your Quantumworks Lab Python SDK version to begin using batch-based consensus for all new projects.\u003c/p\u003e\u003cp\u003eIn mid-Q1 2023, schedule the migration for all of your dataset-based projects created before QueueMode deprecation. Quantumworks Lab will provide a detailed migration path for your dataset-based projects. No action required for now.\u003c/p\u003e\u003ch2 id=\"why-are-we-making-these-changes\"\u003eWhy are we making these changes?\u003c/h2\u003e\u003cp\u003eAdding an entire dataset to a project’s labeling queue is the old way of queueing Data Rows to a project. Sending batches of Data Rows from a dataset to a project’s labeling queue provides your team with flexibility to build more complex review pipelines for your team.\u003c/p\u003e\u003ch2 id=\"where-can-i-learn-more\"\u003eWhere can I learn more?\u003c/h2\u003e\u003cp\u003eFor more information, visit our \u003ca href=\"https://docs.labelbox.com/docs/migrating-to-workflows?ref=labelbox-guides.ghost.io\"\u003eMigration guide\u003c/a\u003e.\u003c/p\u003e","comment_id":"635be086a31ffb004d18ffa3","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/unnamed--1-.png","featured":false,"visibility":"public","created_at":"2022-10-28T14:00:38.000+00:00","updated_at":"2023-10-26T18:18:08.000+00:00","published_at":"2022-10-28T14:01:11.000+00:00","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/sdk-changes-a-new-way-to-queue-review/","excerpt":"What is changing?\n\nLabelbox is deprecating QueueMode, meaning all projects will be required to use Batch-based queueing for all projects by the end of Q1 2023. We will release a new SDK version that will automatically set up all new projects with this new functionality.\n\n\nLabelbox will release the following changes:\n\nFree / EDU / Starter customers can expect the changes on November 21st.\n\nPro and Enterprise customers can expect the changes on a rolling basis starting December 14th.\n\nAutomatic up","reading_time":2,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"SDK Changes: A new way to queue \u0026 review | Quantumworks Lab","meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63336dc3875dd8004d80eb50","uuid":"4b053b4c-821c-41ff-8494-93ff9ef80fd2","title":"How to find and fix label errors","slug":"how-to-find-and-fix-label-errors","html":"\u003cp\u003eIn this guide, we'll be walking you through how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/8hpp57mhw0\" title=\"How to find and fix label errors Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTry the Colab: \u003ca href=\"https://colab.research.google.com/drive/1SRcoEo4-UMiKiX4CPBP82UlLi6OtItn5?ref=labelbox-guides.ghost.io#scrollTo=8bedb521\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eML models are only as good as the data they are trained on and it's actually more common than you think for there to be labeling errors in your training data. In order to improve your ML models, you'll need to improve your training data by finding label errors and then sending them to be corrected. \u003c/p\u003e\u003cp\u003eOnce you upload your model predictions and model metrics to Quantumworks Lab, you can unlock powerful workflows to label high-impact data, faster and more efficiently. This not only helps speed up your labeling efforts and increases label quality, but can help reduce your labeling budget. \u003c/p\u003e\u003cp\u003eLearn more about our methods to drive data-centric iterations by improving your training data:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/improve-model-performance?ref=labelbox-guides.ghost.io\"\u003eFind model errors\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/active-learning?ref=labelbox-guides.ghost.io\"\u003eActive learning: Prioritize high-value data\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","comment_id":"63336dc3875dd8004d80eb50","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2791--2-.png","featured":false,"visibility":"public","created_at":"2022-09-27T21:40:19.000+00:00","updated_at":"2023-10-27T17:20:00.000+00:00","published_at":"2022-10-10T23:18:00.000+00:00","custom_excerpt":"Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-find-and-fix-label-errors/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-find-and-fix-label-errors/","excerpt":"Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. ","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2791-1.png","og_title":"How to find and fix label errors","og_description":"Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2791.png","twitter_title":"How to find and fix label errors","twitter_description":"Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. ","meta_title":"How to find and fix label errors | Quantumworks Lab","meta_description":"Learn how you can use Quantumworks Lab Model to visually compare your ground truths and predictions to identify and fix label errors. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"632c712a16e912003d39b1ea","uuid":"43d97c24-4694-4599-b510-00cf170294a5","title":"How to curate and version your training datasets and hyperparameters","slug":"how-to-curate-and-version-your-training-datasets-and-hyperparameters","html":"\u003cp\u003eIt's important for teams to be able to manage their model input and outputs in a single place. Being able to understand and visualize how different models compare to each other is a crucial aspect of a successful data engine. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/9mas8rc1d7\" title=\"How to curate and version your training datasets and hyperparameters Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTry the Colab: \u003ca href=\"https://colab.research.google.com/drive/1nWoWb-EwuQFZK1UpuF2y-q6VVETU-GyH?ref=labelbox-guides.ghost.io#scrollTo=QDMX8CvqWcGk\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eRather than flying blind, teams can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. You can easily track and reproduce model experiments to observe the differences and share best practices with your team. \u003c/p\u003e\u003cp\u003eML teams may want to kick off multiple model runs with different sets of hyperparameters and compare performance. Having a single place to track model configurations and performance is crucial to model iteration and for reproducing model results. Learn how you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/create-a-model?ref=labelbox-guides.ghost.io\"\u003eCreate a model\u003c/a\u003e \u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/create-a-model-run?ref=labelbox-guides.ghost.io\"\u003eCreate a model run\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/curate-data-splits?ref=labelbox-guides.ghost.io\"\u003eCurate data splits\u003c/a\u003e \u003c/li\u003e\u003cli\u003eCreate and modify a model run configuration \u003ca href=\"https://docs.labelbox.com/docs/add-model-run-config-to-track-hyperparameters?ref=labelbox-guides.ghost.io#create-and-modify-model-run-config-in-the-ui\"\u003ethrough our UI\u003c/a\u003e \u003c/li\u003e\u003cli\u003eCreate and modify a model run configuration \u003ca href=\"https://docs.labelbox.com/docs/add-model-run-config-to-track-hyperparameters?ref=labelbox-guides.ghost.io#create-and-modify-model-run-config-in-sdk\"\u003ethrough the SDK\u003c/a\u003e \u003c/li\u003e\u003cli\u003eUse our \u003ca href=\"https://colab.research.google.com/drive/1nWoWb-EwuQFZK1UpuF2y-q6VVETU-GyH?ref=labelbox-guides.ghost.io#scrollTo=QDMX8CvqWcGk\"\u003eGoogle Colab Notebook\u003c/a\u003e (as shown in the video) to curate and version your training datasets and hyperparameters\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo learn more about model run configurations, feel free to visit our \u003ca href=\"https://docs.labelbox.com/docs/add-model-run-config-to-track-hyperparameters?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e","comment_id":"632c712a16e912003d39b1ea","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2721.png","featured":false,"visibility":"public","created_at":"2022-09-22T14:28:58.000+00:00","updated_at":"2023-10-27T17:23:25.000+00:00","published_at":"2022-10-10T15:36:00.000+00:00","custom_excerpt":"Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. Easily track and reproduce model experiments to observe the differences and share best practices with your team. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-curate-and-version-your-training-datasets-and-hyperparameters/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-curate-and-version-your-training-datasets-and-hyperparameters/","excerpt":"Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. Easily track and reproduce model experiments to observe the differences and share best practices with your team. ","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2721-2.png","og_title":"How to curate and version your training datasets and hyperparameters","og_description":"Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. Easily track and reproduce model experiments to observe the differences and share best practices with your team. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2721-1.png","twitter_title":"How to curate and version your training datasets and hyperparameters","twitter_description":"Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits. Easily track and reproduce model experiments to observe the differences and share best practices with your team. ","meta_title":"How to curate and version your training datasets and hyperparameters | Quantumworks Lab","meta_description":"Learn how you can use Model to configure, track, and compare essential model training hyperparameters alongside training data and data splits.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63275a80bb4a31003d53748b","uuid":"c94ec715-9ba1-4a83-bed9-a5ec592f18dc","title":"How to natively annotate a PDF document","slug":"how-to-natively-annotate-a-pdf-document","html":"\u003cp\u003ePDF documents are inherently complex – they often contain lots of text, images, charts, graphs, and more. Information within PDFs can be interpreted in many different ways and traditional OCR solutions are not sufficient in capturing both text and visual information, which is vital for document image understanding and can limit the accuracy of your model. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/iwvumt91xu\" title=\"How to natively annotate a PDF document Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOur Document editor is a \u003cstrong\u003emultimodal annotation platform.\u003c/strong\u003e You can easily turn stores of PDF files and documents into performant ML models. With the ability to use an NER text layer, you can easily annotate text of interest alongside OCR, without losing context. \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWith our Document editor, teams can: \u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNatively \u003ca href=\"https://docs.labelbox.com/reference/documents?ref=labelbox-guides.ghost.io\"\u003eupload\u003c/a\u003e whole PDF files\u003c/li\u003e\u003cli\u003eEasily \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#navigate-the-document\"\u003enavigate pages and zoom in \u0026amp; out\u003c/a\u003e \u003c/li\u003e\u003cli\u003eCreate and use a \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#custom-text-layer\"\u003ecustom text layer\u003c/a\u003e\u003c/li\u003e\u003cli\u003eSave and \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#exporting-raw-text\"\u003eexport raw text\u003c/a\u003e \u003c/li\u003e\u003cli\u003eCreate \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#entity\"\u003eentities\u003c/a\u003e (for NER) - including \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#token-selection\"\u003etokenization\u003c/a\u003e at the word-level \u0026amp; character-level \u003c/li\u003e\u003cli\u003eCreate \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#bounding-box\"\u003ebounding boxes\u003c/a\u003e (for OCR) \u003c/li\u003e\u003cli\u003eUse \u003ca href=\"https://docs.labelbox.com/reference/import-document-annotations?ref=labelbox-guides.ghost.io#bounding-box\"\u003emodel-assisted labeling\u003c/a\u003e to import bounding boxes \u003c/li\u003e\u003cli\u003eCreate \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#relationships\"\u003eannotation relationships\u003c/a\u003e - including between annotations that span different pages\u003c/li\u003e\u003cli\u003eClassify your PDF - with \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#radio-classification\"\u003eradio\u003c/a\u003e, \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#checklist-classification\"\u003echecklist\u003c/a\u003e, and \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#free-text-classification\"\u003efree-text\u003c/a\u003e classification \u003c/li\u003e\u003cli\u003eUse \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io#document-specific-hotkeys\"\u003ehotkeys\u003c/a\u003e to speed up your workflow \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo learn more about our Document editor, please refer to our \u003ca href=\"https://docs.labelbox.com/docs/document-annotations?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e","comment_id":"63275a80bb4a31003d53748b","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2681.png","featured":false,"visibility":"public","created_at":"2022-09-18T17:50:56.000+00:00","updated_at":"2023-10-26T18:17:00.000+00:00","published_at":"2022-10-09T18:14:00.000+00:00","custom_excerpt":"Easily turn stores of documents and PDF files into performant ML models with our Document editor. With the ability to use an NER text layer alongside OCR techniques, teams can annotate text, images, graphs, and more without losing context. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-natively-annotate-a-pdf-document/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-natively-annotate-a-pdf-document/","excerpt":"Easily turn stores of documents and PDF files into performant ML models with our Document editor. With the ability to use an NER text layer alongside OCR techniques, teams can annotate text, images, graphs, and more without losing context. ","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2681-2.png","og_title":"How to natively annotate a PDF document","og_description":"In this guide, we'll be walking through how teams can easily turn stores of documents and PDF files into performant ML models with our Document editor. With the ability to use an NER text layer alongside OCR techniques, teams can annotate text, images, graphs, and more without losing context. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2681-1.png","twitter_title":"How to natively annotate a PDF document","twitter_description":"In this guide, we'll be walking through how teams can easily turn stores of documents and PDF files into performant ML models with our Document editor. With the ability to use an NER text layer alongside OCR techniques, teams can annotate text, images, graphs, and more without losing context. ","meta_title":"How to natively annotate a PDF document | Quantumworks Lab","meta_description":"With the ability to use an NER text layer alongside OCR techniques, teams can annotate text, images, graphs, and more without losing context. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"633c4d4bc39859004d2e16cf","uuid":"ed5983f1-aa56-41ca-8d13-4287a88a28b2","title":"How to find and fix model errors","slug":"how-to-find-and-fix-model-errors","html":"\u003cp\u003eA great way to boost model performance is to surface edge cases on which the model is struggling. You can fix those model failures with targeted improvements to your training data so that the model becomes better on these edge cases. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/jzt02qjlts\" title=\"How to find and fix model errors Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-center\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1SRcoEo4-UMiKiX4CPBP82UlLi6OtItn5?ref=labelbox-guides.ghost.io#scrollTo=8bedb521\" class=\"kg-btn kg-btn-accent\"\u003eTry the Google Colab\u003c/a\u003e\u003c/div\u003e\u003cp\u003eOnce you upload your model predictions and model metrics to Quantumworks Lab, you can use your trained model as a guide to find model failure and edge cases. With quantitative and visual inspection, you can identify challenging edge cases and then use \u003ca href=\"https://labelbox.com/product/catalog/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Catalog\u003c/a\u003e to find similar unlabeled data to improve your model. \u003c/p\u003e\u003cp\u003e\u003cu\u003eHere's a systematic process that can help teams easily surface and fix model errors:\u003c/u\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStep 1\u003c/strong\u003e: Look for assets where your model predictions and labels disagree – one way to do that is to look at model metrics and surface clusters of data points where the model is struggling.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStep 2:\u003c/strong\u003e Visualize these challenge cases and identify patterns of model failures. Prioritize the most important model failures to fix.  \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStep 3:\u003c/strong\u003e Now that you've identified edge cases that need fixing, the goal is to find unlabeled data points that are most similar to the challenge case – these are high-impact data points that you want to label in priority.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eStep 4:\u003c/strong\u003e When you re-train the model on this newly labeled data, the model will learn to make better predictions on the newly added data points and won't struggle as it did before. In a few steps, you've fixed your model errors and have boosted model performance. \u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow else can you benefit from a data-engine like Quantumworks Lab?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTeams can also drive data-centric iterations by finding and fixing label errors to improve model performance. Learn more about how to find and fix label errors in in this \u003ca href=\"https://labelbox.com/guides/how-to-find-and-fix-label-errors/?ref=labelbox-guides.ghost.io\"\u003eguide\u003c/a\u003e our \u003ca href=\"https://docs.labelbox.com/docs/identify-labeling-mistakes?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e","comment_id":"633c4d4bc39859004d2e16cf","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2818--1-.png","featured":false,"visibility":"public","created_at":"2022-10-04T15:12:11.000+00:00","updated_at":"2023-10-26T18:23:24.000+00:00","published_at":"2022-10-05T13:43:59.000+00:00","custom_excerpt":"A great way to boost model performance is to surface edge cases on which the model might be struggling. You can fix those model failures with targeted improvements to your training data so that the model is better trained on these edge cases. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-find-and-fix-model-errors/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-find-and-fix-model-errors/","excerpt":"A great way to boost model performance is to surface edge cases on which the model might be struggling. You can fix those model failures with targeted improvements to your training data so that the model is better trained on these edge cases. ","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2818-2.png","og_title":"How to find and fix model errors","og_description":"A great way to boost model performance is to surface edge cases on which the model might be struggling. You can fix those model failures with targeted improvements to your training data so that the model is better trained on these edge cases. ","twitter_image":null,"twitter_title":"How to find and fix model errors","twitter_description":"A great way to boost model performance is to surface edge cases on which the model might be struggling. You can fix those model failures with targeted improvements to your training data so that the model is better trained on these edge cases. ","meta_title":"How to find and fix model errors | Quantumworks Lab","meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"633629c01beec0003d38bdf3","uuid":"b2f4bcf2-b0bf-4906-a6bd-2571ed491fea","title":"How to annotate conversational text for chatbot use cases","slug":"how-to-annotate-conversational-text-for-chatbot-use-cases","html":"\u003cp\u003eAs AI continues to grow, there's been a rapid adoption in the industrial application of language-based models. Teams interested in training chatbots or other language models can now use Quantumworks Lab's Conversational Text editor. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7b3knr04l0\" title=\"How to annotate conversational text for chatbot use cases Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eOur Conversational Text editor allows teams to create unique message-based classifications that can identify user intent or sentiment. Teams can easily train an open-source model on their own data and use Quantumworks Lab's suite of tools across Annotate, Catalog, and Model to quickly tailor their language model to meet their specific business needs.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWith our Conversational Text editor, teams can:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eNatively \u003ca href=\"https://docs.labelbox.com/reference/text-conversational?ref=labelbox-guides.ghost.io\"\u003eimport\u003c/a\u003e conversational text or thread-based messages\u003c/li\u003e\u003cli\u003eCreate \u003ca href=\"https://docs.labelbox.com/docs/conversational-annotations?ref=labelbox-guides.ghost.io#entity\"\u003eentities\u003c/a\u003e for NER \u003c/li\u003e\u003cli\u003eCreate \u003ca href=\"https://docs.labelbox.com/docs/conversational-annotations?ref=labelbox-guides.ghost.io#message-based-classifications---radio\"\u003emessage-based classifications\u003c/a\u003e to identify user intent or sentiment \u003c/li\u003e\u003cli\u003eCreate radio, checklist, or free-form text classifications\u003c/li\u003e\u003cli\u003eCreate \u003ca href=\"https://docs.labelbox.com/docs/conversational-annotations?ref=labelbox-guides.ghost.io#relationship\"\u003eannotation relationships\u003c/a\u003e between entities\u003c/li\u003e\u003cli\u003eUse \u003ca href=\"https://docs.labelbox.com/docs/conversational-annotations?ref=labelbox-guides.ghost.io#text-specific-hotkeys\"\u003ehotkeys\u003c/a\u003e to speed up labeling\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo learn more about our Conversational Text editor, refer to our \u003ca href=\"https://docs.labelbox.com/docs/conversational-annotations?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e","comment_id":"633629c01beec0003d38bdf3","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2792.png","featured":false,"visibility":"public","created_at":"2022-09-29T23:26:56.000+00:00","updated_at":"2023-10-26T18:19:27.000+00:00","published_at":"2022-09-30T21:17:29.000+00:00","custom_excerpt":"Teams can easily train an open-source model on their own data and use Quantumworks Lab's suite of tools across Annotate, Catalog, and Model to quickly tailor their language model to meet their specific business needs.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-annotate-conversational-text-for-chatbot-use-cases/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5fc375d13000123d7f8","name":"Industry: Retail \u0026 e-commerce","slug":"industry-retail-e-commerce","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-retail-e-commerce/"},{"id":"653aa630375d13000123d800","name":"Industry: Manufacturing","slug":"industry-manufacturing","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-manufacturing/"},{"id":"653aa623375d13000123d7fe","name":"Industry: Internet \u0026 media","slug":"industry-internet-media","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-internet-media/"},{"id":"653aa608375d13000123d7fa","name":"Industry: Finance \u0026 insurance","slug":"industry-finance-insurance","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-finance-insurance/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-annotate-conversational-text-for-chatbot-use-cases/","excerpt":"Teams can easily train an open-source model on their own data and use Quantumworks Lab's suite of tools across Annotate, Catalog, and Model to quickly tailor their language model to meet their specific business needs.","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2792-2.png","og_title":"How to annotate conversational text for chatbot use cases","og_description":"Teams can easily train an open-source model on their own data and use Quantumworks Lab's suite of tools across Annotate, Catalog, and Model to quickly tailor their language model to meet their specific business needs.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2792-1.png","twitter_title":"How to annotate conversational text for chatbot use cases","twitter_description":"Teams can easily train an open-source model on their own data and use Quantumworks Lab's suite of tools across Annotate, Catalog, and Model to quickly tailor their language model to meet their specific business needs.","meta_title":"How to annotate conversational text for chatbot use cases | Quantumworks Lab","meta_description":"Teams can easily train an open-source model and use Quantumworks Lab to quickly tailor their language model to meet their specific business needs.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"632e177f16e912003d39b2bb","uuid":"3a1d718f-8dba-46ed-a8a3-d45ed71360b3","title":"How to run model-assisted labeling and active learning on NER data with a 🤗Hugging Face model","slug":"how-to-run-model-assisted-labeling-with-active-learning-on-ner-data-with-a-hugging-face-model","html":"\u003cp\u003eNot all data impacts model performance equally. In fact, a huge roadblock many teams face is being able to leverage automation to speed up labeling to go through even faster data-centric iterations. \u003c/p\u003e\u003cp\u003eIn this guide, we'll be showing you how you can efficiently improve models in development and production by using a third-party model, such as 🤗Hugging Face, to guide and identify targeted improvements in your training data to boost model performance. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/bn1is51w76\" title=\"How to run model-assisted labeling and active learning on NER data with a HuggingFace model Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003e\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTry the Colab: \u003ca href=\"https://colab.research.google.com/drive/1ovL6BiLqh81KXoSC1CYdOKuaOF6lAG3X?ref=labelbox-guides.ghost.io#scrollTo=-ZW42JeD22fq\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eLearn more about \u003ca href=\"https://docs.labelbox.com/docs/upload-model-predictions?ref=labelbox-guides.ghost.io\"\u003eActive Learning\u003c/a\u003e and the benefits of prioritizing high-value data\u003c/li\u003e\u003cli\u003eLearn more about \u003ca href=\"https://docs.labelbox.com/docs/upload-model-predictions?ref=labelbox-guides.ghost.io\"\u003ehow to import your model predictions \u0026amp; metrics\u003c/a\u003e in Quantumworks Lab\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1ovL6BiLqh81KXoSC1CYdOKuaOF6lAG3X?ref=labelbox-guides.ghost.io#scrollTo=-ZW42JeD22fq\"\u003eGoogle Colab Notebook\u003c/a\u003e (used in the video) to run MAL and active learning on NER data with the Hugging Face model \u003c/li\u003e\u003c/ul\u003e","comment_id":"632e177f16e912003d39b2bb","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2760.png","featured":false,"visibility":"public","created_at":"2022-09-23T20:30:55.000+00:00","updated_at":"2023-10-26T18:16:11.000+00:00","published_at":"2022-09-23T22:47:09.000+00:00","custom_excerpt":"Efficiently improve models in development and production by using a third-party model, such as HuggingFace, to guide and identify targeted improvements in your training data to boost model performance. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-run-model-assisted-labeling-with-active-learning-on-ner-data-with-a-hugging-face-model/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa5be375d13000123d7f4","name":"Train \u0026 fine-tune AI","slug":"train-fine-tune-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#F68C00","url":"https://labelbox-guides.ghost.io/tag/train-fine-tune-ai/"},"url":"https://labelbox-guides.ghost.io/how-to-run-model-assisted-labeling-with-active-learning-on-ner-data-with-a-hugging-face-model/","excerpt":"Efficiently improve models in development and production by using a third-party model, such as HuggingFace, to guide and identify targeted improvements in your training data to boost model performance. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":"How to run model-assisted labeling and active learning on NER data with a HuggingFace model","og_description":"Efficiently improve models in development and production by using a third-party model, such as HuggingFace, to guide and identify targeted improvements in your training data to boost model performance. ","twitter_image":null,"twitter_title":"How to run model-assisted labeling and active learning on NER data with a HuggingFace model","twitter_description":"Efficiently improve models in development and production by using a third-party model, such as HuggingFace, to guide and identify targeted improvements in your training data to boost model performance. ","meta_title":"how-to-run-model-assisted-labeling-with-active-learning-on-ner-data-with-a-hugging-face-model | Quantumworks Lab","meta_description":"Efficiently improve models in development and production by using a third-party model, such as HuggingFace to boost model performance. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"632b4fb516e912003d39b1b6","uuid":"7e2371f4-daa6-4da2-a67a-6130f8fed8e6","title":"Fundamental elements of the Quantumworks Lab editor","slug":"fundamental-elements-of-the-labelbox-editor","html":"\u003cp\u003eAligning with your team on key terms used in Quantumworks Lab will serve to greatly enhance collaboration and cohesiveness throughout your work in the platform. In this brief video, we introduce the fundamental elements of the Quantumworks Lab editor.\u003c/p\u003e\u003cp\u003eFor a complete list of key definitions, check out our documentation \u003ca href=\"https://docs.labelbox.com/docs/key-definitions?ref=labelbox-guides.ghost.io\" rel=\"noopener noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/fkluhsbns2\" title=\"Fundamental elements of the Quantumworks Lab Editor Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/annotation?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eAnnotation\u003c/strong\u003e\u003c/a\u003e: An instance of a Feature. Annotations can be imported as ground truth, model predictions, or can be created in the Quantumworks Lab editor. Annotations are categorized as \u003cem\u003eObjects\u003c/em\u003e (e.g. bounding box, polygon, etc) or \u003cem\u003eClassifications\u003c/em\u003e (e.g. radio, checklist, etc).\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/data-import-format-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eAsset\u003c/strong\u003e\u003c/a\u003e: A single cloud-hosted file to be labeled (e.g., an image, a video, or a text file).\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/data-row?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eData Row\u003c/strong\u003e\u003c/a\u003e: The container that houses all of the following information for a single Asset:\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\t\u003cem\u003e\tURL to your cloud-hosted file\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tMetadata\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tMedia attributes (e.g data type, size, etc.)       \u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\t\tAttachments (files that provide context for your labelers) \u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/annotate-overview?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eEditor\u003c/strong\u003e\u003c/a\u003e: The labeling interface you can use to create, review, and edit annotations. When you create a project, you will be prompted to configure your editor (i.e., select an ontology, add labeling instructions, etc).\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/working-with-features?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eFeature\u003c/strong\u003e\u003c/a\u003e: A feature is the master definition of what you want the model to predict. It is also the blueprint for your ground truth. An ontology is made up of a set of features. There are two kinds of features: objects (e.g., Bounding box) and classifications (e.g., Radio). A feature can have multiple deeply nested sub-classifications.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/reference/label-export?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eLabel\u003c/strong\u003e\u003c/a\u003e: A collection of all annotations on a Data Row. For example, all Bounding boxes, Polylines, and Radio classifications on an image would be considered the “Label”.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eOntology\u003c/strong\u003e\u003c/a\u003e: A collection of Features and their relationships (also known as a taxonomy). Ontologies can be reused across different projects. It is essential for data labeling, model training, and evaluation. When you are in the editor, the ontology is what appears in the “Tools” panel.\u003c/li\u003e\u003c/ul\u003e","comment_id":"632b4fb516e912003d39b1b6","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2710.png","featured":false,"visibility":"public","created_at":"2022-09-21T17:53:57.000+00:00","updated_at":"2023-10-26T18:02:56.000+00:00","published_at":"2022-09-22T15:33:25.000+00:00","custom_excerpt":"Aligning with your team on key terms used in Quantumworks Lab will serve to greatly enhance collaboration and cohesiveness throughout your work in the platform. In this brief video, we introduce the fundamental elements of the Quantumworks Lab Editor.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/fundamental-elements-of-the-labelbox-editor/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/fundamental-elements-of-the-labelbox-editor/","excerpt":"Aligning with your team on key terms used in Quantumworks Lab will serve to greatly enhance collaboration and cohesiveness throughout your work in the platform. In this brief video, we introduce the fundamental elements of the Quantumworks Lab Editor.","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2710-2.png","og_title":"Fundamental elements of the Quantumworks Lab editor","og_description":"Aligning with your team on key terms used in Quantumworks Lab will serve to greatly enhance collaboration and cohesiveness throughout your work in the platform. In this brief video, we introduce the fundamental elements of the Quantumworks Lab Editor.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2710-1.png","twitter_title":"Fundamental elements of the Quantumworks Lab editor","twitter_description":"Aligning with your team on key terms used in Quantumworks Lab will serve to greatly enhance collaboration and cohesiveness throughout your work in the platform. In this brief video, we introduce the fundamental elements of the Quantumworks Lab Editor.","meta_title":"Fundamental elements of the Quantumworks Lab editor | Quantumworks Lab","meta_description":"Aligning with your team on key terms used in Quantumworks Lab will  greatly enhance collaboration and cohesiveness throughout your work in the platform","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63312d8216e912003d39b326","uuid":"aede2bf8-e2c5-4f2c-8630-dfe192ab4e95","title":"How to sync your cloud storage with Catalog","slug":"how-to-sync-your-cloud-storage-with-catalog","html":"\u003cp\u003eFor many ML teams, a data pipeline that keeps data, such as Data Rows, attachments, and metadata, between their cloud storage bucket and Quantumworks Lab Catalog in sync is critical. In this guide, we'll show you how to setup Google Cloud Functions to keep your data in sync. A similar approach can be applied to Amazon S3 and Microsoft Azure. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/t88happbah\" title=\"How to sync your cloud storage with Catalog Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eYou can sync your cloud storage with Catalog by using two cloud functions: upload_asset.py, triggered upon uploading a new asset to the bucket, and \u003cem\u003edelete_asset\u003c/em\u003e.py, triggered when an asset has been deleted from the bucket.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://github.com/Quantumworks Lab/Quantumworks Lab/blob/master/Guide/Data%20curation/Google%20cloud%20function/upload_asset.py?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eupload_asset.py \u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab\nfrom Quantumworks Lab import Client, Dataset\n\n# Add your API key below\nLABELBOX_API_KEY = \"\"\nclient = Client(api_key=LABELBOX_API_KEY)\n\n\ndef upload_asset(event, context):\n    \"\"\"Uploads an asset to Catalog when a new asset is uploaded to GCP bucket. \n       If a dataset with bucket_name exists in Catalog, then an asset is added to that dataset. Otherwise, a new dataset is created.\n    Args:\n         event (dict): Event payload.\n         context (google.cloud.functions.Context): Metadata for the event.\n    \"\"\"\n    file = event\n    bucket_name = file['bucket']\n    object_name = file[\"name\"]\n    datasets = client.get_datasets(where=Dataset.name == bucket_name)\n    dataset = next(datasets, None)\n    if not dataset:\n      dataset = client.create_dataset(name=bucket_name, iam_integration='DEFAULT')\n    url = f\"gs://{bucket_name}/{object_name}\"\n    dataset.create_data_row(row_data=url, global_key=object_name)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003ca href=\"https://github.com/Quantumworks Lab/Quantumworks Lab/blob/master/Guide/Data%20curation/Google%20cloud%20function/delete_asset.py?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003edelete_asset.py\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport Quantumworks Lab\nfrom Quantumworks Lab import Client\nfrom labelbox.schema.data_row import DataRow\n\n# Add your API key below\nLABELBOX_API_KEY = \"\"\nclient = Client(api_key=LABELBOX_API_KEY)\n\n\ndef delete_asset(event, context):\n    \"\"\"Deletes the asset from Catalog when the asset is deleted from GCP bucket using global key.\n    Args:\n          event (dict): Event payload.\n          context (google.cloud.functions.Context): Metadata for the event.\n    \"\"\"\n    file = event\n\n    # Sets the file's name as global key\n    global_key = file[\"name\"]\n    res = client.get_data_row_ids_for_global_keys(global_key)\n\n    if res[\"status\"] == \"SUCCESS\":\n        delete_datarow_id = res[\"results\"][0]\n        data_row = client._get_single(DataRow, delete_datarow_id)\n        data_row.delete()\n    else:\n        print(\"Global key does not exist\")\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou can also learn more about how Catalog can help you curate your unstructured data with precision in our \u003ca href=\"https://docs.labelbox.com/docs/catalog-overview?ref=labelbox-guides.ghost.io#getting-started-with-catalog\"\u003edocumentation\u003c/a\u003e. \u003c/p\u003e","comment_id":"63312d8216e912003d39b326","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/09/Group-2772.png","featured":false,"visibility":"public","created_at":"2022-09-26T04:41:38.000+00:00","updated_at":"2023-10-26T18:09:49.000+00:00","published_at":"2022-09-02T17:17:00.000+00:00","custom_excerpt":"For many ML teams, a data pipeline that keeps data between their cloud storage bucket and Quantumworks Lab Catalog in sync is critical. Learn how to setup Google Cloud Functions to keep your data in sync. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-sync-your-cloud-storage-with-catalog/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-sync-your-cloud-storage-with-catalog/","excerpt":"For many ML teams, a data pipeline that keeps data between their cloud storage bucket and Quantumworks Lab Catalog in sync is critical. Learn how to setup Google Cloud Functions to keep your data in sync. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":"How to sync your cloud storage with Catalog","og_description":"For many ML teams, a data pipeline that keeps data between their cloud storage bucket and Quantumworks Lab Catalog in sync is critical. Learn how to setup Google Cloud Functions to keep your data in sync. ","twitter_image":null,"twitter_title":"How to sync your cloud storage with Catalog","twitter_description":"For many ML teams, a data pipeline that keeps data between their cloud storage bucket and Quantumworks Lab Catalog in sync is critical. Learn how to setup Google Cloud Functions to keep your data in sync. ","meta_title":"How to sync your cloud storage with Catalog | Quantumworks Lab","meta_description":"For many ML teams, a data pipeline that keeps data between their cloud storage bucket and Quantumworks Lab Catalog in sync is critical. Learn how to setup Google Cloud Functions to keep your data in sync. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6344b98cb75421003d6e6019","uuid":"5a7fea79-24bd-4241-a5cd-841e07066f08","title":"How to create and manage ontologies and features","slug":"how-to-create-and-manage-ontologies","html":"\u003cp\u003eA clean, thoughtful ontology is critical for creating high-quality labeled data with minimal errors and inconsistencies. Ontologies are an essential part of the Quantumworks Lab labeling platform. Every time you create a project or a model in Quantumworks Lab, you will need to select an ontology.\u003c/p\u003e\u003cp\u003eOntologies and features should be created and managed with the goals of proper labeling, efficiency, and reusability in mind.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/nuxpfgxd6x\" title=\"How to create and manage ontologies \u0026amp; features Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003cp\u003eWhen creating and implementing an ontology, here are some helpful tips to consider:\u003c/p\u003e\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThoroughly understand the task and the tools at your disposal.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReview the basic documentation for \u003ca href=\"https://docs.labelbox.com/docs/labelbox-ontology?ref=labelbox-guides.ghost.io\"\u003eOntologies\u003c/a\u003e and \u003ca href=\"https://docs.labelbox.com/docs/working-with-features?ref=labelbox-guides.ghost.io\"\u003eFeatures\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eCheck the \u003ca href=\"https://docs.labelbox.com/docs/image-annotations?ref=labelbox-guides.ghost.io\"\u003esupported annotation types\u003c/a\u003e for different editors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate an ontology that follows the most logical workflow for a labeler.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eChoose tools that will allow labelers to reach the highest speed and quality possible.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate and upload detailed \u003ca href=\"https://docs.labelbox.com/docs/labeling-instructions?ref=labelbox-guides.ghost.io\"\u003einstructions\u003c/a\u003e, considering known edge cases that labelers will encounter.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTest your ontology in a brief trial run before sending the project to the labeling team.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"python-sdk\"\u003ePython SDK\u003c/h3\u003e\u003cdiv class=\"kg-card kg-button-card kg-align-left\"\u003e\u003ca href=\"https://colab.research.google.com/github/Quantumworks Lab/labelbox-python/blob/develop/examples/basics/ontologies.ipynb?ref=labelbox-guides.ghost.io\" class=\"kg-btn kg-btn-accent\"\u003eTry the Google Colab\u003c/a\u003e\u003c/div\u003e\u003cp\u003e\u003cbr\u003e\u003cstrong\u003eCreate features and ontologies:\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\n# Import packages\nfrom Quantumworks Lab import OntologyBuilder, Tool, Classification, Option, Client\n\n# Establish client\nclient = Client()\n\n# Create a bounding box tool with a nested radio subclass\nobject_features = [\n    Tool(\n        tool=Tool.Type.BBOX,\n        name=\"regulatory-sign\",\n        color=\"#ff0000\",\n        classifications=[\n            Classification(\n                class_type=Classification.Type.RADIO,\n                instructions=\"Sign type\",\n                options=[\n                    Option(value=\"stop\", label=\"Stop\"),\n                    Option(value=\"one_way\", label=\"One way\")\n                ]\n            )\n        ]\n    )\n]\n\n# Create a global checklist classification\nclassification_features = [\n    Classification(\n        class_type=Classification.Type.CHECKLIST,\n        instructions=\"Quality Issues\",\n        options=[\n            Option(value=\"blurry\", label=\"Blurry\"),\n            Option(value=\"distorted\", label=\"Distorted\")\n        ]\n    )\n\n]\n\n# Use the above features to create an ontology\nontology_builder = OntologyBuilder(\n    tools=object_features,\n    classifications=classification_features\n)\n\nontology = client.create_ontology(\n    \"SDK Demo\",\n    ontology_builder.asdict()\n)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eCreate an ontology from existing features:\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Search for feature by name in your org\nfeature_1 = next(client.get_feature_schemas(\"\u0026lt;TOOL_NAME\u0026gt;\"))\nfeature_2 = next(client.get_feature_schemas(\"\u0026lt;TOOL_NAME\u0026gt;\"))\n\n# Get feature by feature schema ID (you can find this in the UI)\nfeature_3 = client.get_feature_schema(\"\u0026lt;FEATURE_SCHEMA_ID\u0026gt;\")\n\n# View the features\nprint(feature_1)\nprint(feature_2)\nprint(feature_3)\n\nontology = client.create_ontology_from_feature_schemas(\n    \"Ontology of shared features\", \n    [feature_1.uid, feature_2.uid, feature_3.uid]\n)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eTo view more common SDK methods for ontology management, check out our documentation \u003ca href=\"https://docs.labelbox.com/reference/ontology?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","comment_id":"6344b98cb75421003d6e6019","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2791--9-.png","featured":false,"visibility":"public","created_at":"2022-10-11T00:32:12.000+00:00","updated_at":"2024-11-21T18:10:27.000+00:00","published_at":"2022-09-02T14:54:00.000+00:00","custom_excerpt":"Ontologies are an essential part of Quantumworks Lab's platform. You'll need to select an ontology when you create a new project or model. Learn how to create, reuse, and manage your ontologies and features. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-create-and-manage-ontologies/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"6323919944f246003d341ab8","name":"Labeling automation","slug":"labeling-automation","description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Guides | Labeling operations","meta_description":"Leverage an end-to-end system that features the full set of capabilities needed to improve your model’s performance.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/labeling-automation/"},"url":"https://labelbox-guides.ghost.io/how-to-create-and-manage-ontologies/","excerpt":"Ontologies are an essential part of Quantumworks Lab's platform. You'll need to select an ontology when you create a new project or model. Learn how to create, reuse, and manage your ontologies and features. ","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2791--9--2.png","og_title":"How to create and manage ontologies and features","og_description":"Ontologies are an essential part of Quantumworks Lab's platform. You'll need to select an ontology when you create a new project or model. Learn how to create and manage your ontologies and features. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2791--9--1.png","twitter_title":"How to create and manage ontologies and features","twitter_description":"Ontologies are an essential part of Quantumworks Lab's platform. You'll need to select an ontology when you create a new project or model. Learn how to create and manage your ontologies and features. ","meta_title":"How to create and manage ontologies and features | Quantumworks Lab","meta_description":"Ontologies are an essential part of Quantumworks Lab's platform. Learn how to create and manage your ontologies and features. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63500b00928368003d1c831b","uuid":"8fe0b6ba-82e0-424d-8e40-a82fc7c03013","title":"How to set up a delegated access integration between Amazon S3 \u0026 Quantumworks Lab","slug":"how-to-set-up-a-delegated-access-integration-between-amazons3-and-labelbox","html":"\u003cp\u003eAt Quantumworks Lab, we believe that your data should stay your data. We give all users the option to integrate their cloud buckets via IAM Delegated Access. \u003c/p\u003e\u003cp\u003eThe AWS delegated access integration allows you to keep your data rows in Amazon S3 while being able to work with it in Labelbox.\u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/rfyru01x9x\" title=\"How to set up a delegated access integration between Amazon S3 \u0026amp; Quantumworks Lab Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"try-it-now-recipe-link\"\u003eTry it now: \u003ca href=\"https://docs.labelbox.com/docs/import-aws-s3-data?ref=labelbox-guides.ghost.io\"\u003eRecipe link\u003c/a\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eData will be represented as signed ephemeral URLs in the Quantumworks Lab platform and be securely rendered in our Catalog, Annotate, and Models products.\u003c/li\u003e\u003cli\u003eThis allows you to keep your bucket private and secured while annotating, searching, and diagnosing model performance on datasets.\u003c/li\u003e\u003cli\u003eKeep in mind you will need Admin permissions in AWS to complete this integration.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDelegated Access allows you to securely and seamlessly host your unlabeled data in your preferred cloud storage provider while providing Quantumworks Lab with the limited access necessary so you can view and label your data in Labelbox. This allows you to use native Identity and Access Management (IAM) roles and policies to control access.\u003c/p\u003e\u003cp\u003eYou can learn more about IAM Delegated Access or the integration with Amazon S3 in our \u003ca href=\"https://docs.labelbox.com/docs/import-aws-s3-data?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e","comment_id":"63500b00928368003d1c831b","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2852--1-.png","featured":false,"visibility":"public","created_at":"2022-10-19T14:34:40.000+00:00","updated_at":"2023-10-26T18:08:51.000+00:00","published_at":"2022-09-01T20:19:00.000+00:00","custom_excerpt":"The AWS delegated access integration allows you to keep your data rows in Amazon S3 while being able to work with it in Labelbox.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-amazons3-and-labelbox/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-set-up-a-delegated-access-integration-between-amazons3-and-labelbox/","excerpt":"The AWS delegated access integration allows you to keep your data rows in Amazon S3 while being able to work with it in Labelbox.","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2852--1--2.png","og_title":"How to set up a delegated access integration between Amazon S3 \u0026 Quantumworks Lab","og_description":"The AWS delegated access integration allows you to keep your data rows in Amazon S3 while being able to work with it in Labelbox.","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2852--1--1.png","twitter_title":"How to set up a delegated access integration between Amazon S3 \u0026 Quantumworks Lab","twitter_description":"The AWS delegated access integration allows you to keep your data rows in Amazon S3 while being able to work with it in Labelbox.","meta_title":"How to set up a delegated access integration between Amazon S3 \u0026 Quantumworks Lab","meta_description":"The AWS delegated access integration allows you to keep your data rows in Amazon S3 while being able to work with it in Labelbox.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"634ffb83928368003d1c82bb","uuid":"d41df20c-83db-4fc6-a649-14895006fb0f","title":"How to set up a delegated access integration between Azure Blob Storage \u0026 Quantumworks Lab","slug":"how-to-set-up-a-delegated-access-integration-between-azure-and-labelbox","html":"\u003cp\u003eAt Quantumworks Lab, we believe that your data should stay your data. We give all users the option to integrate their cloud buckets via IAM Delegated Access. \u003c/p\u003e\u003cp\u003eThe Azure Delegated Access integration allows you to keep your data rows in Azure Blob Storage while being able to work with it in the Quantumworks Lab platform. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/ejl828cqj6\" title=\"How to set up a delegated access integration between Azure Blob Storage \u0026amp; Quantumworks Lab Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"try-it-now\"\u003eTry it now\u003c/h3\u003e\u003cul\u003e\u003cli\u003eData will be represented as signed ephemeral URLs in the Quantumworks Lab platform and be securely rendered in our Catalog, Annotate, and Models products.\u003c/li\u003e\u003cli\u003eThis allows you to keep your bucket private and secure while annotating, searching, and diagnosing model performance on datasets.\u003c/li\u003e\u003cli\u003eKeep in mind that you will need Admin permissions in Azure to complete this integration. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDelegated Access allows you to securely and seamlessly host your unlabeled data in your preferred cloud storage provider while providing Quantumworks Lab with the limited access necessary so you can view and label your data in Labelbox. This allows you to use native Identity and Access Management (IAM) roles and policies to control access.\u003c/p\u003e\u003cp\u003eYou can learn more about IAM Delegated Access or the integration with Azure Blob Storage in our \u003ca href=\"https://docs.labelbox.com/docs/microsoft-azure-blob-storage?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e","comment_id":"634ffb83928368003d1c82bb","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2851.png","featured":false,"visibility":"public","created_at":"2022-10-19T13:28:35.000+00:00","updated_at":"2024-03-21T15:09:40.000+00:00","published_at":"2022-09-01T20:19:00.000+00:00","custom_excerpt":"The Azure Delegated Access integration allows you to keep your data rows in Azure Blob Storage while being able to work with it in the Quantumworks Lab platform. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-azure-and-labelbox/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-set-up-a-delegated-access-integration-between-azure-and-labelbox/","excerpt":"The Azure Delegated Access integration allows you to keep your data rows in Azure Blob Storage while being able to work with it in the Quantumworks Lab platform. ","reading_time":1,"access":true,"comments":false,"og_image":null,"og_title":"How to set up a delegated access integration between Azure Blob Storage \u0026 Quantumworks Lab","og_description":"The Azure Delegated Access integration allows you to keep your data rows in Azure Blob Storage while being able to work with it in the Quantumworks Lab platform. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2851-1.png","twitter_title":"How to set up a delegated access integration between Azure Blob Storage \u0026 Quantumworks Lab","twitter_description":"The Azure Delegated Access integration allows you to keep your data rows in Azure Blob Storage while being able to work with it in the Quantumworks Lab platform. ","meta_title":"How to set up a delegated access integration between Azure Blob Storage \u0026 Quantumworks Lab | Quantumworks Lab","meta_description":"The Azure Delegated Access integration allows you to keep your data rows in Azure Blob Storage while being able to work with it in the Quantumworks Lab platform. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"63500fbf928368003d1c8351","uuid":"a7f1f447-315c-4f4c-8d9f-199bad27c991","title":"How to set up a delegated access integration between GCP Storage \u0026 Quantumworks Lab","slug":"how-to-set-up-a-delegated-access-integration-between-gcp-storage-labelbox","html":"\u003cp\u003eAt Quantumworks Lab, we believe that your data should stay your data. We give all users the option to integrate their cloud buckets via IAM Delegated Access. \u003c/p\u003e\u003cp\u003eThe GCP Delegated Access integration allows you to keep your data rows in GCP Storage while being able to work with it in the Quantumworks Lab platform. \u003c/p\u003e\u003cfigure class=\"kg-card kg-embed-card\"\u003e\u003ciframe src=\"https://fast.wistia.net/embed/iframe/7avv949lho\" title=\"How to set up a delegated access integration between GCP Storage \u0026amp; Quantumworks Lab Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen=\"\" width=\"960\" height=\"540\"\u003e\u003c/iframe\u003e\n\u003cscript src=\"https://fast.wistia.net/assets/external/E-v1.js\" async=\"\"\u003e\u003c/script\u003e\u003c/figure\u003e\u003ch3 id=\"try-it-now-recipe-link\"\u003eTry it now: \u003ca href=\"https://docs.labelbox.com/recipes/google-delegated-access?ref=labelbox-guides.ghost.io\"\u003eRecipe link\u003c/a\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eData will be represented as signed ephemeral URLs in the Quantumworks Lab platform and be securely rendered in our Catalog, Annotate, and Models products.\u003c/li\u003e\u003cli\u003eThis allows you to keep your bucket private and secured while annotating, searching, and diagnosing model performance on datasets.\u003c/li\u003e\u003cli\u003eKeep in mind you will need Admin permissions in GCP to complete this integration.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDelegated Access allows you to securely and seamlessly host your unlabeled data in your preferred cloud storage provider while providing Quantumworks Lab with the limited access necessary so you can view and label your data in Labelbox. This allows you to use native Identity and Access Management (IAM) roles and policies to control access.\u003c/p\u003e\u003cp\u003eYou can learn more about IAM Delegated Access or the integration with GCP Storage in our \u003ca href=\"https://docs.labelbox.com/docs/using-google-cloud-storage?ref=labelbox-guides.ghost.io\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e","comment_id":"63500fbf928368003d1c8351","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2851--1-.png","featured":false,"visibility":"public","created_at":"2022-10-19T14:54:55.000+00:00","updated_at":"2023-10-26T18:07:06.000+00:00","published_at":"2022-09-01T20:18:00.000+00:00","custom_excerpt":"The GCP Delegated Access integration allows you to keep your data rows in GCP Storage while being able to work with it in the Quantumworks Lab platform. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/how-to-set-up-a-delegated-access-integration-between-gcp-storage-labelbox/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"},{"id":"653aa598375d13000123d7f2","name":"MLOps","slug":"mlops","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/mlops/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa4e3375d13000123d7e4","name":"Explore \u0026 manage data","slug":"explore-manage-data","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#E7BF00","url":"https://labelbox-guides.ghost.io/tag/explore-manage-data/"},"url":"https://labelbox-guides.ghost.io/how-to-set-up-a-delegated-access-integration-between-gcp-storage-labelbox/","excerpt":"The GCP Delegated Access integration allows you to keep your data rows in GCP Storage while being able to work with it in the Quantumworks Lab platform. ","reading_time":1,"access":true,"comments":false,"og_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2851--1--3.png","og_title":"How to set up a delegated access between GCP Storage \u0026 Quantumworks Lab","og_description":"The GCP Delegated Access integration allows you to keep your data rows in GCP Storage while being able to work with it in the Quantumworks Lab platform. ","twitter_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Group-2851--1--2.png","twitter_title":"How to set up a delegated access between GCP Storage \u0026 Quantumworks Lab","twitter_description":"The GCP Delegated Access integration allows you to keep your data rows in GCP Storage while being able to work with it in the Quantumworks Lab platform. ","meta_title":"How to set up a delegated access between GCP Storage \u0026 Quantumworks Lab","meta_description":"The GCP Delegated Access integration allows you to keep your data rows in GCP Storage while being able to work with it in the Quantumworks Lab platform. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"636e0d66e7ffab003d39b064","uuid":"f24bb5f6-74f2-4279-8164-4d976bf4278e","title":"Introduction to Computer Vision","slug":"computer-vision","html":"\u003ch2 id=\"what-is-computer-vision\"\u003e\u003cstrong\u003eWhat is computer vision?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eComputer vision is a field of Artificial Intelligence (AI) technology that enables computer systems to perform tasks that require visual perception. In simpler terms, computer vision helps a system mimic human sight. It’s typically used in different visual applications such as computer security surveillance, manufacturing line quality inspection, cars, drones and robots navigation, medical imaging diagnosis, or automated retail stores.\u003c/p\u003e\u003cp\u003eComputer vision is a challenge in machine learning as the visual world is complex and vast, and computers are designed to work best with constrained problems.\u003c/p\u003e\u003cp\u003eFor example, a computer can quickly be programmed to perform a task like identifying images in a database with “flower” in the file name, but asking it to identify flowers within a set of images is a far more difficult task. Not all flowers look the same, and the computer will have a hard time identifying the differences in images due to lighting, camera angles, camera types, image quality, and more.\u003c/p\u003e\u003cp\u003eHowever, computer vision algorithms have recently become more robust and accessible with the availability of public datasets, which help train models. However, for enterprises embracing computer vision initiatives, publicly available algorithms and datasets may not meet their requirements.\u003c/p\u003e\u003cp\u003eEnterprise businesses need models trained on their own data for specific use cases, whether it's identifying cancer cells in microscope images of tissue samples, tracking vehicle movement, or finding weeds in photos of crops in a field. Many companies are also discovering the need for computer vision models trained on video data, or a combination of video and image data, to ensure that the training data is more representative of what the model will need to achieve performant AI. As a result of these challenges, machine learning models built for enterprise use cases are frequently more complex and more difficult to create.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/23ABhlO-DZ-8f2LGZ7mmu0VWASGDp9qRLp0efXiIqCmJL5BdAVBUWWy9UzcKWC701XB4s4XDMCtud7ZoRWE_I1D_hGyyk6OadSlqrP9Y-IA-HyRfKcQGwf1FNXj_DTvPmi0X0FUGeM09bc7E_OALgG7iR12W3pXsRLy-fLC4IJFa6_DCX0CBl81ncyFFfw\" class=\"kg-image\" alt=\"An example of video data annotated to train a computer vision model.\" loading=\"lazy\" width=\"624\" height=\"427\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eAn example of video data annotated to train a computer vision model.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"common-computer-vision-use-cases\"\u003e\u003cstrong\u003eCommon computer vision use cases\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eLet’s explore a couple of basic computer vision applications, along with some examples of enterprise use cases for computer vision.\u003c/p\u003e\u003ch3 id=\"object-detection\"\u003e\u003cstrong\u003eObject detection\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eObject detection is used for locating an object or objects within an image or a video frame from a video source. Object detection is one of the computer vision technologies used to automatically identify instances of semantic objects in natural scene images. To detect an object means to estimate its location relative to other objects within a framework model.\u003cbr\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/vjK1k186uqiovmGCR3oTrEiVp6YUK5jpT3N3X-2G7-JBVOSoxDQTo1e7M1cXKUFURIJDDc8p3-JZwKDBPaFk9zSngyrK50nYyBU_URN5oOm9hRhLgeY81Qf9Ru3ISk-jDNFuYuTTY2jN2u6CkRvcZl4wwABKZGuf0cuJrYIvwXd6V8C9RnRrfntN27Gz\" class=\"kg-image\" alt=\"This ultrasound image has potentially cancerous cell clumps outlined in bounding boxes.\" loading=\"lazy\" width=\"624\" height=\"480\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eThis ultrasound image has potentially cancerous cell clumps outlined in bounding boxes.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThis is a common computer vision use case used across industries. For example, farmers use AI solutions with models trained for object detection to find weeds, damaged plants, or other problems in photos of their field, saving them significant time and money. This type of machine learning model allows for more efficiency within crop growing, such as the ability to spray weeds only in affected areas rather than the entire field.\u003c/p\u003e\u003cp\u003eIn order to properly train a model for object detection, AI teams will need to first gather their dataset — images of the field — and have an expert botanist or agronomist label them by hand. These annotated images are then fed into the algorithm to train it. To achieve the level of accuracy required to allow farming businesses to rely on the model, the algorithm will likely need to be trained over several iteration cycles.\u003c/p\u003e\u003cp\u003eObject detection is often used in tandem with many other computer vision applications, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eObject classification:\u003c/strong\u003e The model identifies the category for each object in the image, such as the type of weed in an image of a field\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eObject identification:\u003c/strong\u003e The model finds each object in the image and identifies it\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eObject verification:\u003c/strong\u003e The model decides whether a specific object is in an image\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"image-segmentation\"\u003e\u003cstrong\u003eImage segmentation\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\"\u003eImage segmentation\u003c/a\u003e examines every pixel in an image and classifies it based on whether it belongs to a specific object. This creates a mask that defines the exact shape of the object, rather than encompassing the object in a box. Object segmentation is particularly useful when it’s important not only to identify an object but also the shape or orientation of the object.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh3.googleusercontent.com/z78RXXWr-_hiSBee6os9pOANmdLERTOAMGYMhkL4pXmHypZ7FWR-RxQSLSW4cQJbYfaapeBuUSQXjS5qE9b2Vf5nPGyXawNdEb4_EuCBw_Ymfznd3srw-FCn4nMIqjqoEgY9RwAfYUrwFbaEl68zo7u4sljD211B4tBJLlsfZ3orlyTcSZFJRqyqcWB3hw\" class=\"kg-image\" alt=\"This image captured from an aerial image shows objects such as rooftops, pools and vegetation using image segmentation masks.\" loading=\"lazy\" width=\"624\" height=\"412\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eThis image captured from an aerial image shows objects such as rooftops, pools and vegetation using image segmentation masks.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eOne example of image segmentation is an insurance use case, where a computer vision model evaluates buildings and their surroundings in an aerial image to assess risk and damage. In this case, it’s important for the model to detect the exact shape of an object — say, a collapsed wall — rather than identify the object with a \u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\"\u003ebounding box\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"how-to-train-a-computer-vision-algorithm\"\u003e\u003cstrong\u003eHow to train a computer vision algorithm\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThere are four basic steps for training a computer vision model:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnderstanding the business requirement for computer vision\u003c/li\u003e\u003cli\u003eCreating a training dataset\u003c/li\u003e\u003cli\u003eTraining the model and assessing its accuracy\u003c/li\u003e\u003cli\u003eIterating with new training data until it reaches the desired accuracy\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"understanding-the-business-requirement-for-computer-vision\"\u003eUnderstanding the business requirement for computer vision\u003c/h3\u003e\u003cp\u003eThe first step for any computer vision project is understanding the problem you're trying to solve. For example, if an e-commerce clothing company wants to tailor store recommendations based on their customers' personal style, it would benefit from a computer vision model that identifies and categorizes each piece of clothing in their catalog. A relatively simple model might be trained to analyze an image and note the following information about the piece of clothing within it:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSKU#\u003c/li\u003e\u003cli\u003eWhat type of item it is (top, bottom, dress)\u003c/li\u003e\u003cli\u003eThe color of the item\u003c/li\u003e\u003cli\u003eAny patterns or prints on the item\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWith a computer vision model that accurately identifies this data, the company would then be able to ensure that a prospective customer receives tailored recommendations based on their purchase history and other demographic or psychographic data.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh3.googleusercontent.com/fG99wPEW1A8s1MJVRN0IqfxTTnJIMSa_ucTB1Jf0GOJefrIKjTKkr32lTok8ibRenZ4IKmi8svivu80lcgtKIVDIykC7AOsJ9Ftv67sIzHaTyixpxcuOQVMob4soJnGfDiPPn-SGCrb11dSnukoJayJxWRfCHKL0tLukBS_1uFjfAfSa3NQfXKzElZ6YSw\" class=\"kg-image\" alt=\"This image shows a computer vision model being trained to detect different types of retail products such as pants, shirts, hoodies, and sweatshirts.\" loading=\"lazy\" width=\"624\" height=\"316\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eThis image shows a computer vision model being trained to detect different types of retail products such as pants, shirts, hoodies, and sweatshirts.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eA more complex version of this computer vision model would be trained to categorize apparel further down to the type of fabric, style (such as a button-down compared to a v-neck shirt), what other pieces in the catalog it might be paired with, and more.\u003c/p\u003e\u003ch3 id=\"creating-a-training-dataset\"\u003eCreating a training dataset\u003c/h3\u003e\u003cp\u003eOnce the AI team has determined exactly what they want their computer vision model to do and what role it will play, they must then develop the training data set.\u003c/p\u003e\u003cp\u003eTraining data is arguably the most crucial element of the machine learning process. High-quality training data will accelerate your computer vision model's path to production while low-quality data may cause expensive delays and a poor performing model. The model learns to “see” based on what it is given, so ensuring that your training data is accurate and representative of what the model will need to evaluate in production is paramount to success.\u003c/p\u003e\u003cp\u003eTo create robust training data, your AI team will need to start by gathering a relevant, diverse dataset. For the e-commerce example mentioned earlier, the dataset would need to include images of every type of item in their catalog. If the training dataset consisted only of images of tops, the model wouldn’t be able to wear pants or dresses. Gathering a dataset for this use case is relatively simple since presumably the company would already have images of every item they’re selling.\u003c/p\u003e\u003cp\u003eHowever in some other use cases, this data may not be so easy to get. For example, an AI team tackling a medical imaging use case might have a much more difficult task in gathering the necessary data due to privacy restrictions, a lack of demographic variety in the available data, or variations caused by different cameras or microscopes, etc.\u003c/p\u003e\u003cp\u003eOnce a dataset has been gathered, the team will need to annotate each image by drawing bounding boxes or segmentation masks over each object and carefully labeling them according to the project guidelines in order to train the computer vision model. These guidelines will be the basis for the dataset’s ontology — the organizational system that classifies each item identified in the dataset.\u003c/p\u003e\u003cp\u003eFor example, a machine learning model being trained to identify and assess the ripeness of bananas might use an ontology like this:\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh6.googleusercontent.com/xKu4ZYTpcdIsZQ2jyet8jxYRYCfM5VwvIv4m1ULt7-9RJrMkVHbPCinRwTt1P5esrnG-IyTk0QB2byV3c5XbV6aDQjPmZgQz8B86hCMw3kYBs-51B2umjKLqJCFpOY5c77w50BXa6ygdp7_y1YstOKzhLoyV46Hfyl0gglZ95XQ6p-2IWmMB1HW6FlC-\" class=\"kg-image\" alt=\"An image of computer vision model being trained to identify and assess the ripeness of bananas using a specific ontology\" loading=\"lazy\" width=\"624\" height=\"468\"\u003e\u003c/figure\u003e\u003cp\u003eCategorizing every item pictured in a dataset is more complicated than it sounds. The team needs to take the following into consideration:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe geographic location of those who are labeling images as names of items can differ widely in different places. For example, “pants” has a different meaning in the United Kingdom than it does in the United States.\u003c/li\u003e\u003cli\u003eHow to nest categories. Will the model recognize a jumpsuit as a type of pants, a type of dress, or will it get its own category?\u003c/li\u003e\u003cli\u003eHow they’ll address any ambiguities that come up during the process.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOne of the best impact-for-cost actions an AI team can take to improve model performance is by iterating and updating the annotations, ontology, along with labeling guidelines, to fine tune the outputted training data, while they train and iterate on the model itself.\u003c/p\u003e\u003ch3 id=\"training-assessing-and-iterating-on-your-computer-vision-model\"\u003eTraining, assessing, and iterating on your computer vision model\u003c/h3\u003e\u003cp\u003eOnce the AI team has generated the first training dataset, they’ll feed the dataset into the model and assess its accuracy. The AI engineers might determine where the model has the lowest accuracy — for example, it might have extra difficulty finding the difference between long-sleeved shirts from sweaters — and use that information to create another dataset consisting mainly of long-sleeved shirts and sweaters, or update the existing labels on images of these shirts and sweaters, so the model can better identify them.\u003c/p\u003e\u003cp\u003eUsually, the process of getting a computer vision model to production-level accuracy requires multiple training datasets, and the improvement in model performance will generally be smaller with each iteration.\u003c/p\u003e\u003ch2 id=\"best-practices-for-enterprise-computer-vision\"\u003e\u003cstrong\u003eBest practices for enterprise computer vision\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTraining a computer vision model for an enterprise use case can be an extensive process, but there are a few actions that AI teams can take to ensure a smoother, faster journey to production.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEstablish a robust labeling operation.\u003c/strong\u003e \u003ca href=\"https://labelbox.com/guides/data-labeling/?ref=labelbox-guides.ghost.io\"\u003eData labeling\u003c/a\u003e is a fundamental part of the training process as it can make or break your entire AI initiative. Take time to find \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003ean experienced labeling team\u003c/a\u003e that understands your business requirements, implement quality management systems within your labeling workflow that incorporates domain expertise, and ensure that your entire labeling pipeline, from data lake to model input, is secure and seamless.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnsure that you also find the right tools for the job.\u003c/strong\u003e Fledgling AI teams often use free, open source labeling tools and pull together disparate systems and workforces to annotate their data, using USB drives and spreadsheets to transfer data and organize their operations. However, enterprise teams with the intention to grow would do better to invest in best-in-class solutions. An enterprise-ready AI data engine, for example, will help them scale and securely connect all the people, processes, and data for their needs.To learn more about how an AI data engine can benefit your enterprise computer vision initiatives, \u003ca href=\"https://labelbox.com/learn/library/complete-guide-data-engines-for-ai/?ref=labelbox-guides.ghost.io\"\u003eread the guide to AI data engines\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e","comment_id":"636e0d66e7ffab003d39b064","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/11/Social-Card_Computer-Vision.png","featured":false,"visibility":"public","created_at":"2022-11-11T08:52:54.000+00:00","updated_at":"2023-10-26T18:06:17.000+00:00","published_at":"2022-08-22T09:00:00.000+00:00","custom_excerpt":"Computer vision is a challenge in machine learning as the visual world is complex and vast. Learn more about computer vision.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/computer-vision/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},"url":"https://labelbox-guides.ghost.io/computer-vision/","excerpt":"Computer vision is a challenge in machine learning as the visual world is complex and vast. Learn more about computer vision.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Introduction to Computer Vision","meta_description":"Computer vision is a challenge in machine learning as the visual world is complex and vast. Learn more about computer vision.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"635768d8a31ffb004d18ff6b","uuid":"48399a28-049e-4e5f-8556-5c654373a659","title":"Data labeling for AI","slug":"data-labeling","html":"\u003cp\u003eHaving an efficient data labeling and model evaluation process is an important foundation for any successful AI product. Your model is only as good as the data it's trained with, and part of the training process includes getting your data labeled quickly and accurately by highly skilled experts.\u003c/p\u003e\u003cp\u003eHowever, many companies typically approach this process by gathering and quickly labeling as much data as they possibly can to train their model. In reality, AI teams today need to focus more on the quality of their data in order to add advanced capabilities and reasoning to their frontier and task-specific\u0026nbsp; models.\u003c/p\u003e\u003cp\u003eHaving larger, low-quality datasets prolong the data labeling process and makes getting to production AI harder. Wading through a vast amount of unstructured data to get accurately labeled data requires a tremendous amount of patience, organization, and time. Ensuring that you have high quality data will save you time and money from decreased labeling costs.\u003c/p\u003e\u003ch2 id=\"what-is-data-labeling\"\u003eWhat is data labeling?\u003c/h2\u003e\u003cp\u003eData labeling has become a broad term that can apply to everything from annotating specific data types to providing feedback and ratings on complex responses from generative AI (GenAI) and LLMs. \u003c/p\u003e\u003cp\u003eHistorically, it has referred to the task of annotating data such as images, PDFs, text, videos or audio with the purpose of helping to teach a machine learning model to make similar annotations. Labels can include bounding boxes and \u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003esegmentation masks for image\u003c/u\u003e\u003c/a\u003e and text data, for example.\u003c/p\u003e\u003cp\u003eWith the rapid rise of GenAI, data labeling tools now often include powerful solutions for \u003ca href=\"https://labelbox.com/product/evals/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eevaluating and rating multimodal AI models\u003c/u\u003e\u003c/a\u003e in a chat arena style experience, \u003ca href=\"https://labelbox.com/solutions/response-generation/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003egenerating prompt/response pairs\u003c/u\u003e\u003c/a\u003e, \u003ca href=\"https://labelbox.com/blog/multi-step-reasoning-teach-llms-to-think-critically/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eevaluating step-by-step reasoning\u003c/u\u003e\u003c/a\u003e, and more. These advanced rating tasks are often performed by highly skilled experts in a specific topic, such as coding, math, physics, medicine, and finance. \u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcmewYa8dZy5fqgwzjazBpu-8RytD6u34P9KlBdZuyCvzMsq3XiX08C3NzpUjhNviNJOpBRc4hELzoYvOKl9eHiDZAShb37hMMciMjEJKLZoL6WusnSAXXKsdCkrp62NrqN3d95mg?key=ggURVl5ZMBIeZKbEDFF3wmqE\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"377\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eExample of advanced data labeling of a complex, multi-step response \u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe data labeling process typically involves human-powered work in order to manually curate datasets or create new responses, and in some cases, computer-assisted help. The types of labels are predetermined by a machine learning engineer and are chosen to give a machine learning model specific information to train and improve the model from these examples. Labels can be as simple as deciding whether a photo contains a human all the way down to determining if the fifth step in a multi-step response is incorrect or unclear.\u003c/p\u003e\u003cp\u003eThe process of data labeling also helps machine learning engineers hone in on important factors that determine the overall precision and accuracy of their model. Example considerations include possible naming and categorization issues, how to represent occluded objects, how a model reasoned through an answer for a given prompt, etc. \u003c/p\u003e\u003ch2 id=\"how-does-data-labeling-work-and-why-is-it-important\"\u003eHow does data labeling work and why is it important?\u003c/h2\u003e\u003cp\u003eData labeling is a central part of the data pre-processing workflow for machine learning. Data labeling structures data to make it meaningful.\u003c/p\u003e\u003cp\u003eThis labeled data is then used to train a machine learning model to find “meaning” in new, relevantly similar data. Throughout this process, machine learning practitioners strive for both quality and quantity. Accurately labeled data coupled with a larger quantity creates more useful deep learning models, as the resulting machine learning model bases their decisions on all the labeled data.\u003c/p\u003e\u003cp\u003eTo illustrate from the example below, a human labeler applies a series of labels on an image asset by applying bounding boxes to the relevant objects, otherwise known as image labeling or \u003ca href=\"https://labelbox.com/guides/image-annotation/?ref=labelbox-guides.ghost.io\"\u003eimage annotation\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIn the example below, pedestrians are marked in blue and taxis are marked in yellow, while trucks are marked in yellow. Accurately identifying the cars from the pedestrians will yield a more successful model, which is defined as a model that can make accurate predictions when presented with new data (which in this case, are images of objects in a street view).\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://lh6.googleusercontent.com/KGoZKy4JrIcD-B6COxYVIYksQ-yNkK-D7c6kNKmgIWds8xcOyJbWYf1HjR4aUTjmWmruyvM9ZUw79T6zko2uJjYTR7yBde1AUkNezqg6LMN1J1tAv-tGUsVo0rwFQiNYsR2_zCiLHh5I1rf7KJf8AYIeR7SCz6lYo6ljf9RoOwvPj-Jmt21OYVmTUg\" class=\"kg-image\" alt=\"In the example below, pedestrians are marked in blue and taxis are marked in yellow, while trucks are marked in yellow.\" loading=\"lazy\" width=\"728\" height=\"538\"\u003e\u003c/figure\u003e\u003cp\u003eThis process is then repeated, and depending on the business use case and project, the quantity of labels on each image can vary. Some projects will require only one label to represent the content of an entire image (e.g. \u003ca href=\"https://labelbox.com/usecases/computer-vision/image-classification/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eimage classification\u003c/a\u003e). Other projects could require multiple objects to be tagged within a single image, each with a different label (e.g., bounding boxes).\u003c/p\u003e\u003ch2 id=\"what-are-the-different-types-of-data-labeling\"\u003eWhat are the different types of data labeling?\u003c/h2\u003e\u003cp\u003eThere are many fields of AI, each working with a different type of data and requiring different data labeling types. The most common fields are \u003ca href=\"https://labelbox.com/solutions/generative-ai/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003egenerative AI\u003c/a\u003e, \u003ca href=\"https://labelbox.com/usecases/computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ecomputer vision for image and video\u003c/a\u003e, \u003ca href=\"https://labelbox.com/usecases/natural-language-processing/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003enatural language processing\u003c/a\u003e (NLP) for text, and audio processing for speech recognition.\u003c/p\u003e\u003ch3 id=\"data-labeling-for-generative-ai\"\u003eData labeling for generative AI\u003c/h3\u003e\u003cp\u003eWith its potential to revolutionize industries and its impressive capabilities in content generation and problem-solving, generative AI has gained significant popularity in the AI space. By learning from existing data patterns, it creates original content, such as code, text, or images. To ensure optimal performance, generative AI models require high-quality, diverse data and human expertise for tasks like\u003ca href=\"https://labelbox.com/product/evals/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e model evaluation\u003c/a\u003e,\u003ca href=\"https://labelbox.com/solutions/supervised-fine-tuning/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003e supervised fine-tuning (SFT)\u003c/a\u003e, \u003ca href=\"https://labelbox.com/solutions/rlhf/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eRLHF\u003c/a\u003e, and \u003ca href=\"https://labelbox.com/solutions/red-teaming/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ered teaming\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eThese post-training tasks rely on quality data to expand the capabilities of frontier and task-specific models in areas like coding, text-to-image, text-to-audio, multilingual understanding, complex \u0026amp; agentic reasoning, and multimodal reasoning.  \u003c/p\u003e\u003ch3 id=\"data-labeling-for-computer-vision-with-image-and-video\"\u003eData labeling for computer vision with image and video\u003c/h3\u003e\u003cp\u003eA \u003ca href=\"https://labelbox.com/guides/computer-vision/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision model\u003c/a\u003e is built to interpret visual data from images and videos to identify, classify, and extract further information about objects that appear in the data. The data labeling process for this type of model includes labeling images, much like in the example above. The computer vision model would then be trained with the labeled data to categorize images, recognize the position of objects, or identify objects of importance in an image. A real-world use case for this type of model includes helping retailers manage inventory by identifying different products on a shelf and the quantity of their stock.\u003c/p\u003e\u003ch3 id=\"data-labeling-for-nlp\"\u003eData labeling for NLP\u003c/h3\u003e\u003cp\u003eNatural language processing (NLP) is a branch of AI that gives models the ability to understand natural language as it is spoken or written. This form of data labeling requires labelers to identify important sections of text or tag text with specific labels to train the model. The model would then develop the ability to understand and interpret the text, even when it's worded slightly differently. \u003c/p\u003e\u003cp\u003eA common real-world use case for this model is a chatbot built for customer support. Using this model, a \u003ca href=\"https://labelbox.com/guides/how-to-train-a-chatbot/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003echatbot\u003c/a\u003e would be able to understand the question, “When is my package being delivered?” even when phrased differently by different customers, such as “When will my package be delivered?” or “What is the delivery date of my package?” and answer accordingly.\u003c/p\u003e\u003ch3 id=\"audio-processing-for-speech-recognition\"\u003eAudio processing for speech recognition\u003c/h3\u003e\u003cp\u003eAudio processing converts sounds into structured data so it can be used for model training and improvement. This data labeling process actually goes hand-in-hand with NLP, as it typically requires the audio to first be transcribed into text before it is labeled.\u003c/p\u003e\u003cp\u003eA common real-world use case for this is any type of virtual assistant commands. When you ask your phone, “What is the weather like today?” and receive an answer, this interaction is enabled by the data labeling process for audio.\u003c/p\u003e\u003ch2 id=\"how-does-labelbox-support-data-labeling\"\u003eHow does Quantumworks Lab support data labeling?\u003c/h2\u003e\u003cp\u003eData labeling projects begin by identifying and instructing human labelers (otherwise known as annotators or raters) to perform labeling tasks. Annotators must be thoroughly trained on the specifications and guidelines of each annotation project, as every use case, team, and organization will have different requirements.\u003c/p\u003e\u003cp\u003eIn the specific case of images and videos, once the annotators are trained on how to annotate or label the data, they will begin labeling hundreds or thousands of images or videos, often using home-grown or open-source labeling tools. \u003c/p\u003e\u003cp\u003eLabelbox’s AI data factory offers three key components necessary to delivering high-quality labeled data: highly-skilled humans, best-in-class software, and operational excellence. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/services/labeling/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox’s Labeling Services\u003c/u\u003e\u003c/a\u003e, powered by Alignerrs, offers a proven community of subject matter experts in a wide range of domains and languages to help align and improve your AI models by generating high-quality data. If you are looking to quickly onboard and customize your own team of experts, use \u003ca href=\"https://labelbox.com/services/alignerr-connect/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eAlignerr Connect\u003c/u\u003e\u003c/a\u003e to directly discover, select, and recruit qualified AI trainers to connect these experts directly with your internal processes.\u0026nbsp;\u003c/p\u003e\u003cp\u003eOur data-centric AI platform is software that is designed to have all the necessary tools for labeling any data modality. This type of software also promotes an iterative approach to data labeling. Instead of using one large dataset to train your model, Quantumworks Lab equips AI teams with the tools they need to label data in smaller batches. This approach means AI teams give more supervision and feedback at the beginning of the project and create a more agile process. This type of approach prioritizes two-way collaboration between the labelers and AI teams to ensure that the data labeling process is efficient and accurate.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, you can achieve operational excellence while maintaining total transparency. Our services and platform offers customizable workflows, multi-step review and rework, and \u003ca href=\"https://labelbox.com/blog/ai-code-and-grammar-critic-improve-quality/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLLM as a judge\u003c/u\u003e\u003c/a\u003e, all accessible through a user-friendly interface to help you continuously generate the highest quality data.\u003c/p\u003e\u003cp\u003eLabelbox sets itself apart by combining a scientific approach to data quality with large-scale operations. Last month alone, we facilitated the creation of over 50 million annotations, requiring over 200,000 human hours. By continuously monitoring and analyzing data quality, we ensure immediate improvements and optimize your data labeling projects.\u003c/p\u003e\u003cp\u003e\u0026nbsp;To further enhance data quality and efficiency, we offer a range of features to optimize your data labeling projects.\u003c/p\u003e\u003ch3 id=\"high-performance-data-labeling-tools\"\u003e\u003cstrong\u003eHigh-performance data labeling tools\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWhen looking for the right AI platform for your team, it’s important to ensure that the software supports enough labels and annotations per asset without sacrificing loading times. This way, you’ll be able to use the AI platform for both simple and complex use cases, which may be a requirement in the future for your team.\u003c/p\u003e\u003ch3 id=\"customization-based-on-ontology-requirements\"\u003e\u003cstrong\u003eCustomization based on ontology requirements\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe ability to configure an AI platform to your exact data structure (ontology) requirements enables you to ensure consistency and scalability in the data labeling process as your use cases expand. Quantumworks Lab provides a convenient way to copy your ontology across multiple projects so that you can make cascading changes or use an existing ontology as a starting point rather than starting from scratch.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh6.googleusercontent.com/yi9z-000uuamEgNcR1Rx6Zv7dNaGKi_KARwDF5L11YTMVeILEcTw-EKue0lz5QsYLc6waBK-xoVIDH5WLCOGlB7Ui8ECPWNcxBIpl1KPpZ9deWNNZSsbKB36SAKpKQgC3Uxx26mmmNXIoZ2ZnCNvcK3ymBfducw_HEASEqIrXqjsVIABWiHGWwaj8A\" class=\"kg-image\" alt=\"Quantumworks Lab allows you to configure the label editor to your ontology requirements. Bring additional attachments such as text, videos, images, overlays, or even custom widgets to aid data labelers to create perfect labels.\" loading=\"lazy\" width=\"1444\" height=\"1174\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eLabelbox allows you to configure the label editor to your ontology requirements. Bring additional attachments such as text, videos, images, overlays, or even custom widgets to aid data labelers to create perfect labels.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"an-emphasis-on-performance-for-a-wide-array-of-devices\"\u003e\u003cstrong\u003eAn emphasis on performance for a wide array of devices\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eA data-centric AI platform includes an intuitive user interface, which helps lower the cognitive load on labelers and enables fast data labeling. Even on lower spec PCs and laptops, high performance is critical for professional annotators who are working in an editor all day.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/TLqBOVgb-6XCHSr3BE42mhNlOm5MZUSSvGYbIWEi7JHek6tREPFQYH6Vb_G_AitECjDCy-iZngI4Sck3xSfFhphCxnP1PvR-Ivd-pOiKtMVu1SgoN2R9PYsmR13rPvj8NMDJYTSXa9swmSa2MOcELgwDNha71jTf6w52UfBmybbEK3fhywCobZgl8A\" class=\"kg-image\" alt=\"A simple, intuitive UI reduces friction in the data labeling process.\" loading=\"lazy\" width=\"1600\" height=\"936\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eA simple, intuitive UI reduces friction in the data labeling process.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"seamlessly-connect-your-data-via-python-sdk-or-api-for-easy-data-labeling\"\u003e\u003cstrong\u003eSeamlessly connect your data via Python SDK or API for easy data labeling\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eStream data into an AI platform and push labeled data into training environments like TensorFlow and PyTorch. Quantumworks Lab was built to be developer friendly and API-first, so you can use it as infrastructure to scale up and connect your ML models to accelerate data labeling productivity and orchestrate active learning.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/A7Kv8_sI8c0PaU8KobZPqI38H4rHFByIUHRPSxMf8NgCbB7aqoYZosGNlzanv2HiRHoN3_mw5s_aV201P12sC87ezaP7FciE5K-YgN4HXnZFk7y7vmxBqxriOPBpJzdBje3F6isYITYtekx6Djsd_un3d2i7-TqkBj0h2vm_EDpBIq7qZe0QEHotxA\" class=\"kg-image\" alt=\"Simplified data import without writing and maintaining your own scripts.\" loading=\"lazy\" width=\"1600\" height=\"1000\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eSimplified data import without writing and maintaining your own scripts.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"benchmarks-consensus-for-data-labeling\"\u003e\u003cstrong\u003eBenchmarks \u0026amp; consensus for data labeling\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eQuality is measured by both the consistency and the accuracy of labeled data. The industry standard methods for calculating data quality are benchmarks (aka gold standard), consensus, and review.\u003c/p\u003e\u003cp\u003eFiguring out what combination of these quality assurance procedures is right for your machine learning project is an essential part of an AI data scientist’s job.\u0026nbsp;In a recent blog post, we \u003ca href=\"https://labelbox.com/blog/inside-the-data-factory-how-labelbox-produces-the-highest-quality-data-at-scale/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003elook deep inside the Quantumworks Lab AI data factory\u003c/u\u003e\u003c/a\u003e, revealing important tools, techniques and processes that are the bedrock for producing the highest-grade data at scale.\u0026nbsp;\u0026nbsp;\u003c/p\u003e\u003cp\u003eQuality assurance is an automated process that operates continuously throughout your training data development and improvement processes. \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io#:~:text=The%20Benchmarks%20tool%20allows%20you,Benchmark%20Label%20on%20the%20asset.\"\u003eWith Quantumworks Lab consensus and benchmark features\u003c/a\u003e, you can automate consistency and accuracy tests. These tests allow you to customize the percentage of your data to test and the number of labelers that will annotate the test data.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/10/Untitled-removebg-preview-1.png\" class=\"kg-image\" alt=\"Benchmarks in action, highlighting the example labeled asset with a gold star.\" loading=\"lazy\" width=\"678\" height=\"368\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/10/Untitled-removebg-preview-1.png 600w, https://labelbox-guides.ghost.io/content/images/2022/10/Untitled-removebg-preview-1.png 678w\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eBenchmarks in action, highlighting the example labeled asset with a gold star.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eLabelbox also offers private, human-centric evaluations to complement traditional benchmarks with what we believe is a more accurate assessment of AI models. By incorporating expert human judgement, addressing challenges around current benchmarks, and providing comprehensive metrics for various AI modalities, \u003ca href=\"https://labelbox.com/blog/labelbox-leaderboards-redefining-ai-evaluation-with-private-transparent-and-human-centric-assessments/?ref=labelbox-guides.ghost.io\"\u003e\u003cu\u003eLabelbox Leaderboards\u003c/u\u003e\u003c/a\u003e aims to offer a more accurate, innovative evaluation of genAI models. \u003c/p\u003e\u003ch3 id=\"collaboration-and-performance-monitoring\"\u003e\u003cstrong\u003eCollaboration and performance monitoring\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eHaving an organized system to invite and supervise all your labelers during the data labeling process is important for both scalability and security. A data-centric AI platform should include granular options to invite users and review the work of each one.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh3.googleusercontent.com/t6RnaBQEsZjkXnXK3IKe-AhfvcSW-SH79XxpjXjweMkFa6swQg2KMEUo07Gj0NuHi2Qn_ZCWhaAJQtllLPeSjyDjeoMqBvtoi0_87xlU9EaLS4tWW0gLcDKGyapTOwWBQVas9g6n6zyblSPbKe7-vPB2o7p0tWBfaNimEfsQEy0tWJ3GCHRRKJw3SQ\" class=\"kg-image\" alt=\"Seamless collaboration between data science teams, domain experts, and dedicated internal \u0026amp; external labeling teams.\" loading=\"lazy\" width=\"1344\" height=\"1024\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eSeamless collaboration between data science teams, domain experts, and dedicated internal \u0026amp; external labeling teams.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eTo ensure top-tier data quality, we also offer Quantumworks Lab Monitor, a powerful tool for granular performance monitoring. Quantumworks Lab Monitor provides a centralized dashboard to visualize and analyze data labeling operations, enabling users to enhance data quality, monitor performance, make data-driven decisions, and streamline management all in one simple click.\u0026nbsp;\u003c/p\u003e\u003cp\u003eTake a quick tour of Monitor in this \u003ca href=\"https://demo.arcade.software/fQYL8I9rMzAi7vuigL8k?embed=\u0026ref=labelbox-guides.ghost.io\"\u003equick, click-through demo t\u003c/a\u003eo learn a bit more. \u003c/p\u003e\u003ch2 id=\"final-thoughts-on-data-labeling\"\u003eFinal thoughts on data labeling\u003c/h2\u003e\u003cp\u003eThe traditional method of training your model with one large training dataset is no longer effective. Machine learning and AI training has moved past this approach to be more agile: carefully curating datasets to accelerate the data labeling process and train the model, examining its performance, and modifying the next dataset accordingly.\u003c/p\u003e\u003cp\u003eThe Quantumworks Lab data factory promotes this iterative process and enables AI teams with the tools needed to accelerate their data labeling and model evaluation process — empowering teams to create powerful training datasets. As such, investing in the right platform and services is key for deploying successful AI products. \u003ca href=\"https://app.labelbox.com/signup?_r=https://www.google.com/?utm_keyword=Quantumworks Lab\u0026utm_source=google\u0026utm_medium=paid-search\u0026utm_campaign=20490363302\u0026gclid=CjwKCAiArva5BhBiEiwA-oTnXdVg1Vj42KRQMjKvBPOVflRG3KbY0t6f-ns2DQADGe_vyVPXN_k0ARoC-jAQAvD_BwE\u0026attr=arcade\u0026landingPageAnonymousId=%22ce322bab-56ea-43b8-a113-000851a3ebaf%22\u0026referrer_url=https://www.google.com/\"\u003eTry Quantumworks Lab for free\u003c/a\u003e.\u003c/p\u003e","comment_id":"635768d8a31ffb004d18ff6b","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Social-Card_Image-annotation--1-.png","featured":false,"visibility":"public","created_at":"2022-10-25T04:40:56.000+00:00","updated_at":"2024-11-27T19:31:15.000+00:00","published_at":"2022-08-19T04:41:00.000+00:00","custom_excerpt":"Get a primer on data labeling, defined as the task of detecting and tagging data with labels, most commonly in the form of images, videos, audio and text assets.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/data-labeling/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa474375d13000123d7e0","name":"Building LLMs","slug":"building-llms","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-llms/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},"url":"https://labelbox-guides.ghost.io/data-labeling/","excerpt":"Get a primer on data labeling, defined as the task of detecting and tagging data with labels, most commonly in the form of images, videos, audio and text assets.","reading_time":10,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Introduction to Data Labeling for Machine Learning and AI","meta_description":"Data labeling is the task of detecting and tagging data with labels, most commonly in the form of images, videos, audio and text assets","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},{"id":"6345d52db75421003d6e6197","uuid":"b3e5add7-62f4-4aea-9a09-b610a3705cf2","title":"How to create high-quality image segmentation masks quickly and easily","slug":"image-segmentation","html":"\u003cp\u003eEnterprise AI teams often spend an enormous amount of time and energy on the development and curation of datasets. This is because, unlike academic machine learning models, the use of open source datasets for a commercial application is unlikely to yield an accurate representation of the real world. Instead, teams need to create a new labeled dataset tailored for their specific project.\u003c/p\u003e\u003cp\u003eIn order to create this novel labeled dataset, data scientists and ML engineers have the choice between a variety of annotation types. In \u003ca href=\"https://labelbox.com/guides/computer-vision/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003ecomputer vision\u003c/a\u003e, the frequent choice for differentiating between objects with the highest degree of accuracy is image segmentation.\u003c/p\u003e\u003cp\u003eIt’s important to stress that without the right tooling, however, image segmentation can be prohibitive for many projects, as it becomes very costly to label the amount of training data necessary to achieve performant model results.\u003c/p\u003e\u003ch2 id=\"what-is-image-segmentation\"\u003eWhat is image segmentation?\u003c/h2\u003e\u003cp\u003eImage segmentation is one of the most labor intensive annotation tasks because it requires pixel level accuracy. Labeling a single image can take up to 30 minutes. With image segmentation, each annotated pixel in an image belongs to a single class. The output is a mask that outlines the shape of the object in the image.\u003c/p\u003e\u003cp\u003eAlthough image segmentation annotations come in a lot of different types (such as semantic segmentation, instance segmentation, and panoptic segmentation), the practice of image segmentation generally describes the need to annotate every pixel of the image with a class.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/ACJ2PcVw1AaqlIWoxZNCTUGS9j3EsF1-0cqn4A0u9xlERfpnmsfJdEff-Vmt0lW1IqOHULdHzpC18or_19GFUwKKZnljTrlfsZ8Gr-Bx-cMOJGhZug2H-D_2Gj0Js6Lbn2-4sSDCfxvf_UPYCgCoIaqJ0xxn6o7W-JThV3A33gSuFHWT7Jfv_t3NAA\" class=\"kg-image\" alt=\"Image segmentation masks used to annotate every pixel and distinguish between items such as sky, ground, and vehicle.\" loading=\"lazy\" width=\"732\" height=\"376\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eImage segmentation masks used to annotate every pixel and distinguish between items such as sky, ground, and vehicle.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"what-are-the-benefits-of-using-image-segmentation-for-my-machine-learning-model\"\u003eWhat are the benefits of using image segmentation for my machine learning model?\u003c/h2\u003e\u003cp\u003eThe primary benefit of image segmentation can be best understood by comparing the three most common annotation types within computer vision.\u003c/p\u003e\u003cul\u003e\u003cli\u003eImage classification: The goal is to simply identify which objects and other properties exist in an image.\u003c/li\u003e\u003cli\u003eImage object detection: This goes one step beyond classification to find the position (using bounding boxes) of individual objects.\u003c/li\u003e\u003cli\u003eImage segmentation: The goal is to recognize and understand what's in the image at the pixel level. Every pixel in an image belongs to a single class, as opposed to object detection where the bounding boxes of objects can overlap.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/-MwGV8z6bBoJIK8pCjzyanHVH21aYa6BJRxqK6VbD2oN3tthDBp76k8tcEcGk8RdGCpPZXbkaGxg3UrDeifRAdKWszU10QcfERG9Ph2mHiFwmj20gQEw3UNGT2u3D582N1tyOYsw0h6bdVK_YlBCFJ6dEcmyYKT0AumHb_Akgu9dq_xYr9Rs1AHlWA\" class=\"kg-image\" alt=\"One image labeled in four ways for four separate computer vision tasks: classification, object detection, image segmentation, and image segmentation with instances.\" loading=\"lazy\" width=\"1600\" height=\"551\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eOne image labeled in four ways for four separate computer vision tasks: classification, object detection, image segmentation, and image segmentation with instances.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eFor a point of comparison, employing image segmentation is particularly useful when dealing with use cases in a model where you need to definitively know whether or not an image contains the object of interest — and also what \u003cem\u003eisn’t\u003c/em\u003e an object of interest.\u003c/p\u003e\u003cp\u003eThis is in contrast to other annotation types such as image classification or bounding boxes, which may be faster to label, but less accurate. In short, annotations generated from image segmentation tend to end up with the most widely applicable and versatile models, because they are the most focused on what is in the contents of an image.\u003c/p\u003e\u003ch2 id=\"how-does-an-ai-data-factory-support-complex-image-segmentation\"\u003eHow does an AI data factory support complex image segmentation?\u003c/h2\u003e\u003cp\u003eAI data factories are commonly equipped with tools which allow labelers to outline complex shapes for image segmentation. The Quantumworks Lab pen tool allows you to draw freehand as well as straight lines. Having fast and ergonomic drawing tools help reduce the time-consuming nature of creating consistent, pixel-perfect labels.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh3.googleusercontent.com/xFTJLKe1HRXJTSGm3fi3V7aPOvI4jxeTq5mlOJtxcFWSp7QKGMJnRUIXOOslCzniw7nrxwp47x4QyB58l_HrtfEhH5G8cXet6zJGR16olzEjX_7UyLNsGYzozsAY7QHs5VBktG8qla16hsTlcXzRYToqW7IwXHO-DT4lzBvCtrbTcFKmA3XL5KMURg\" class=\"kg-image\" alt=\"The Quantumworks Lab pen tool makes image segmentation quick and easy.\" loading=\"lazy\" width=\"900\" height=\"532\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eThe Quantumworks Lab pen tool makes image segmentation quick and easy.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIn addition, AI data factories typically include additional features that specifically help optimize your image segmentation project.\u003c/p\u003e\u003ch3 id=\"automating-the-image-segmentation-labeling-task\"\u003eAutomating the image segmentation labeling task\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/10/My-Movie-35--1--1.gif\" class=\"kg-image\" alt=\"A best-in-class AI data engine helps accelerate the image segmentation process through automation\" loading=\"lazy\" width=\"1280\" height=\"720\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/10/My-Movie-35--1--1.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/10/My-Movie-35--1--1.gif 1000w, https://labelbox-guides.ghost.io/content/images/2022/10/My-Movie-35--1--1.gif 1280w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eA best-in-class AI data factory helps accelerate the image segmentation process through automation\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eA best-in-class AI data factory helps users automate parts of the image segmentation process to accelerate efforts without diminishing the quality of annotations. One crucial part of this automation includes the incorporation of auto-segmentation tools that enable labelers to complete the  complex image segmentation drawing tasks in seconds.\u003c/p\u003e\u003cp\u003eAnother labeling automation technique, called pre-labeling or model-assisted labeling, has been proven to reduce labeling time and costs by up to 50% for AI teams. Model-assisted labeling involves the use of a model — whether it’s a generic off-the-shelf model, your own model in training, or a model built specifically to generate labels. The model’s output is used as pre-labels, enabling labelers to simply correct or edit them instead of labeling data from scratch.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/10/2022-10-14_16-13-28--1--2.gif\" class=\"kg-image\" alt=\"Model-assisted labeling, or pre-labeling, helps accelerating model training by eliminating the need to label data from scratch\" loading=\"lazy\" width=\"1778\" height=\"884\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/10/2022-10-14_16-13-28--1--2.gif 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/10/2022-10-14_16-13-28--1--2.gif 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/10/2022-10-14_16-13-28--1--2.gif 1600w, https://labelbox-guides.ghost.io/content/images/2022/10/2022-10-14_16-13-28--1--2.gif 1778w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eModel-assisted labeling, or pre-labeling, helps accelerating model training by eliminating the need to label data from scratch\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003ePre-labeling decreases labeling costs as the model gets smarter with every iteration, leaving teams more time to focus on manually labeling edge cases or areas where the model might not be performing as well. It’s not only faster and less expensive, but delivers better model performance.\u003c/p\u003e\u003ch3 id=\"customization-based-on-ontology-for-image-segmentation\"\u003e\u003cstrong\u003eCustomization based on ontology for image segmentation\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh6.googleusercontent.com/lztPGENgnYbTrWp-jfg19tsQyeWeOvRZY0aXZ2Q7qXPIKn3s6pXwD0aXT-cDA5mgtN6q2Z-9-g347uyDnJqGYeoGSN5-wZGLSM3PlQsgvMnRujDaJp8O77UzPk5_7VdNfw469frXI0GM0_IUxTyu5l9NMR35R7CVoqnGq_0VRPIyIUr_TmORUJt_Ug\" class=\"kg-image\" alt=\"Configure the label editor to your ontology requirements. Bring additional attachments such as text, videos, images, overlays, or even custom widgets to aid data labelers to create perfect labels.\" loading=\"lazy\" width=\"1444\" height=\"1174\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eConfigure the label editor to your ontology requirements. Bring additional attachments such as text, videos, images, overlays, or even custom widgets to aid data labelers to create perfect labels.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eBeing able to configure the label editor to your exact data structure (ontology) requirements, with the ability to further classify instances that you have segmented, is important for accurate and easy image segmentation. Quantumworks Lab’s ontology management feature includes classifications, custom attributes, hierarchical relationships, and more.\u003c/p\u003e\u003ch3 id=\"an-emphasis-on-performance-for-a-wide-array-of-devices\"\u003e\u003cstrong\u003eAn emphasis on performance for a wide array of devices\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/M67Xt9O5E6AM7y9vUFeOLsUVeBNyp8zqKBkJAe_BFE1n32qdcq0nVrfu5-HhbwlIvwknEL7GzhpVJSLNW1gLJ_fpKTo5WtRQ9gitkiKGYu5Ir4UQ-Vihubw2sGe5WTl6AzKtQR3H2P9C6v-R52tVUILOJblI4E_UXKJlQfE1RhZWzB7yWhI5Ding4A\" class=\"kg-image\" alt=\"A simple, intuitive UI reduces friction in the image segmentation process.\" loading=\"lazy\" width=\"1600\" height=\"936\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eA simple, intuitive UI reduces friction in the image segmentation process.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAs previously mentioned, image segmentation can be a time consuming process, taking up to 30 minutes to complete a single image. A focus on intuitive UI across multiple devices (even on lower-spec PCs or laptops) is important to reduce fatigue for professional labelers who are working on image segmentation labeling tasks for hours on end.\u003c/p\u003e\u003ch3 id=\"dynamic-queuing-for-large-scale-image-segmentation-projects\"\u003e\u003cstrong\u003eDynamic queuing for large scale image segmentation projects\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/yIz5zll3BvFXsk-kqFn7-fL3vvbqjsCiz0ESTeCulEb0GLH0w_h3J8-sjOpSb6mI3W_A_SlPxoD8ydkdXh06KMPAVPos7pUbM0Z5yky1YCYc35TKVn64D8JHrqG84suq7QyJwHfvs9kycFWJwvGlZTNeFYe5vGfC2MjtOsEy3lAxuV9gFqDYK8HHrw\" class=\"kg-image\" alt=\"Dynamic queuing helps cut down on time spent on image segmentation tasks\" loading=\"lazy\" width=\"940\" height=\"581\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eDynamic queuing helps cut down on time spent on image segmentation tasks\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eStandard queuing methods have image segmentation tasks distributed equally to active annotators. These equal distribution methods typically lead to inefficient workflows as not all annotators work at the same speeds and not all image segmentation tasks are created equal. With large scale projects, this also potentially leads to longer delays as annotators sit idle, waiting for new tasks.\u003c/p\u003e\u003cp\u003eAn AI data factory supports more efficient labeling queues for image segmentation tasks by enabling a continuous workflow for annotators. With this method, new tasks will automatically be distributed at the rate of completion to eliminate idle time and help get your image segmentation project finished faster.\u003c/p\u003e\u003ch3 id=\"software-first-approach-to-image-segmentation\"\u003e\u003cstrong\u003eSoftware-first approach to image segmentation\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/2aLRRrCSJUbvVKcLwvDRC0r6dPTlkYL24pE1Ta2PPWAOHYk2EsNK9ceOrFp1VX3FNcaW8lduNfeoMCr3YuALeMezMMmjNWTy_CtOtGnUpVDan_5mEXIZMQbJFoA3Kl4dDY3or7nErq5z47uQLkDtOOfkz7ulwLcsWKIzPu1gGpe7bIFgM-ntaa4yHA\" class=\"kg-image\" alt=\"With a software-first approach, you pay only for productive screen activity time from your human image segmentation labeling service\" loading=\"lazy\" width=\"1600\" height=\"1080\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eWith a software-first approach, you pay only for productive screen activity time from your human image segmentation labeling service\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eA best-in-class data factory gives you options when it comes to labeling configuration for image segmentation tasks, adopting a software-first approach.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/blog/how-a-software-first-approach-to-labeling-ai-data-gets-you-a-better-roi-than-tech-enabled-bpos/?ref=labelbox-guides.ghost.io\"\u003eThis software-first approach\u003c/a\u003e allows AI teams with image segmentation projects to use any labeling service or vendor with full transparency, collaborate easily with all stakeholders throughout the labeling process, and train their own models to automate labeling thereby significantly reducing their unit costs for image segmentation tasks.\u003c/p\u003e\u003cp\u003eThis includes using an in-house image segmentation team, a BPO (business process outsourcing), or on-demand labeling services from \u003ca href=\"https://labelbox.com/product/boost/?ref=labelbox-guides.ghost.io\"\u003eLabelbox Boost\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"enabling-the-creation-of-synthetic-data\"\u003e\u003cstrong\u003eEnabling the creation of synthetic data\u003c/strong\u003e \u003c/h3\u003e\u003cp\u003eAI teams are increasingly turning to synthetic data to meet their needs when real data is thin, or when handling sensitive information as the dataset won’t contain any real information during the model training process.\u003c/p\u003e\u003cp\u003eSynthetic data can also simulate edge cases and conditions that aren’t represented in real data or mitigate issues caused by potential changes in the camera sensor or lighting conditions, helping teams fill in the gaps in their dataset.\u003c/p\u003e\u003cp\u003eAI data factory enables teams to build a VAE(variational auto-encoder) or GAN (Generative Adversarial Networks) model to generate image data.\u003c/p\u003e\u003cp\u003eSynthetic data generation can be used for preserving privacy, and overcoming some imbalanced dataset problems (for example, if you are training on a cancer diagnosis dataset from patient records but there are only 2% of positive cases, you would want to generate a few synthetic positive examples that are anonymous).\u003c/p\u003e\u003cp\u003eWhile it can be helpful to bootstrap a model training with a larger dataset,  synthetic data will inherit biases from the data it is modeling from, and the model might still struggle when facing with production data it has not learned about before.  Thus, it is often recommended to collect diverse real-world data for training (and use active learning techniques to curate more efficiently), rather than relying on synthetic data.\u003c/p\u003e\u003ch3 id=\"support-for-shared-borders-when-creating-image-segmentation-masks\"\u003e\u003cstrong\u003eSupport for shared borders when creating image segmentation masks\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cbr\u003eWhen creating image segmentation masks, it’s important to be able to share borders between objects. With the Quantumworks Lab editor, it’s simple. Whenever you draw a new object, if you overlap the border of an already existing object, the new border you’re drawing will be shared.\u003c/p\u003e\u003cp\u003eThis method works well when you are labeling objects from the background first. Sometimes, though, you want to be able to draw foreground objects first, and then be able to draw an object behind without messing up the masks you’ve already created. A best-in-class AI data factory will support shared borders to help you accelerate the image segmentation process.\u003c/p\u003e\u003ch2 id=\"what-are-some-example-real-world-image-segmentation-use-cases\"\u003eWhat are some example real world image segmentation use cases?\u003c/h2\u003e\u003cp\u003eImage segmentation is popular for real world machine learning models when high accuracy is required of the computer vision application being built.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/usecases/computer-vision/image-segmentation/?ref=labelbox-guides.ghost.io\" rel=\"noreferrer\"\u003eUse cases employing image segmentation\u003c/a\u003e can be found as follows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAutonomous vehicles\u003c/li\u003e\u003cli\u003eMedical imagery\u003c/li\u003e\u003cli\u003eRetail applications\u003c/li\u003e\u003cli\u003eFace recognition and analysis\u003c/li\u003e\u003cli\u003eVideo surveillance\u003c/li\u003e\u003cli\u003eSatellite image analysis\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/ukjoguzCSt8X_qtZ9GKD04C1dYp-hNXTa9CCnU5lcUtjv2Lic0bq96gJZ6i9EEiDPRiH-LL3BOg0bGgE70qqVParJsHRfDpEaHvfTIiJn0mnpysc5hPiVLjbWiGtxPrQRdwUYWDr4Pu2sc0wi4S_PpcVlboXGK_5h7ki4yePRdiUjIMVX3nAusiOjw\" class=\"kg-image\" alt=\"Machine learning models built to help retailers better manage inventory might use image segmentation masks to identify and count products on shelves.\" loading=\"lazy\" width=\"1280\" height=\"720\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eMachine learning models built to help retailers better manage inventory might use image segmentation masks to identify and count products on shelves.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"final-thoughts-on-image-segmentation-with-an-ai-data-factory\"\u003eFinal thoughts on image segmentation with an AI data factory\u003c/h2\u003e\u003cp\u003eHistorically, image segmentation has been prohibitive for many projects, despite the benefits of pin-point accuracy, because the costs associated with labeling the amount of training data necessary to achieve performant model results can become astronomical.\u003c/p\u003e\u003cp\u003eHowever, an AI data factory can rewrite this precedent. With the features included in a best-in-class AI data such as a flexible pen tool, auto-segmentation, and model-assisted labeling techniques, image segmentation becomes a fast and accurate process — thus also becoming a more accessible technology to AI teams.\u003c/p\u003e\u003cp\u003eInvesting in an AI data factory like Quantumworks Lab empowers teams with the tools to quickly build better image segmentation AI products. \u003c/p\u003e","comment_id":"6345d52db75421003d6e6197","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Social-Card_Image-annotation-1.png","featured":false,"visibility":"public","created_at":"2022-10-11T20:42:21.000+00:00","updated_at":"2024-11-20T21:34:50.000+00:00","published_at":"2022-08-17T20:55:00.000+00:00","custom_excerpt":"Image segmentation is used to label images for applications that require high accuracy and is manually intensive.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/image-segmentation/","authors":[{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"}],"tags":[{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6328cbf816e912003d39b1a4","name":"Quantumworks Lab","slug":"Quantumworks Lab","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/Quantumworks Lab/"},"primary_tag":{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},"url":"https://labelbox-guides.ghost.io/image-segmentation/","excerpt":"Image segmentation is used to label images for applications that require high accuracy and is manually intensive.","reading_time":8,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Introduction to Image Segmentation for Machine Learning \u0026 AI","meta_description":"Image segmentation is used to label images for applications that require high accuracy and is manually intensive.","email_subject":null,"frontmatter":null,"feature_image_alt":"How to create high-quality image segmentation masks quickly and easily","feature_image_caption":null},{"id":"6336244e1beec0003d38bdcc","uuid":"9183edaf-81ca-46b5-a0d7-d4d5c1d56cf0","title":"Best practices for successful image annotation","slug":"image-annotation","html":"\u003ch2 id=\"what-is-image-annotation\"\u003eWhat is image annotation?\u003c/h2\u003e\u003cp\u003eImage annotation is the task of labeling digital images, typically involving human input and, in some cases, computer-assisted help. Labels are predetermined by a machine learning (ML) engineer and are chosen to give the \u003ca href=\"https://labelbox.com/guides/computer-vision/?ref=labelbox-guides.ghost.io\"\u003ecomputer vision model\u003c/a\u003e information about the objects present in the image. The process of labeling images also helps machine learning engineers hone in on important factors in the image data that determine the overall precision and accuracy of their model.\u003c/p\u003e\u003cp\u003eExample considerations include possible naming and categorization issues, how to represent occluded objects (objects hidden by other objects in the image), how to deal with parts of the image that are unrecognizable, etc.\u003c/p\u003e\u003ch2 id=\"how-do-you-annotate-an-image\"\u003eHow do you annotate an image?\u003c/h2\u003e\u003cp\u003eFrom the example image below, a person has used an image annotation tool to apply a series of labels by placing bounding boxes around the relevant objects, thereby annotating the image. In this case, pedestrians are marked in blue and taxis are marked in yellow, while trucks are marked in yellow.\u003c/p\u003e\u003cp\u003eDepending on the business use case and project, the number of image annotations on each image can vary. Some projects will require only one label to represent the content of an entire image (e.g. image classification). Other projects could require multiple objects to be tagged within a single image, each with a different label (e.g. a bounding box).\u003c/p\u003e\u003cp\u003eImage annotation software is designed to make image labeling as easy as possible. A good image annotation app will include features like a bounding box annotation tool and a pen tool for freehand \u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\"\u003eimage segmentation\u003c/a\u003e.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/fSf1jZJOJk9eRfx9AIsU4jqg9TriNm0nOfNU8u0ibv5r1c_2iAptWQjpNUMY_KDBjiYI-Z6ldPujqKtNGjxMJDyHd8cKEoUq47rLTpD_ISkhPxd-rXxVnNGHd8ITlQwl5xDZ-SmbVaZrJYQZuIHkN99RSgsqCPMZQmif5g4cxepWYwXj8pDRcxg0iA\" class=\"kg-image\" alt=\"Bounding boxes applied to identify vehicle types and pedestrians.\" loading=\"lazy\" width=\"728\" height=\"538\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eBounding boxes applied to identify vehicle types and pedestrians.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"what-are-the-different-types-of-image-annotation\"\u003eWhat are the different types of image annotation?\u003c/h2\u003e\u003cp\u003eTo create a novel labeled dataset for use in computer vision projects, data scientists and ML engineers have the choice between a variety of annotation types they can apply to images. Researchers will use an image markup tool to help with the actual labeling. The three most common image annotation types within computer vision are:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eClassification:\u003c/strong\u003e With whole-image classification, the goal is to simply identify which objects and other properties exist in an image without localizing them within the image\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eObject detection:\u003c/strong\u003e With image object detection, the goal is to find the location (established by using bounding boxes) of individual objects within the image\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eImage segmentation\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e: \u003c/strong\u003eWith image segmentation, the goal is to recognize and understand what's in the image at the pixel level. Every pixel in an image is assigned to at least one class, as opposed to object detection, where the bounding boxes of objects can overlap. This is also known as semantic segmentation.\u003c/li\u003e\u003c/ul\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/bnfgDITr0oB6E_CybfiAyOPjvXNe5L_cbjTykcjXEXxBlnwzyKE4F0ZTWkcWcFjdwNoLQBI8Iqk-e5pp-7FO3ZNrwpFtFIH61oxbRQJXNv2tQm4hpKDFKSQyUE5B3IEjZexd7-4fTg3S2ya_ksa1w2KtkFizfNfuMb5C7etXkbkBqU2D9eUIC4OjoA\" class=\"kg-image\" alt=\"Different types of image annotation\" loading=\"lazy\" width=\"1600\" height=\"551\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eDifferent types of image annotation\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWhole image classification provides a broad categorization of an image and is a step up from unsupervised learning as it associates an entire image with just one label. It is by far the easiest and quickest to annotate out of the other common options. Whole-image classification is also a good option for abstract information such as scene detection or time of day.\u003c/p\u003e\u003cp\u003eBounding boxes, on the other hand, are the standard for most object detection use cases and require a higher level of granularity than whole-image classification. They provide a balance between annotation speed and targeting items of interest.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/guides/image-segmentation/?ref=labelbox-guides.ghost.io\"\u003eImage segmentation\u003c/a\u003e is usually chosen to support use cases in a model where you need to definitively know whether or not an image contains the object of interest as well as what isn’t an object of interest. This is in contrast to other annotation types such as classification or bounding boxes, which may be faster but usually convey less information.\u003c/p\u003e\u003ch2 id=\"why-is-image-annotation-useful\"\u003eWhy is image annotation useful?\u003c/h2\u003e\u003cp\u003eImage annotation is a vital part of training computer vision models that process image data for object detection, classification, segmentation, and more. A dataset of images that have been labeled and annotated to identify and classify specific objects, for example, is required to train an object detection model.\u003c/p\u003e\u003cp\u003eThis kind of computer vision model is an increasingly important technology. For example, a self-driving vehicle relies on a sophisticated computer vision image annotation algorithm. This model labels all the objects in the vehicle's environment, such as cars, pedestrians, bicycles, trees, etc. This data is then processed by the vehicle's computer and used to navigate traffic successfully and safely.\u003c/p\u003e\u003cp\u003eThere are many off-the-shelf image annotation models available. One such model is YOLO, an object detection model that generates bounding box annotations in real time. YOLO stands for \"You only look once,\" indicating that the algorithm analyzes the image and applies image annotations in one pass, prioritizing speed.\u003c/p\u003e\u003ch2 id=\"how-does-an-ai-data-engine-support-complex-image-annotation\"\u003eHow does an AI data engine support complex image annotation?\u003c/h2\u003e\u003cp\u003eImage annotation projects begin by determining what should be labeled in the images and then instructing annotators to perform the annotation tasks using an image annotation tool.\u003c/p\u003e\u003cp\u003eAnnotators must be thoroughly trained on the specifications and guidelines of each image annotation project, as every company will have different image labeling requirements. The annotation process will also differ depending upon the image annotation tool used.\u003c/p\u003e\u003cp\u003eOnce the annotators are trained on proper data annotation procedures for the project, they will begin annotating hundreds or thousands of images on an image annotation tool.\u003c/p\u003e\u003cp\u003eData engine software like Quantumworks Lab is not only equipped with an image annotation tool, but also allows AI teams to organize and store their structured and unstructured data while providing a model training framework.\u003c/p\u003e\u003cp\u003eThis scalable and flexible image annotation tool allows you to perform all the tasks mentioned above, from image classification to advanced semantic segmentation.\u003c/p\u003e\u003cp\u003eIn addition, a best-in-class data engine will typically include additional features that specifically help optimize your image annotation projects.\u003c/p\u003e\u003ch3 id=\"model-assisted-labeling\"\u003e\u003cstrong\u003eModel-assisted labeling\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eAn AI data engine helps users automate several parts of their image annotation process to accelerate efforts without diminishing the quality of annotations.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://labelbox-guides.ghost.io/content/images/2022/10/doc-hero.png\" class=\"kg-image\" alt=\"Quantumworks Lab enables AI teams to quickly and easily annotate images with powerful, flexible, and configurable labeling editors.\" loading=\"lazy\" width=\"1824\" height=\"1140\" srcset=\"https://labelbox-guides.ghost.io/content/images/size/w600/2022/10/doc-hero.png 600w, https://labelbox-guides.ghost.io/content/images/size/w1000/2022/10/doc-hero.png 1000w, https://labelbox-guides.ghost.io/content/images/size/w1600/2022/10/doc-hero.png 1600w, https://labelbox-guides.ghost.io/content/images/2022/10/doc-hero.png 1824w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eLabelbox enables AI teams to quickly and easily annotate images with powerful, flexible, and configurable labeling editors.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e1. Automated queuing enables labelers to work continuously, eliminating the delays that occur as they wait to receive datasets, instructions, and other materials \u003c/p\u003e\u003cp\u003e2. Auto-segmentation tools that cut complex image segmentation drawing tasks down to seconds \u003c/p\u003e\u003cp\u003e3. Automate data operations and workflows programmatically with a Python SDK \u003c/p\u003e\u003cp\u003e4. AI teams can import model predictions as pre-labels, so that labelers can review and correct them instead of labeling data from scratch \u003c/p\u003e\u003cp\u003eThis final labeling automation technique, called pre-labeling or model-assisted labeling, has been proven to reduce labeling time and costs by up to 50% for AI teams. \u003c/p\u003e\u003cp\u003ePre-labeling decreases labeling costs as the model gets smarter with every iteration, leaving teams more time to focus on manually labeling edge cases or areas where the model might not be performing as well. It’s not only faster and less expensive, but delivers better model performance.\u003c/p\u003e\u003ch3 id=\"high-performance-image-annotation-tools\"\u003e\u003cstrong\u003eHigh-performance image annotation tools\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe image annotation tool on the AI data engine you are testing can support a high number of objects and labels per image without sacrificing loading times.\u003c/p\u003e\u003cp\u003eLabelbox’s fast and ergonomic drawing tools provide efficiency to help reduce the time-consuming nature of creating consistent, pixel-perfect labels. A vector pen tool, for instance, allows users to draw freehand as well as generate straight lines. When you have the right tool for the job, image annotation is much easier.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/tsRLs-Hvh8Hk4l_RihmDxalaObPvHw9DlgLfuwYr38lH9aMamuQL98ZmUistHlSBDt0d1HbrE-7qaxE4FIDZbNUiuuF40rE96sKR8z6J4uYDAb5cSx1ZOfFcmiv_DSyRuV4Ppro5A9ZXQXGgW2l4C4c4DOwFXIaiwzSN7FllMqrZHfJwDxNKHLXZtw\" class=\"kg-image\" alt=\"The Quantumworks Lab segmentation pen tool makes image annotation quick and easy.\" loading=\"lazy\" width=\"1600\" height=\"1056\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eThe Quantumworks Lab segmentation pen tool makes image annotation quick and easy.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"customization-based-on-ontology-requirements\"\u003e\u003cstrong\u003eCustomization based on ontology requirements\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://labelbox.com/product/annotate/image/?ref=labelbox-guides.ghost.io\"\u003eLabelbox’s suite of image annotation tools\u003c/a\u003e gives you the ability to configure the label editor to your exact data structure (ontology) requirements, with the ability to further classify instances that you have segmented.\u003c/p\u003e\u003cp\u003eOntology management includes classifications, custom attributes, hierarchical relationships, and more. You'll be able to quickly annotate images with the labels that matter to you, without the clutter of irrelevant options.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/Eo5Nk92kyinI-jRWdHIi50Yn-yuWMHhDrhARjTprS56BUUoqBLtosk0ic9XRjV1SmEBBP1E6-PByGZWkE1YCKE4w5rNYNcHT2R5V6PaaMAkV9Kd85PyyPPNnu_Gh-aXzsdZM4eymU_FOlMVAidc1wm2zAqKTmE3khVTtYfbz7MIjhucekyQ4QVZSDw\" class=\"kg-image\" alt=\"Configure the label editor to your exact data structure (ontology) requirements. Bring additional attachments such as text, videos, images, overlays or even custom  widgets to aid data labelers to create perfect labels.\" loading=\"lazy\" width=\"1444\" height=\"1174\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eConfigure the label editor to your exact data structure (ontology) requirements. Bring additional attachments such as text, videos, images, overlays or even custom widgets to aid data labelers to create perfect labels.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"a-streamlined-user-interface-that-emphasizes-performance-for-a-wide-array-of-devices\"\u003e\u003cstrong\u003eA streamlined user interface that emphasizes performance for a wide array of devices\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eAn intuitive design helps lower the cognitive load on image labelers which enables faster image annotation. Moreover, an uncluttered online image annotation tool is built to run quickly, even on lower spec PCs and laptops. Both are critical for professional labelers who are working in an annotation editor all day.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh5.googleusercontent.com/wdXPZTvdOxnK78h7aB5ALH89zRK4Ag2RZsoGX0DPN3u14TCwFgFqmjE97e6SPgqd7sepjlZU4EEUfcDowJ1qRmFTgYvz9nO_036oLdk3xVe94uAsiQtY4HRCJrQdYrf7mgZa1PPKbbpRrRe69BDKKlpJ8Rrg9-I3jmulZif52YCJCLX6BA_fW9URSA\" class=\"kg-image\" alt=\"A simple, intuitive UI reduces friction.\" loading=\"lazy\" width=\"1600\" height=\"936\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eA simple, intuitive UI reduces friction.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"seamlessly-connect-your-data-via-python-sdk-or-api\"\u003e\u003cstrong\u003eSeamlessly connect your data via Python SDK or API\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eStream data into your AI data engine and push labeled data into training environments like \u003ca href=\"https://labelbox.com/blog/tensorflow-launch-partners/?ref=labelbox-guides.ghost.io\"\u003eTensorFlow\u003c/a\u003e and PyTorch. Quantumworks Lab was built to be developer-friendly and API first, so you can use it as infrastructure to scale up and connect your computer vision models to accelerate labeling productivity and orchestrate active learning.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh4.googleusercontent.com/3hP2i26_j9k7FomFavaQnvOQSFiQo5KXW3GSj1OyPLT0LWFI22ENYRvQ01N_kjnJTrfJvn1YrFYOrOM5qnDwMCBzMJiDb8SNqNBWHWVz6nW8peW2TP2Yr_lGsgOiSw22zebxhpK1ooOaIJ1GTWi-SyjawwXoKEI7bAQe7cWqdeATWx2jbPcSZT0xgA\" class=\"kg-image\" alt=\"Simplified data import without writing and maintaining your own scripts.\" loading=\"lazy\" width=\"1600\" height=\"1000\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eSimplified data import without writing and maintaining your own scripts.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"benchmarks-and-consensus\"\u003e\u003cstrong\u003eBenchmarks and consensus\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eData quality is measured by both the consistency and the accuracy of labeled data. The industry-standard methods for calculating data quality are benchmarks (aka gold standard), consensus, and review.\u003c/p\u003e\u003cp\u003eAn essential part of an AI data scientist’s job is figuring out what combination of these quality assurance procedures is right for annotated images used in your ML project. Quality assurance is an automated process that operates continuously throughout your training data development and improvement processes. \u003ca href=\"https://docs.labelbox.com/docs/benchmarks?ref=labelbox-guides.ghost.io#:~:text=The%20Benchmarks%20tool%20allows%20you,Benchmark%20Label%20on%20the%20asset.\"\u003eWith Quantumworks Lab consensus and benchmark features\u003c/a\u003e, you can automate consistency and accuracy tests. These tests allow you to customize the percentage of your data to test and the number of labelers that will annotate the test data.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh6.googleusercontent.com/zeTWFetAKuzB57EcIrdcnzJEMDOi_ZgfnN0xeCfK6rz5Ahdnj9rk4NXqKrIYSg7DgoYHJaLgqfP6aZpTj3KADi1wniVnFyloPWHOnnUbmdZbrW9TCoErBfNY95WRSyrRpDiv24hVxMo0mjUdZzvHbD2bMaZWuDq1kGJjSoDqfHO-hz3I5F967_aLAQ\" class=\"kg-image\" alt=\"Benchmarks in action, highlighting the example labeled asset with a gold star.\" loading=\"lazy\" width=\"1514\" height=\"822\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eBenchmarks in action, highlighting the example labeled asset with a gold star.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"collaboration-and-performance-monitoring\"\u003e\u003cstrong\u003eCollaboration and performance monitoring\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eHaving an organized system to invite and supervise all your labelers during an image annotation project is important for both scalability and security. An AI data engine should include granular options to invite users and review the work of each one.\u003c/p\u003e\u003cp\u003eWith Quantumworks Lab, setting up a project and inviting new members is extremely easy, and there are many options for \u003ca href=\"https://docs.labelbox.com/docs/performance-dashboard?ref=labelbox-guides.ghost.io\"\u003emonitoring their performance\u003c/a\u003e, including statistics on seconds needed to label an image. You can implement several quality control mechanisms, including activating automatic consensus between different labelers or setting gold standard benchmarks.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh6.googleusercontent.com/wQieRUTWPY1MIuy-QJu2btbCzmw5Arm-ZR42vAIFe8ZmvHMGQNqJU7XyhDxhVgoxYD6hXR7dspTEUCm-e1zMH5DvlcfN9ys0wRgFE2JXfTokHgdqt26MHFvS4WFaBCvD_S3qKwD-iTMWCwqfGn2FjCSM0pLcHqXKchs-95raW5oCCg01QWWmrs9yjA\" class=\"kg-image\" alt=\"Seamless collaboration between data science teams, domain experts, and dedicated external labeling teams.\" loading=\"lazy\" width=\"1344\" height=\"1024\"\u003e\u003cfigcaption\u003e\u003ci\u003e\u003cem class=\"italic\" style=\"white-space: pre-wrap;\"\u003eSeamless collaboration between data science teams, domain experts, and dedicated external labeling teams.\u003c/em\u003e\u003c/i\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"automatic-annotation-tool\"\u003e\u003cstrong\u003eAutomatic annotation tool\u003c/strong\u003e\u003c/h3\u003e\u003cfigure class=\"kg-card kg-image-card kg-card-hascaption\"\u003e\u003cimg src=\"https://lh6.googleusercontent.com/xvu89pZOSbVC5_EPrM2ZRi56xwCvF_tS-KECnDwXOE2ReGprVoEQZzd8Mee-m5e3CZFu4KVcoBsSVLK52hMnWwsjJb-Cvyfj1OArbozIgwxNMkeFjdkYIeS8MFcHMFji-YaMOrKZnLyGrn1rkYgzSTuZQorjL-Viw9-0Gt3Q3Sc5EEjrn7p_Al_D1Q\" class=\"kg-image\" alt=\"Automatic annotation tools help you save time\" loading=\"lazy\" width=\"1600\" height=\"990\"\u003e\u003cfigcaption\u003e\u003cspan style=\"white-space: pre-wrap;\"\u003eAutomatic annotation tools help you save time\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eIf you are using image annotation to train a machine learning model, Quantumworks Lab allows you to use your model to create pre-labeled images for your labeling team using an \u003ca href=\"https://labelbox.com/blog/use-automation-to-reduce-your-labeling-time-and-spend/?ref=labelbox-guides.ghost.io\"\u003eautomatic image segmentation tool\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eLabelers can then review the output of the computer vision annotation tool and make any necessary corrections or adjustments. Instead of starting from scratch, much of the work is already done, resulting in significant time savings.\u003c/p\u003e\u003ch2 id=\"final-thoughts-on-image-annotation-with-an-ai-data-engine\"\u003eFinal thoughts on image annotation with an AI data engine\u003c/h2\u003e\u003cp\u003eThe real-world applications for image annotation are endless, from content moderation to self-driving cars to security and surveillance. And, while there are many components to image annotation (classification, detection, segmentation), ultimately the annotation process itself is just a way to produce high quality data for model training. \u003c/p\u003e\u003cp\u003eWhen engineers at Tesla developed their Full Self Driving (FSD) vehicle technology in 2020, a key part of their success was an AI data engine. OpenAI currently uses their own proprietary AI data engine to train, deploy and maintain popular successful models such as GPT-3 and DALL-E 2.\u003c/p\u003e\u003cp\u003eFrom these examples, we can see how an AI data engine is key to deploying successful AI products, as it is the foundational infrastructure for how team members interface with data and models. Unfortunately, not all teams have the time and resources to architect an intricate and complex data engine for every use case.\u003c/p\u003e\u003cp\u003eLuckily, AI teams today don’t have to build and maintain data engines for their projects like Tesla and OpenAI did — they can invest in one instead. A best-in-class AI data engine gives you the ability to visualize, curate, organize, label data to improve model performance. Quantumworks Lab can help you get there.\u003c/p\u003e\u003cp\u003e\u003cbr\u003eDownload the \u003ca href=\"https://labelbox.com/learn/library/complete-guide-data-engines-for-ai/?ref=labelbox-guides.ghost.io\"\u003e\u003cstrong\u003eComplete guide to data engines for AI\u003c/strong\u003e\u003c/a\u003e to learn how investing in a data engine can help your team build transformative AI products fast.\u003c/p\u003e","comment_id":"6336244e1beec0003d38bdcc","feature_image":"https://labelbox-guides.ghost.io/content/images/2022/10/Screen-Shot-2022-10-03-at-10.17.25-AM.png","featured":false,"visibility":"public","created_at":"2022-09-29T23:03:42.000+00:00","updated_at":"2024-05-29T23:56:57.000+00:00","published_at":"2022-08-01T23:07:00.000+00:00","custom_excerpt":"Image annotation is defined as the task of annotating an image with labels. Discover how an AI data engine supports image annotation at scale.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"https://labelbox.com/guides/image-annotation/","authors":[{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"}],"tags":[{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},{"id":"653aa45d375d13000123d7de","name":"Build AI","slug":"build-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/build-ai/"},{"id":"653aa484375d13000123d7e2","name":"Building computer vision","slug":"building-computer-vision","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#22D3EE","url":"https://labelbox-guides.ghost.io/tag/building-computer-vision/"},{"id":"653aa5ec375d13000123d7f6","name":"Industry: Any","slug":"industry-any","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#4ADE80","url":"https://labelbox-guides.ghost.io/tag/industry-any/"}],"primary_author":{"id":"6336232f1beec0003d38bdc9","name":"Lisa Dimyadi","slug":"lisa","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://labelbox-guides.ghost.io/author/lisa/"},"primary_tag":{"id":"653aa53f375d13000123d7ec","name":"Label data for AI","slug":"label-data-for-ai","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#5CAAFF","url":"https://labelbox-guides.ghost.io/tag/label-data-for-ai/"},"url":"https://labelbox-guides.ghost.io/image-annotation/","excerpt":"Image annotation is defined as the task of annotating an image with labels. Discover how an AI data engine supports image annotation at scale.","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Image Annotation Best Practices \u0026 How to Annotate Images","meta_description":"Image annotation is defined as the task of annotating an image with labels. Discover how an AI data engine supports image annotation at scale.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null}],"featured":null,"currentPage":7,"url":"/guides"},"__N_SSG":true},"page":"/guides/page/[id]","query":{"id":"7"},"buildId":"Tltx2tBe97VOu7U0vVrr8","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script id="setOrignalReferrer" type="text/javascript">
          if (typeof jQuery != 'undefined') {
            $('#image-viewer').hide();
            setTimeout(() => {$('#image-viewer').hide();}, 500);
        $(".content img").click(function () {
            if($(this).attr("src"))
            {
              $("#full-image").attr("src", $(this).attr("src"));
              $('#image-viewer').show();
            }
            
        });
        $("#image-viewer").click(function () {
            $('#image-viewer').hide();
        });
            
            $(document).ready(function () {
                var store = window.localStorage;
                var newRef = document.referrer;
                
                if (store) {
                    if (!newRef || newRef == '') {
                        store.removeItem('origin');
                    }
        
                    var origin = store.getItem('origin');
        
                    if ((newRef && !newRef.match(/labelbox.com/i)) && origin != newRef) {
                        origin = newRef;
                        store.setItem('origin', newRef);
                    }
        
                    if (origin) {
                       
                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if (!lHref.match(/_r=/)) {
                                if (lHref.indexOf('?') > 0) {
                                    lHref = lHref + '&';
                                } else {
                                    lHref = lHref + '?';
                                }
                            }
        
                            lHref = lHref + '_r=' + encodeURI(origin);
                            $(e).attr('href', lHref);
                        });
                    }
                }
        
                function getCookie(name) {
                    const cookieString = document.cookie;
                    const cookies = cookieString.split('; ');
                    for (const cookie of cookies) {
                        const [cookieName, cookieValue] = cookie.split('=');
                        if (cookieName === name) {
                            return cookieValue;
                        }
                    }
                    return null;
                }

                function readPPCCookies() {
                  var cookies = document.cookie.split(';');
                  var cookieData = {};
                  
                  for (var i = 0; i < cookies.length; i++) {
                    var parts = cookies[i].split('=');
                    var cookieName = parts[0].trim();
                    
                    if (cookieName.startsWith('ppc')) {
                      var cookieValue = parts[1];
                      cookieData[cookieName] = cookieValue;
                    }
                  }
                  
                  return cookieData;
                }
                

                    var link = '';
                
                    const cookieData = readPPCCookies();
                    
                    for (var cookieName in cookieData) {
                      var utmName = cookieName.replace('ppc', 'utm_').toLowerCase();
                      var utmValue = cookieData[cookieName];
                      link += utmName + '=' + utmValue + '&';
                    }
                    
                    if(getCookie('gclid')) link+='gclid='+getCookie('gclid');
                    if(getCookie('attr')) link+='&attr='+getCookie('attr');
                    if(window.localStorage.getItem('ajs_anonymous_id')) link+='&landingPageAnonymousId='+window.localStorage.getItem('ajs_anonymous_id');
                    if(getCookie('referrer_url')) link+='&referrer_url='+getCookie('referrer_url');

                        $('a').filter((i, e) => {
                            var h = $(e).attr('href');
                            if (h) return h.match(/app.labelbox.com/);
                        }).each((i, e) => {
                            var lHref = $(e).attr('href');
                            if(!lHref.includes('gclid')) {
                               lHref = lHref + '?' + link;
                               $(e).attr('href', lHref);
                            }
                        });
            
            });
        }
        
          
          </script><script src="../../../../cdn.lr-in-prod.com/LogRocket.min.js" crossorigin="anonymous"></script></body>
<!-- Mirrored from labelbox.com/guides/page/7/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 02 Aug 2025 13:30:55 GMT -->
</html>